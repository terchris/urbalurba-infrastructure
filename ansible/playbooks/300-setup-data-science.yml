---
# file: ansible/playbooks/300-setup-data-science.yml
# Description:
# Set up Databricks Replacement Data Science stack on Kubernetes
# Phase 1: Spark Kubernetes Operator - Distributed processing engine for data workloads
# Phase 2: JupyterHub - Notebook interface for data scientists with secret-based authentication
#
# Part of: Databricks Replacement Project - Processing Engine + Notebook Interface
# Replaces: Databricks compute clusters, job execution, and workspace notebooks
#
# Prerequisites:
# - Kubernetes cluster with sufficient resources (6+ CPUs, 8+ GB RAM)
# - kubectl configured for target cluster
# - Helm 3.x installed
# - Traefik ingress controller installed
# - urbalurba-secrets applied to jupyterhub namespace (contains JUPYTERHUB_AUTH_PASSWORD)
# - Required manifests: 300-spark-config.yaml, 310-jupyterhub-config.yaml, 311-jupyterhub-ingress.yaml
#
# Architecture:
# - Spark Operator manages Spark applications as Kubernetes custom resources
# - JupyterHub provides web-based notebook interface with PySpark integration
# - Jobs run as ephemeral pods with automatic resource allocation
# - ARM64 compatible for Apple Silicon hardware
# - Clean job lifecycle management with automatic cleanup
# - Helm manages all RBAC resources with proper ownership
#
# Usage:
# ansible-playbook playbooks/300-setup-data-science.yml -e kube_context="rancher-desktop"

- name: Set up Databricks Replacement Data Science stack on Kubernetes
  hosts: localhost
  gather_facts: false
  vars:
    manifests_folder: "/mnt/urbalurbadisk/manifests"
    merged_kubeconf_file: "/mnt/urbalurbadisk/kubeconfig/kubeconf-all"
    spark_namespace: "spark-operator"
    jupyterhub_namespace: "jupyterhub"
    installation_timeout: 300  # 5 minutes timeout for installations
    pod_readiness_timeout: 180  # 3 minutes timeout for pod readiness
    
    # Helm chart references
    spark_operator_chart: "spark-kubernetes-operator/spark-kubernetes-operator"
    spark_operator_repo_url: "https://apache.github.io/spark-kubernetes-operator"
    jupyterhub_chart: "jupyterhub/jupyterhub"
    jupyterhub_repo_url: "https://hub.jupyter.org/helm-chart/"
    
    # Config files
    spark_config_file: "{{ manifests_folder }}/300-spark-config.yaml"
    jupyterhub_config_file: "{{ manifests_folder }}/310-jupyterhub-config.yaml"
    jupyterhub_ingress_file: "{{ manifests_folder }}/311-jupyterhub-ingress.yaml"

  tasks:

    - name: 1. Print playbook description
      ansible.builtin.debug:
        msg: |
          üöÄ Setting up Databricks Replacement Data Science Stack
          üìä Phase 1: Apache Spark Kubernetes Operator (Processing Engine)
          üìä Phase 2: JupyterHub (Notebook Interface with Secret Authentication)
          üéØ Target: {{ kube_context | default('rancher-desktop') }}
          üìÅ Namespaces: {{ spark_namespace }}, {{ jupyterhub_namespace }}
          üîß RBAC: Managed by Helm (no manual RBAC files)
          üîê Authentication: Password from urbalurba-secrets

    # ============= PHASE 1: SPARK KUBERNETES OPERATOR =============

    - name: 2. Create spark-operator namespace
      kubernetes.core.k8s:
        name: "{{ spark_namespace }}"
        api_version: v1
        kind: Namespace
        state: present
        kubeconfig: "{{ merged_kubeconf_file }}"

    - name: 3. Check existing Helm repositories
      ansible.builtin.command: helm repo list
      register: helm_repo_list
      changed_when: false

    - name: 4. Add Spark Kubernetes Operator Helm repository if needed
      kubernetes.core.helm_repository:
        name: "spark-kubernetes-operator"
        repo_url: "{{ spark_operator_repo_url }}"
      when: "'spark-kubernetes-operator' not in helm_repo_list.stdout"
      register: helm_repo_result

    - name: 5. Update Helm repositories
      ansible.builtin.command: helm repo update
      changed_when: false
    
    # Deploy Spark Kubernetes Operator (Helm will create RBAC automatically)
    - name: 6. Deploy Spark Kubernetes Operator with Helm-managed RBAC
      ansible.builtin.command: >-
        helm upgrade --install spark-kubernetes-operator {{ spark_operator_chart }} 
        -f {{ spark_config_file }} 
        --namespace {{ spark_namespace }}
        --timeout {{ installation_timeout }}s
      environment:
        KUBECONFIG: "{{ merged_kubeconf_file }}"
      register: spark_operator_result
      changed_when: true
    
    - name: 7. Display Spark Operator deployment result
      ansible.builtin.debug:
        msg: "Apache Spark Kubernetes Operator deployment initiated. Waiting for readiness..."
    
    - name: 8. Wait for Spark Operator pod to be ready
      ansible.builtin.shell: >-
        kubectl wait --for=condition=ready pod 
        -l app.kubernetes.io/name=spark-kubernetes-operator 
        -n {{ spark_namespace }} --timeout={{ pod_readiness_timeout }}s
      environment:
        KUBECONFIG: "{{ merged_kubeconf_file }}"
      register: spark_wait_result
      changed_when: false
      ignore_errors: true
    
    - name: 9. Display Spark Operator readiness status
      ansible.builtin.debug:
        msg: "Spark Operator readiness status: {{ 'Ready' if spark_wait_result.rc == 0 else 'Not ready yet, continuing anyway' }}"
    
    # Verify SparkApplication CRDs are installed
    - name: 10. Verify Apache Spark SparkApplication CRDs are installed
      ansible.builtin.shell: kubectl get crd sparkapplications.spark.apache.org
      environment:
        KUBECONFIG: "{{ merged_kubeconf_file }}"
      register: crd_check_result
      changed_when: false
      ignore_errors: true
    
    - name: 11. Display CRD verification result
      ansible.builtin.debug:
        msg: "SparkApplication CRDs: {{ 'Installed' if crd_check_result.rc == 0 else 'Not found - may still be installing' }}"

    # ============= PHASE 2: JUPYTERHUB NOTEBOOK INTERFACE =============

    - name: 12. Create jupyterhub namespace
      kubernetes.core.k8s:
        name: "{{ jupyterhub_namespace }}"
        api_version: v1
        kind: Namespace
        state: present
        kubeconfig: "{{ merged_kubeconf_file }}"

    - name: 13. Add JupyterHub Helm repository if needed
      kubernetes.core.helm_repository:
        name: "jupyterhub"
        repo_url: "{{ jupyterhub_repo_url }}"
      when: "'jupyterhub' not in helm_repo_list.stdout"
      register: jupyterhub_helm_repo_result

    - name: 14. Update Helm repositories for JupyterHub
      ansible.builtin.command: helm repo update
      changed_when: false

    - name: 15. Check if urbalurba-secrets exists in jupyterhub namespace
      ansible.builtin.shell: >-
        kubectl get secret urbalurba-secrets -n {{ jupyterhub_namespace }} 
        -o jsonpath='{.data.JUPYTERHUB_AUTH_PASSWORD}' 2>/dev/null | base64 -d || echo "NOT_FOUND"
      environment:
        KUBECONFIG: "{{ merged_kubeconf_file }}"
      register: jupyterhub_secret_check
      changed_when: false
      ignore_errors: true

    - name: 16. Display JupyterHub secret status
      ansible.builtin.debug:
        msg: |
          JupyterHub authentication secret status: 
          {{ 'Found - Password configured ‚úÖ' if jupyterhub_secret_check.stdout != 'NOT_FOUND' else 'NOT FOUND ‚ö†Ô∏è - Apply urbalurba-secrets to jupyterhub namespace' }}

    - name: 17. Deploy JupyterHub with PySpark integration and secret authentication
      ansible.builtin.command: >-
        helm upgrade --install jupyterhub {{ jupyterhub_chart }}
        -f {{ jupyterhub_config_file }}
        --namespace {{ jupyterhub_namespace }}
        --timeout {{ installation_timeout }}s
      environment:
        KUBECONFIG: "{{ merged_kubeconf_file }}"
      register: jupyterhub_result
      changed_when: true

    - name: 18. Display JupyterHub deployment result
      ansible.builtin.debug:
        msg: "JupyterHub deployment initiated. Waiting for readiness..."

    - name: 19. Wait for JupyterHub hub pod to be ready
      ansible.builtin.shell: >-
        kubectl wait --for=condition=ready pod 
        -l app=jupyterhub,component=hub 
        -n {{ jupyterhub_namespace }} --timeout={{ pod_readiness_timeout }}s
      environment:
        KUBECONFIG: "{{ merged_kubeconf_file }}"
      register: jupyterhub_hub_wait_result
      changed_when: false
      ignore_errors: true

    - name: 20. Wait for JupyterHub proxy pod to be ready
      ansible.builtin.shell: >-
        kubectl wait --for=condition=ready pod 
        -l app=jupyterhub,component=proxy 
        -n {{ jupyterhub_namespace }} --timeout={{ pod_readiness_timeout }}s
      environment:
        KUBECONFIG: "{{ merged_kubeconf_file }}"
      register: jupyterhub_proxy_wait_result
      changed_when: false
      ignore_errors: true

    - name: 21. Display JupyterHub readiness status
      ansible.builtin.debug:
        msg: |
          JupyterHub readiness status:
          - Hub: {{ 'Ready' if jupyterhub_hub_wait_result.rc == 0 else 'Not ready yet' }}
          - Proxy: {{ 'Ready' if jupyterhub_proxy_wait_result.rc == 0 else 'Not ready yet' }}

    - name: 22. Apply JupyterHub ingress configuration
      kubernetes.core.k8s:
        src: "{{ jupyterhub_ingress_file }}"
        state: present
        kubeconfig: "{{ merged_kubeconf_file }}"
      register: jupyterhub_ingress_result

    - name: 23. Verify JupyterHub service is accessible
      ansible.builtin.shell: >-
        kubectl get service proxy-public -n {{ jupyterhub_namespace }} 
        -o jsonpath='{.spec.ports[0].port}'
      environment:
        KUBECONFIG: "{{ merged_kubeconf_file }}"
      register: jupyterhub_service_port
      changed_when: false
      ignore_errors: true

    # ============= SIMPLIFIED JUPYTERHUB HEALTH CHECKS =============

    - name: 24. Check JupyterHub service exists
      ansible.builtin.shell: >-
        kubectl get service proxy-public -n {{ jupyterhub_namespace }} --no-headers 2>/dev/null | wc -l
      environment:
        KUBECONFIG: "{{ merged_kubeconf_file }}"
      register: jupyterhub_service_check
      changed_when: false
      ignore_errors: true

    - name: 25. Display JupyterHub service status
      ansible.builtin.debug:
        msg: "JupyterHub service: {{ 'Available ‚úÖ' if (jupyterhub_service_check.stdout | int) > 0 else 'Not found ‚ö†Ô∏è' }}"

    - name: 26. Check JupyterHub ingress status
      ansible.builtin.shell: >-
        kubectl get ingress jupyterhub -n {{ jupyterhub_namespace }} --no-headers 2>/dev/null | wc -l
      environment:
        KUBECONFIG: "{{ merged_kubeconf_file }}"
      register: jupyterhub_ingress_check
      changed_when: false
      ignore_errors: true

    - name: 27. Display JupyterHub ingress status
      ansible.builtin.debug:
        msg: "JupyterHub ingress: {{ 'Configured ‚úÖ' if (jupyterhub_ingress_check.stdout | int) > 0 else 'Not found ‚ö†Ô∏è' }}"

    # ============= INTEGRATION TESTING =============

    - name: 28. Test Spark Operator with sample Pi calculation
      ansible.builtin.shell: |
        cat <<EOF | kubectl apply -f - -n {{ spark_namespace }}
        apiVersion: spark.apache.org/v1alpha1
        kind: SparkApplication
        metadata:
          name: spark-pi-test
          namespace: {{ spark_namespace }}
        spec:
          mainClass: "org.apache.spark.examples.SparkPi"
          jars: "local:///opt/spark/examples/jars/spark-examples.jar"
          sparkConf:
            spark.dynamicAllocation.enabled: "false"
            spark.kubernetes.authenticate.driver.serviceAccountName: "spark"
            spark.kubernetes.container.image: "apache/spark:4.0.0"
            spark.driver.cores: "1"
            spark.driver.memory: "512m"
            spark.executor.instances: "1"
            spark.executor.cores: "1"
            spark.executor.memory: "512m"
          runtimeVersions:
            scalaVersion: "2.13"
            sparkVersion: "4.0.0"
        EOF
      environment:
        KUBECONFIG: "{{ merged_kubeconf_file }}"
      register: test_job_result
      changed_when: true
      ignore_errors: true
    
    - name: 29. Display test job submission result
      ansible.builtin.debug:
        msg: "Spark Pi test job: {{ 'Submitted successfully' if test_job_result.rc == 0 else 'Submission failed - this is expected if CRDs are not ready yet' }}"
    
    # Give a brief pause for job to start
    - name: 30. Brief pause for test job to start
      ansible.builtin.pause:
        seconds: 10
      when: test_job_result.rc == 0
    
    # Check test job status
    - name: 31. Check test job status
      ansible.builtin.shell: >-
        kubectl get sparkapp spark-pi-test -n {{ spark_namespace }} 
        -o jsonpath='{.status.currentState.currentStateSummary}' 2>/dev/null || echo "NOT_FOUND"
      environment:
        KUBECONFIG: "{{ merged_kubeconf_file }}"
      register: test_job_status
      changed_when: false
      ignore_errors: true
      when: test_job_result.rc == 0
    
    - name: 32. Display test job status
      ansible.builtin.debug:
        msg: "Test job status: {{ test_job_status.stdout | default('Job not submitted') }}"
      when: test_job_result.rc == 0
    
    # Clean up test job
    - name: 33. Clean up test job
      ansible.builtin.shell: kubectl delete sparkapp spark-pi-test -n {{ spark_namespace }} --ignore-not-found=true
      environment:
        KUBECONFIG: "{{ merged_kubeconf_file }}"
      changed_when: false
      ignore_errors: true

    # ============= VERIFICATION AND STATUS =============
    
    # Verify final deployment status
    - name: 34. Get Spark Operator pods
      ansible.builtin.shell: kubectl get pods -n {{ spark_namespace }}
      environment:
        KUBECONFIG: "{{ merged_kubeconf_file }}"
      register: spark_pods
      changed_when: false
    
    - name: 35. Get JupyterHub pods
      ansible.builtin.shell: kubectl get pods -n {{ jupyterhub_namespace }}
      environment:
        KUBECONFIG: "{{ merged_kubeconf_file }}"
      register: jupyterhub_pods
      changed_when: false
    
    - name: 36. Get JupyterHub ingress
      ansible.builtin.shell: kubectl get ingress -n {{ jupyterhub_namespace }}
      environment:
        KUBECONFIG: "{{ merged_kubeconf_file }}"
      register: jupyterhub_ingress
      changed_when: false
    
    - name: 37. Display deployment status
      ansible.builtin.debug:
        msg: |
          Spark Operator pods:
          {{ spark_pods.stdout }}
          
          JupyterHub pods:
          {{ jupyterhub_pods.stdout }}
          
          JupyterHub ingress:
          {{ jupyterhub_ingress.stdout }}
    
    # Count running pods for both services
    - name: 38. Count running pods
      ansible.builtin.shell: >-
        echo "Spark: $(kubectl get pods -n {{ spark_namespace }} | grep -v NAME | grep -c Running || echo "0")"
        echo "JupyterHub: $(kubectl get pods -n {{ jupyterhub_namespace }} | grep -v NAME | grep -c Running || echo "0")"
      environment:
        KUBECONFIG: "{{ merged_kubeconf_file }}"
      register: running_pods_count
      changed_when: false
      ignore_errors: true
    
    - name: 39. Determine installation success
      ansible.builtin.set_fact:
        spark_running: "{{ spark_pods.stdout.find('Running') != -1 }}"
        jupyterhub_running: "{{ jupyterhub_pods.stdout.find('Running') != -1 }}"
        installation_successful: "{{ (spark_pods.stdout.find('Running') != -1) and (jupyterhub_pods.stdout.find('Running') != -1) }}"
        secret_configured: "{{ jupyterhub_secret_check.stdout != 'NOT_FOUND' }}"
        jupyterhub_service_healthy: "{{ (jupyterhub_service_check.stdout | int) > 0 }}"
        jupyterhub_ingress_working: "{{ (jupyterhub_ingress_check.stdout | int) > 0 }}"
    
    - name: 40. Display final installation status
      ansible.builtin.debug:
        msg: |
          ===============================================
          üöÄ Databricks Replacement Data Science Stack
          ===============================================

          {{ '‚úÖ SUCCESS - Full Databricks Replacement Stack Running!' if installation_successful else '‚ö†Ô∏è PARTIAL SUCCESS - Some components may not be ready yet' }}

          üì¶ Components installed:
          ‚Ä¢ Phase 1: Apache Spark Kubernetes Operator {{ '‚úÖ' if spark_running else '‚ö†Ô∏è' }}
          ‚Ä¢ Phase 2: JupyterHub Notebook Interface {{ '‚úÖ' if jupyterhub_running else '‚ö†Ô∏è' }}
          ‚Ä¢ RBAC Configuration (Helm-managed with proper ownership)
          ‚Ä¢ SparkApplication CRDs (spark.apache.org API group)
          ‚Ä¢ Traefik Ingress for web access

          üîê Authentication Configuration:
          ‚Ä¢ Secret Status: {{ 'Configured ‚úÖ' if secret_configured else 'Missing ‚ö†Ô∏è' }}
          ‚Ä¢ Password Source: urbalurba-secrets/JUPYTERHUB_AUTH_PASSWORD
          ‚Ä¢ Login Username: admin (or any username with DummyAuthenticator)
          ‚Ä¢ Login Password: {{ 'From secret' if secret_configured else 'NOT CONFIGURED - Apply urbalurba-secrets!' }}

          üîÑ Status:
          {{ running_pods_count.stdout }}
          ‚Ä¢ Test job: {{ 'Executed successfully' if test_job_result.rc == 0 else 'Skipped (CRDs may still be installing)' }}
          ‚Ä¢ Ingress: {{ 'Applied ‚úÖ' if jupyterhub_ingress_result is defined else 'Not applied ‚ö†Ô∏è' }}
          ‚Ä¢ JupyterHub Service: {{ 'Healthy ‚úÖ' if jupyterhub_service_healthy else 'Not responding ‚ö†Ô∏è' }}
          ‚Ä¢ JupyterHub Ingress: {{ 'Working ‚úÖ' if jupyterhub_ingress_working else 'Not configured ‚ö†Ô∏è' }}

          üìä Databricks Replacement Progress:
          ‚Ä¢ Phase 1: Processing Engine ‚úÖ COMPLETE
          ‚Ä¢ Phase 2: Notebook Interface ‚úÖ COMPLETE
          ‚Ä¢ Phase 3: SQL Analytics (Superset) - Future (15% remaining)

          üåê Access Information:
          ‚Ä¢ JupyterHub Web Interface: http://jupyterhub.localhost
          ‚Ä¢ Login: Username: admin, Password: {{ 'from urbalurba-secrets' if secret_configured else 'NOT CONFIGURED' }}
          ‚Ä¢ Alternative access: kubectl port-forward -n {{ jupyterhub_namespace }} svc/proxy-public 8888:80

          üöÄ What You Can Do Now:
          1. Access JupyterHub at http://jupyterhub.localhost
          2. Create new Python notebooks with PySpark
          3. Run distributed Spark computations
          4. Execute SQL queries with spark.sql()
          5. Perform data analytics and machine learning

          üìù Example PySpark code for notebooks:
             import findspark; findspark.init()
             from pyspark.sql import SparkSession
             spark = SparkSession.builder.appName("Test").getOrCreate()
             df = spark.createDataFrame([("Alice", 25)], ["name", "age"])
             df.show()

          üîß Monitoring and Troubleshooting:
          ‚Ä¢ Check Spark pods: kubectl get pods -n {{ spark_namespace }}
          ‚Ä¢ Check JupyterHub pods: kubectl get pods -n {{ jupyterhub_namespace }}
          ‚Ä¢ View Spark logs: kubectl logs -n {{ spark_namespace }} deployment/spark-kubernetes-operator
          ‚Ä¢ View JupyterHub logs: kubectl logs -n {{ jupyterhub_namespace }} deployment/hub
          ‚Ä¢ List Spark jobs: kubectl get sparkapp -A
          ‚Ä¢ Check ingress: kubectl get ingress -n {{ jupyterhub_namespace }}
          ‚Ä¢ Verify secret: kubectl get secret urbalurba-secrets -n {{ jupyterhub_namespace }}
          ‚Ä¢ Manual access test: kubectl port-forward -n {{ jupyterhub_namespace }} svc/proxy-public 8888:80

          {{ '‚ö†Ô∏è ACTION REQUIRED: Apply urbalurba-secrets to jupyterhub namespace before accessing JupyterHub!' if not secret_configured else '' }}

          ===============================================
          {{ 'üéâ DATABRICKS REPLACEMENT READY - 85% FUNCTIONALITY ACHIEVED!' if installation_successful and secret_configured and jupyterhub_service_healthy else '‚ö†Ô∏è CHECK DEPLOYMENT STATUS AND VERIFICATION RESULTS' }}
          ===============================================