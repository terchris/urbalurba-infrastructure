{
  "@context": "https://schema.org",
  "@type": "ItemList",
  "name": "UIS Stacks",
  "description": "Service stacks - groups of services that work together in the Urbalurba Infrastructure Stack",
  "itemListElement": [
    {
      "@type": "SoftwareSourceCode",
      "identifier": "observability",
      "name": "Observability Stack",
      "description": "Complete monitoring with metrics, logs, and distributed tracing",
      "category": "OBSERVABILITY",
      "tags": ["monitoring", "metrics", "logs", "tracing", "grafana", "prometheus"],
      "abstract": "Full observability with Prometheus, Loki, Tempo, and Grafana",
      "logo": "observability-stack-logo.svg",
      "summary": "The observability stack provides full visibility into your infrastructure. Prometheus collects metrics, Loki aggregates logs, Tempo stores traces, and Grafana visualizes everything in unified dashboards. The OpenTelemetry Collector receives telemetry data from applications.",
      "docs": "/docs/stacks/observability",
      "components": [
        {
          "service": "prometheus",
          "position": 1,
          "note": "Metrics collection and storage"
        }
,
        {
          "service": "tempo",
          "position": 2,
          "note": "Distributed tracing backend"
        }
,
        {
          "service": "loki",
          "position": 3,
          "note": "Log aggregation"
        }
,
        {
          "service": "otel-collector",
          "position": 4,
          "note": "Telemetry data receiver",
          "optional": true
        }
,
        {
          "service": "grafana",
          "position": 5,
          "note": "Visualization and dashboards"
        }
      ]
    },
    {
      "@type": "SoftwareSourceCode",
      "identifier": "ai-local",
      "name": "Local AI Stack",
      "description": "Run AI models locally with a unified API and chat interface",
      "category": "AI",
      "tags": ["ai", "llm", "ollama", "openai", "chat", "local"],
      "abstract": "Self-hosted AI with LiteLLM proxy and Open WebUI",
      "logo": "ai-local-stack-logo.svg",
      "summary": "The local AI stack provides a unified LLM interface. LiteLLM proxies to external Ollama (on host Mac) and cloud providers, while Open WebUI offers a ChatGPT-like interface for users. Note: Ollama runs on the host machine, not in-cluster.",
      "docs": "/docs/stacks/ai-local",
      "components": [
        {
          "service": "litellm",
          "position": 1,
          "note": "Unified API gateway"
        }
,
        {
          "service": "openwebui",
          "position": 2,
          "note": "User-facing chat interface"
        }
      ]
    },
    {
      "@type": "SoftwareSourceCode",
      "identifier": "analytics",
      "name": "Analytics Stack",
      "description": "Collaborative data science platform with notebooks and distributed computing",
      "category": "ANALYTICS",
      "tags": ["analytics", "spark", "jupyter", "notebooks", "datascience"],
      "abstract": "Analytics platform with Spark, JupyterHub, and Unity Catalog",
      "logo": "analytics-stack-logo.svg",
      "summary": "The analytics stack provides a complete platform for data analysis and ML workflows. Apache Spark handles distributed computing, JupyterHub provides collaborative notebooks, and Unity Catalog manages data governance.",
      "docs": "/docs/stacks/analytics",
      "components": [
        {
          "service": "spark",
          "position": 1,
          "note": "Distributed computing engine"
        }
,
        {
          "service": "jupyterhub",
          "position": 2,
          "note": "Multi-user notebook server"
        }
,
        {
          "service": "unity-catalog",
          "position": 3,
          "note": "Data catalog and governance",
          "optional": true
        }
      ]
    }
  ]
}
