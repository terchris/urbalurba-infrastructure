# manifests/220-litellm-config.yaml
# Description:
# LiteLLM proxy configuration with multiple model sources:
# 1. In-cluster Ollama qwen3:0.6b-incluster model (always available in the cluster)
# 2. External Mac Ollama gemma3:4b model (requires network access to 192.168.68.61)
# 3. Cloud provider models (available when valid API keys are provided)
#
# IMPORTANT: This configuration does NOT manage models running on the host machine.
# Models running on the host's Ollama instance (like deepseek-r1) should be accessed 
# directly by OpenWebUI through the OLLAMA_API_BASE_URL environment variable.
# 
# Usage:
# installing: helm upgrade --install litellm -f 207-litellm-config.yaml oci://ghcr.io/berriai/litellm-helm --namespace ai --create-namespace
# upgrading:  helm upgrade litellm -f 207-litellm-config.yaml oci://ghcr.io/berriai/litellm-helm --namespace ai
# uninstalling: helm uninstall litellm --namespace ai
#
# Prerequisites:
# 1. The 'urbalurba-secrets' secret must exist with required keys
# 2. Network connectivity to 192.168.68.61:11434 for the external Mac model
#
# API Keys:
# - Valid API keys must be set in the 'urbalurba-secrets' secret for cloud models to work
# - Models with invalid/dummy keys will appear in the model list but will fail when called
# - Update keys in the secret without changing this config file to activate cloud models:
#   kubectl edit secret urbalurba-secrets -n ai
#
# Testing:
# 1. Get your master key from the secret:
#    LITELLM_PROXY_MASTER_KEY=$(kubectl get secret urbalurba-secrets -n ai -o jsonpath="{.data.LITELLM_PROXY_MASTER_KEY}" | base64 --decode)
#
# 2. Forward the LiteLLM port to your local machine:
#    kubectl port-forward svc/litellm 4000:4000 -n ai
#
# 3. List available models:
#    curl -X GET http://localhost:4000/v1/models -H "Authorization: Bearer $LITELLM_PROXY_MASTER_KEY"
#
# 4. Test completion with the in-cluster Qwen model:
#    curl -X POST http://localhost:4000/v1/chat/completions \
#      -H "Content-Type: application/json" \
#      -H "Authorization: Bearer $LITELLM_PROXY_MASTER_KEY" \
#      -d '{"model": "qwen3-0.6b-incluster", "messages": [{"role": "user", "content": "Tell me a short joke"}]}'
#
# 5. Test completion with external Mac with gemma3:4b:
#    curl -X POST http://localhost:4000/v1/chat/completions \
#      -H "Content-Type: application/json" \
#      -H "Authorization: Bearer $LITELLM_PROXY_MASTER_KEY" \
#      -d '{"model": "external-ollama-gemma3", "messages": [{"role": "user", "content": "Explain quantum computing in simple terms"}]}'

# Container image configuration
image:
  repository: ghcr.io/berriai/litellm-database
  pullPolicy: Always
  tag: "main-latest"  # Back to latest - should have the fix

# Disable internal PostgreSQL - use shared PostgreSQL instead
db:
  useExisting: true
  deployStandalone: false
  endpoint: postgresql.default.svc.cluster.local
  database: litellm
  secret:
    name: urbalurba-secrets
    usernameKey: LITELLM_POSTGRESQL__USER
    passwordKey: LITELLM_POSTGRESQL__PASSWORD

postgresql:
  enabled: false

# Reference to Kubernetes secrets with env vars
environmentSecrets:
  - urbalurba-secrets

# Configure DATABASE_URL and enable debug logging and database model storage
extraEnvVars:
  - name: DATABASE_URL
    value: "postgresql://litellm:$(LITELLM_POSTGRESQL__PASSWORD)@postgresql.default.svc.cluster.local:5432/litellm"
  - name: LITELLM_LOG
    value: "DEBUG"
  - name: STORE_MODEL_IN_DB
    value: "True"
  # UI authentication environment variables (ENTERPRISE/PREMIUM FEATURE ONLY)
  # NOTE: These require LiteLLM Enterprise license - commenting out for free version
  # - name: UI_USERNAME
  #   valueFrom:
  #     secretKeyRef:
  #       name: urbalurba-secrets
  #       key: UI_USERNAME
  # - name: UI_PASSWORD
  #   valueFrom:
  #     secretKeyRef:
  #       name: urbalurba-secrets
  #       key: UI_PASSWORD

# Master key for authentication
masterkeySecretName: urbalurba-secrets
masterkeySecretKey: LITELLM_PROXY_MASTER_KEY

# Use existing ConfigMap instead of inline config
configMapRef:
  name: ai-models-litellm
  key: config.yaml

# Disable Helm-managed ConfigMap creation and use our existing one
proxyConfigMap:
  create: false
  name: ai-models-litellm  # Name of the existing ConfigMap to use

# Remove inline config since we're using external ConfigMap
# The model configuration is now managed in topsecret/kubernetes/kubernetes-secrets.yml