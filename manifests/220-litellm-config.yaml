# 207-litellm-config.yaml
#  7may25: not in use anymore
# Description:
# LiteLLM proxy configuration with multiple model sources:
# 1. In-cluster Ollama qwen3:0.6b-incluster model (always available in the cluster)
# 2. External Mac Ollama gemma3:4b model (requires network access to 192.168.68.61)
# 3. Cloud provider models (available when valid API keys are provided)
#
# IMPORTANT: This configuration does NOT manage models running on the host machine.
# Models running on the host's Ollama instance (like deepseek-r1) should be accessed 
# directly by OpenWebUI through the OLLAMA_API_BASE_URL environment variable.
# 
# Usage:
# installing: helm upgrade --install litellm -f 207-litellm-config.yaml oci://ghcr.io/berriai/litellm-helm --namespace ai --create-namespace
# upgrading:  helm upgrade litellm -f 207-litellm-config.yaml oci://ghcr.io/berriai/litellm-helm --namespace ai
# uninstalling: helm uninstall litellm --namespace ai
#
# Prerequisites:
# 1. The 'urbalurba-secrets' secret must exist with required keys
# 2. Network connectivity to 192.168.68.61:11434 for the external Mac model
#
# API Keys:
# - Valid API keys must be set in the 'urbalurba-secrets' secret for cloud models to work
# - Models with invalid/dummy keys will appear in the model list but will fail when called
# - Update keys in the secret without changing this config file to activate cloud models:
#   kubectl edit secret urbalurba-secrets -n ai
#
# Testing:
# 1. Get your master key from the secret:
#    LITELLM_PROXY_MASTER_KEY=$(kubectl get secret urbalurba-secrets -n ai -o jsonpath="{.data.LITELLM_PROXY_MASTER_KEY}" | base64 --decode)
#
# 2. Forward the LiteLLM port to your local machine:
#    kubectl port-forward svc/litellm 4000:4000 -n ai
#
# 3. List available models:
#    curl -X GET http://localhost:4000/v1/models -H "Authorization: Bearer $LITELLM_PROXY_MASTER_KEY"
#
# 4. Test completion with the in-cluster Qwen model:
#    curl -X POST http://localhost:4000/v1/chat/completions \
#      -H "Content-Type: application/json" \
#      -H "Authorization: Bearer $LITELLM_PROXY_MASTER_KEY" \
#      -d '{"model": "qwen3-0.6b-incluster", "messages": [{"role": "user", "content": "Tell me a short joke"}]}'
#
# 5. Test completion with external Mac with gemma3:4b:
#    curl -X POST http://localhost:4000/v1/chat/completions \
#      -H "Content-Type: application/json" \
#      -H "Authorization: Bearer $LITELLM_PROXY_MASTER_KEY" \
#      -d '{"model": "external-ollama-gemma3", "messages": [{"role": "user", "content": "Explain quantum computing in simple terms"}]}'

# Container image configuration
image:
  repository: ghcr.io/berriai/litellm-database
  pullPolicy: Always
  tag: "main-latest"  # Back to latest - should have the fix

# Keep internal PostgreSQL - it's already working
# postgresql:
#   enabled: false

# No need for extra DATABASE_URL since we're using internal PostgreSQL
# extraEnvVars:
#   - name: DATABASE_URL
#     value: "postgresql://postgres:postgres@postgres.default.svc.cluster.local:5432/litellm"

# Reference to Kubernetes secrets with env vars
environmentSecrets:
  - urbalurba-secrets

# Enable debug logging
extraEnvVars:
  - name: LITELLM_LOG
    value: "DEBUG"

# Master key for authentication
masterkeySecretName: urbalurba-secrets
masterkeySecretKey: LITELLM_PROXY_MASTER_KEY

# Configure the proxy with fallbacks and error handling
proxy_config:
  # Configure general settings for reliable operation
  general_settings:
    # If cloud models fail, still allow local models to work
    allow_model_caching_in_memory: True
    # Return useful error messages
    detailed_error_messaging: True
  
  # Configure router settings for better reliability
  router_settings:
    routing_strategy: "simple-shuffle"
    # Define fallbacks from cloud models to local models
    fallbacks: [
      {"gpt-4o": ["host-mac-gpt-oss", "qwen3-0.6b-incluster"]},
      {"claude-3-opus": ["host-mac-gpt-oss", "qwen3-0.6b-incluster"]},
      {"gpt-3.5-turbo": ["host-mac-gpt-oss", "qwen3-0.6b-incluster"]}
    ]
  
  # LiteLLM settings
  litellm_settings:
    # Set a reasonable timeout
    request_timeout: 120
    # Enable fallbacks for content policy errors
    content_policy_fallbacks: [
      {"gpt-4o": ["external-ollama-gemma3", "qwen3-0.6b-incluster"]},
      {"claude-3-opus": ["external-ollama-gemma3", "qwen3-0.6b-incluster"]}
    ]
    # Retry on connection issues (helps with intermittent network problems)
    num_retries: 3
  
  # Model list configuration
  model_list:
    # In-cluster Ollama model (always available)
    - model_name: qwen3-0.6b-incluster
      litellm_params:
        model: ollama/qwen3:0.6b
        api_base: "http://open-webui-ollama:11434"
        temperature: 0.7
        max_tokens: 1000
        stop: ["<|im_start|>", "<|im_end|>"]
      model_info:
        description: "Qwen3 0.6B (small, in-cluster, for testing only)"
        status: "active"
        input_cost_per_token: 0
        output_cost_per_token: 0
        max_input_tokens: 32768
        mode: "chat"
    
    # Host Mac Ollama - accessible from cluster
    - model_name: host-mac-gpt-oss
      litellm_params:
        model: ollama/gpt-oss:20b
        api_base: "http://host.lima.internal:11434"
        temperature: 0.7
        max_tokens: 2000
        stream: false  # Disable streaming to avoid thinking field issues
        format: ""  # Ensure no special format is set
      model_info:
        description: "Ollama running on host Mac"
        status: "active"
        input_cost_per_token: 0
        output_cost_per_token: 0
        max_input_tokens: 8192
        max_output_tokens: 2000
        mode: "chat"

    # External Mac Ollama gemma3:4b model (old IP, keeping for reference)
    - model_name: external-ollama-gemma3
      litellm_params:
        model: ollama/gemma3:4b
        api_base: "http://192.168.68.61:11434"
        temperature: 0.7
        max_tokens: 2000
        stop: ["<|im_start|>", "<|im_end|>"]
      model_info:
        description: "Gemma 3 4B model running on external Mac Ollama"
        status: "active"
        input_cost_per_token: 0
        output_cost_per_token: 0
        max_input_tokens: 8192
        max_output_tokens: 2000
        mode: "chat"
        architecture: "gemma3"
        parameters: "4B"
    
    # OpenAI GPT-4o (requires valid API key)
    - model_name: gpt-4o
      litellm_params:
        model: openai/gpt-4o
        api_key: "os.environ/OPENAI_API_KEY"
        max_tokens: 4000
      model_info:
        description: "GPT-4o from OpenAI (requires valid API key in secret)"
        status: "requires_api_key"
        input_cost_per_token: 0.00005
        output_cost_per_token: 0.00015
        max_input_tokens: 128000
        mode: "chat"
    
    # Azure OpenAI model (requires valid API key)
    - model_name: azure-gpt-4
      litellm_params:
        model: azure/gpt-4
        api_base: "os.environ/AZURE_API_BASE"
        api_key: "os.environ/AZURE_API_KEY"
        api_version: "2023-07-01-preview"
        max_tokens: 4000
      model_info:
        description: "GPT-4 deployed on Azure OpenAI (requires valid Azure credentials in secret)"
        status: "requires_api_key"
        input_cost_per_token: 0.00003
        output_cost_per_token: 0.00006
        max_input_tokens: 8192
        mode: "chat"
    
    # Anthropic Claude model (requires valid API key)
    - model_name: claude-3-opus
      litellm_params:
        model: anthropic/claude-3-opus-20240229
        api_key: "os.environ/ANTHROPIC_API_KEY"
        max_tokens: 4000
      model_info:
        description: "Claude 3 Opus from Anthropic (requires valid API key in secret)"
        status: "requires_api_key"
        input_cost_per_token: 0.00003
        output_cost_per_token: 0.00015
        max_input_tokens: 200000
        mode: "chat"