{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Urbalurba Infrastructure","text":"<p>Complete datacenter on your laptop - A zero-friction developer platform that provides production-grade infrastructure locally.</p>"},{"location":"#what-is-this","title":"What is This?","text":"<p>Urbalurba Infrastructure is a comprehensive Kubernetes-based platform that runs the same configuration in development and production:</p> <ul> <li>Local Development: Run everything on your laptop with Rancher Desktop</li> <li>Production Ready: Deploy the exact same configuration to Azure AKS or any Kubernetes cluster</li> <li>Zero Cloud Dependencies: Develop and test without internet connectivity</li> <li>Privacy-First AI: Run LLMs locally on your own data</li> </ul>"},{"location":"#run-anywhere","title":"Run Anywhere","text":"Platform Architecture Use Case Laptop (Rancher Desktop) ARM64 / x86_64 Local development Azure AKS x86_64 Production cloud Ubuntu Server ARM64 / x86_64 Self-hosted production Raspberry Pi ARM64 Edge computing, home lab <p>One codebase. Any platform. Same result.</p> <p>Once your Kubernetes cluster is running, everything else is identical regardless of where it runs. Same manifests, same Ansible playbooks, same services, same URLs.</p>"},{"location":"#services-included","title":"Services Included","text":""},{"location":"#core-infrastructure","title":"Core Infrastructure","text":"<ul> <li>Kubernetes - Container orchestration via Rancher Desktop</li> <li>Traefik - Ingress controller with automatic TLS</li> <li>Nginx - Web server</li> </ul>"},{"location":"#observability-stack","title":"Observability Stack","text":"<ul> <li>Prometheus - Metrics collection</li> <li>Grafana - Visualization and dashboards</li> <li>Loki - Log aggregation</li> <li>Tempo - Distributed tracing</li> <li>OpenTelemetry Collector - Telemetry pipeline</li> </ul>"},{"location":"#databases","title":"Databases","text":"<ul> <li>PostgreSQL - Primary relational database</li> <li>MySQL - Alternative SQL database</li> <li>MongoDB - Document database</li> <li>Qdrant - Vector database for AI</li> <li>Redis - Cache and message broker</li> <li>Elasticsearch - Search engine</li> </ul>"},{"location":"#ai-machine-learning","title":"AI &amp; Machine Learning","text":"<ul> <li>OpenWebUI - ChatGPT-like interface</li> <li>LiteLLM - LLM proxy for multiple providers</li> <li>Ollama - Local LLM runtime</li> <li>Tika - Document extraction</li> </ul>"},{"location":"#authentication","title":"Authentication","text":"<ul> <li>Authentik - SSO and identity provider with blueprints</li> </ul>"},{"location":"#message-queues","title":"Message Queues","text":"<ul> <li>RabbitMQ - Message broker</li> </ul>"},{"location":"#development-tools","title":"Development Tools","text":"<ul> <li>ArgoCD - GitOps continuous deployment</li> <li>pgAdmin - PostgreSQL administration</li> <li>RedisInsight - Redis administration</li> </ul>"},{"location":"#networking","title":"Networking","text":"<ul> <li>Tailscale - Secure mesh VPN</li> <li>Cloudflare Tunnels - Public access without port forwarding</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>macOS, Linux, or Windows with WSL2</li> <li>16GB RAM minimum (32GB recommended)</li> <li>50GB free disk space</li> <li>Rancher Desktop installed</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code># Clone the repository\ngit clone https://github.com/terchris/urbalurba-infrastructure.git\ncd urbalurba-infrastructure\n\n# Run the installer\n./install-rancher.sh\n\n# Access the provision host\n./login-provision-host.sh\n\n# Configure secrets\ncd topsecret\n./create-kubernetes-secrets.sh\n# Edit your values in secrets-config/\n./create-kubernetes-secrets.sh\nkubectl apply -f kubernetes/kubernetes-secrets.yml\n\n# Deploy services using Ansible\ncd /mnt/urbalurbadisk/ansible\nansible-playbook playbooks/034-setup-grafana.yml\n</code></pre>"},{"location":"#access-your-services","title":"Access Your Services","text":"<p>After deployment, access services at:</p> Service URL Nginx http://localhost Grafana http://grafana.localhost Prometheus http://prometheus.localhost Authentik http://authentik.localhost OpenWebUI http://openwebui.localhost pgAdmin http://pgadmin.localhost ArgoCD http://argocd.localhost"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>Getting Started - First steps and quick start guide</li> <li>Hosts &amp; Platforms - Supported platforms and setup guides</li> <li>Packages - Service documentation by category</li> <li>Networking - External access via Tailscale and Cloudflare</li> <li>Rules &amp; Standards - Development conventions and patterns</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"#repository-structure","title":"Repository Structure","text":"<pre><code>urbalurba-infrastructure/\n\u251c\u2500\u2500 ansible/              # Ansible playbooks for deployment\n\u251c\u2500\u2500 docs/                 # Documentation (this site)\n\u251c\u2500\u2500 manifests/            # Kubernetes manifests (numbered by category)\n\u251c\u2500\u2500 provision-host/       # Provision host container scripts\n\u251c\u2500\u2500 topsecret/            # Secrets management (gitignored)\n\u251c\u2500\u2500 install-rancher.sh    # Main installer script\n\u2514\u2500\u2500 mkdocs.yml            # Documentation site configuration\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please read the development workflow and git workflow guides before submitting changes.</p>"},{"location":"#license","title":"License","text":"<p>This project is maintained by the Urbalurba development team.</p>"},{"location":"hosts-azure-aks/","title":"Azure AKS Host Documentation","text":"<p>File: <code>docs/hosts-azure-aks.md</code> Purpose: Complete operational documentation for Urbalurba Infrastructure on Azure Kubernetes Service (AKS) Target Audience: Infrastructure engineers deploying to Azure AKS Last Updated: September 22, 2024</p> <p>Version 4.0 - All components tested and working in production.</p>"},{"location":"hosts-azure-aks/#executive-summary","title":"Executive Summary","text":"<p>\u2705 Production Ready - Urbalurba Infrastructure is fully operational on Azure AKS with zero changes to existing manifests. The system's context-based architecture provides seamless multi-environment support via kubectl context switching.</p>"},{"location":"hosts-azure-aks/#operational-status","title":"Operational Status:","text":"<ul> <li>\u2705 Complete AKS deployment - All services running successfully</li> <li>\u2705 Storage compatibility - Azure Disk CSI with transparent aliases</li> <li>\u2705 Cost management - Comprehensive cluster and internet access control</li> <li>\u2705 Automated provisioning - End-to-end deployment scripts</li> <li>\u2705 Multi-environment - Seamless switching between local and cloud contexts</li> <li>\u2705 Production tested - Validated with real workloads and cost analysis</li> </ul>"},{"location":"hosts-azure-aks/#deployment-guide","title":"Deployment Guide","text":""},{"location":"hosts-azure-aks/#quick-start","title":"Quick Start","text":"<pre><code># 1. Ensure provision-host is running\ndocker exec -it provision-host bash\ncd /mnt/urbalurbadisk\n\n# 2. Configure Azure settings (see Configuration section below)\ncp hosts/azure-aks/azure-aks-config.sh-template hosts/azure-aks/azure-aks-config.sh\nnano hosts/azure-aks/azure-aks-config.sh\n\n# 3. Deploy AKS cluster\n./hosts/install-azure-aks.sh\n\n# 4. Deploy all services\ncd /mnt/urbalurbadisk/provision-host/kubernetes\n./provision-kubernetes.sh azure-aks\n\n# 5. Manage cluster\n./hosts/azure-aks/manage-aks-cluster.sh\n</code></pre>"},{"location":"hosts-azure-aks/#configuration","title":"Configuration","text":"<p>IMPORTANT: Before running any deployment scripts, you must configure your Azure credentials:</p> <ol> <li> <p>Copy the template file:    <pre><code>cd hosts/azure-aks\ncp azure-aks-config.sh-template azure-aks-config.sh\n</code></pre></p> </li> <li> <p>Edit the configuration file and replace placeholder values with your actual Azure information:    <pre><code>nano azure-aks-config.sh\n</code></pre></p> </li> </ol> <p>Replace these placeholder values:    - <code>TENANT_ID=\"your-tenant-id\"</code> \u2192 Your Azure tenant ID    - <code>SUBSCRIPTION_ID=\"your-subscription-id\"</code> \u2192 Your Azure subscription ID    - <code>your-email@organization.com</code> \u2192 Your actual email address    - <code>your-cost-center</code> \u2192 Your organization's cost center</p> <ol> <li>Security Note: The <code>azure-aks-config.sh</code> file contains sensitive information and is automatically excluded from git commits via <code>.gitignore</code>.</li> </ol>"},{"location":"hosts-azure-aks/#azure-settings","title":"Azure Settings","text":"<p>Example configuration (<code>azure-aks-config.sh</code>): <pre><code>TENANT_ID=\"your-tenant-id\"\nSUBSCRIPTION_ID=\"your-subscription-id\"\nRESOURCE_GROUP=\"rg-urbalurba-aks-weu\"\nCLUSTER_NAME=\"azure-aks\"\nLOCATION=\"westeurope\"\nNODE_COUNT=2\nNODE_SIZE=\"Standard_B2ms\"\n</code></pre></p> <p>Prerequisites: - Azure subscription with Contributor access - PIM role activation capability - Provision-host container running - Configuration setup: Copy template and add your Azure credentials (see Configuration section above)</p>"},{"location":"hosts-azure-aks/#deployment-components","title":"Deployment Components","text":""},{"location":"hosts-azure-aks/#available-scripts","title":"Available Scripts","text":"<ol> <li>Core Deployment Scripts \u2705</li> <li>\u2705 <code>install-azure-aks.sh</code> - Complete automated deployment orchestrator</li> <li>\u2705 <code>01-azure-aks-create.sh</code> - AKS cluster creation with PIM handling</li> <li>\u2705 <code>02-azure-aks-setup.sh</code> - Post-creation configuration and Traefik setup</li> <li> <p>\u2705 <code>03-azure-aks-cleanup.sh</code> - Comprehensive cluster removal</p> </li> <li> <p>Management and Support Scripts \u2705</p> </li> <li>\u2705 <code>azure-aks-config.sh</code> - Centralized configuration management</li> <li>\u2705 <code>check-aks-quota.sh</code> - Pre-deployment quota validation</li> <li> <p>\u2705 <code>manage-aks-cluster.sh</code> - Operations management (internet, costs, cluster control)</p> </li> <li> <p>Infrastructure Configuration \u2705</p> </li> <li>\u2705 <code>manifests-overrides/000-storage-class-azure-alias.yaml</code> - Storage class compatibility</li> <li>\u2705 Full integration with existing Urbalurba manifests and playbooks</li> </ol>"},{"location":"hosts-azure-aks/#deployment-workflow","title":"Deployment Workflow \u2705","text":"<ol> <li>\u2705 Prerequisites: Provision-host container running (<code>install-rancher.sh</code>)</li> <li>\u2705 Enter container: <code>docker exec -it provision-host bash</code></li> <li>\u2705 Configure Azure: Edit <code>azure-aks-config.sh</code> with your Azure details</li> <li>\u2705 Deploy cluster: <code>./hosts/install-azure-aks.sh</code> (fully automated)</li> <li>\u2705 Deploy services: <code>cd /mnt/urbalurbadisk/provision-host/kubernetes &amp;&amp; ./provision-kubernetes.sh azure-aks</code></li> <li>\u2705 Manage cluster: <code>./hosts/azure-aks/manage-aks-cluster.sh [command]</code></li> </ol>"},{"location":"hosts-azure-aks/#cluster-management","title":"Cluster Management","text":""},{"location":"hosts-azure-aks/#management-commands","title":"Management Commands","text":"<pre><code># Check cluster status and costs\n./manage-aks-cluster.sh\n\n# Control internet access\n./manage-aks-cluster.sh internet on   # Enable external access\n./manage-aks-cluster.sh internet off  # Disable (save costs)\n\n# Control cluster state\n./manage-aks-cluster.sh cluster stop  # Stop cluster (save ~$120/month)\n./manage-aks-cluster.sh cluster start # Restart cluster\n\n# View cost analysis\n./manage-aks-cluster.sh costs         # Detailed cost breakdown\n</code></pre>"},{"location":"hosts-azure-aks/#context-switching","title":"Context Switching","text":"<pre><code># Switch between environments\nexport KUBECONFIG=/mnt/urbalurbadisk/kubeconfig/kubeconf-all\nkubectl config use-context rancher-desktop  # Local development\nkubectl config use-context azure-aks       # Cloud production\n\n# Verify current context\nkubectl config current-context\nkubectl get nodes\n</code></pre>"},{"location":"hosts-azure-aks/#service-deployment","title":"Service Deployment","text":"<pre><code># Deploy all Urbalurba services to AKS\ncd /mnt/urbalurbadisk/provision-host/kubernetes\n./provision-kubernetes.sh azure-aks\n\n# Verify deployment\nkubectl get pods --all-namespaces\nkubectl get pvc --all-namespaces\nkubectl get ingressroute --all-namespaces\n</code></pre>"},{"location":"hosts-azure-aks/#cost-management","title":"Cost Management","text":""},{"location":"hosts-azure-aks/#cost-analysis","title":"Cost Analysis","text":"<p>Typical costs for 2x Standard_B2ms configuration: - AKS Control Plane: $0 (Free tier) - Compute (2x Standard_B2ms): ~$120/month - Storage (Azure managed disks): ~$30/month - Load Balancer: ~$20/month (when internet enabled) - Total: ~$170/month active | ~$30/month when stopped</p> <p>Real cost tracking: Use <code>./manage-aks-cluster.sh costs</code> or check Azure Portal Cost Management</p>"},{"location":"hosts-azure-aks/#cost-optimization","title":"Cost Optimization","text":"<pre><code># Immediate savings\n./manage-aks-cluster.sh internet off     # Save ~$20/month (LoadBalancer)\n./manage-aks-cluster.sh cluster stop     # Save ~$120/month (compute)\n\n# Restart when needed\n./manage-aks-cluster.sh cluster start\n./manage-aks-cluster.sh internet on\n\n# Complete cleanup (default behavior)\n./hosts/azure-aks/03-azure-aks-cleanup.sh         # Remove everything\n# OR keep resource group\n./hosts/azure-aks/03-azure-aks-cleanup.sh --keep-rg  # Keep RG, delete cluster only\n</code></pre>"},{"location":"hosts-azure-aks/#architecture-details","title":"Architecture Details","text":""},{"location":"hosts-azure-aks/#storage-integration","title":"Storage Integration","text":"<ul> <li>Storage Classes: <code>local-path</code> and <code>microk8s-hostpath</code> aliases map to Azure Disk CSI</li> <li>Persistent Volumes: Automatic provisioning via Azure managed disks</li> <li>Compatibility: All existing PVC manifests work unchanged</li> </ul>"},{"location":"hosts-azure-aks/#networking","title":"Networking","text":"<ul> <li>CNI: Azure CNI with network policies</li> <li>Ingress: Traefik with Azure LoadBalancer service</li> <li>Internet Access: Controllable via service type switching</li> <li>Internal Services: ClusterIP services for internal communication</li> </ul>"},{"location":"hosts-azure-aks/#security","title":"Security","text":"<ul> <li>RBAC: Azure AD integration with Kubernetes RBAC</li> <li>PIM: Privileged Identity Management for Azure access</li> <li>Network Policies: Azure CNI network policy enforcement</li> <li>Secrets: Kubernetes secrets management for sensitive data</li> </ul>"},{"location":"hosts-azure-aks/#troubleshooting","title":"Troubleshooting","text":""},{"location":"hosts-azure-aks/#common-issues","title":"Common Issues","text":"<p>PIM Role Activation: <pre><code># If you get permission errors:\n# 1. Go to https://portal.azure.com PIM\n# 2. Activate Contributor role\n# 3. Wait 2 minutes\n# 4. Retry operation\n</code></pre></p> <p>Context Switching: <pre><code># If contexts are missing:\ncd /mnt/urbalurbadisk\nansible-playbook ansible/playbooks/04-merge-kubeconf.yml\nexport KUBECONFIG=/mnt/urbalurbadisk/kubeconfig/kubeconf-all\n</code></pre></p> <p>Storage Issues: <pre><code># If PVCs are pending:\nkubectl get storageclass  # Verify aliases exist\nkubectl describe pvc &lt;name&gt;  # Check events\n</code></pre></p>"},{"location":"hosts-azure-aks/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code># Check cluster resources\nkubectl top nodes\nkubectl top pods --all-namespaces\n\n# Monitor costs\n./manage-aks-cluster.sh costs\n\n# Check service status\nkubectl get pods --all-namespaces\nkubectl get pvc --all-namespaces\nkubectl get ingressroute --all-namespaces\n</code></pre>"},{"location":"hosts-azure-aks/#production-operations","title":"Production Operations","text":""},{"location":"hosts-azure-aks/#daily-operations","title":"Daily Operations","text":"<ul> <li>Monitor costs: Check Azure Portal or run cost analysis</li> <li>Control internet access: Enable only when needed</li> <li>Context switching: Seamless development \u2194 production</li> <li>Service deployment: Standard Kubernetes workflows</li> </ul>"},{"location":"hosts-azure-aks/#backup-and-recovery","title":"Backup and Recovery","text":"<ul> <li>Persistent data: Stored in Azure managed disks</li> <li>Configuration: All infrastructure as code in git</li> <li>Cluster recreation: Fully automated via scripts</li> <li>Service restoration: Standard manifest redeployment</li> </ul>"},{"location":"hosts-azure-aks/#summary","title":"Summary","text":"<p>Urbalurba Infrastructure on Azure AKS provides a complete, production-ready Kubernetes platform with:</p> <p>\u2705 Zero-modification deployment - All existing manifests work unchanged \u2705 Cost-effective operations - Granular control over compute and networking costs \u2705 Multi-environment support - Seamless local \u2194 cloud development workflow \u2705 Enterprise security - Azure AD, PIM, network policies, and RBAC \u2705 Operational simplicity - Automated deployment, management, and cleanup  </p> <p>The system is battle-tested and ready for production workloads.</p> <p>Version 4.0 - Complete operational documentation. All components tested and validated in production deployment.</p>"},{"location":"hosts-azure-microk8s/","title":"Azure MicroK8s Host Documentation","text":"<p>File: <code>docs/hosts-azure-microk8s.md</code> Purpose: Deployment guide for Azure VM with MicroK8s using Cloud Adoption Framework (CAF) Target Audience: Infrastructure engineers deploying to Azure environments Last Updated: September 22, 2024</p>"},{"location":"hosts-azure-microk8s/#overview","title":"\ud83d\udccb Overview","text":"<p>This guide covers setting up a VM in Azure using the Cloud Adoption Framework (CAF) with MicroK8s. In a CAF environment, resources are locked down by default, requiring explicit permissions for access. While this enhances security, it adds complexity to the setup process.</p>"},{"location":"hosts-azure-microk8s/#key-features","title":"Key Features","text":"<ul> <li>Azure VM deployment with automated MicroK8s installation</li> <li>CAF compliance with proper resource group and networking setup</li> <li>Tailscale VPN integration for secure remote access</li> <li>Cloud-init automation for consistent provisioning</li> <li>Ansible integration for ongoing management</li> </ul>"},{"location":"hosts-azure-microk8s/#automated-setup-with-enhanced-script","title":"Automated Setup with Enhanced Script","text":"<p>We've created an improved script (<code>01-azure-vm-create-redcross-v2.sh</code>) that handles the entire deployment process, including error handling and detailed feedback.</p>"},{"location":"hosts-azure-microk8s/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure subscription with Contributor role access</li> <li>Tailscale account and network setup</li> <li>SSH keys for the ansible user</li> <li>Configuration setup: Copy the template file and add your Azure credentials (see Configuration section below)</li> </ul>"},{"location":"hosts-azure-microk8s/#deployment-process","title":"Deployment Process","text":""},{"location":"hosts-azure-microk8s/#configuration","title":"Configuration","text":"<p>IMPORTANT: Before running any deployment scripts, you must configure your Azure credentials:</p> <ol> <li> <p>Copy the template file:    <pre><code>cd hosts/azure-microk8s\ncp azure-vm-config-redcross-sandbox.sh-template azure-vm-config-redcross-sandbox.sh\n</code></pre></p> </li> <li> <p>Edit the configuration file and replace placeholder values with your actual Azure information:    <pre><code>nano azure-vm-config-redcross-sandbox.sh\n</code></pre></p> </li> </ol> <p>Replace these placeholder values:    - <code>TENANT_ID=\"your-tenant-id\"</code> \u2192 Your Azure tenant ID    - <code>SUBSCRIPTION_ID=\"your-subscription-id\"</code> \u2192 Your Azure subscription ID    - <code>your-email@organization.com</code> \u2192 Your actual email address    - <code>your-cost-center</code> \u2192 Your organization's cost center</p> <ol> <li>Security Note: The <code>azure-vm-config-redcross-sandbox.sh</code> file contains sensitive information and is automatically excluded from git commits via <code>.gitignore</code>.</li> </ol>"},{"location":"hosts-azure-microk8s/#deployment-process_1","title":"Deployment Process","text":"<p>The deployment uses these files:</p> <ul> <li>VM configuration: <code>azure-vm-config-redcross-sandbox.sh</code> (created from template above)</li> <li>VM creation script: <code>01-azure-vm-create-redcross-v2.sh</code></li> <li>Ansible inventory: <code>02-azure-ansible-inventory.sh</code></li> <li>Resource cleanup: <code>azure-vm-cleanup-redcross-v2.sh</code></li> </ul>"},{"location":"hosts-azure-microk8s/#running-the-creation-script","title":"Running the Creation Script","text":"<pre><code>./01-azure-vm-create-redcross-v2.sh &lt;admin_username&gt; &lt;admin_password&gt; &lt;vm_instance&gt;\n</code></pre> <p>Example: <pre><code>./01-azure-vm-create-redcross-v2.sh ansible &lt;your-password&gt; azure-microk8s\n</code></pre></p> <p>The script performs these steps: 1. Checks for required configuration files 2. Prompts you to activate your Privileged Identity Management (PIM) role if needed 3. Logs into Azure and sets the subscription 4. Creates resource groups for VM and networking if they don't exist 5. Sets up network components (VNet, subnet, NSG) 6. Creates and configures the VM with MicroK8s 7. Formats and mounts the data disk 8. Verifies Tailscale connectivity 9. Creates cluster information file 10. Tests SSH access through Tailscale 11. Shows MicroK8s status and provides comprehensive summary</p> <p>Important: The script uses cloud-init to configure the VM, including Tailscale setup. The cloud-init file should include a valid Tailscale auth key.</p>"},{"location":"hosts-azure-microk8s/#cloud-init-configuration","title":"Cloud-Init Configuration","text":"<p>The cloud-init file (<code>/mnt/urbalurbadisk/cloud-init/azure-cloud-init.yml</code>) handles initial VM setup: - Creates the <code>ansible</code> user with SSH key access - Installs MicroK8s with core add-ons - Installs and configures Tailscale for secure remote access</p> <p>For more information about cloud-init configuration, see <code>cloud-init/cloud-init-readme.md</code></p>"},{"location":"hosts-azure-microk8s/#accessing-the-vm","title":"Accessing the VM","text":"<p>After deployment, access the VM via Tailscale:</p> <pre><code>ssh -i /mnt/urbalurbadisk/secrets/id_rsa_ansible -F /dev/null ansible@&lt;tailscale-ip&gt;\n</code></pre> <p>Where <code>&lt;tailscale-ip&gt;</code> is the Tailscale IP displayed at the end of the script execution.</p> <p>The provision-host (the local container or VM) must also be connected to the Tailscale network to connect over the Tailscale network. For more about Tailscale see <code>networking/vpn-tailscale-howto.md</code></p>"},{"location":"hosts-azure-microk8s/#deleting-the-vm","title":"Deleting the VM","text":"<p>To remove the VM and associated resources, use the improved cleanup script:</p> <pre><code>./azure-vm-cleanup-redcross-v2.sh &lt;vm_instance&gt;\n</code></pre> <p>Example: <pre><code>./azure-vm-cleanup-redcross-v2.sh azure-microk8s\n</code></pre></p> <p>The cleanup script will: 1. Prompt you to activate your PIM role if needed 2. Log into Azure and set the subscription 3. List all resources that will be deleted and ask for confirmation 4. If the VM is the only one in its resource group, offer to delete the entire group (faster) 5. Remove all associated resources (VM, disks, NIC, NSG) 6. Optionally wait to verify that the resource group has been fully deleted 7. Remind you to manually delete the machine from your Tailscale network</p>"},{"location":"hosts-azure-microk8s/#manual-installation-for-troubleshooting","title":"Manual Installation (For Troubleshooting)","text":"<p>If you need to troubleshoot or understand the underlying process, here are the manual steps:</p>"},{"location":"hosts-azure-microk8s/#step-1-set-up-the-azure-environment","title":"Step 1: Set Up the Azure Environment","text":"<ol> <li> <p>Create resource groups: <pre><code>az group create --name rg-sandbox-k8s-weu --location westeurope\naz group create --name rg-sandbox-network-weu --location westeurope\n</code></pre></p> </li> <li> <p>Create networking components: <pre><code>az network vnet create --resource-group rg-sandbox-network-weu --name vnet-sandbox-network-weu --address-prefix 10.2.0.0/16 --location westeurope\naz network vnet subnet create --resource-group rg-sandbox-network-weu --vnet-name vnet-sandbox-network-weu --name sub-sandbox-k8s-weu --address-prefix 10.2.1.0/24\naz network nsg create --resource-group rg-sandbox-k8s-weu --name nsg-sandbox-k8s-azure-microk8s-weu --location westeurope\n</code></pre></p> </li> <li> <p>Create network interface: <pre><code>az network nic create --resource-group rg-sandbox-k8s-weu --name nic-sandbox-k8s-azure-microk8s-weu --subnet /subscriptions/&lt;subscription-id&gt;/resourceGroups/rg-sandbox-network-weu/providers/Microsoft.Network/virtualNetworks/vnet-sandbox-network-weu/subnets/sub-sandbox-k8s-weu --network-security-group nsg-sandbox-k8s-azure-microk8s-weu\n</code></pre></p> </li> <li> <p>Create VM with cloud-init: <pre><code>az vm create --resource-group rg-sandbox-k8s-weu --name vm-sandbox-k8s-azure-microk8s-weu --image Ubuntu2204 --size Standard_B2ms --location westeurope --admin-username \"ansible\" --admin-password \"&lt;your-password&gt;\" --authentication-type password --storage-sku Standard_LRS --data-disk-sizes-gb 50 --nics nic-sandbox-k8s-azure-microk8s-weu --custom-data @/mnt/urbalurbadisk/cloud-init/azure-cloud-init.yml\n</code></pre></p> </li> </ol>"},{"location":"hosts-azure-microk8s/#step-2-verify-tailscale-connectivity","title":"Step 2: Verify Tailscale Connectivity","text":"<ol> <li> <p>Check if Tailscale is running on the VM: <pre><code>az vm run-command invoke --resource-group rg-sandbox-k8s-weu --name vm-sandbox-k8s-azure-microk8s-weu --command-id RunShellScript --scripts \"tailscale status\"\n</code></pre></p> </li> <li> <p>Get the Tailscale IP: <pre><code>az vm run-command invoke --resource-group rg-sandbox-k8s-weu --name vm-sandbox-k8s-azure-microk8s-weu --command-id RunShellScript --scripts \"tailscale ip -4\"\n</code></pre></p> </li> <li> <p>Test SSH access: <pre><code>ssh -i /mnt/urbalurbadisk/secrets/id_rsa_ansible -F /dev/null ansible@&lt;tailscale-ip&gt;\n</code></pre></p> </li> </ol>"},{"location":"hosts-azure-microk8s/#step-3-format-and-mount-data-disk","title":"Step 3: Format and Mount Data Disk","text":"<pre><code>az vm run-command invoke --resource-group rg-sandbox-k8s-weu --name vm-sandbox-k8s-azure-microk8s-weu --command-id RunShellScript --scripts \"sudo parted /dev/sdc --script mklabel gpt mkpart xfspart xfs 0% 100% &amp;&amp; sudo mkfs.xfs /dev/sdc1 &amp;&amp; sudo partprobe /dev/sdc1 &amp;&amp; sudo mkdir -p /mnt/urbalurbadisk &amp;&amp; sudo mount /dev/sdc1 /mnt/urbalurbadisk &amp;&amp; echo '/dev/sdc1 /mnt/urbalurbadisk xfs defaults 0 2' | sudo tee -a /etc/fstab &amp;&amp; sudo chown ansible:ansible /mnt/urbalurbadisk &amp;&amp; sudo chmod 755 /mnt/urbalurbadisk\"\n</code></pre>"},{"location":"hosts-azure-microk8s/#step-4-verify-microk8s-installation","title":"Step 4: Verify MicroK8s Installation","text":"<pre><code>az vm run-command invoke --resource-group rg-sandbox-k8s-weu --name vm-sandbox-k8s-azure-microk8s-weu --command-id RunShellScript --scripts \"microk8s status\"\n</code></pre>"},{"location":"hosts-azure-microk8s/#troubleshooting","title":"Troubleshooting","text":""},{"location":"hosts-azure-microk8s/#cloud-init-issues","title":"Cloud-Init Issues","text":"<p>If cloud-init fails, check the logs: <pre><code>az vm run-command invoke --resource-group rg-sandbox-k8s-weu --name vm-sandbox-k8s-azure-microk8s-weu --command-id RunShellScript --scripts \"cat /var/log/cloud-init-output.log | tail -n 50\"\n</code></pre></p>"},{"location":"hosts-azure-microk8s/#tailscale-connectivity","title":"Tailscale Connectivity","text":"<p>If Tailscale isn't connecting: 1. Verify Tailscale installation: <pre><code>az vm run-command invoke --resource-group rg-sandbox-k8s-weu --name vm-sandbox-k8s-azure-microk8s-weu --command-id RunShellScript --scripts \"which tailscale &amp;&amp; systemctl status tailscaled\"\n</code></pre></p> <ol> <li>Check for valid auth keys in your cloud-init file and create a new auth key if needed from the Tailscale admin console.</li> </ol>"},{"location":"hosts-azure-microk8s/#ssh-access-problems","title":"SSH Access Problems","text":"<p>If SSH access fails: 1. Verify the SSH key path: <code>/mnt/urbalurbadisk/secrets/id_rsa_ansible</code> 2. Check SSH key permissions: <code>chmod 600 /mnt/urbalurbadisk/secrets/id_rsa_ansible</code> 3. Bypass SSH config with the <code>-F /dev/null</code> option</p>"},{"location":"hosts-azure-microk8s/#sample-output-logs","title":"Sample Output Logs","text":""},{"location":"hosts-azure-microk8s/#create-vm-example-log","title":"Create VM Example Log","text":"<p>Below is a truncated example of a successful VM creation:</p> <pre><code>./01-azure-vm-create-redcross-v2.sh ansible Secretp@ssword1 azure-microk8s\n================================================\n    Azure VM Creation Script v2 (Full Setup)    \n================================================\nStarting Azure VM creation process for instance: azure-microk8s\n\n=== STEP 1: Checking required files and loading configuration ===\n  \u2192 Checking if ./azure-vm-config-redcross-sandbox.sh exists\n  \u2713 File ./azure-vm-config-redcross-sandbox.sh found\n  \u2192 Checking if /mnt/urbalurbadisk/cloud-init/azure-cloud-init.yml exists\n  \u2713 File /mnt/urbalurbadisk/cloud-init/azure-cloud-init.yml found\n  \u2192 Loading configuration from ./azure-vm-config-redcross-sandbox.sh\n  \u2713 Configuration loaded\n  \u2192 Generating resource names\nConfigured variables:\n  VM Instance Name: vm-sandbox-k8s-azure-microk8s-weu\n  VM Resource Group: rg-sandbox-k8s-weu\n  Network Resource Group: rg-sandbox-network-weu\n  Subscription ID: &lt;your-subscription-id&gt;\n\n=== STEP 1.5: Activating Privileged Identity Management (PIM) role ===\nIMPORTANT: You need the Contributor role on the Azure subscription to run this script.\nIn Azure this is a ClickOps operation (the M$ guys did not grow up using a command line tool)\nTo activate the Contributor role in Azure:\n  1) Search for PIM in the Azure portal search bar\n  2) Click on Microsoft Entra Privileged Identity Management\n  3) On the PIM page, click \"My roles\"\n  4) Click \"Azure resources\"\n  5) Click \"Activate\" next to the Contributor role\n\nAlternatively, click on this URL (Ctrl+Click in most terminals):\nhttps://portal.azure.com/?feature.msaljs=true#view/Microsoft_Azure_PIMCommon/ActivationMenuBlade/~/azurerbac\nSearch for your resource, click on the name, and then click on the Contributor role.\n\nAfter you have activated your Contributor role, press Enter to continue...\nContinuing with script execution...\n\n=== STEP 2: Setting up Azure CLI and login ===\n  \u2192 Running: Setting login experience\n  \u2713 Azure CLI: Setting login experience\n  \u2192 Initiating Azure login\nPlease follow these steps to log in:\n  1. Open a web browser and go to: https://microsoft.com/devicelogin\n  2. Enter the code that will be displayed below\n  3. Follow the prompts to complete the login process\nRunning: az login --tenant &lt;your-tenant-id&gt; --use-device-code\nTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code XXXXXXXX to authenticate.\n[\n  {\n    \"cloudName\": \"AzureCloud\",\n    \"homeTenantId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n    \"id\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n    \"isDefault\": true,\n    \"managedByTenants\": [],\n    \"name\": \"Your Subscription Name\",\n    \"state\": \"Enabled\",\n    \"tenantId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n    \"user\": {\n      \"name\": \"user@organization.com\",\n      \"type\": \"user\"\n    }\n  }\n  // Additional subscriptions may appear here\n]\n  \u2713 Azure login successful\n  \u2192 Running: Set subscription\n  \u2713 Azure: Set subscription\n\n=== STEP 3: Checking for existing resources ===\n  \u2192 Checking if VM already exists: vm-sandbox-k8s-azure-microk8s-weu\n  \u2713 VM does not exist, can proceed with creation\n\n=== STEP 4: Creating Resource Groups ===\n  \u2192 Checking if Resource Group rg-sandbox-k8s-weu exists\n  \u2192 Creating Resource Group: rg-sandbox-k8s-weu in westeurope\n  \u2192 Running: Create rg-sandbox-k8s-weu\n  \u2713 Resource Group: Create rg-sandbox-k8s-weu\n  \u2192 Checking if Resource Group rg-sandbox-network-weu exists\n  \u2192 Creating Resource Group: rg-sandbox-network-weu in westeurope\n  \u2192 Running: Create rg-sandbox-network-weu\n  \u2713 Resource Group: Create rg-sandbox-network-weu\n\n=== STEP 5: Creating VNet and Subnet ===\n  \u2192 Checking if VNet vnet-sandbox-network-weu exists\n  \u2713 VNet vnet-sandbox-network-weu already exists\n  \u2192 Checking if Subnet sub-sandbox-k8s-weu exists\n/subscriptions/&lt;subscription-id&gt;/resourceGroups/rg-sandbox-network-weu/providers/Microsoft.Network/virtualNetworks/vnet-sandbox-network-weu/subnets/sub-sandbox-k8s-weu\n  \u2713 Subnet sub-sandbox-k8s-weu already exists\n  \u2192 Updating SUBNET_FULL_PATH variable\n\n=== STEP 6: Creating Network Security Group ===\n  \u2192 Checking if NSG nsg-sandbox-k8s-azure-microk8s-weu exists\n  \u2192 Creating NSG: nsg-sandbox-k8s-azure-microk8s-weu\n  \u2192 Running: Create NSG\n  \u2713 Network: Create NSG\n  \u2192 Using Tailscale for secure access - no external SSH rule needed\n\n=== STEP 7: Creating Network Interface ===\n  \u2192 Creating network interface: nic-sandbox-k8s-azure-microk8s-weu\n  \u2192 Running: Create interface\n  \u2713 Network: Create interface\n\n=== STEP 8: Creating Virtual Machine with Data Disk ===\n  \u2192 Starting VM creation (this may take several minutes)\n  \u2192 Running: Create\n  \u2713 VM: Create\n\n=== STEP 9: Waiting for VM to be fully provisioned ===\n  \u2192 Running: Wait for creation\n  \u2713 VM: Wait for creation\n\n=== STEP 10: Waiting for cloud-init to complete ===\n  \u2192 Checking if cloud-init has completed processing\n  \u2192 Unable to confirm cloud-init completion, checking MicroK8s status\n  \u2713 MicroK8s is running - cloud-init has likely completed successfully\n\n=== STEP 11: Formatting and mounting data disk ===\n  \u2192 Running on VM: Formatting and mounting data disk\n  \u2713 VM Command: Formatting and mounting data disk\nEnable succeeded: \n[stdout]\nmeta-data=/dev/sdc1              isize=512    agcount=4, agsize=3276672 blks\n         =                       sectsz=4096  attr=2, projid32bit=1\n         =                       crc=1        finobt=1, sparse=1, rmapbt=0\n         =                       reflink=1    bigtime=0 inobtcount=0\ndata     =                       bsize=4096   blocks=13106688, imaxpct=25\n         =                       sunit=0      swidth=0 blks\nnaming   =version 2              bsize=4096   ascii-ci=0, ftype=1\nlog      =internal log           bsize=4096   blocks=6399, version=2\n         =                       sectsz=4096  sunit=1 blks, lazy-count=1\nrealtime =none                   extsz=4096   blocks=0, rtextents=0\nDiscarding blocks...Done.\n/dev/sdc1 /mnt/urbalurbadisk xfs defaults 0 2\nDisk formatted, mounted, and permissions set\n\n[stderr]\n\n=== STEP 12: Getting Tailscale IP and Hostname ===\n  \u2192 Getting Tailscale IP (may take multiple attempts)\n  \u2192 Attempt 1 of 10 to get Tailscale IP\n  \u2192 Attempt failed. Waiting for 30 seconds before retrying...\n  \u2192 Attempt 2 of 10 to get Tailscale IP\n  \u2713 Successfully retrieved Tailscale IP: 100.64.192.33\n  \u2192 Using instance name as Tailscale hostname: azure-microk8s\n\n=== STEP 13: Creating cluster information file ===\n  \u2192 Creating: azure-microk8s.sh\n  \u2713 Created azure-microk8s.sh\n\n=== STEP 14: Setting execute permissions for info file ===\n  \u2713 Set executable permissions\n\n=== STEP 15: Testing Ansible user SSH access ===\n  \u2192 Testing SSH connection to 100.64.192.33\nWarning: Permanently added '100.64.192.33' (ED25519) to the list of known hosts.\nAnsible user SSH test successful\n  \u2713 Ansible SSH access successful\n\n=== STEP 16: Displaying cluster information ===\n  \u2192 Checking MicroK8s status\n  \u2713 MicroK8s is running\nEnabled addons:\n  \u2713 dashboard # (core) The Kubernetes dashboard\n  \u2713 dns # (core) CoreDNS\n  \u2713 ha-cluster # (core) Configure high availability on the current node\n  \u2713 helm # (core) Helm - the package manager for Kubernetes\n  \u2713 helm3 # (core) Helm 3 - the package manager for Kubernetes\n  \u2713 hostpath-storage # (core) Storage class; allocates storage from host directory\n  \u2713 metrics-server # (core) K8s Metrics Server for API access to service metrics\n  \u2713 storage # (core) Alias to hostpath-storage add-on, deprecated\n\n===== Azure VM Creation Summary =====\nComponent Status:\n  Network - Update variables: OK\n  Cloud-Init - Completion: Likely successful (MicroK8s running)\n  Resource Group - Create rg-sandbox-k8s-weu: OK\n  Network - Create NSG: OK\n  Network - Create interface: OK\n  Ansible - SSH Test: Successful\n  Resource Group - Create rg-sandbox-network-weu: OK\n  VM Command - Formatting and mounting data disk: OK\n  Azure - Set subscription: OK\n  VM - Create: OK\n  Tailscale - Hostname: azure-microk8s\n  VM - Wait for creation: OK\n  Configuration - File Permissions: Set successfully\n  MicroK8s - Status: Running\n  Network - SSH Access: Secured via Tailscale\n  Azure CLI - Setting login experience: OK\n  Tailscale - IP: 100.64.192.33\n\nVM creation completed successfully.\n  Virtual Machine: vm-sandbox-k8s-azure-microk8s-weu\n  Resource Group: rg-sandbox-k8s-weu\n  Tailscale IP: 100.64.192.33\n  Tailscale Hostname: azure-microk8s\n\nTo SSH to the VM, use:\n  ssh -i /mnt/urbalurbadisk/secrets/id_rsa_ansible ansible@100.64.192.33\n</code></pre>"},{"location":"hosts-azure-microk8s/#inventory-log","title":"Inventory log","text":"<pre><code>./02-azure-ansible-inventory-v2.sh\n================================================\n      Azure VM Ansible Inventory Update V2      \n================================================\n\n=== STEP 1: Checking current directory ===\n  \u2192 Current directory: azure-microk8s\n  \u2713 Correct directory confirmed: hosts/azure-microk8s\n\n=== STEP 2: Loading configuration ===\n  \u2192 Checking if ./azure-vm-config-redcross-sandbox.sh exists\n  \u2713 File ./azure-vm-config-redcross-sandbox.sh found\n  \u2192 Loading configuration from ./azure-vm-config-redcross-sandbox.sh\n  \u2713 Successfully loaded configuration from ./azure-vm-config-redcross-sandbox.sh\n  \u2192 Checking if azure-microk8s.sh exists\n  \u2713 File azure-microk8s.sh found\n  \u2192 Loading cluster information from azure-microk8s.sh\n  \u2713 Successfully loaded cluster information from azure-microk8s.sh\n  \u2192 Loaded configuration:\n  Cluster Name: azure-microk8s\n  Tailscale IP: 100.64.192.33\n  Host Name: vm-sandbox-k8s-azure-microk8s-weu\n  Ansible Directory: /mnt/urbalurbadisk/ansible\n\n=== STEP 3: Updating Ansible inventory ===\n  \u2192 Validating configuration variables\n  \u2713 Configuration variables validated\n  \u2192 Checking if Ansible directory exists: /mnt/urbalurbadisk/ansible\n  \u2713 Ansible directory exists\n  \u2192 Checking if inventory playbook exists: /mnt/urbalurbadisk/ansible/playbooks/02-update-ansible-inventory.yml\n  \u2713 Inventory playbook exists\n  \u2192 Pre-checking SSH connectivity to 100.64.192.33 (port 22)\n  \u2192 Running: SSH Connectivity Test\n  \u2713 Network: SSH Connectivity Test\n  \u2713 SSH connectivity to 100.64.192.33 confirmed\n  \u2192 Running Ansible playbook to update inventory\n  \u2192 Running: Update Inventory\n  \u2713 Ansible: Update Inventory\n  \u2713 Ansible inventory updated successfully via playbook\n\n=== STEP 5: Validating Ansible inventory file ===\n  \u2192 Checking if inventory file exists: /mnt/urbalurbadisk/ansible/inventory.yml\n  \u2713 Inventory file exists\n  \u2192 Checking if inventory file is valid YAML\n  \u2192 Running: YAML Syntax\n  \u2713 Validation: YAML Syntax\n  \u2713 Inventory file is valid YAML\n  \u2192 Checking if azure-microk8s exists in inventory\n  \u2713 Cluster name azure-microk8s found in inventory\n  \u2192 Checking if IP address is correct\n  \u2713 IP address 100.x.x.x correctly set for azure-microk8s\n\n=== STEP 4: Testing Ansible connection ===\n  \u2192 Checking network connectivity to 100.x.x.x\n  \u2192 Running: ICMP Ping Test\n  \u2713 Network: ICMP Ping Test\n  \u2713 Host 100.x.x.x is reachable via ICMP ping\n  \u2192 Checking if SSH key exists: /mnt/urbalurbadisk/secrets/id_rsa_ansible\n  \u2713 SSH key found\n  \u2192 Pinging host azure-microk8s via Ansible\n  \u2192 Running: Connection Test\n  \u2713 Ansible: Connection Test\n  \u2713 Successfully connected to azure-microk8s via Ansible\n\n=== STEP 6: Summary ===\n===== Ansible Inventory Update Summary =====\nComponent Status:\n  Configuration - Cluster Info: OK\n  Ansible - Connection Test: OK\n  Ansible - Update Inventory: OK\n  Network - SSH Connectivity: OK\n  Configuration - Main Config: OK\n  Ansible - Inventory Update: Successful\n  Environment - Directory: OK\n  Network - ICMP Ping: OK\n  Network - SSH Connectivity Test: OK\n  Validation - YAML Syntax: OK\n  Network - ICMP Ping Test: OK\n  Validation - IP Address: Correct\n\n\u2705 Ansible inventory update completed successfully.\n  Cluster Name: azure-microk8s\n  Host IP: 100.x.x.x\n  Inventory File: /mnt/urbalurbadisk/ansible/inventory.yml\n\nYou can now manage this VM through Ansible:\n  ansible azure-microk8s -m ping\n  ansible-playbook your-playbook.yml -l azure-microk8s\n\nCommon Ansible commands:\n  # Check system facts\n  ansible azure-microk8s -m setup\n  # Run ad-hoc command\n  ansible azure-microk8s -a \"microk8s status\"\n  # Apply a playbook to this host\n  ansible-playbook path/to/playbook.yml -l azure-microk8s\n</code></pre>"},{"location":"hosts-azure-microk8s/#cleanup-example-log","title":"Cleanup Example Log","text":"<p>Below is a truncated example of a successful cleanup operation:</p> <pre><code>./azure-vm-cleanup-redcross-v2.sh azure-microk8s\n\u2192 Loading configuration from ./azure-vm-config-redcross-sandbox.sh\n\u2713 Configuration loaded\nThe following resources will be deleted:\n  VM Instance Name: vm-sandbox-k8s-azure-microk8s-weu\n  Resource Group: rg-sandbox-k8s-weu\n  Network Interface: nic-sandbox-k8s-azure-microk8s-weu\n  Network Security Group: nsg-sandbox-k8s-azure-microk8s-weu\n  OS Disk: disk-os-sandbox-k8s-azure-microk8s-weu\n  Data Disk: disk-data-sandbox-k8s-azure-microk8s-weu\n\nAre you sure you want to delete these resources? (yes/no): yes\n\n=== STEP 1: Activating Privileged Identity Management (PIM) role ===\nIMPORTANT: You need the Contributor role on the Azure subscription to run this script.\nIn Azure this is a ClickOps operation (the M$ guys did not grow up using a command line tool)\nTo activate the Contributor role in Azure:\n  1) Search for PIM in the Azure portal search bar\n  2) Click on Microsoft Entra Privileged Identity Management\n  3) On the PIM page, click \"My roles\"\n  4) Click \"Azure resources\"\n  5) Click \"Activate\" next to the Contributor role\n\nAlternatively, click on this URL (Ctrl+Click in most terminals):\nhttps://portal.azure.com/?feature.msaljs=true#view/Microsoft_Azure_PIMCommon/ActivationMenuBlade/~/azurerbac\nSearch for your resource, click on the name, and then click on the Contributor role.\n\nAfter you have activated your Contributor role, press Enter to continue...\nContinuing with script execution...\n\n=== STEP 2: Setting up Azure CLI and login ===\n\u2192 Setting Azure CLI login experience\nCommand group 'config' is experimental and under development. Reference and support levels: https://aka.ms/CLI_refstatus\n\u2713 Setting Azure CLI login experience succeeded\n\u2192 Initiating Azure login with device code authentication\nPlease follow these steps to log in:\n  1. Open a web browser and go to: https://microsoft.com/devicelogin\n  2. Enter the code that will be displayed below\n  3. Follow the prompts to complete the login process\nTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code XXXXXXXX to authenticate.\n[\n  // Azure subscription output - multiple subscriptions may appear\n  {\n    \"cloudName\": \"AzureCloud\",\n    \"homeTenantId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n    \"id\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n    \"isDefault\": true,\n    \"managedByTenants\": [],\n    \"name\": \"Your Subscription Name\",\n    \"state\": \"Enabled\",\n    \"tenantId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n    \"user\": {\n      \"name\": \"user@organization.com\",\n      \"type\": \"user\"\n    }\n  }\n  // Additional subscriptions removed for brevity\n]\n\u2713 Azure login successful\n\u2192 Setting Azure subscription to &lt;your-subscription-id&gt;\n\u2713 Setting Azure subscription succeeded\n\n=== STEP 3: Checking for resource existence ===\n\u2192 Checking if VM exists: vm-sandbox-k8s-azure-microk8s-weu\n\u2713 VM vm-sandbox-k8s-azure-microk8s-weu found - will be deleted\n\n=== STEP 4: Deleting Azure resources ===\n\u2192 Checking if we should delete resource group rg-sandbox-k8s-weu\n\u2192 Only one VM exists in resource group rg-sandbox-k8s-weu - safe to delete entire group\n\u2192 Deleting entire resource group: rg-sandbox-k8s-weu\n\u2713 Deleting resource group rg-sandbox-k8s-weu succeeded\nResource deletion initiated.\nNote: Resource group deletion happens asynchronously and may take several minutes to complete.\n\nWould you like this script to wait and confirm complete deletion? (yes/no): yes\n\u2192 Waiting for resource group deletion to complete (this may take 5-10 minutes)...\n\u2192 Attempt 1/20: Resource group is in state: Deleting. Waiting 30s...\n\u2192 Attempt 2/20: Resource group is in state: Deleting. Waiting 30s...\n\u2192 Attempt 3/20: Resource group is in state: Deleting. Waiting 30s...\n\u2192 Attempt 4/20: Resource group is in state: Deleting. Waiting 30s...\n\u2192 Attempt 5/20: Resource group is in state: Deleting. Waiting 30s...\n\u2192 Attempt 6/20: Resource group is in state: Deleting. Waiting 30s...\n\u2713 Resource group rg-sandbox-k8s-weu has been successfully deleted!\n\n=== STEP 5: Cleanup complete ===\nAzure resources cleanup completed!\nDon't forget to manually delete the host from tailscale network if needed.\nGo to https://login.tailscale.com/admin/machines and delete the machine named: azure-microk8s\n</code></pre>"},{"location":"hosts-cloud-init-readme/","title":"Cloud-Init Documentation","text":"<p>File: <code>docs/hosts-cloud-init-readme.md</code> Purpose: Cloud-init configuration and templates for automated host provisioning Target Audience: Infrastructure engineers working with cloud-init deployments Last Updated: September 22, 2024</p>"},{"location":"hosts-cloud-init-readme/#overview","title":"\ud83d\udccb Overview","text":"<p>Cloud-init is a critical component for automated host provisioning across different environments in the Urbalurba infrastructure. It provides a standardized way to configure hosts during their initial boot process, ensuring consistent setup across various deployment scenarios. In addition to basic system configuration, cloud-init also handles:</p> <ul> <li>Connection to the Tailscale network for secure remote management</li> <li>Standardized Kubernetes (MicroK8s) installation and configuration</li> <li>Consistent security hardening across all environments</li> </ul> <p>This ensures that all hosts, regardless of their deployment environment, are: - Securely accessible for management - Properly configured for their intended role - Integrated into the infrastructure's network - Ready for service deployment</p>"},{"location":"hosts-cloud-init-readme/#prerequisites","title":"\ud83d\udd11 Prerequisites","text":"<p>IMPORTANT: Before using cloud-init templates, you must set up SSH keys for Ansible authentication:</p> <p>\ud83d\udc49 SSH Key Setup Guide - Create the required SSH keys for cloud-init deployments</p> <p>The SSH keys created in this prerequisite step are essential for: - Ansible automation and host management - Secure SSH access to provisioned hosts - Cloud-init user authentication setup</p>"},{"location":"hosts-cloud-init-readme/#cloud-init-templates","title":"Cloud-Init Templates","text":"<p>Located in <code>cloud-init/</code>: - <code>azure-cloud-init-template.yml</code> - Azure VM configuration - <code>gcp-cloud-init-template.yml</code> - Google Cloud Platform configuration - <code>oci-cloud-init-template.yml</code> - Oracle Cloud Infrastructure configuration - <code>multipass-cloud-init-template.yml</code> - Multipass VM configuration - <code>raspberry-cloud-init-template.yml</code> - Raspberry Pi configuration - <code>provision-cloud-init-template.yml</code> - Generic provisioning configuration</p>"},{"location":"hosts-cloud-init-readme/#cloud-init-generation","title":"Cloud-Init Generation","text":"<p>The <code>create-cloud-init.sh</code> script generates cloud-init configurations by: 1. Reading template files 2. Extracting secrets from Kubernetes secrets file 3. Replacing placeholders with actual values 4. Generating environment-specific cloud-init files</p>"},{"location":"hosts-cloud-init-readme/#usage","title":"Usage","text":"<pre><code>./create-cloud-init.sh &lt;hostname&gt; &lt;template-name&gt;\n</code></pre> <p>Where: - <code>hostname</code>: The name of the host to be configured - <code>template-name</code>: The name of the cloud-init template to use (e.g., azure, gcp, multipass)</p>"},{"location":"hosts-cloud-init-readme/#common-cloud-init-features","title":"Common Cloud-Init Features","text":"<p>All cloud-init configurations include:</p>"},{"location":"hosts-cloud-init-readme/#1-system-configuration","title":"1. System Configuration","text":"<ul> <li>Hostname setting</li> <li>Timezone configuration</li> <li>System updates</li> <li>Package management</li> </ul>"},{"location":"hosts-cloud-init-readme/#2-user-management","title":"2. User Management","text":"<ul> <li>Ansible user creation</li> <li>SSH key configuration</li> <li>Sudo privileges</li> <li>User groups</li> </ul>"},{"location":"hosts-cloud-init-readme/#3-security-setup","title":"3. Security Setup","text":"<ul> <li>SSH hardening</li> <li>Firewall configuration</li> <li>Security updates</li> <li>Access control</li> </ul>"},{"location":"hosts-cloud-init-readme/#4-kubernetes-setup","title":"4. Kubernetes Setup","text":"<ul> <li>MicroK8s installation</li> <li>Cluster configuration</li> <li>Network plugin setup</li> <li>Storage configuration</li> </ul>"},{"location":"hosts-cloud-init-readme/#5-network-configuration","title":"5. Network Configuration","text":"<ul> <li>Tailscale VPN setup</li> <li>Network interface configuration</li> <li>DNS settings</li> <li>Proxy configuration (if needed)</li> </ul>"},{"location":"hosts-cloud-init-readme/#6-environment-setup","title":"6. Environment Setup","text":"<ul> <li>Path configuration</li> <li>Environment variables</li> <li>System limits</li> <li>Resource allocation</li> </ul>"},{"location":"hosts-cloud-init-readme/#template-structure","title":"Template Structure","text":"<p>Each cloud-init template follows a standard structure:</p> <pre><code>#cloud-config\nusers:\n  - name: ansible\n    # User configuration\n\nwrite_files:\n  # System configuration files\n\npackage_update: true\npackage_upgrade: true\npackages:\n  # Required packages\n\nruncmd:\n  # Post-installation commands\n</code></pre>"},{"location":"hosts-cloud-init-secrets/","title":"Cloud-Init SSH Key Setup","text":"<p>File: <code>docs/hosts-cloud-init-secrets.md</code> Purpose: SSH key generation for cloud-init and Ansible automation Target Audience: Infrastructure engineers setting up cloud-init deployments Last Updated: September 22, 2024</p>"},{"location":"hosts-cloud-init-secrets/#overview","title":"\ud83d\udccb Overview","text":"<p>All VMs that we create are configured using cloud-init.yml. An \"ansible\" user is created and set up so that it can log in using SSH key authentication. This document provides instructions for creating the SSH key that is used by ansible to connect to all hosts.</p> <p>This is a prerequisite step for cloud-init deployment - see hosts-cloud-init-readme.md for the main cloud-init documentation.</p> <p>Important: The SSH key files (<code>id_rsa_ansible</code> and <code>id_rsa_ansible.pub</code>) will be created on your local disk and should never be committed to the repository. They contain sensitive authentication credentials.</p>"},{"location":"hosts-cloud-init-secrets/#prerequisites","title":"Prerequisites","text":"<p>First, change directory to the secrets directory:</p> <pre><code>cd secrets\npwd\n</code></pre> <p>You MUST be in the folder <code>secrets</code> for the rest of the commands to work.</p>"},{"location":"hosts-cloud-init-secrets/#creating-the-ansible-ssh-keys","title":"Creating the Ansible SSH Keys","text":"<p>There are two methods to create the necessary SSH keys:</p>"},{"location":"hosts-cloud-init-secrets/#method-1-using-the-automated-script-recommended","title":"Method 1: Using the Automated Script (Recommended)","text":"<p>We've created a script that automates the key creation process:</p> <pre><code>./create-secrets.sh\n</code></pre> <p>The script will: 1. Verify it's running from the correct directory 2. Create an RSA 4096-bit key pair without a passphrase 3. Verify the keys were created successfully 4. Provide a summary of operations</p> <p>If successful, you'll see a message indicating \"All OK\".</p>"},{"location":"hosts-cloud-init-secrets/#method-2-manual-creation","title":"Method 2: Manual Creation","text":"<p>If you prefer to create the keys manually:</p> <pre><code>ssh-keygen -t rsa -b 4096 -f ./id_rsa_ansible\n</code></pre> <p>When prompted for a passphrase, you can leave it empty.</p> <pre><code>Generating public/private rsa key pair.\nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in ./id_rsa_ansible\nYour public key has been saved in ./id_rsa_ansible.pub\n</code></pre>"},{"location":"hosts-cloud-init-secrets/#using-the-generated-keys","title":"Using the Generated Keys","text":"<p>The public key (id_rsa_ansible.pub) is what you'll add to your cloud-init.yml file. You can view it with:</p> <pre><code>cat ./id_rsa_ansible.pub\n</code></pre> <p>For security, the private key must be set to read-only for the user:</p> <pre><code>chmod 400 ./id_rsa_ansible\n</code></pre> <p>After setting permissions, your key files should appear like this:</p> <pre><code>ls -la\n</code></pre> <pre><code>-r--------  1 user  staff  3401 Jun  7 12:27 id_rsa_ansible\n-rw-r--r--  1 user  staff   755 Jun  7 12:27 id_rsa_ansible.pub\n</code></pre>"},{"location":"hosts-cloud-init-secrets/#using-the-key-to-connect","title":"Using the Key to Connect","text":"<p>To log in to a host with the username ansible using this key:</p> <pre><code>ssh -i ./id_rsa_ansible ansible@&lt;ip or hostname&gt;\n</code></pre>"},{"location":"hosts-multipass-microk8s/","title":"Multipass MicroK8s Host Documentation","text":"<p>File: <code>docs/hosts-multipass-microk8s.md</code> Purpose: LEGACY deployment guide for MicroK8s on Ubuntu VM using Multipass Target Audience: Historical reference - USE RANCHER DESKTOP INSTEAD Last Updated: September 22, 2024</p> <p>\u26a0\ufe0f LEGACY DOCUMENTATION - This setup has been REPLACED BY RANCHER DESKTOP</p> <p>For new installations, use the default Rancher Desktop setup instead. See hosts-rancher-kubernetes.md for current instructions.</p>"},{"location":"hosts-multipass-microk8s/#overview","title":"\ud83d\udccb Overview","text":"<p>This guide covers provisioning Kubernetes (MicroK8s) on Ubuntu running in Multipass. This setup is ideal for testing the system on your local machine before deploying to physical servers or cloud environments.</p>"},{"location":"hosts-multipass-microk8s/#key-features","title":"Key Features","text":"<ul> <li>Local virtualization using Multipass for lightweight VMs</li> <li>Cloud-init automation for consistent VM provisioning</li> <li>MicroK8s installation with core add-ons</li> <li>Bridge networking for local network access</li> <li>Development-focused with configurable resources</li> </ul>"},{"location":"hosts-multipass-microk8s/#use-cases","title":"Use Cases","text":"<ul> <li>Local development and testing</li> <li>GitOps workflow validation</li> <li>Infrastructure experimentation</li> <li>Pre-deployment testing</li> </ul> <p>The host that is created is named: <code>multipass-microk8s</code></p>"},{"location":"hosts-multipass-microk8s/#create-the-vm-named-multipass-microk8s","title":"create the VM named multipass-microk8s","text":"<p>Make sure you are in the folder <code>hosts/multipass-microk8s</code> when you run the script.</p> <p>By default, the script will create a VM with minimal resources that can only be used to verify that GitOps works. If you want to use it for development you must provide parameters that give it more resources. Run it with the command:</p> <p>For development (we use this):</p> <pre><code>cd hosts/multipass-microk8s\n./01-create-multipass-microk8s.sh --cpus 6 --memory \"10G\" --disk \"50G\"\n</code></pre> <p>Just for testing use a smaller VM.:</p> <pre><code>cd hosts/multipass-microk8s\n./01-create-multipass-microk8s.sh\n</code></pre> <p>The VM is now created. To set it up we use the VM named provision-host.</p>"},{"location":"hosts-multipass-microk8s/#register-multipass-microk8s-in-ansible-inventory","title":"Register multipass-microk8s in ansible inventory","text":"<p>For the provision-host VM to be able to reach the multipass-microk8s VM we must set up the ansible inventory file. This is described in the file provision-host/02-register-ansible-inventory.md.</p> <p>(there is now a script that does this) Tasks:</p> <ul> <li>log in to the provision-host VM</li> <li>set up the ansible inventory file</li> <li>do the ansible ping to check that the multipass-microk8s VM is reachable</li> </ul> <p>Then return here for instructions on how to install the kubernetes cluster on the multipass-microk8s VM.</p>"},{"location":"hosts-multipass-microk8s/#install-microk8s-on-vm-multipass-microk8s-running-on-your-host","title":"Install microk8s on VM multipass-microk8s running on your host","text":""},{"location":"hosts-multipass-microk8s/#the-automatic-way-of-installing-microk8s-on-the-multipass-microk8s-vm","title":"The automatic way of installing microk8s on the multipass-microk8s VM","text":"<p>In the folder <code>hosts/multipass-microk8s</code>run the script to install microk8s on the VM multipass-microk8s. A lot of things are done in the script. It will install microk8s, set up the dashboard and set up metallb.</p> <p>The below example will install microk8s on the VM multipass-microk8s and set up metallb to provide IP addresses in a suitable range for the cluster. Replace the IP range with values appropriate for your network.</p> <pre><code>cd hosts/multipass-microk8s\n./03-setup-multipass-microk8s.sh 192.168.x.240-192.168.x.242  # Replace x with your network\n</code></pre> <p>At the bottom you will see.</p> <pre><code>------ Summary of installation statuses for: $0 ------\nTesting connection to multipass-microk8s: OK\nInstalling microk8s: OK\nMerging kubeconfig files: OK\n--------------- All OK ------------------------\nKubernetes Dashboard is accessible on:\n&lt;link displayed here&gt;\n\nClick on the link and paste this token:\n&lt;token displayed here&gt;    \n</code></pre> <p>Click on the URL and you should see the Kubernetes dashboard. Use the token to log in.</p> <p>If it does not work automatically you can do it manually. The manual way is described below.</p>"},{"location":"hosts-multipass-microk8s/#the-manual-way-of-installing-microk8s-on-the-multipass-microk8s-vm","title":"The manual way of installing microk8s on the multipass-microk8s VM","text":"<p>The playbook for installing microk8s is <code>02-install-microk8s.yml</code> is in the folder playbooks. It needs two parameters. The hostname and the IP range that the cluster can provide IP addresses from.</p> <p>For the VM multipass-microk8s the hostname is <code>multipass-microk8s</code>. In order to provide an IP range we must know something about the network that the VM is running on. Running <code>multipass info multipass-microk8s</code> will give us the IP address of the VM. In IPv4 there are two addresses listed. One for NAT and one for the local network. We will use the local network address. That is the second address. For example, it might be <code>192.168.x.65</code> On your local network, identify a range of addresses that are not currently used. For example, if addresses above <code>192.168.x.240</code> are available, you can use the range <code>192.168.x.240-192.168.x.242</code> for the cluster. This gives the cluster three IP addresses to use. If you do this on a corporate network you must check with the network administrator what range you can use.</p> <p>Log in to the provision-host VM and change to the folder <code>/mnt/urbalurbadisk/ansible</code>. Run the command:</p> <pre><code>ansible-playbook playbooks/02-install-microk8s.yml -e target_host=\"multipass-microk8s\" -e metallb_ip_range=\"192.168.x.240-192.168.x.242\"\n</code></pre> <p>The install will also set up the dashboard.</p> <p>The output will show the token and the url to use to log in to the dashboard.</p> <pre><code>TASK [Display the NodePort for Kubernetes Dashboard for all IPs] ****************************************************************\nok: [multipass-microk8s] =&gt; (item=192.168.x.23) =&gt; {\n    \"msg\": \"Kubernetes Dashboard is accessible on: https://192.168.x.23:31630\"\n}\nok: [multipass-microk8s] =&gt; (item=192.168.x.81) =&gt; {\n    \"msg\": \"Kubernetes Dashboard is accessible on: https://192.168.x.81:31630\"\n}\n</code></pre> <p>Two files are created in <code>provision-host</code> folder <code>/mnt/urbalurbadisk/kubeconfig</code> the files are:</p> <ul> <li>multipass-microk8s-dashboardtoken: contains the token for the dashboard</li> <li>multipass-microk8s-kubeconfig: contains the kubeconfig file for the cluster</li> </ul>"},{"location":"hosts-multipass-microk8s/#merge-the-kubeconfig-file-into-one-file-for-all-clusters","title":"Merge the kubeconfig file into one file for all clusters","text":"<p>The kubeconfig file for the cluster is stored in the file <code>multipass-microk8s-kubeconfig</code>. As we are setting up more clusters we need to merge the kubeconfig files into one file. This is done with the playbook <code>04-merge-kubeconf.yml</code>.</p> <pre><code> ansible-playbook playbooks/04-merge-kubeconf.yml\n</code></pre> <p>The playbook merges all *-kubeconfig files in the folder <code>/mnt/urbalurbadisk/kubeconfig</code> into one file <code>/mnt/urbalurbadisk/kubeconfig/kubeconf-all</code>. It then sets the KUBECONFIG environment variable to the file. But in order to use it you must run the command:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>Now you can use the kubectl command to manage the cluster. The commands are described in the file /commands/kubectl-commands.md</p>"},{"location":"hosts-multipass-microk8s/#install-helm","title":"Install Helm","text":"<p>Helm is a package manager for kubernetes. It is used to install applications in the cluster. Helm is installed with the playbook playbooks/install-helm.yml</p> <pre><code>ansible-playbook playbooks/03-install-microk8s-helm.yml\n</code></pre>"},{"location":"hosts-multipass-microk8s/#log-in-to-the-dashboard","title":"Log in to the dashboard","text":"<p>Now we need to test if the cluster is working and we can do that by logging in to the dashboard. Use the url on your host (Mac in my case) computer to log in to the dashboard. As you see there are two IP addresses to use. One for NAT and one that is available on the local network.</p> <p>So just click on the link and use the token that is in the file <code>multipass-microk8s-dashboardtoken</code> to log in.</p> <p>There is nothing in the cluster yet. To see something you can click on \"Nodes\" in the left menu and you should see a graph of CPU and memory usage.</p> <p>We now know that the cluster is working.</p>"},{"location":"hosts-multipass-microk8s/#k9s-a-terminal-based-ui-for-kubernetes","title":"k9s a terminal based UI for kubernetes","text":"<p>k9s is a terminal based UI for kubernetes. It is a great tool to use when you are working in the terminal. When kubectl works you can just start it with the command <code>k9s</code> and you will see a UI that is easy to use. the k9s is installed on the provision-host VM. You can use it from there. </p> <pre><code>k9s\n</code></pre>"},{"location":"hosts-multipass-microk8s/#log-in-to-multipass-microk8s-from-your-computer","title":"Log in to multipass-microk8s from your computer","text":"<p>To use the multipass-microk8s VM we need to log in to it using ssh key that was created in the secrets step.</p> <pre><code>ssh -i ./secrets/id_rsa_ansible ansible@$(multipass info multipass-microk8s | grep IPv4 | awk '{print $2}')\n</code></pre>"},{"location":"hosts-rancher-kubernetes/","title":"Rancher Kubernetes Host Documentation","text":"<p>File: <code>docs/hosts-rancher-kubernetes.md</code> Purpose: Deployment guide for Rancher Desktop Kubernetes cluster setup Target Audience: Developers setting up local Kubernetes development environment Last Updated: September 22, 2024</p>"},{"location":"hosts-rancher-kubernetes/#overview","title":"\ud83d\udccb Overview","text":"<p>Rancher Desktop is the default Kubernetes environment for Urbalurba infrastructure. When you run <code>./install-rancher.sh</code>, it automatically starts both the provision-host container and a Rancher Desktop Kubernetes cluster, providing a complete local development environment.</p> <p>Unlike other host types, Rancher Desktop comes with Kubernetes (k3s) pre-installed, so the setup process focuses on configuration and integration with the Urbalurba infrastructure.</p>"},{"location":"hosts-rancher-kubernetes/#default-setup","title":"Default Setup","text":"<ul> <li>Automatic provisioning - Started automatically with <code>./install-rancher.sh</code></li> <li>Provision-host integration - Container and cluster work together seamlessly</li> <li>Default context - All scripts use <code>rancher-desktop</code> context by default</li> <li>Zero configuration - Works out of the box for development</li> </ul>"},{"location":"hosts-rancher-kubernetes/#key-features","title":"Key Features","text":"<ul> <li>Pre-installed Kubernetes - No manual cluster setup required</li> <li>Local development - Ideal for testing and development workflows</li> <li>Docker integration - Built-in container runtime</li> <li>GUI management - Easy cluster management through Rancher Desktop UI</li> <li>No cloud-init required - Uses Rancher Desktop's built-in provisioning</li> </ul>"},{"location":"hosts-rancher-kubernetes/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>The Rancher Desktop cluster is automatically set up when you start Urbalurba:</p> <pre><code># This single command sets up everything:\n# 1. Provision-host container\n# 2. Rancher Desktop Kubernetes cluster\n# 3. All Urbalurba services\n./install-rancher.sh\n</code></pre> <p>That's it! The provision-host container and Rancher cluster start together, and all services deploy automatically.</p>"},{"location":"hosts-rancher-kubernetes/#prerequisites","title":"\ud83d\udcd6 Prerequisites","text":"<ol> <li>Rancher Desktop Installation</li> <li>Download and install Rancher Desktop from rancherdesktop.io</li> <li>Start Rancher Desktop and enable Kubernetes</li> <li> <p>The <code>./install-rancher.sh</code> script will automatically configure everything else</p> </li> <li> <p>Linux Users: Enable Privileged Ports</p> </li> </ol> <p>On Linux, port 80 and 443 are \"privileged\" and require extra configuration. Without this, <code>http://localhost</code> won't work.</p> <pre><code># Enable port 80/443 access (temporary - until reboot)\nsudo sysctl -w net.ipv4.ip_unprivileged_port_start=80\n\n# Make permanent (survives reboot)\necho \"net.ipv4.ip_unprivileged_port_start=80\" | sudo tee -a /etc/sysctl.conf\n</code></pre> <p>After running this command, restart Rancher Desktop for port forwarding to take effect.</p> <p>See Rancher Desktop Installation docs for more details on \"Traefik Port Binding Access\".</p> <ol> <li>What's Included Automatically</li> <li>kubectl (included with Rancher Desktop)</li> <li>Helm (for package management)</li> <li>Docker (included with Rancher Desktop)</li> <li>Provision-host container with all tools</li> <li>Kubernetes cluster configuration</li> <li>All Urbalurba services</li> </ol>"},{"location":"hosts-rancher-kubernetes/#installation-process","title":"\ud83d\udd27 Installation Process","text":"<p>The installation script (<code>install-rancher-kubernetes.sh</code>) runs automatically inside the provision-host container when you execute <code>./install-rancher.sh</code>. This script performs these steps:</p> <ol> <li>Verify Kubernetes Cluster - Ensures Rancher Desktop Kubernetes is running</li> <li>Apply Secrets - Configures necessary Kubernetes secrets</li> <li>Setup Storage - Configures local storage classes</li> <li>Configure Networking - Sets up ingress and networking components</li> </ol> <p>\u2139\ufe0f Automatic Execution: This script is called automatically by the main installation process. You typically don't need to run it manually unless troubleshooting specific issues.</p>"},{"location":"hosts-rancher-kubernetes/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"hosts-rancher-kubernetes/#cluster-configuration","title":"Cluster Configuration","text":"<ul> <li>Single-node cluster - All components run on local machine</li> <li>Storage - Local path provisioner for persistent volumes</li> <li>Networking - Traefik ingress controller with localhost access</li> <li>Container Runtime - Docker (default) or containerd</li> <li>Browser Access - All services accessible via <code>http://localhost</code> URLs</li> </ul>"},{"location":"hosts-rancher-kubernetes/#integration-points","title":"Integration Points","text":"<ul> <li>Local development - Seamless integration with local IDE and tools</li> <li>Port forwarding - Easy access to services via localhost</li> <li>Volume mounts - Direct access to local filesystem</li> <li>Resource management - Configurable CPU and memory limits</li> </ul>"},{"location":"hosts-rancher-kubernetes/#configuration","title":"\ud83d\udee0\ufe0f Configuration","text":""},{"location":"hosts-rancher-kubernetes/#rancher-desktop-settings","title":"Rancher Desktop Settings","text":"<pre><code># Recommended Rancher Desktop configuration\nkubernetes:\n  enabled: true\n  version: \"v1.28.x\"  # Latest stable\n\ncontainer:\n  runtime: docker  # or containerd\n\nresources:\n  memory: 8GB      # Adjust based on your system\n  cpus: 4          # Adjust based on your system\n</code></pre>"},{"location":"hosts-rancher-kubernetes/#storage-configuration","title":"Storage Configuration","text":"<ul> <li>Default storage class - <code>local-path</code> (compatible with Urbalurba manifests)</li> <li>Persistent volumes - Stored in local directories</li> <li>Volume size - Limited by available disk space</li> </ul>"},{"location":"hosts-rancher-kubernetes/#service-access","title":"Service Access","text":"<p>Browser Access (Primary Method) All Urbalurba services are automatically configured for browser access:</p> <pre><code># Services are accessible directly in your browser at:\nhttp://&lt;service&gt;.localhost\n\n# Examples of common services:\nhttp://whoami.localhost          # Whoami test service\nhttp://grafana.localhost         # Grafana monitoring\nhttp://pgadmin.localhost         # PostgreSQL admin\nhttp://openwebui.localhost       # OpenWebUI AI interface\n</code></pre>"},{"location":"hosts-rancher-kubernetes/#complete-reset-factory-reset","title":"Complete Reset (Factory Reset)","text":"<p>If you need to completely reset the Urbalurba infrastructure, you can perform a factory reset in Rancher Desktop:</p> <p>\u26a0\ufe0f Warning: All data, configurations, and certificates will be permanently lost. If you have multiple clusters configured, the kubeconfig file that provides access to them will be deleted. Use <code>kubeconf-copy2local.sh</code> to backup this file before proceeding if you have multiple clusters.</p> <ol> <li>Open Rancher Desktop application</li> <li>Go to Troubleshooting \u2192 Factory Reset</li> <li>Confirm the reset - This will delete all data and configurations</li> <li>Restart Rancher Desktop and enable Kubernetes</li> <li>Redeploy by running <code>./install-rancher.sh</code></li> </ol> <p>Warning: Factory reset will permanently delete all deployed services, persistent volumes, and configurations. Make sure to backup any important data before proceeding.</p>"},{"location":"hosts-rancher-kubernetes/#performance-optimization","title":"\ud83d\udcc8 Performance Optimization","text":""},{"location":"hosts-rancher-kubernetes/#resource-allocation","title":"Resource Allocation","text":"<ul> <li>Memory - Allocate at least 8GB for full Urbalurba stack</li> <li>CPU - 4+ cores recommended for good performance</li> <li>Disk - Ensure sufficient space for persistent volumes</li> </ul>"},{"location":"hosts-rancher-kubernetes/#development-workflow","title":"Development Workflow","text":"<ul> <li>Hot reloading - Use volume mounts for development</li> <li>Service mesh - Optional for advanced testing scenarios</li> <li>Monitoring - Use kubectl top or Rancher Desktop metrics</li> </ul>"},{"location":"hosts-rancher-kubernetes/#integration-with-urbalurba","title":"\ud83d\udd17 Integration with Urbalurba","text":""},{"location":"hosts-rancher-kubernetes/#service-deployment","title":"Service Deployment","text":"<pre><code># Deploy all Urbalurba services\ncd /mnt/urbalurbadisk/provision-host/kubernetes\n./provision-kubernetes.sh rancher-desktop\n\n# Deploy specific services\nkubectl apply -f manifests/&lt;service-manifest&gt;.yaml\n</code></pre>"},{"location":"hosts-rancher-kubernetes/#context-switching","title":"Context Switching","text":"<pre><code># Switch between different Kubernetes contexts\nkubectl config use-context rancher-desktop  # Local development\nkubectl config use-context azure-aks       # Cloud production\n\n# Verify current context\nkubectl config current-context\n</code></pre>"},{"location":"hosts-rancher-kubernetes/#related-documentation","title":"\ud83d\udcd6 Related Documentation","text":"<ul> <li>hosts-readme.md - Main hosts overview</li> <li>CLAUDE.md - Repository instructions</li> <li>install-rancher-desktop-mac.md - Rancher Desktop installation guide</li> <li>rules-git-workflow.md - Development workflow standards</li> </ul>"},{"location":"hosts-rancher-kubernetes/#notes","title":"\ud83d\udcdd Notes","text":"<ul> <li>No cloud-init required - Rancher Desktop handles all provisioning</li> <li>GUI management - Use Rancher Desktop application for cluster management</li> <li>Local only - This setup is for development and testing, not production</li> <li>Resource limits - Performance depends on your local machine specifications</li> </ul>"},{"location":"hosts-raspberry-microk8s/","title":"Raspberry Pi MicroK8s Host Documentation","text":"<p>File: <code>docs/hosts-raspberry-microk8s.md</code> Purpose: Deployment guide for MicroK8s on Raspberry Pi with Tailscale integration Target Audience: Edge computing enthusiasts and IoT developers Last Updated: September 22, 2024</p>"},{"location":"hosts-raspberry-microk8s/#overview","title":"\ud83d\udccb Overview","text":"<p>This guide covers setting up a Raspberry Pi with Tailscale and MicroK8s for edge computing scenarios. The Raspberry Pi provides a low-power, ARM-based Kubernetes environment suitable for IoT applications and distributed computing.</p>"},{"location":"hosts-raspberry-microk8s/#key-features","title":"Key Features","text":"<ul> <li>ARM architecture optimized for Raspberry Pi hardware</li> <li>Low power consumption ideal for edge deployments</li> <li>Tailscale VPN integration for secure remote management</li> <li>Cloud-init automation for consistent setup</li> <li>Edge computing capabilities for distributed workloads</li> </ul>"},{"location":"hosts-raspberry-microk8s/#prerequisites","title":"Prerequisites","text":"<ul> <li>Raspberry Pi 4 (8GB+ RAM recommended)</li> <li>MicroSD card (32GB+ recommended)</li> <li>Network connectivity (WiFi or Ethernet)</li> <li>Tailscale account for VPN access</li> </ul>"},{"location":"hosts-raspberry-microk8s/#setup-methods","title":"\ud83d\ude80 Setup Methods","text":""},{"location":"hosts-raspberry-microk8s/#automatic-setup","title":"Automatic Setup","text":"<p>TODO: Describe automatic setup process when available</p>"},{"location":"hosts-raspberry-microk8s/#manual-setup","title":"Manual Setup","text":"<p>This manual process assumes: - Raspberry Pi is set up with Ubuntu - Connected to Tailscale network - User named <code>ansible</code> is configured</p> <p>Step 1: Provide the information needed to set up the raspberry in the ansible inventory.</p> <p>In order to set up using ansible you need to add the raspberry to the ansible inventory.  The information we need is stored in the file raspberry-microk8s.sh in the raspberry-microk8s folder. This file should be created by the automatic setup, but we do it manually here.</p> <pre><code>cat &lt;&lt; EOF &gt; raspberry-microk8s.sh\n#!/bin/bash\nfilename: raspberry-microk8s.sh\ndescription: manually created created info about the raspberry\nTAILSCALE_IP=100.xxx.xxx.xxx  # Replace with your actual Tailscale IP\nCLUSTER_NAME=raspberry-microk8s\nHOST_NAME=raspberry-microk8s\nEOF\n\nchmod +x raspberry-microk8s.sh\n</code></pre> <p>Step 2: Add the raspberry to the ansible inventory.</p> <pre><code>./02-raspberry-ansible-inventory.sh \n</code></pre> <p>TODO: finish the rasperry setup (whare is my old notes?)</p>"},{"location":"hosts-readme/","title":"Hosts Documentation","text":"<p>File: <code>docs/hosts-readme.md</code> Purpose: Comprehensive guide to Urbalurba infrastructure host types and deployment strategies Target Audience: Infrastructure engineers and developers deploying Urbalurba Last Updated: September 22, 2024</p>"},{"location":"hosts-readme/#overview","title":"\ud83d\udccb Overview","text":"<p>The Urbalurba infrastructure is designed to be highly flexible and can be deployed across various environments and hardware configurations. This document provides a standardized overview of all supported host types and their deployment strategies.</p>"},{"location":"hosts-readme/#hardware-support","title":"Hardware Support","text":"<ul> <li>Raspberry Pi (ARM architecture) - Edge computing and development</li> <li>Standard x86/AMD64 servers - On-premises and bare metal</li> <li>Cloud virtual machines - Azure, AWS, GCP</li> <li>Local virtualization - Multipass, Rancher Desktop</li> </ul> <p>The software and programs in the system work consistently across all host types.</p>"},{"location":"hosts-readme/#host-provisioning-strategy","title":"\ud83d\ude80 Host Provisioning Strategy","text":"<p>Ubuntu-based hosts (Azure MicroK8s, Multipass, Raspberry Pi) use cloud-init for automated provisioning, while managed services (Azure AKS, Rancher Desktop) use their own provisioning mechanisms. For detailed information about cloud-init configuration and templates, see hosts-cloud-init-readme.md.</p>"},{"location":"hosts-readme/#host-types","title":"\ud83c\udfd7\ufe0f Host Types","text":"<p>The system supports several types of host configurations:</p>"},{"location":"hosts-readme/#1-rancher-kubernetes-hosts","title":"1. Rancher Kubernetes Hosts","text":"<p>Documentation: hosts-rancher-kubernetes.md | Scripts: <code>hosts/rancher-kubernetes/</code> - Deploys Rancher-managed Kubernetes clusters - Default local development environment - Supports multi-node clusters - Includes Rancher-specific configurations - Provides cluster management interface - No cloud-init required (uses Rancher Desktop)</p>"},{"location":"hosts-readme/#2-azure-aks-hosts","title":"2. Azure AKS Hosts","text":"<p>Documentation: hosts-azure-aks.md | Scripts: <code>hosts/azure-aks/</code> - Managed Kubernetes service on Azure - Production-ready with Azure integration - Cost management and scaling features - No cloud-init required (managed service)</p>"},{"location":"hosts-readme/#3-azure-microk8s-hosts","title":"3. Azure MicroK8s Hosts","text":"<p>Documentation: hosts-azure-microk8s.md | Scripts: <code>hosts/azure-microk8s/</code> - Deploys MicroK8s on Azure VMs - Uses Azure-specific cloud-init configuration \u2705 - Supports automatic scaling - Integrates with Azure services - Includes Tailscale VPN for secure access</p>"},{"location":"hosts-readme/#4-raspberry-pi-microk8s-hosts","title":"4. Raspberry Pi MicroK8s Hosts","text":"<p>Documentation: hosts-raspberry-microk8s.md | Scripts: <code>hosts/raspberry-microk8s/</code> - Uses Raspberry Pi-specific cloud-init configuration \u2705 - Optimized for ARM architecture - Resource-efficient configuration - Edge computing support - Low-power operation - Includes WiFi configuration</p>"},{"location":"hosts-readme/#5-multipass-microk8s-hosts-legacy","title":"5. Multipass MicroK8s Hosts (LEGACY)","text":"<p>Documentation: hosts-multipass-microk8s.md | Scripts: <code>hosts/multipass-microk8s/</code> - REPLACED BY RANCHER DESKTOP - Kept for historical reference - Deploys MicroK8s using Multipass - Uses Multipass-specific cloud-init configuration \u2705 - Previously used for local development - Lightweight virtualization</p>"},{"location":"hosts-readme/#host-setup-commands","title":"\ud83d\ude80 Host Setup Commands","text":"<p>Each host type provides Kubernetes cluster setup commands. These are run from the provision-host container to prepare different types of Kubernetes clusters.</p> <p>To set up clusters (and machines that run clusters) log in to <code>provision-host</code> container. Then change working directory to </p> <pre><code>cd /mnt/urbalurbadisk/hosts\n</code></pre>"},{"location":"hosts-readme/#to-set-up-azure-aks","title":"To set up Azure AKS","text":"<p>Read documentation: hosts-azure-aks.md</p> <pre><code>./install-azure-aks.sh\n</code></pre>"},{"location":"hosts-readme/#to-set-up-a-vm-in-azure-and-then-prepare-microk8s-kubernetes-on-it","title":"To set up a VM in Azure and then prepare microk8s kubernetes on it","text":"<p>Read documentation: hosts-azure-microk8s.md</p> <pre><code>./install-azure-microk8s-v2.sh\n</code></pre>"},{"location":"hosts-readme/#to-set-up-ubuntu-on-a-raspberry-pi-and-then-prepare-microk8s-kubernetes-on-it","title":"To set up Ubuntu on a Raspberry Pi and then prepare microk8s kubernetes on it","text":"<p>Raspberry Pi (manual setup required)</p> <p>See documentation: hosts-raspberry-microk8s.md</p>"},{"location":"hosts-readme/#multipass-microk8s-legacy-replaced-by-rancher-desktop","title":"Multipass MicroK8s (LEGACY - replaced by Rancher Desktop)","text":"<p>See documentation: hosts-multipass-microk8s.md</p>"},{"location":"hosts-readme/#after-cluster-setup-deploy-all-services","title":"After Cluster Setup: Deploy All Services","text":"<p>The benefit of Kubernetes is that once you have a cluster running, the application deployment process is identical across all cluster types.</p> <p>Once your Kubernetes cluster is ready, deploy all Urbalurba services:</p> <pre><code># From inside provision-host container:\ncd /mnt/urbalurbadisk/provision-host/kubernetes\n./provision-kubernetes.sh &lt;cluster-context&gt;\n\n# Examples:\n./provision-kubernetes.sh rancher-desktop    # For Rancher Desktop\n./provision-kubernetes.sh azure-aks          # For Azure AKS\n./provision-kubernetes.sh multipass-microk8s # For Multipass (legacy)\n</code></pre> <p>This script automatically installs all services in the correct order (core systems, databases, AI services, monitoring, etc.).</p>"},{"location":"hosts-readme/#multi-cluster-management","title":"\ud83d\udd04 Multi-Cluster Management","text":"<p>When you set up multiple Kubernetes clusters, Urbalurba automatically merges their kubeconfig files for seamless context switching:</p>"},{"location":"hosts-readme/#automatic-kubeconfig-merging","title":"Automatic Kubeconfig Merging","text":"<p>Each time a new cluster is added, the system runs: <pre><code># From inside provision-host container:\nansible-playbook ansible/playbooks/04-merge-kubeconf.yml\n</code></pre></p> <p>This merges all <code>*-kubeconfig</code> files from <code>/mnt/urbalurbadisk/kubeconfig/</code> into a single file: <code>/mnt/urbalurbadisk/kubeconfig/kubeconf-all</code></p>"},{"location":"hosts-readme/#context-switching-between-clusters","title":"Context Switching Between Clusters","text":"<p>Once merged, you can easily switch between different Kubernetes environments:</p> <pre><code># Set the merged kubeconfig\nexport KUBECONFIG=/mnt/urbalurbadisk/kubeconfig/kubeconf-all\n\n# Switch between clusters\nkubectl config use-context rancher-desktop  # Local development\nkubectl config use-context azure-aks       # Cloud production\nkubectl config use-context azure-microk8s  # Azure VM\nkubectl config use-context multipass-microk8s # Legacy multipass\n\n# Check current context\nkubectl config current-context\n\n# List all available contexts\nkubectl config get-contexts\n</code></pre>"},{"location":"hosts-readme/#benefits-of-multi-cluster-setup","title":"Benefits of Multi-Cluster Setup","text":"<ul> <li>Development \u2192 Production workflow - Test locally, deploy to cloud</li> <li>Cross-cloud redundancy - Multiple cloud providers</li> <li>Environment isolation - Separate dev/staging/prod clusters</li> <li>Unified management - One kubectl interface for all clusters</li> </ul>"},{"location":"hosts-readme/#detailed-documentation","title":"\ud83d\udcda Detailed Documentation","text":"<p>For comprehensive setup guides, troubleshooting, and configuration details:</p> <ul> <li>hosts-cloud-init-readme.md - Cloud-init configuration and templates</li> <li>hosts-azure-microk8s.md - Azure MicroK8s deployment guide</li> <li>hosts-azure-aks.md - Azure AKS deployment guide</li> <li>hosts-multipass-microk8s.md - Multipass MicroK8s deployment guide</li> <li>hosts-raspberry-microk8s.md - Raspberry Pi MicroK8s deployment guide</li> <li>hosts-rancher-kubernetes.md - Rancher Kubernetes deployment guide</li> </ul>"},{"location":"manifests-readme/","title":"Kubernetes Manifests","text":"<p>This folder contains the Kubernetes manifests for the applications and services that are deployed to the infrastructure.</p>"},{"location":"manifests-readme/#manifest-organization","title":"Manifest Organization","text":"<p>Manifests are organized by deployment order and functionality:</p>"},{"location":"manifests-readme/#000-099-core-infrastructure","title":"000-099: Core Infrastructure","text":"<ul> <li><code>000-storage-class-alias.yaml</code> - Storage class configuration</li> <li><code>001-002</code> - Storage testing manifests</li> <li><code>010-012</code> - Tailscale and Traefik configuration</li> <li><code>020</code> - Nginx configuration and ingress</li> <li><code>030-039</code> - Observability stack (Grafana, Loki, Tempo, Prometheus, OpenTelemetry)</li> </ul>"},{"location":"manifests-readme/#040-099-data-and-messaging","title":"040-099: Data and Messaging","text":"<ul> <li><code>040-044</code> - Database configurations (MongoDB, PostgreSQL, MySQL, Qdrant)</li> <li><code>050</code> - Redis configuration</li> <li><code>060</code> - Elasticsearch configuration</li> <li><code>070-071</code> - Authentik identity provider and SSO</li> <li><code>080</code> - RabbitMQ configuration</li> <li><code>090</code> - Gravitee API management</li> </ul>"},{"location":"manifests-readme/#200-299-ai-and-development","title":"200-299: AI and Development","text":"<ul> <li><code>200-201</code> - AI persistent storage and Open WebUI</li> <li><code>205</code> - Ollama configuration</li> <li><code>208-210</code> - Open WebUI configuration and ingress</li> <li><code>220-221</code> - ArgoCD and LiteLLM</li> <li><code>230</code> - Prometheus stack</li> </ul>"},{"location":"manifests-readme/#300-399-data-science","title":"300-399: Data Science","text":"<ul> <li><code>300</code> - Apache Spark configuration</li> <li><code>310-311</code> - JupyterHub configuration and ingress</li> <li><code>320-321</code> - Unity Catalog configuration and ingress</li> </ul>"},{"location":"manifests-readme/#600-699-administration","title":"600-699: Administration","text":"<ul> <li><code>640-641</code> - pgAdmin configuration</li> <li><code>740</code> - pgAdmin ingress</li> </ul>"},{"location":"manifests-readme/#700-799-networking","title":"700-799: Networking","text":"<ul> <li><code>751-752</code> - Cloudflare tunnel configuration</li> <li><code>net2-*</code> - Tailscale cluster networking</li> </ul>"},{"location":"manifests-readme/#recent-additions","title":"Recent Additions","text":""},{"location":"manifests-readme/#authentik-identity-provider-070-071","title":"Authentik Identity Provider (070-071)","text":"<ul> <li><code>070-authentik-config.yaml</code> - Helm values for Authentik deployment</li> <li><code>071-authentik-ingress.yaml</code> - Ingress configuration for external access</li> </ul> <p>Features: - Single Sign-On (SSO) via OpenID Connect and SAML - User management and directory services - Multi-factor authentication (MFA) - Application integration and provisioning - Audit logging and compliance features</p> <p>Usage: <pre><code># Deploy Authentik\nhelm upgrade --install authentik authentik/authentik -f manifests/070-authentik-config.yaml -n authentik\n\n# Apply ingress\nkubectl apply -f manifests/071-authentik-ingress.yaml\n</code></pre></p>"},{"location":"manifests-readme/#manifest-patterns","title":"Manifest Patterns","text":""},{"location":"manifests-readme/#configuration-files","title":"Configuration Files","text":"<ul> <li>Use numbered sequencing for deployment order</li> <li>Include comprehensive documentation headers</li> <li>Reference secrets from <code>urbalurba-secrets</code> Kubernetes secret</li> <li>Separate configuration from ingress where possible</li> </ul>"},{"location":"manifests-readme/#ingress-files","title":"Ingress Files","text":"<ul> <li>Follow the pattern: <code>XXX-service-ingress.yaml</code></li> <li>Use Traefik ingress controller by default</li> <li>Configure for localhost testing initially</li> <li>Include TLS configuration for production use</li> </ul>"},{"location":"manifests-readme/#secret-management","title":"Secret Management","text":"<ul> <li>All sensitive values reference <code>urbalurba-secrets</code> secret</li> <li>Secrets are managed centrally in the <code>topsecret/kubernetes/</code> directory</li> <li>Use the <code>update-kubernetes-secrets-v2.sh</code> script for deployment</li> </ul>"},{"location":"manifests-readme/#deployment-workflow","title":"Deployment Workflow","text":"<ol> <li>Update secrets using the automated script</li> <li>Deploy configuration with Helm or kubectl</li> <li>Apply ingress for external access</li> <li>Verify deployment and functionality</li> </ol>"},{"location":"manifests-readme/#best-practices","title":"Best Practices","text":"<ul> <li>Always check the documentation header in each manifest</li> <li>Use consistent naming conventions</li> <li>Test manifests with <code>--dry-run=client</code> before applying</li> <li>Keep configuration and ingress manifests separate</li> <li>Document all environment-specific values</li> </ul>"},{"location":"networking-cloudflare-setup/","title":"Cloudflare Tunnel Setup Guide","text":"<p>Purpose: Professional internet access with custom domains Audience: Users wanting production-ready setup with own domains Time Required: 15-20 minutes Prerequisites: Working cluster with Traefik ingress</p>"},{"location":"networking-cloudflare-setup/#quick-summary","title":"\ud83d\ude80 Quick Summary","text":"<p>Transform your local cluster from <code>http://service.localhost</code> to <code>https://service.yourcompany.com</code> with enterprise-grade security. Uses your Cloudflare-managed domain to provide global CDN, DDoS protection, and professional appearance.</p>"},{"location":"networking-cloudflare-setup/#prerequisites","title":"\u2705 Prerequisites","text":"<p>Before starting, ensure you have: - [ ] Kubernetes cluster running (Rancher Desktop or similar) - [ ] Traefik ingress controller deployed - [ ] Services accessible locally (e.g., <code>http://whoami.localhost</code>) - [ ] Domain already added to Cloudflare (e.g., <code>urbalurba.no</code>) - [ ] Access to provision-host container - [ ] Logged into Cloudflare dashboard before running setup</p> <p>\u26a0\ufe0f CRITICAL: You MUST be logged into dash.cloudflare.com before running the setup script!</p>"},{"location":"networking-cloudflare-setup/#how-cloudflare-tunnel-works","title":"\ud83d\udd04 How Cloudflare Tunnel Works","text":"<p>The Cloudflare tunnel creates a secure outbound connection from your cluster to Cloudflare's edge:</p> <pre><code>Internet \u2192 Cloudflare Edge \u2192 Tunnel \u2192 Traefik \u2192 Your Services\n</code></pre> <p>Key Benefits: - No port forwarding or firewall configuration needed - Automatic SSL/TLS certificates - DDoS protection and global CDN - Works behind NAT/firewalls</p>"},{"location":"networking-cloudflare-setup/#script-overview","title":"\ud83d\udccb Script Overview","text":"<p>Three scripts manage the complete tunnel lifecycle:</p> Script Purpose When to Use Parameters <code>820-cloudflare-tunnel-setup.sh</code> Creates tunnel &amp; configures DNS First time setup <code>&lt;domain&gt;</code> required <code>821-cloudflare-tunnel-deploy.sh</code> Deploys tunnel to Kubernetes After setup or updates None (auto-detects) <code>822-cloudflare-tunnel-delete.sh</code> Removes tunnel completely Clean up / start over None (auto-detects)"},{"location":"networking-cloudflare-setup/#quick-start-guide","title":"\ud83d\ude80 Quick Start Guide","text":""},{"location":"networking-cloudflare-setup/#step-1-create-tunnel-and-configure-dns","title":"Step 1: Create Tunnel and Configure DNS","text":"<pre><code># Inside provision-host container\ndocker exec -it provision-host bash\ncd /mnt/urbalurbadisk\n\n# Create tunnel (interactive - will open browser for auth)\n./networking/cloudflare/820-cloudflare-tunnel-setup.sh urbalurba.no\n</code></pre> <p>What happens: - Checks if tunnel already exists (smart detection) - Opens browser for Cloudflare authentication (see authentication steps below) - Creates tunnel with unique credentials - Configures DNS: <code>urbalurba.no</code> AND <code>*.urbalurba.no</code> \u2192 tunnel - Stores credentials for persistence - Updates <code>kubernetes-secrets.yml</code> with tunnel credentials</p>"},{"location":"networking-cloudflare-setup/#browser-authentication-process","title":"Browser Authentication Process","text":"<p>When the script runs, you'll need to complete a 2-step browser authentication:</p> <p>Step 1: Select Domain Zone - A browser URL will appear in the terminal - click or copy it to your browser - You'll see \"Authorize Cloudflare Tunnel\" page with all your domains - ACTION: Click on the row for your specific domain (e.g., urbalurba.no) - All domains should show \"Active\" status with green checkmarks</p> <p>Step 2: Authorize Tunnel Creation - You'll see a confirmation dialog: \"Authorize Tunnel for [your-domain]\" - Message: \"To finish configuring Tunnel for your zone, click Authorize below\" - ACTION: Click the blue \"Authorize\" button (NOT \"Cancel\")</p> <p>Step 3: Success Confirmation - You'll see a \"Success\" page - Message: \"Cloudflared has installed a certificate allowing your origin to create a Tunnel on this zone\" - ACTION: Close the browser window and return to the terminal</p> <p>\u26a0\ufe0f Important: You must complete BOTH browser steps (select domain AND authorize) or you'll get \"Unauthorized\" errors. The authentication link has a timeout, so complete it quickly.</p>"},{"location":"networking-cloudflare-setup/#step-2-deploy-tunnel-to-kubernetes","title":"Step 2: Deploy Tunnel to Kubernetes","text":"<pre><code># Deploy to current cluster (no parameters needed)\n./networking/cloudflare/821-cloudflare-tunnel-deploy.sh\n</code></pre> <p>What happens: - Creates Kubernetes secret with credentials - Deploys tunnel connector pod - Routes traffic to Traefik ingress - Establishes connection to Cloudflare edge</p>"},{"location":"networking-cloudflare-setup/#step-3-root-domain-configuration-automatic","title":"Step 3: Root Domain Configuration (Automatic)","text":"<p>The setup script now automatically configures both: - Root domain: <code>urbalurba.no</code> \u2192 tunnel - Wildcard subdomains: <code>*.urbalurba.no</code> \u2192 tunnel</p> <p>No manual configuration needed! Both domains are set up automatically during the tunnel creation process.</p> <p>Note: If you have Cloudflare Workers intercepting the root domain, you may need to:    - Check Workers &amp; Pages \u2192 Remove any custom domains    - Check Workers Routes \u2192 Delete routes for your domain</p>"},{"location":"networking-cloudflare-setup/#step-4-verify-setup","title":"Step 4: Verify Setup","text":"<pre><code># Test both root domain and subdomain routing\ncurl https://urbalurba.no\ncurl https://test.urbalurba.no\ncurl https://whoami.urbalurba.no\ncurl https://openwebui.urbalurba.no\n</code></pre> <p>Both root domain and subdomains should work automatically!</p> <p>\u26a0\ufe0f Authentication Note: If you want to protect services with Authentik authentication on external domains, see <code>docs/rules-ingress-traefik.md</code> section \"External Domain Authentication Limitations\" for important manual configuration requirements.</p>"},{"location":"networking-cloudflare-setup/#complete-cleanup","title":"\ud83d\uddd1\ufe0f Complete Cleanup","text":"<p>To completely remove a tunnel and start over:</p> <pre><code># Delete everything (no parameters needed)\n./networking/cloudflare/822-cloudflare-tunnel-delete.sh\n</code></pre> <p>What gets deleted: - Kubernetes deployment, configmap, and secrets - Cloudflare tunnel - Local configuration files - TODO: Cloudflare DNS routes (you must do it manually ) - TODO: Cloudflare API tokens (you must do it manually )</p>"},{"location":"networking-cloudflare-setup/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"networking-cloudflare-setup/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"Problem Cause Solution \"Worker is Running!\" on root domain Cloudflare Worker intercepting traffic Check Workers &amp; Pages for custom domains, remove Worker routes 502 Bad Gateway Tunnel can't reach service Verify Traefik is running, check tunnel logs DNS_PROBE_FINISHED_NXDOMAIN Missing DNS record Run setup script or manually add CNAME in Cloudflare DNS Tunnel pod not starting Missing credentials Re-run setup script to generate credentials Certificate error during setup Not logged into Cloudflare Login to dash.cloudflare.com first \"Cannot have more than 50 tokens\" Too many API tokens created Clean up unused tokens (see below) REST API unauthorized errors Incomplete browser authentication Complete BOTH steps: select domain AND click Authorize Authentication timeout Took too long to complete browser steps Run script again for fresh link, complete quickly Wrong domain selected Multiple domains in account Ensure you click the correct domain row that matches script parameter Permission denied errors File ownership issues Script now automatically fixes ownership using <code>docker exec -u root</code> \"Unauthorized: Failed to get tunnel\" Credentials mismatch Script now properly updates both Kubernetes secret and ConfigMap"},{"location":"networking-cloudflare-setup/#cleaning-up-api-tokens-50-token-limit","title":"Cleaning Up API Tokens (50 Token Limit)","text":"<p>Cloudflare has a limit of 50 API tokens per account. Each tunnel creation attempt generates a new token, so repeated testing can hit this limit.</p> <p>To clean up unused tokens: 1. Go to https://dash.cloudflare.com/profile/api-tokens 2. Look for tokens with names like:    - <code>cloudflared-*</code> (from tunnel creation attempts)    - Old/duplicate tokens from testing    - Tokens you no longer use 3. Click the \"Delete\" button (trash icon) next to each unused token 4. Confirm the deletion</p> <p>Prevention tips: - Use <code>822-cloudflare-tunnel-delete.sh</code> to properly clean up tunnels - Avoid repeatedly running setup without proper cleanup - Delete test tunnels when done testing</p>"},{"location":"networking-cloudflare-setup/#checking-tunnel-status","title":"Checking Tunnel Status","text":"<pre><code># View tunnel pod status\nkubectl get pods -n default -l app=cloudflared\n\n# Check tunnel logs\nkubectl logs -n default -l app=cloudflared --tail=50\n\n# Verify DNS records in Cloudflare Dashboard\n# DNS \u2192 Records \u2192 Look for CNAME entries pointing to .cfargotunnel.com\n</code></pre>"},{"location":"networking-cloudflare-setup/#important-paths","title":"Important Paths","text":"File Path Purpose Certificate <code>/mnt/urbalurbadisk/cloudflare/cloudflare-certificate.pem</code> Global Cloudflare auth (created during browser auth) Credentials <code>/mnt/urbalurbadisk/cloudflare/cloudflare-tunnel.json</code> Tunnel-specific secrets (encrypted) Config <code>/mnt/urbalurbadisk/cloudflare/cloudflare-tunnel-config.yml</code> Tunnel configuration Manifest <code>/mnt/urbalurbadisk/manifests/cloudflare-tunnel-manifest.yaml</code> K8s deployment <p>\u26a0\ufe0f Security Note: Never share the certificate or credential files. They provide access to your Cloudflare account and tunnel.</p>"},{"location":"networking-cloudflare-setup/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"networking-cloudflare-setup/#traffic-flow","title":"Traffic Flow","text":"<pre><code>User Request \u2192 Cloudflare Edge \u2192 Tunnel Pod \u2192 Traefik \u2192 Service\n</code></pre>"},{"location":"networking-cloudflare-setup/#components","title":"Components","text":"<ul> <li>Cloudflare Edge: Global CDN and security layer</li> <li>Tunnel Connector: Pod running <code>cloudflared</code> in your cluster</li> <li>Traefik: Ingress controller routing to services</li> <li>Services: Your applications with IngressRoute definitions</li> </ul>"},{"location":"networking-cloudflare-setup/#dns-configuration","title":"DNS Configuration","text":"<ul> <li>Root domain: <code>urbalurba.no</code> \u2192 tunnel (automatically configured)</li> <li>Wildcard: <code>*.urbalurba.no</code> \u2192 All subdomains route to tunnel</li> <li>Proxied: Orange cloud enabled for CDN and security</li> </ul>"},{"location":"networking-cloudflare-setup/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Domain setup: Adding a domain to Cloudflare</li> <li>Tunnel docs: Cloudflare Tunnel documentation</li> <li>Traefik integration: IngressRoute configuration</li> </ul>"},{"location":"networking-readme/","title":"Networking Overview","text":"<p>Status: \ud83c\udf10 Dual-Tunnel Internet Access Architecture Updated: September 8, 2025 Architecture: HostRegexp routing with Tailscale + Cloudflare support</p>"},{"location":"networking-readme/#quick-start-connect-your-cluster-to-the-internet","title":"\ud83d\ude80 Quick Start - Connect Your Cluster to the Internet","text":"<p>Your cluster works perfectly on <code>.localhost</code> domains for development. Ready to connect to the internet?</p> <p>You have two options - both are supported by your existing manifests:</p>"},{"location":"networking-readme/#option-a-tailscale-funnel-quick-free","title":"Option A: Tailscale Funnel \ud83d\udd35 (Quick &amp; Free)","text":"<ul> <li>Get online in: 15 minutes</li> <li>Cost: Free </li> <li>URLs: <code>https://whoami.your-device.ts.net</code></li> <li>Perfect for: Personal projects, demos, team development</li> </ul>"},{"location":"networking-readme/#option-b-cloudflare-tunnel-professional","title":"Option B: Cloudflare Tunnel \u26a1 (Professional)","text":"<ul> <li>Get online in: 45 minutes</li> <li>Cost: ~$10-15/year (domain)</li> <li>URLs: <code>https://whoami.your-domain.com</code></li> <li>Perfect for: Business sites, production apps, custom branding</li> </ul> <p>\ud83d\udc49 Jump to tunnel selection guide</p>"},{"location":"networking-readme/#introduction","title":"Introduction","text":"<p>This document provides a complete guide to: * Internet access options - Choose between Tailscale Funnel and Cloudflare Tunnel * Architecture overview - How the dual-tunnel system works * Setup guidance - Get your services online quickly * DevOps access - Secure administration with Tailscale VPN</p>"},{"location":"networking-readme/#internet-access-options-tailscale-vs-cloudflare","title":"\ud83c\udf10 Internet Access Options: Tailscale vs Cloudflare","text":""},{"location":"networking-readme/#your-current-status","title":"Your Current Status \u2705","text":"<p>You have successfully deployed your cluster and everything works on development domains: - \u2705 <code>http://whoami.localhost</code> - Working great - \u2705 <code>http://openwebui.localhost</code> - AI chat accessible - \u2705 <code>http://authentik.localhost</code> - Authentication ready - \u2705 All services running smoothly</p> <p>Next Step: Connect to the internet so others can access your services.</p>"},{"location":"networking-readme/#which-option-should-you-choose","title":"\ud83e\udd14 Which Option Should You Choose?","text":""},{"location":"networking-readme/#choose-tailscale-funnel-if-you-want","title":"Choose Tailscale Funnel if you want:","text":"<p>\u2705 Quick &amp; Free Setup - No domain purchase required - Automatic HTTPS certificates - Working in 15 minutes</p> <p>\u2705 Personal/Learning Projects - Perfect for demos and testing - Share with friends easily - No ongoing domain costs</p> <p>\u2705 Built-in Security - Only people you invite can access - VPN-level security by default - Fine-grained access controls</p> <p>\u2705 Simple Management - One dashboard for everything - No DNS configuration needed - Automatic updates and renewal</p> <p>Best for: Personal projects, learning, demos, team development, secure internal tools</p>"},{"location":"networking-readme/#choose-cloudflare-tunnel-if-you-want","title":"Choose Cloudflare Tunnel if you want:","text":"<p>\u2705 Professional Domains - Your own custom domain (<code>yourcompany.com</code>) - Professional appearance for clients - Brand consistency</p> <p>\u2705 Production-Ready Features - Global CDN and caching - DDoS protection included - Web Application Firewall (WAF) - Analytics and monitoring</p> <p>\u2705 Public Access - Anyone can access (no VPN needed) - Perfect for public-facing services - SEO-friendly URLs</p> <p>\u2705 Scalability - Handle high traffic loads - Multiple domains supported - Enterprise-grade infrastructure</p> <p>Best for: Business websites, public services, client demos, production applications</p>"},{"location":"networking-readme/#quick-comparison","title":"\ud83d\udcca Quick Comparison","text":"Feature Tailscale Cloudflare Setup Time 15 minutes 45 minutes Domain Cost Free $10-15/year Custom Domain No Yes Security VPN-style (invite only) Public + WAF protection Performance Direct connection Global CDN Maintenance Minimal Minimal Production Ready Personal/team use Enterprise grade Access Control Tailscale accounts Internet + optional auth"},{"location":"networking-readme/#can-you-use-both","title":"\ud83d\ude80 Can You Use Both?","text":"<p>Yes! Your HostRegexp routing architecture supports both simultaneously:</p> <ul> <li>Development: <code>service.localhost</code> (local testing)</li> <li>Team Access: <code>service.your-device.ts.net</code> (secure team sharing)  </li> <li>Public Access: <code>service.yourcompany.com</code> (customer-facing)</li> </ul> <p>Technical Implementation: <pre><code># Your manifests use HostRegexp patterns like this:\nmatch: HostRegexp(`whoami\\..+`)\n\n# This automatically handles:\n# - whoami.localhost (development)\n# - whoami.provision-host.dog-pence.ts.net (Tailscale Funnel)  \n# - whoami.yourcompany.com (Cloudflare Tunnel)\n</code></pre></p> <p>Common Pattern: 1. Start with Tailscale for immediate internet access 2. Add Cloudflare later when you need custom domains 3. Use both for different purposes (internal vs external)</p> <p>See Traefik Ingress Rules for complete technical details on HostRegexp routing.</p>"},{"location":"networking-readme/#recommended-decision-path","title":"\ud83c\udfaf Recommended Decision Path","text":""},{"location":"networking-readme/#quick-decision-questions","title":"Quick Decision Questions:","text":"<ol> <li>Do you need a custom domain (yourcompany.com)?</li> <li>Yes \u2192 Cloudflare</li> <li> <p>No \u2192 Tailscale</p> </li> <li> <p>Is this for business/client use?</p> </li> <li>Yes \u2192 Cloudflare  </li> <li> <p>No \u2192 Tailscale</p> </li> <li> <p>Do you want to spend money on a domain?</p> </li> <li>Yes \u2192 Cloudflare</li> <li> <p>No \u2192 Tailscale</p> </li> <li> <p>Do you need public internet access?</p> </li> <li>Yes \u2192 Cloudflare</li> <li>Team only \u2192 Tailscale</li> </ol> <p>When in doubt, start with Tailscale - you can always add Cloudflare later!</p>"},{"location":"networking-readme/#architecture-overview","title":"\ud83c\udfd7\ufe0f Architecture Overview","text":""},{"location":"networking-readme/#microsoft-cloud-adoption-framework-caf-principles","title":"Microsoft Cloud Adoption Framework (CAF) principles","text":"<p>The networking architecture follows Microsoft Cloud Adoption Framework (CAF) principles for enterprise-grade infrastructure design.</p> <p>Kubernetes clusters can be defined as Landing Zones - either standalone or within larger Azure/cloud Landing Zones, providing isolation, governance, and security boundaries.</p>"},{"location":"networking-readme/#dual-tunnel-architecture-benefits","title":"Dual-Tunnel Architecture Benefits","text":"<p>Your cluster uses HostRegexp routing patterns in Traefik IngressRoutes that automatically work with both tunnel types:</p> <pre><code># Example from your working manifests:\napiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nspec:\n  routes:\n    - match: HostRegexp(`whoami\\..+`)  # Matches any domain starting with \"whoami.\"\n      kind: Rule\n      services:\n        - name: whoami\n          port: 80\n\n# This pattern automatically handles:\n# - whoami.localhost (development - auto-routes to 127.0.0.1)\n# - whoami.provision-host.dog-pence.ts.net (Tailscale Funnel)  \n# - whoami.yourcompany.com (Cloudflare Tunnel)\n</code></pre> <p>Key Technical Advantages: - \u2705 Unified Routing: Single IngressRoute handles multiple domains - \u2705 Zero Configuration: No manifest changes when adding tunnel types - \u2705 Internal DNS Support: CoreDNS resolves <code>.localhost</code> for pod-to-pod communication - \u2705 Authentication Ready: Works with Authentik forward auth middleware - \u2705 Future-Proof: Automatically supports any new domains</p> <p>Technical details: Traefik Ingress Rules Guide</p>"},{"location":"networking-readme/#external-access-with-cloudflare","title":"External Access with Cloudflare","text":"<p>Cloudflare provides secure, scalable exposure of services to the internet:</p> <ul> <li>Purpose: Professional public access with custom domains</li> <li>Benefits: Global CDN, DDoS protection, WAF, rate limiting, zero-trust access</li> <li>Implementation: Cloudflare Tunnels connect to Kubernetes without inbound ports</li> <li>URLs: <code>https://service.yourcompany.com</code></li> </ul>"},{"location":"networking-readme/#internet-access-with-tailscale-funnel","title":"Internet Access with Tailscale Funnel","text":"<p>Tailscale Funnel provides quick, secure internet access without custom domains:</p> <ul> <li>Purpose: Fast internet access for personal/team projects</li> <li>Benefits: Zero configuration, automatic HTTPS, invite-based security</li> <li>Implementation: Tailscale Funnel exposes services via .ts.net domains</li> <li>URLs: <code>https://service.your-device.ts.net</code></li> </ul>"},{"location":"networking-readme/#internal-networking-with-tailscale-vpn","title":"Internal Networking with Tailscale VPN","text":"<p>Tailscale VPN provides secure DevOps access to clusters:</p> <ul> <li>Purpose: Private communication for administration and maintenance</li> <li>Benefits: Works across any environment (local, cloud, multi-cloud)</li> <li>Security: Zero-trust networking with MagicDNS and ACL-based access control</li> </ul>"},{"location":"networking-readme/#high-level-architecture-for-internet-access","title":"High-Level Architecture for Internet Access","text":"<p>The diagram below shows how external traffic reaches your services through either tunnel type:</p> <p>:::mermaid flowchart TD     User[\"External User\"]     User2[\"Team Member\"]     Dev[\"Developer\"]</p> <pre><code>subgraph Internet_Access [\"Internet Access Options\"]\n    Tailscale_Funnel[\"Tailscale Funnel&lt;br&gt;service.device.ts.net\"]\n    CF_Tunnel[\"Cloudflare Tunnel&lt;br&gt;service.domain.com\"]\n    Localhost[\"Localhost&lt;br&gt;service.localhost\"]\nend\n\nsubgraph Your_Cluster [\"Your Kubernetes Cluster\"]\n    subgraph Traefik_Layer [\"Traefik Layer\"]\n        Traefik[\"Traefik 3.3.6&lt;br&gt;IngressRoute CRDs\"]\n        HostRegexp[\"HostRegexp Routing&lt;br&gt;`service\\..+`\"]\n        Internal_DNS[\"Internal DNS&lt;br&gt;CoreDNS Rewrites\"]\n    end\n\n    subgraph Services_Layer [\"Services Layer\"]\n        Services[\"Your Services&lt;br&gt;(whoami, openwebui, authentik)\"]\n        Auth[\"Authentik Forward Auth&lt;br&gt;(Optional)\"]\n    end\nend\n\nUser --&gt; CF_Tunnel\nUser2 --&gt; Tailscale_Funnel\nDev --&gt; Localhost\n\nCF_Tunnel --&gt; Traefik\nTailscale_Funnel --&gt; Traefik\nLocalhost --&gt; Traefik\n\nTraefik --&gt; HostRegexp\nHostRegexp --&gt; Auth\nHostRegexp --&gt; Services\nAuth --&gt; Services\n\nInternal_DNS -.-&gt; Services\n</code></pre> <p>:::</p> <p>Key Technical Features: - HostRegexp routing: <code>match: HostRegexp('service\\..+')</code> patterns handle all domain types - Traefik IngressRoute CRDs: Using <code>traefik.io/v1alpha1</code> API (current stable version) - Internal DNS: CoreDNS rewrites enable pod-to-pod communication on same hostnames - Authentication: Optional Authentik forward auth middleware for protected services - Zero Configuration: No manifest changes needed when adding/switching tunnels</p>"},{"location":"networking-readme/#high-level-architecture-for-devops-access","title":"High-Level Architecture for DevOps Access","text":"<p>The diagram below shows how DevOps engineers securely connect to clusters:</p> <p>:::mermaid flowchart TD     subgraph DevOps[\"DevOps Engineer's Machine\"]         subgraph provision_host_container [\"provision-host container\"]             kubectl[\"kubectl\"]             tools[\"SSHAnsibleCloud CLIs\"]             Tailscale_Client[\"Tailscale Client\"]</p> <pre><code>        kubectl --- tools --- Tailscale_Client\n        linkStyle 0 stroke-width:0,stroke-opacity:0,fill-opacity:0\n        linkStyle 1 stroke-width:0,stroke-opacity:0,fill-opacity:0\n    end\nend\n\nsubgraph Tailscale_Network [\"Tailscale Secure Network\"]\n    TS_Cloud[\"Tailscale Control Plane&lt;br&gt;(Authentication &amp; ACLs)\"]\nend\n\nsubgraph Your_Infrastructure [\"Your Infrastructure\"]\n    subgraph Cluster_VM[\"Cluster Host\"]\n        Tailscale_Agent[\"Tailscale Client\"]\n        K8s_Cluster[\"Kubernetes Cluster\"]\n\n        Tailscale_Agent --- K8s_Cluster\n        linkStyle 3 stroke-width:0,stroke-opacity:0,fill-opacity:0\n    end\nend\n\nTailscale_Client -- \"Secure Connection\" --&gt; TS_Cloud\nTailscale_Agent -- \"Secure Connection\" --&gt; TS_Cloud\n\nTailscale_Client -. \"Encrypted P2P&lt;br&gt;Administration\" .-&gt; Tailscale_Agent\n</code></pre> <p>:::</p>"},{"location":"networking-readme/#technology-choices-for-optimal-security-and-performance","title":"Technology Choices for Optimal Security and Performance","text":""},{"location":"networking-readme/#why-cloudflare","title":"Why Cloudflare?","text":"<p>Cloudflare provides enterprise-grade security and performance for public-facing services:</p> <p>Security Features: - Advanced Web Application Firewall (WAF) blocks malicious traffic - DDoS protection mitigates attacks of any size - Zero-trust tunnel approach - no inbound ports needed - SSL/TLS termination with automatic certificate management</p> <p>Performance Features: - Global network spans 300+ cities for low latency - Intelligent edge caching reduces server load - Cloudflare Workers enable custom routing logic - Analytics and monitoring for traffic insights</p> <p>Implementation: - Cloudflare Tunnels create secure outbound-only connections - No public IP addresses or firewall ports needed - Programmable routing based on domain, path, or headers - Automatic failover and load balancing</p> <p>For detailed setup instructions, see Cloudflare Tunnel Setup Guide.</p>"},{"location":"networking-readme/#why-tailscale","title":"Why Tailscale?","text":"<p>Tailscale provides zero-configuration secure networking for both internet access and DevOps administration:</p> <p>For Internet Access (Tailscale Funnel): - Instant HTTPS endpoints on .ts.net domains - No domain purchase or DNS configuration required - Invite-based access control for security - Perfect for personal projects and team development</p> <p>For DevOps Access (Tailscale VPN): - WireGuard-based encrypted peer-to-peer connections - Works seamlessly across NATs and firewalls - Identity-based access control with fine-grained ACLs - \"Zero config\" approach eliminates traditional VPN complexity</p> <p>Architecture Benefits: - True zero-trust networking model - Eliminates central VPN server points of failure - Automatic key rotation and device management - Cross-platform support for all environments</p> <p>For detailed setup instructions, see Tailscale Funnel Setup Guide.</p>"},{"location":"networking-readme/#next-steps","title":"Next Steps","text":""},{"location":"networking-readme/#ready-to-connect-to-the-internet","title":"Ready to Connect to the Internet?","text":"<p>Choose your tunnel type and follow the setup guide:</p> <ol> <li>\ud83d\udd35 Tailscale Funnel Setup - Get online in 15 minutes (free)</li> <li>\u26a1 Cloudflare Tunnel Setup - Professional setup with custom domain</li> </ol>"},{"location":"networking-readme/#internal-developer-access-sovereignsky","title":"Internal Developer Access (SovereignSky)","text":"<p>For internal-only access from within the Tailnet (no public internet):</p> <ol> <li>\ud83d\udd12 Tailscale Internal Access - HTTP access for SovereignSky developers via Tailnet VPN</li> </ol> <p>Your HostRegexp architecture makes switching between tunnel types seamless - the same manifests work for localhost development, Tailscale team access, and Cloudflare production without any modifications.</p>"},{"location":"networking-tailscale-internal-ingress/","title":"Tailscale Internal Access Setup Guide","text":"<p>Purpose: Internal-only Tailnet access for SovereignSky developers Audience: Developers needing secure access to cluster services without public internet exposure Time Required: 5-10 minutes Prerequisites: Working cluster with Traefik ingress, Tailscale account with OAuth credentials</p>"},{"location":"networking-tailscale-internal-ingress/#overview","title":"Overview","text":"<p>This guide explains how to deploy internal-only Tailscale access to your Kubernetes cluster using a LoadBalancer Service. Unlike the Funnel-based setup (see <code>networking-tailscale-setup.md</code>), this deployment:</p> <ul> <li>NO public internet access (internal Tailnet only)</li> <li>Accessible ONLY from devices on the same Tailnet</li> <li>Works with HTTP (no HTTPS requirement on tailnet)</li> <li>Designed for SovereignSky developers using devcontainers with Tailscale VPN</li> </ul>"},{"location":"networking-tailscale-internal-ingress/#architecture","title":"Architecture","text":""},{"location":"networking-tailscale-internal-ingress/#how-it-works","title":"How It Works","text":"<pre><code>SovereignSky Devcontainer (with Tailscale)\n    |\n    v\nTailnet (private network)\n    |\n    v\nk8s-terje (Tailscale LoadBalancer)  &lt;-- internal URL: http://k8s-terje.taile269d.ts.net\n    |\n    v\nTraefik Ingress Controller\n    |\n    +-&gt; grafana.sovereignsky.no\n    +-&gt; otel.sovereignsky.no\n    +-&gt; litellm.sovereignsky.no\n</code></pre>"},{"location":"networking-tailscale-internal-ingress/#tailscale-devices-per-cluster","title":"Tailscale Devices Per Cluster","text":"<p>Each cluster creates two devices in Tailscale Admin Console:</p> Device Name Purpose Description <code>k8s-terje-tailscale-operator</code> Controller Manages Tailscale resources in the cluster <code>k8s-terje</code> LoadBalancer proxy Routes HTTP traffic to Traefik <p>Multi-cluster naming convention: <pre><code>MacBook cluster:    k8s-terje-tailscale-operator, k8s-terje\niMac cluster:       k8s-imac-tailscale-operator, k8s-imac\ntecmacdev cluster:  k8s-tecmacdev-tailscale-operator, k8s-tecmacdev\n</code></pre></p> <p>The <code>-tailscale-operator</code> suffix matches the default naming from Tailscale Kubernetes Operator documentation, making it clear this is the official Tailscale component.</p>"},{"location":"networking-tailscale-internal-ingress/#quick-start","title":"Quick Start","text":""},{"location":"networking-tailscale-internal-ingress/#step-1-configure-secrets","title":"Step 1: Configure Secrets","text":"<p>Ensure your <code>00-common-values.env</code> (or Kubernetes secrets) includes:</p> <pre><code># Tailscale OAuth credentials (from https://login.tailscale.com/admin/settings/oauth)\nTAILSCALE_CLIENTID=k7Gdhr7mdf11CNTRL\nTAILSCALE_CLIENTSECRET=tskey-client-k7Gdhr7mdf11CNTRL-xxxxx\n\n# Tailnet info\nTAILSCALE_TAILNET=businessmodel.io\nTAILSCALE_DOMAIN=taile269d.ts.net\n\n# Internal hostname - UNIQUE PER CLUSTER\nTAILSCALE_INTERNAL_HOSTNAME=k8s-terje   # k8s-imac for iMac, k8s-tecmacdev for tecmacdev\n</code></pre>"},{"location":"networking-tailscale-internal-ingress/#step-2-deploy-internal-ingress","title":"Step 2: Deploy Internal Ingress","text":"<p>From the provision-host container:</p> <pre><code># Deploy Tailscale internal ingress\ndocker exec provision-host ansible-playbook \\\n  /mnt/urbalurbadisk/ansible/playbooks/805-deploy-tailscale-internal-ingress.yml\n</code></pre> <p>Or using the setup script: <pre><code>docker exec provision-host /mnt/urbalurbadisk/provision-host/kubernetes/network/03-setup-tailscale-internal.sh\n</code></pre></p>"},{"location":"networking-tailscale-internal-ingress/#step-3-verify-deployment","title":"Step 3: Verify Deployment","text":"<pre><code># Check pods in tailscale namespace\nkubectl get pods -n tailscale\n\n# Expected output:\n# NAME                                              READY   STATUS\n# operator-84987b6fc7-xxxxx                         1/1     Running\n# ts-tailscale-internal-ingress-xxxxx-0             1/1     Running\n\n# Check Tailscale Admin Console for devices:\n# - k8s-terje-tailscale-operator\n# - k8s-terje\n</code></pre>"},{"location":"networking-tailscale-internal-ingress/#configuration-files","title":"Configuration Files","text":""},{"location":"networking-tailscale-internal-ingress/#manifests","title":"Manifests","text":"File Purpose <code>manifests/800-tailscale-operator-config.yaml.j2</code> Helm values for Tailscale operator (Jinja2 template) <code>manifests/805-tailscale-internal-ingress.yaml.j2</code> LoadBalancer Service for Tailnet-only access (Jinja2 template)"},{"location":"networking-tailscale-internal-ingress/#ansible-playbooks","title":"Ansible Playbooks","text":"File Purpose <code>ansible/playbooks/805-deploy-tailscale-internal-ingress.yml</code> Deploy internal ingress <code>ansible/playbooks/806-remove-tailscale-internal-ingress.yml</code> Remove internal ingress"},{"location":"networking-tailscale-internal-ingress/#setup-scripts","title":"Setup Scripts","text":"File Purpose <code>provision-host/kubernetes/network/03-setup-tailscale-internal.sh</code> Setup script <code>provision-host/kubernetes/network/03-remove-tailscale-internal.sh</code> Removal script"},{"location":"networking-tailscale-internal-ingress/#technical-details","title":"Technical Details","text":""},{"location":"networking-tailscale-internal-ingress/#why-two-devices","title":"Why Two Devices?","text":"<p>The Tailscale Kubernetes Operator always registers itself as a device on the Tailnet. This cannot be disabled. Per the official documentation:</p> <p>The Tailscale Kubernetes operator creates a tailnet device for itself when deployed.</p> <p>This is expected behavior, not a bug. Each cluster needs: 1. Operator device - Manages Tailscale resources 2. Ingress device - Routes actual traffic</p>"},{"location":"networking-tailscale-internal-ingress/#loadbalancer-service-configuration-key-points","title":"LoadBalancer Service Configuration Key Points","text":"<p>The LoadBalancer Service uses <code>loadBalancerClass: tailscale</code> which enables HTTP traffic:</p> <pre><code># From 805-tailscale-internal-ingress.yaml.j2\napiVersion: v1\nkind: Service\nmetadata:\n  name: traefik-tailscale\n  namespace: kube-system\n  annotations:\n    tailscale.com/hostname: \"{{ TAILSCALE_INTERNAL_HOSTNAME }}\"  # Device name\n    tailscale.com/tags: \"tag:k8s-operator\"\nspec:\n  type: LoadBalancer\n  loadBalancerClass: tailscale\n  selector:\n    app.kubernetes.io/name: traefik\n    app.kubernetes.io/instance: traefik-kube-system\n  ports:\n    - name: http\n      port: 80\n      targetPort: web\n    - name: https\n      port: 443\n      targetPort: websecure\n</code></pre> <p>This approach: - Works with HTTP (no HTTPS requirement on tailnet) - Does NOT use Funnel (internal Tailnet access only) - Exposes both HTTP (80) and HTTPS (443) ports to Traefik</p>"},{"location":"networking-tailscale-internal-ingress/#operator-configuration-key-points","title":"Operator Configuration Key Points","text":"<p>The operator gets a unique hostname per cluster:</p> <pre><code># From 800-tailscale-operator-config.yaml.j2\noperatorConfig:\n  hostname: \"{{ TAILSCALE_INTERNAL_HOSTNAME }}-tailscale-operator\"\n  tags: \"tag:k8s-operator\"\n  logging: \"info\"\n</code></pre>"},{"location":"networking-tailscale-internal-ingress/#developer-access","title":"Developer Access","text":""},{"location":"networking-tailscale-internal-ingress/#from-sovereignsky-devcontainer","title":"From SovereignSky Devcontainer","text":"<ol> <li>Devcontainer has Tailscale VPN configured</li> <li>DNS resolves <code>*.sovereignsky.no</code> to the cluster's Tailscale IP</li> <li>Access services directly: <code>http://grafana.sovereignsky.no</code></li> </ol> <pre><code># From inside a devcontainer with Tailscale\ncurl http://grafana.sovereignsky.no\ncurl http://otel.sovereignsky.no\ncurl http://litellm.sovereignsky.no\n</code></pre>"},{"location":"networking-tailscale-internal-ingress/#direct-tailscale-access","title":"Direct Tailscale Access","text":"<p>You can also access the cluster directly via Tailscale (HTTP works now):</p> <pre><code># Direct Tailscale URL (HTTP, internal only)\ncurl http://k8s-terje.taile269d.ts.net\n\n# With Host header for service routing\ncurl -H \"Host: grafana.localhost\" http://k8s-terje.taile269d.ts.net\n</code></pre>"},{"location":"networking-tailscale-internal-ingress/#removal","title":"Removal","text":"<p>To remove the internal ingress:</p> <pre><code># Remove ingress only (keep operator)\ndocker exec provision-host ansible-playbook \\\n  /mnt/urbalurbadisk/ansible/playbooks/806-remove-tailscale-internal-ingress.yml\n\n# Remove everything (ingress + operator)\ndocker exec provision-host ansible-playbook \\\n  /mnt/urbalurbadisk/ansible/playbooks/806-remove-tailscale-internal-ingress.yml \\\n  -e remove_operator=true\n</code></pre> <p>Or using the script: <pre><code>docker exec provision-host /mnt/urbalurbadisk/provision-host/kubernetes/network/03-remove-tailscale-internal.sh\n</code></pre></p>"},{"location":"networking-tailscale-internal-ingress/#comparison-internal-vs-funnel","title":"Comparison: Internal vs Funnel","text":"Feature Internal LoadBalancer (805) Funnel Ingress (802-803) Public internet access No Yes Requires Tailscale on client Yes No Use case Developer access Public services Security Tailnet-only Public with HTTPS HTTP support Yes No (HTTPS only) HTTPS requirement on tailnet No Yes Kubernetes resource type LoadBalancer Service Ingress Setup scripts 805/806 801-804"},{"location":"networking-tailscale-internal-ingress/#troubleshooting","title":"Troubleshooting","text":""},{"location":"networking-tailscale-internal-ingress/#device-not-appearing-in-tailscale-admin","title":"Device Not Appearing in Tailscale Admin","text":"<ol> <li> <p>Check operator pod logs:    <pre><code>kubectl logs -n tailscale -l app=operator\n</code></pre></p> </li> <li> <p>Verify OAuth credentials are correct in secrets</p> </li> <li> <p>Check that <code>tag:k8s-operator</code> exists in your Tailnet ACL policy</p> </li> </ol>"},{"location":"networking-tailscale-internal-ingress/#connection-refused","title":"Connection Refused","text":"<ol> <li> <p>Verify Traefik is running:    <pre><code>kubectl get pods -n kube-system -l app.kubernetes.io/name=traefik\n</code></pre></p> </li> <li> <p>Check LoadBalancer Service configuration:    <pre><code>kubectl get svc traefik-tailscale -n kube-system -o yaml\n</code></pre></p> </li> <li> <p>Check if the service has a Tailscale IP assigned:    <pre><code>kubectl get svc traefik-tailscale -n kube-system -o jsonpath='{.status.loadBalancer.ingress}'\n</code></pre></p> </li> </ol>"},{"location":"networking-tailscale-internal-ingress/#wrong-device-name","title":"Wrong Device Name","text":"<p>If the operator device shows as generic <code>tailscale-operator</code> instead of <code>k8s-terje-tailscale-operator</code>:</p> <ol> <li> <p>Remove the deployment:    <pre><code>docker exec provision-host ansible-playbook \\\n  /mnt/urbalurbadisk/ansible/playbooks/806-remove-tailscale-internal-ingress.yml \\\n  -e remove_operator=true\n</code></pre></p> </li> <li> <p>Redeploy - the Helm chart will use the templated operator config with the correct hostname</p> </li> </ol>"},{"location":"networking-tailscale-internal-ingress/#helm-chart-default-behavior","title":"Helm Chart Default Behavior","text":"<p>The Tailscale Helm chart has a default <code>operatorConfig.hostname: \"tailscale-operator\"</code>. Our Jinja2 template (<code>800-tailscale-operator-config.yaml.j2</code>) overrides this with <code>{{ TAILSCALE_INTERNAL_HOSTNAME }}-tailscale-operator</code> to give each cluster a unique name.</p>"},{"location":"networking-tailscale-internal-ingress/#see-also","title":"See Also","text":"<ul> <li>networking-tailscale-setup.md - Funnel-based public internet access</li> <li>Tailscale Kubernetes Operator documentation</li> <li>Tailscale Cluster Ingress documentation</li> </ul>"},{"location":"networking-tailscale-networkisolation/","title":"Tailscale Funnel Security Setup for Rancher Desktop","text":"<p>TODO: This is not implemented - it is a potential solution that isolates the cluster when connected to the internet using tailscale funnel or cloudfrale tunnel.</p>"},{"location":"networking-tailscale-networkisolation/#overview","title":"Overview","text":"<p>This document describes how to securely expose services from a local Rancher Desktop Kubernetes cluster to the internet while protecting your local network from potential security threats.</p>"},{"location":"networking-tailscale-networkisolation/#the-challenge","title":"The Challenge","text":"<p>When you expose services to the internet, you create a potential security risk. Without proper configuration, a compromised service could potentially access your local network, including other computers, printers, and sensitive resources on your home or office network.</p>"},{"location":"networking-tailscale-networkisolation/#the-solution","title":"The Solution","text":"<p>We implement network isolation that creates a secure barrier between internet-facing services and your local network, while preserving the development workflow you're already using.</p>"},{"location":"networking-tailscale-networkisolation/#architecture-overview","title":"Architecture Overview","text":""},{"location":"networking-tailscale-networkisolation/#current-setup","title":"Current Setup","text":"<ul> <li>Rancher Desktop: Running Kubernetes cluster on your computer</li> <li>provision-host: A Docker container for managing the cluster (accessed via <code>docker exec</code>)</li> <li>Services: Web applications running in Kubernetes that you want to expose to the internet</li> </ul>"},{"location":"networking-tailscale-networkisolation/#security-model","title":"Security Model","text":"<p>Our setup creates two separate network zones:</p>"},{"location":"networking-tailscale-networkisolation/#zone-1-management-trusted","title":"Zone 1: Management (Trusted)","text":"<ul> <li>provision-host container: Used for development and cluster management</li> <li>Network access: Can reach the internet through your computer's network</li> <li>Why it's safe: Isolated from internet-facing services, only accessed by developers</li> </ul>"},{"location":"networking-tailscale-networkisolation/#zone-2-public-services-restricted","title":"Zone 2: Public Services (Restricted)","text":"<ul> <li>Kubernetes cluster services: Applications exposed to the internet</li> <li>Network access: Can ONLY reach the internet through Tailscale secure tunnels</li> <li>Why it's safe: Completely blocked from accessing your local network</li> </ul>"},{"location":"networking-tailscale-networkisolation/#network-flow-diagrams","title":"Network Flow Diagrams","text":""},{"location":"networking-tailscale-networkisolation/#secure-inbound-traffic-from-internet-to-your-services","title":"Secure Inbound Traffic (From Internet to Your Services)","text":"<pre><code>Internet Users\n    \u2193\nTailscale Funnel (Secure Gateway)\n    \u2193\nTraefik (Load Balancer)\n    \u2193\nYour Application (e.g., whoami service)\n</code></pre>"},{"location":"networking-tailscale-networkisolation/#secure-outbound-traffic-from-your-services-to-internet","title":"Secure Outbound Traffic (From Your Services to Internet)","text":"<p><pre><code>Your Application (needs to fetch data)\n    \u2193\nTailscale Exit Node (Secure Gateway)\n    \u2193\nInternet\n</code></pre> \u2705 SECURE: Cannot access your local network (printers, other computers, etc.)</p>"},{"location":"networking-tailscale-networkisolation/#development-workflow-unchanged","title":"Development Workflow (Unchanged)","text":"<p><pre><code>Developer\n    \u2193\ndocker exec provision-host\n    \u2193\nkubectl commands / scripts\n    \u2193\nManage Kubernetes cluster\n</code></pre> \u2705 PRESERVED: Your existing development process works exactly the same</p>"},{"location":"networking-tailscale-networkisolation/#security-benefits","title":"Security Benefits","text":""},{"location":"networking-tailscale-networkisolation/#for-non-technical-users","title":"For Non-Technical Users","text":"<ol> <li>Complete Network Protection: Even if a web service gets hacked, attackers cannot access your local computers, printers, or other devices</li> <li>Zero Configuration Changes: Your development workflow stays exactly the same</li> <li>Internet Access Maintained: Your services can still access the internet when needed (for updates, APIs, etc.)</li> <li>No Performance Impact: Local development and management tasks run at full speed</li> </ol>"},{"location":"networking-tailscale-networkisolation/#for-technical-users","title":"For Technical Users","text":"<ol> <li>Network Isolation: All cluster egress traffic routes through Tailscale exit nodes, preventing access to RFC1918 private networks</li> <li>Container-Level Security: provision-host remains isolated from Kubernetes network namespace</li> <li>Defense in Depth: Multiple layers of protection (network policies + routing + container isolation)</li> <li>Minimal Attack Surface: Only explicitly exposed services accessible via Funnel</li> </ol>"},{"location":"networking-tailscale-networkisolation/#implementation-details","title":"Implementation Details","text":""},{"location":"networking-tailscale-networkisolation/#prerequisites","title":"Prerequisites","text":"<ol> <li>Tailscale Account: With admin access to create OAuth credentials</li> <li>Rancher Desktop: Running with Kubernetes enabled</li> <li>provision-host: Docker container for cluster management</li> <li>Traefik: Ingress controller running in the cluster</li> </ol>"},{"location":"networking-tailscale-networkisolation/#step-1-tailscale-configuration","title":"Step 1: Tailscale Configuration","text":""},{"location":"networking-tailscale-networkisolation/#oauth-client-setup","title":"OAuth Client Setup","text":"<ol> <li>Go to Tailscale Admin Console</li> <li>Create OAuth client with:</li> <li>Scopes: <code>Devices Core</code> and <code>Auth Keys</code> (write access)</li> <li>Tags: <code>tag:k8s-operator</code></li> </ol>"},{"location":"networking-tailscale-networkisolation/#acl-policy-configuration","title":"ACL Policy Configuration","text":"<p>Add to your Tailscale ACL policy: <pre><code>{\n  \"tagOwners\": {\n    \"tag:k8s-operator\": [],\n    \"tag:k8s\": [\"tag:k8s-operator\"]\n  }\n}\n</code></pre></p>"},{"location":"networking-tailscale-networkisolation/#step-2-kubernetes-operator-installation","title":"Step 2: Kubernetes Operator Installation","text":"<p>Install the Tailscale Kubernetes operator using Helm:</p> <pre><code># Add Tailscale Helm repository\nhelm repo add tailscale https://pkgs.tailscale.com/helmcharts\nhelm repo update\n\n# Install operator with OAuth credentials\nhelm upgrade --install tailscale-operator tailscale/tailscale-operator \\\n  --namespace=tailscale --create-namespace \\\n  --set-string oauth.clientId=\"&lt;your-oauth-client-id&gt;\" \\\n  --set-string oauth.clientSecret=\"&lt;your-oauth-client-secret&gt;\" \\\n  --wait\n</code></pre>"},{"location":"networking-tailscale-networkisolation/#step-3-network-security-implementation","title":"Step 3: Network Security Implementation","text":""},{"location":"networking-tailscale-networkisolation/#force-all-cluster-traffic-through-tailscale","title":"Force All Cluster Traffic Through Tailscale","text":"<pre><code>apiVersion: tailscale.com/v1alpha1\nkind: Connector\nmetadata:\n  name: cluster-exit-node\n  namespace: tailscale\nspec:\n  subnetRoutes:\n    - \"0.0.0.0/0\"\n  exitNode: true\n</code></pre>"},{"location":"networking-tailscale-networkisolation/#block-access-to-local-networks","title":"Block Access to Local Networks","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-local-networks\n  namespace: default\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\n  egress:\n  # Allow internet access via Tailscale only\n  - to:\n    - ipBlock:\n        cidr: 0.0.0.0/0\n        except:\n        - 10.0.0.0/8      # Block private class A\n        - 172.16.0.0/12   # Block private class B\n        - 192.168.0.0/16  # Block private class C\n        - 169.254.0.0/16  # Block link-local\n        - 127.0.0.0/8     # Block loopback\n  # Allow DNS resolution\n  - to: []\n    ports:\n    - protocol: UDP\n      port: 53\n  # Allow cluster-internal traffic\n  - to: []\n    ports:\n    - protocol: TCP\n    - protocol: UDP\n</code></pre>"},{"location":"networking-tailscale-networkisolation/#step-4-service-exposure","title":"Step 4: Service Exposure","text":""},{"location":"networking-tailscale-networkisolation/#expose-service-to-internet-via-funnel","title":"Expose Service to Internet via Funnel","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: whoami\n  annotations:\n    tailscale.com/expose: \"true\"\nspec:\n  selector:\n    app: whoami\n  ports:\n  - port: 443\n    targetPort: 80\n</code></pre>"},{"location":"networking-tailscale-networkisolation/#enable-funnel-for-public-access","title":"Enable Funnel for Public Access","text":"<pre><code># Find your service's Tailscale hostname\ntailscale status\n\n# Enable Funnel (replace with actual hostname)\ntailscale funnel --bg https://whoami-default-xyz.tail-scale.ts.net:443\n</code></pre>"},{"location":"networking-tailscale-networkisolation/#step-5-validation","title":"Step 5: Validation","text":""},{"location":"networking-tailscale-networkisolation/#test-network-isolation","title":"Test Network Isolation","text":"<pre><code># Deploy test pod\nkubectl run network-test --image=busybox -it --rm -- /bin/sh\n\n# Inside pod - these should FAIL (timeout):\nping 192.168.1.1          # Your router\nping 10.0.0.1             # Docker gateway\nwget -T 5 http://192.168.1.100  # Local services\n\n# This should WORK (via Tailscale):\nping 8.8.8.8              # Internet via exit node\n</code></pre>"},{"location":"networking-tailscale-networkisolation/#test-development-workflow","title":"Test Development Workflow","text":"<pre><code># This should work exactly as before:\ndocker exec -it provision-host /bin/bash\nkubectl get pods\nkubectl apply -f your-manifests.yaml\n</code></pre>"},{"location":"networking-tailscale-networkisolation/#operational-considerations","title":"Operational Considerations","text":""},{"location":"networking-tailscale-networkisolation/#monitoring","title":"Monitoring","text":"<ul> <li>Tailscale Admin Console: Monitor device connections and traffic</li> <li>Kubernetes Logs: Check operator and connector pod logs</li> <li>Network Policies: Use <code>kubectl describe networkpolicy</code> to verify rules</li> </ul>"},{"location":"networking-tailscale-networkisolation/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>Service Not Accessible: Check Funnel configuration and DNS resolution</li> <li>No Internet from Pods: Verify exit node is running and connected</li> <li>Network Policy Issues: Test with temporary policy removal</li> </ol>"},{"location":"networking-tailscale-networkisolation/#performance","title":"Performance","text":"<ul> <li>Latency: Outbound traffic routes through Tailscale exit nodes</li> <li>Bandwidth: Limited by Tailscale plan and exit node capacity</li> <li>Local Traffic: Cluster-internal communication unaffected</li> </ul>"},{"location":"networking-tailscale-networkisolation/#backup-procedures","title":"Backup Procedures","text":"<ul> <li>Configuration Backup: Export Tailscale and Kubernetes configurations</li> <li>Recovery Plan: Document steps to restore connectivity if issues arise</li> </ul>"},{"location":"networking-tailscale-networkisolation/#cost-and-resource-implications","title":"Cost and Resource Implications","text":""},{"location":"networking-tailscale-networkisolation/#tailscale-costs","title":"Tailscale Costs","text":"<ul> <li>Personal Use: Free tier supports up to 3 devices</li> <li>Business Use: Paid plans for larger deployments</li> <li>Exit Node Bandwidth: May affect plan selection</li> </ul>"},{"location":"networking-tailscale-networkisolation/#resource-usage","title":"Resource Usage","text":"<ul> <li>Memory: ~100MB for operator pod</li> <li>CPU: Minimal impact on cluster performance</li> <li>Network: Additional overhead for encrypted tunneling</li> </ul>"},{"location":"networking-tailscale-networkisolation/#security-checklist","title":"Security Checklist","text":""},{"location":"networking-tailscale-networkisolation/#pre-deployment","title":"Pre-Deployment","text":"<ul> <li>[ ] OAuth client created with minimal required scopes</li> <li>[ ] ACL policies configured for least-privilege access</li> <li>[ ] Network policies tested in development environment</li> <li>[ ] Backup procedures documented</li> </ul>"},{"location":"networking-tailscale-networkisolation/#post-deployment","title":"Post-Deployment","text":"<ul> <li>[ ] Network isolation verified with test pods</li> <li>[ ] Funnel access confirmed from external network</li> <li>[ ] Development workflow validated</li> <li>[ ] Monitoring and alerting configured</li> <li>[ ] Incident response procedures documented</li> </ul>"},{"location":"networking-tailscale-networkisolation/#ongoing-maintenance","title":"Ongoing Maintenance","text":"<ul> <li>[ ] Regular security updates for Tailscale operator</li> <li>[ ] Periodic review of exposed services</li> <li>[ ] Monitoring of unusual network patterns</li> <li>[ ] Backup configuration updates</li> </ul>"},{"location":"networking-tailscale-networkisolation/#conclusion","title":"Conclusion","text":"<p>This setup provides enterprise-grade security for exposing local Kubernetes services to the internet while maintaining the development workflow you're already comfortable with.</p> <p>Key Benefits: - \u2705 Complete network isolation prevents lateral movement - \u2705 Zero impact on existing development processes - \u2705 Secure internet access for services when needed - \u2705 Simple to maintain and monitor</p> <p>The Result: You can confidently expose services to the internet knowing that even if they're compromised, your local network remains completely protected.</p>"},{"location":"networking-tailscale-setup/","title":"Tailscale Tunnel Setup Guide","text":"<p>Purpose: Public internet access with automatic .ts.net domains Audience: Users wanting secure, public internet connectivity Time Required: 10-15 minutes Prerequisites: Working cluster with Traefik ingress</p>"},{"location":"networking-tailscale-setup/#critical-limitation-no-wildcard-dns-support","title":"\u26a0\ufe0f Critical Limitation: No Wildcard DNS Support","text":"<p>Tailscale does not support wildcard DNS routing. This means patterns like <code>*.k8s.dog-pence.ts.net</code> will NOT work.</p> <p>Reference: Tailscale GitHub Issue #1196</p> <p>Throughout this document we use the tailscale domain <code>dog-pence.ts.net</code> as an example. You get your own domain in the form <code>.ts.net</code> when signing up to Tailscale."},{"location":"networking-tailscale-setup/#what-this-means","title":"What This Means:","text":"<ul> <li>\u274c Does NOT work: <code>https://whoami.k8s.dog-pence.ts.net</code> (subdomain pattern)</li> <li>\u274c Does NOT work: <code>https://*.k8s.dog-pence.ts.net</code> (wildcard routing)</li> <li>\u2705 DOES work: <code>https://whoami.dog-pence.ts.net</code> (individual service via 803 script)</li> <li>\u2705 DOES work: <code>https://authentik.dog-pence.ts.net</code> (each service gets its own URL)</li> </ul>"},{"location":"networking-tailscale-setup/#the-solution-individual-service-ingresses","title":"The Solution: Individual Service Ingresses","text":"<p>We use the <code>802-tailscale-tunnel-deploy.sh</code> script to create individual Tailscale ingresses for each service. Each service gets its own public URL directly on your tailscale domain.</p>"},{"location":"networking-tailscale-setup/#quick-summary","title":"\ud83d\ude80 Quick Summary","text":"<p>Transform your local cluster from <code>http://service.localhost</code> to public URLs like <code>https://whoami.dog-pence.ts.net</code> with automatic HTTPS. Each service gets its own public URL via individual Tailscale ingresses.</p>"},{"location":"networking-tailscale-setup/#how-tailscale-tunnel-works","title":"\ud83c\udfd7\ufe0f How Tailscale Tunnel Works","text":""},{"location":"networking-tailscale-setup/#architecture-overview","title":"Architecture Overview","text":"<p>Due to Tailscale's lack of wildcard DNS support, each service requires its own Tailscale ingress:</p> <pre><code>External User \u2192 https://whoami.dog-pence.ts.net\n    \u2193\nTailscale MagicDNS \u2192 whoami-ingress (dedicated Tailscale pod)\n    \u2193\nKubernetes Service \u2192 whoami pod\n</code></pre> <p>Key Components: 1. Tailscale MagicDNS - Provides automatic DNS for each service (e.g., <code>whoami.dog-pence.ts.net</code>) 2. Individual Ingresses - Each service gets its own Tailscale pod/device 3. Direct Service Routing - Traffic goes directly to each service 4. Your Services - whoami, openwebui, authentik, etc.</p> <p>Security Benefits: - \u2705 End-to-end encryption through Tailscale network - \u2705 No public IP exposure - services remain private - \u2705 Invite-based access - only your tailnet members can access - \u2705 Zero-trust networking - device authentication required</p>"},{"location":"networking-tailscale-setup/#prerequisites","title":"\u2705 Prerequisites","text":"<p>Before starting, ensure you have: - [ ] Kubernetes cluster running (Rancher Desktop or similar) - [ ] Traefik ingress controller deployed - [ ] Services accessible locally (e.g., <code>http://whoami.localhost</code>) - [ ] Access to provision-host container - [ ] Valid Tailscale account and credentials</p>"},{"location":"networking-tailscale-setup/#script-overview","title":"\ud83d\udccb Script Overview","text":"<p>Five scripts manage the complete Tailscale setup:</p> Script Purpose When to Use Parameters <code>801-tailscale-tunnel-setup.sh</code> Sets up Tailscale on provision-host First time setup None <code>802-tailscale-tunnel-deploy.sh</code> Deploys operator to cluster After host setup <code>[cluster-hostname]</code> <code>802-tailscale-tunnel-deploy.sh</code> Add individual service ingress After operator deployed <code>&lt;service&gt; [hostname]</code> <code>803-tailscale-tunnel-deletehost.sh</code> Remove individual service ingress When removing a service <code>&lt;hostname&gt;</code> <code>804-tailscale-tunnel-delete.sh</code> Removes everything Clean up / start over None"},{"location":"networking-tailscale-setup/#quick-start-guide","title":"\ud83d\ude80 Quick Start Guide","text":""},{"location":"networking-tailscale-setup/#step-1-create-tailscale-account","title":"Step 1: Create Tailscale Account","text":"<ol> <li>Visit tailscale.com and sign up</li> <li>Your tailnet will be created (e.g., <code>yourusername.github</code>) \u2192 Note this as <code>TAILSCALE_TAILNET</code></li> </ol>"},{"location":"networking-tailscale-setup/#step-2-configure-access-control-tags-prepare-for-auth-key","title":"Step 2: Configure Access Control Tags (Prepare for auth key)","text":"<ol> <li>Go to Tailscale Access Controls</li> <li>Click \"JSON editor\" (top right of the policy editor)</li> <li>Replace the entire content with this clean configuration: <pre><code>{\n  \"tagOwners\": {\n    \"tag:k8s-operator\": [\"autogroup:admin\"]\n  },\n  \"nodeAttrs\": [\n    {\n      \"target\": [\"tag:k8s-operator\"],\n      \"attr\": [\"funnel\"]\n    }\n  ],\n  \"acls\": [\n    {\"action\": \"accept\", \"src\": [\"*\"], \"dst\": [\"*:*\"]}\n  ]\n}\n</code></pre></li> <li>Click \"Save\"</li> </ol> <p>What this does: - <code>tagOwners</code>: Allows admins to assign <code>tag:k8s-operator</code> tags - <code>nodeAttrs</code>: Enables Funnel capability for devices with <code>tag:k8s-operator</code> (public internet access) - <code>acls</code>: Allows all devices to communicate with each other (simple setup)</p>"},{"location":"networking-tailscale-setup/#step-3-create-auth-key-for-provision-host-authentication-with-funnel","title":"Step 3: Create Auth Key (for provision-host authentication with Funnel)","text":"<ol> <li>Go to Auth Keys page</li> <li>Click \"Generate auth key\" </li> <li>Description: <code>urbalurba-k8s-funnel</code></li> <li>Reusable: \u2705 Check this box (allows multiple devices)</li> <li>Expiration: <code>90</code> days </li> <li>Ephemeral: \u274c Leave unchecked (permanent infrastructure)</li> <li>Tags: Type <code>tag:k8s-operator</code> and click \"Add tags\" </li> <li>The <code>tag:k8s-operator</code> is required for Funnel capability (public internet access)</li> <li>Click \"Generate key\"</li> <li>Copy the auth key \u2192 This becomes <code>TAILSCALE_SECRET</code></li> </ol> <p>Why tag:k8s-operator?  - The ACL policy grants Funnel capability only to devices with <code>tag:k8s-operator</code> - This allows the device to expose services to the public internet - Without this tag, you'll only get internal tailnet connectivity</p>"},{"location":"networking-tailscale-setup/#step-4-create-oauth-client-for-cluster-operations","title":"Step 4: Create OAuth Client (for cluster operations)","text":"<ol> <li>Go to OAuth clients page </li> <li>Click \"Generate OAuth client\"</li> <li>Description: <code>urbalurba-k8s-oauth</code></li> <li>Select required scopes</li> <li>DNS: Select Write (enable MagicDNS features if needed)</li> <li>Devices \u2192 Core: Select Write (create/delete cluster devices)<ul> <li>Tags (required for write scope): Click \"Add tags\" and add <code>tag:k8s-operator</code></li> <li>This allows the OAuth client to create devices with the k8s-operator tag</li> </ul> </li> <li>Auth keys: Select Write \u2190 REQUIRED (allows operator to create internal auth keys)<ul> <li>Tags (required for write scope): Click \"Add tags\" and add <code>tag:k8s-operator</code> </li> </ul> </li> <li>Feature Settings: Select Write (enable HTTPS/Funnel features)</li> <li>Leave all other scopes unselected (principle of least privilege)</li> <li>Click \"Generate client\"</li> <li>Copy the Client ID \u2192 This becomes <code>TAILSCALE_CLIENTID</code></li> <li>Copy the Client Secret \u2192 This becomes <code>TAILSCALE_CLIENTSECRET</code></li> </ol> <p>\u26a0\ufe0f Important: Save these values immediately - you can't view the secret again!</p> <p>Why these scopes? - Auth keys (Write): CRITICAL - Allows Tailscale operator to create internal auth keys (without this you get 403 errors) - Devices Core (Write): Allows Tailscale operator to create/delete cluster ingress devices - DNS (Write): Enables MagicDNS configuration for wildcard routing - Feature Settings (Write): Allows enabling HTTPS/Funnel for internet access</p>"},{"location":"networking-tailscale-setup/#step-5-configure-magicdns-domain","title":"Step 5: Configure MagicDNS Domain","text":"<ol> <li>Go to Tailscale Admin Console \u2192 DNS</li> <li>Enable MagicDNS </li> <li>Note your MagicDNS domain (e.g., <code>dog-pence.ts.net</code>) \u2192 This becomes <code>TAILSCALE_DOMAIN</code></li> </ol>"},{"location":"networking-tailscale-setup/#step-6-update-kubernetes-secrets","title":"Step 6: Update Kubernetes Secrets","text":"<p>Edit <code>topsecret/kubernetes/kubernetes-secrets.yml</code> with your values: <pre><code># Update these Tailscale variables with values from Steps 1-5:\nTAILSCALE_SECRET: tskey-auth-YOUR-AUTH-KEY           # From Step 3: Auth Key\nTAILSCALE_TAILNET: your-tailnet-name                # From Step 1: Your tailnet name\nTAILSCALE_DOMAIN: your-magic-dns-domain             # From Step 5: MagicDNS domain  \nTAILSCALE_CLUSTER_HOSTNAME: k8s                     # Becomes: k8s.[your-domain].ts.net (cluster ingress only)\nTAILSCALE_CLIENTID: YOUR-OAUTH-CLIENT-ID            # From Step 4: OAuth Client ID\nTAILSCALE_CLIENTSECRET: tskey-client-YOUR-OAUTH-CLIENT-SECRET  # From Step 4: OAuth Client Secret\n</code></pre></p> <p>Important: TAILSCALE_CLUSTER_HOSTNAME: - This is used for the cluster-wide ingress only (when no service parameter is provided) - Example: If set to <code>k8s</code> and your domain is <code>dog-pence.ts.net</code>:   - <code>k8s.dog-pence.ts.net</code> \u2192 Routes to Traefik's default backend (nginx catch-all)   - Individual services get their own URLs: <code>whoami.dog-pence.ts.net</code>, <code>grafana.dog-pence.ts.net</code>   - Note: Tailscale does NOT support wildcard DNS, so <code>*.k8s.dog-pence.ts.net</code> patterns won't work</p>"},{"location":"networking-tailscale-setup/#step-7-apply-secrets-to-kubernetes","title":"Step 7: Apply Secrets to Kubernetes","text":"<pre><code># Apply updated secrets to cluster\nkubectl apply -f topsecret/kubernetes/kubernetes-secrets.yml\n\n# Verify secrets are applied\nkubectl get secret urbalurba-secrets -o yaml | grep TAILSCALE\n</code></pre>"},{"location":"networking-tailscale-setup/#step-8-setup-tailscale-on-provision-host","title":"Step 8: Setup Tailscale on Provision-Host","text":"<pre><code># From your Mac host, copy scripts and access provision-host\n./copy2provisionhost.sh\ndocker exec -it provision-host bash\ncd /mnt/urbalurbadisk\n\n# Setup Tailscale daemon and authenticate\n./networking/tailscale/801-tailscale-tunnel-setup.sh\n</code></pre>"},{"location":"networking-tailscale-setup/#step-9-deploy-tailscale-operator-to-cluster","title":"Step 9: Deploy Tailscale Operator to Cluster","text":"<pre><code># Deploy operator (required for managing individual service ingresses)\n./networking/tailscale/802-tailscale-tunnel-deploy.sh\n</code></pre>"},{"location":"networking-tailscale-setup/#step-10-add-individual-services-the-working-solution","title":"Step 10: Add Individual Services (The Working Solution)","text":"<p>Since Tailscale doesn't support wildcard DNS, use the <code>802-tailscale-tunnel-deploy.sh</code> script to expose each service individually:</p> <pre><code># Basic usage: ./802-tailscale-tunnel-deploy.sh &lt;service-name&gt; [hostname]\n\n# Add whoami service (uses default hostname=whoami)\n./networking/tailscale/802-tailscale-tunnel-deploy.sh whoami\n# Result: https://whoami.dog-pence.ts.net\n\n# Add OpenWebUI with custom hostname\n./networking/tailscale/802-tailscale-tunnel-deploy.sh open-webui openwebui\n# Result: https://openwebui.dog-pence.ts.net\n\n# Add Authentik with clean hostname\n./networking/tailscale/802-tailscale-tunnel-deploy.sh authentik-server authentik\n# Result: https://authentik.dog-pence.ts.net\n\n# Add Grafana\n./networking/tailscale/802-tailscale-tunnel-deploy.sh grafana grafana\n# Result: https://grafana.dog-pence.ts.net\n</code></pre> <p>What the script does: 1. Deploys Tailscale operator (if not already running) 2. Creates a Tailscale ingress pod for that specific service 3. Configures public internet access via Funnel 4. Traefik handles routing to the appropriate service 5. Sets up DNS entry at <code>[hostname].[your-domain].ts.net</code></p> <p>To remove a service: <pre><code># Remove by hostname\n./networking/tailscale/803-tailscale-tunnel-deletehost.sh whoami\n</code></pre></p> <p>Important notes: - Each service requires its own Tailscale pod (slight resource overhead) - Services are directly accessible from the public internet - No authentication by default - add Authentik protection if needed - DNS propagation takes 1-5 minutes globally after adding a service</p>"},{"location":"networking-tailscale-setup/#step-11-test-public-internet-access","title":"Step 11: Test Public Internet Access","text":"<pre><code># Test your exposed services (replace with your actual domain):\ncurl https://whoami.dog-pence.ts.net\ncurl https://openwebui.dog-pence.ts.net\ncurl https://authentik.dog-pence.ts.net\n\n# These URLs work from:\n# - Any browser on any computer\n# - No Tailscale client needed for visitors\n# - Full public internet exposure via Funnel\n\n# To see all your active Tailscale ingresses:\nkubectl get pods -n tailscale -l app.kubernetes.io/name=tailscale-ingress\n</code></pre>"},{"location":"networking-tailscale-setup/#step-12-dns-troubleshooting","title":"Step 12: DNS Troubleshooting","text":"<p>If services are not immediately accessible, use these commands to check DNS resolution:</p> <pre><code># Check basic DNS resolution\nnslookup whoami.dog-pence.ts.net\n\n# Get detailed DNS information\ndig whoami.dog-pence.ts.net\n\n# Test connectivity with verbose output\ncurl -v https://whoami.dog-pence.ts.net\n</code></pre> <p>Expected Results: - <code>nslookup</code> should return a Tailscale Funnel IP (e.g., <code>185.40.234.37</code>) - <code>dig</code> should show the A record with TTL information - <code>curl -v</code> should show successful TLS handshake and HTTP response</p> <p>Common Issues: - \"Could not resolve host\" - DNS propagation still in progress (wait 1-5 minutes) - \"Connection timeout\" - Check if service is running in cluster - \"404 Not Found\" - Service exists but Traefik routing needs adjustment</p>"},{"location":"networking-tailscale-setup/#complete-cleanup","title":"\ud83d\uddd1\ufe0f Complete Cleanup","text":"<p>To completely remove Tailscale and start over: <pre><code># Delete everything\n./networking/tailscale/804-tailscale-tunnel-delete.sh\n</code></pre></p> <p>What gets deleted: - All Tailscale ingresses and services - Tailscale operator from cluster - Tailscale devices from your tailnet - Tailscale daemon on provision-host - Local configuration files</p>"},{"location":"networking-tailscale-setup/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"networking-tailscale-setup/#error-requested-tags-tagk8s-operator-are-invalid-or-not-permitted","title":"Error: \"requested tags [tag:k8s-operator] are invalid or not permitted\"","text":"<p>This error means your OAuth client doesn't have permission for <code>tag:k8s-operator</code>. To fix:</p> <ol> <li>Go to OAuth clients page</li> <li>Edit your <code>urbalurba-k8s-oauth</code> client</li> <li>In Devices \u2192 Core scope, ensure <code>tag:k8s-operator</code> is added</li> <li>In Auth keys scope, ensure <code>tag:k8s-operator</code> is added</li> <li>Generate a new client secret (required after scope changes)</li> <li>Update <code>TAILSCALE_CLIENTSECRET</code> in your secrets file</li> <li>Apply with <code>kubectl apply -f topsecret/kubernetes/kubernetes-secrets.yml</code></li> <li>Run the script again</li> </ol> <p>Key Point: The operator uses <code>tag:k8s-operator</code> for all devices, including itself and cluster ingress devices with Funnel capability.</p>"},{"location":"networking-tailscale-setup/#expired-tailscale-keys","title":"Expired Tailscale Keys","text":"<p>If you get authentication errors, create new keys at Tailscale Admin Console:</p> <p>Create OAuth Client: 1. Click \"Generate auth key\" \u2192 \"OAuth client\" 2. Name: <code>urbalurba-k8s</code> 3. Scopes: <code>device:create</code>, <code>device:delete</code>, <code>device:read</code> 4. Copy Client ID and Client Secret</p> <p>Create Auth Key: 1. Click \"Generate auth key\" \u2192 \"Auth key\" 2. Tags: <code>tag:provision-host</code> (optional) 3. Expiry: 90 days 4. Copy the auth key</p> <p>Update secrets file: <pre><code># Edit topsecret/kubernetes/kubernetes-secrets.yml\nTAILSCALE_SECRET: tskey-auth-YOUR-NEW-AUTH-KEY\nTAILSCALE_CLIENTID: YOUR-NEW-CLIENT-ID\nTAILSCALE_CLIENTSECRET: tskey-client-YOUR-NEW-CLIENT-SECRET\n\n# Apply to cluster\nkubectl apply -f topsecret/kubernetes/kubernetes-secrets.yml\n</code></pre></p>"},{"location":"networking-tailscale-setup/#script-execution-issues","title":"Script Execution Issues","text":"<p>Check Tailscale status in provision-host: <pre><code># Access provision-host container\ndocker exec -it provision-host bash\n\n# Check Tailscale daemon\ntailscale status\n\n# Check cluster operator\nkubectl get pods -n tailscale\nkubectl logs -n tailscale -l app=operator\n</code></pre></p> <p>Check cluster connectivity: <pre><code># From provision-host container\nkubectl get ingressroute -A\nkubectl describe ingress -n kube-system\n</code></pre></p>"},{"location":"networking-tailscale-setup/#tailscale-installation-issues","title":"Tailscale Installation Issues","text":"<p>Tailscale is pre-installed in the provision-host container. If missing: <pre><code># From provision-host container\ncurl -fsSL https://tailscale.com/install.sh | sh\n</code></pre></p>"},{"location":"networking-tailscale-setup/#architecture-details","title":"\ud83d\udcda Architecture Details","text":""},{"location":"networking-tailscale-setup/#per-service-routing-flow","title":"Per-Service Routing Flow","text":"<pre><code>1. External request: https://whoami.dog-pence.ts.net\n2. Tailscale MagicDNS resolves to specific whoami-ingress device\n3. whoami-ingress pod forwards directly to whoami service\n4. No Traefik involvement - direct service connection\n</code></pre>"},{"location":"networking-tailscale-setup/#script-dependencies","title":"Script Dependencies","text":"<ul> <li>801 \u2192 802 \u2192 803 (sequential execution required)</li> <li>803 can be run multiple times to add different services</li> <li>804 removes individual service ingresses</li> <li>805 complete cleanup of everything</li> </ul>"},{"location":"networking-tailscale-setup/#integration-with-other-systems","title":"Integration with Other Systems","text":"<ul> <li>Works alongside Cloudflare tunnels (different domains)</li> <li>Each service gets independent public URL</li> <li>Can add Authentik protection per service if needed</li> </ul>"},{"location":"networking-tailscale-setup/#verification","title":"\u2705 Verification","text":"<p>After setup, verify your services are accessible:</p> <pre><code># Test individual service URLs (after running 803 for each)\ncurl https://whoami.dog-pence.ts.net\ncurl https://openwebui.dog-pence.ts.net\ncurl https://authentik.dog-pence.ts.net\n\n# Check all Tailscale ingress pods\nkubectl get pods -n tailscale\n\n# View Tailscale device status\ntailscale status\n\n# List all service ingresses\nkubectl get pods -n tailscale -l app.kubernetes.io/name=tailscale-ingress\n</code></pre>"},{"location":"networking-tailscale-setup/#benefits-achieved","title":"\ud83c\udf89 Benefits Achieved","text":"<p>\u2705 Public Internet Access: Each service accessible via its own <code>.ts.net</code> URL from anywhere \u2705 Automatic HTTPS: Zero-configuration SSL certificates \u2705 No Port Forwarding: Works behind NAT/firewalls via Tailscale Funnel \u2705 Flexible Service Exposure: Choose exactly which services to make public \u2705 Simple Management: Add/remove services with single command</p>"},{"location":"networking-tailscale-setup/#summary","title":"\ud83d\udcdd Summary","text":"<p>While Tailscale doesn't support wildcard DNS (limiting us from using patterns like <code>*.k8s.dog-pence.ts.net</code>), the <code>802-tailscale-tunnel-deploy.sh</code> script provides a practical workaround. Each service gets its own public URL like <code>https://whoami.dog-pence.ts.net</code>, giving you full control over which services are exposed to the internet.</p> <p>\u26a0\ufe0f Authentication Note: Services exposed via Tailscale are publicly accessible by default. If you need authentication, consider adding Authentik protection. See <code>docs/rules-ingress-traefik.md</code> for authentication setup details.</p>"},{"location":"overview-getting-started/","title":"Getting Started","text":"<p>File: <code>docs/overview-getting-started.md</code> Purpose: Quick start guide for first-time users to get Urbalurba running immediately Target Audience: New users and developers trying Urbalurba for the first time Last Updated: September 22, 2024</p>"},{"location":"overview-getting-started/#first-test-5-minutes-to-running","title":"\ud83d\ude80 First Test - 5 Minutes to Running","text":"<p>Get Urbalurba Infrastructure running on your computer in just 5 minutes:</p>"},{"location":"overview-getting-started/#step-1-install-rancher-desktop-2-minutes","title":"Step 1: Install Rancher Desktop (2 minutes)","text":"<ol> <li>Download Rancher Desktop: Go to https://rancherdesktop.io/</li> <li>Install: Run the installer for your operating system</li> <li>Start Rancher Desktop: Launch the application</li> <li>Wait for Kubernetes: The Kubernetes cluster will start automatically</li> </ol>"},{"location":"overview-getting-started/#step-2-download-and-start-urbalurba-3-minutes","title":"Step 2: Download and Start Urbalurba (3 minutes)","text":"<ol> <li>Download: Go to https://github.com/terchris/urbalurba-infrastructure/releases</li> <li>Download the latest: Click on <code>urbalurba-infrastructure.zip</code></li> <li>Extract: Unzip the file to your desired folder</li> <li>Start: Double-click <code>start-urbalurba.sh</code> (macOS/Linux) or <code>start-urbalurba.bat</code> (Windows)</li> </ol>"},{"location":"overview-getting-started/#step-3-open-your-browser","title":"Step 3: Open Your Browser","text":"<p>Once the startup completes (you'll see \"All services ready!\"), open your browser to:</p> <p>http://localhost</p> <p>You'll see the Urbalurba welcome page \"Hello world\"</p>"},{"location":"overview-getting-started/#starting-services","title":"\ud83c\udf10 Starting services","text":"<p>By default you get a catch-all web page that says \"Hello world\". </p> <p>There are two ways of doing this. Starting manually or defining what service should start when the cluster is built.</p> <p>We will do the simplest way first. Starting sevices manually.</p> <p>All management is done in the provision-host container. So first you must log in to it by starting the script \u00b4login-provision-host.sh`.</p> <p>This takes you into the provision-host and ou should see a prompt like this: <pre><code>[INFO] Logging into provision-host container...\n[INFO] Type 'exit' to return to your local machine\n\nansible@lima-rancher-desktop:/mnt/urbalurbadisk$\n</code></pre></p>"},{"location":"overview-getting-started/#deploy-your-first-service","title":"Deploy Your First Service","text":"<p>Let's deploy a simple test service you can see in your browser:</p> <pre><code># Run the simple setup script\n./provision-host/kubernetes/99-test/not-in-use/01-setup-whoami-public.sh\n</code></pre> <p>The script will: - Test your Kubernetes connection - Deploy the whoami service and ingress - Wait for the pod to be ready - Test that the service responds</p> <p>The output will be:</p> <pre><code>.. many lines ...\n\nPLAY RECAP *************************************************************************************************************************************\nlocalhost                  : ok=17   changed=1    unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   \n\n\u2705 whoami deployment complete\n\ud83c\udf89 Open your browser to: http://whoami.localhost\n</code></pre> <p>When it completes successfully, open your browser to: http://whoami.localhost</p> <p>You'll see a page showing your request details - this proves your Kubernetes cluster and ingress are working perfectly!</p>"},{"location":"overview-getting-started/#monitor-your-cluster-with-k9s","title":"Monitor Your Cluster with k9s","text":"<p>k9s is a terminal-based Kubernetes dashboard that's already installed in the provision-host:</p> <pre><code># Start k9s to see your cluster\nk9s\n</code></pre> <p>k9s Navigation Tips: - 0 - Show all namespaces - :pods - List all pods - :svc - List all services - :deploy - List all deployments - Enter - View details of selected item - l - View logs of selected pod - q - Quit/go back</p> <p>What You'll See:</p> <p>A line like this: <pre><code>default      whoami-76575d99b4-t6q42      1/1   Running\n</code></pre></p> <ul> <li>And several system pods keeping Kubernetes running</li> </ul>"},{"location":"overview-getting-started/#whats-happening-behind-the-scenes","title":"\ud83d\udd27 What's Happening Behind the Scenes","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           Your Computer                     \u2502\n\u2502                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 Provision Host   \u2502  \u2502 Kubernetes      \u2502  \u2502\n\u2502  \u2502 Container        \u2502\u2500\u25ba\u2502 Cluster         \u2502  \u2502\n\u2502  \u2502                  \u2502  \u2502                 \u2502  \u2502\n\u2502  \u2502 \u2022 Installing...  \u2502  \u2502 \u2022 Starting...   \u2502  \u2502\n\u2502  \u2502 \u2022 Configuring... \u2502  \u2502 \u2022 Services...   \u2502  \u2502\n\u2502  \u2502 \u2022 Deploying...   \u2502  \u2502 \u2022 Ready!        \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                            \u25b2                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502        Web Browser                     \u2502 \u2502\n\u2502  \u2502  http://whoami.localhost               \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ol> <li>Provision Host downloads and configures all tools</li> <li>Kubernetes starts your local services</li> <li>Browser connects to the whoami-public and displays its parameters</li> </ol>"},{"location":"overview-getting-started/#how-to-remove-the-whoami-test","title":"How to remove the whoami test","text":"<pre><code># Run the simple setup script\n./provision-host/kubernetes/99-test/not-in-use/01-remove-whoami-public.sh\n</code></pre> <p>The service will be removed and you can verify it by using the <code>k9s</code></p>"},{"location":"overview-getting-started/#next-steps","title":"\ud83c\udfaf Next Steps","text":"<p>Once you have the basic system running:</p> <p>Explore Services: Read the services overview to understand what's available</p> <p>\ud83d\udca1 Goal: Get you from zero to a running local datacenter in 5 minutes with just a browser and two downloads!</p>"},{"location":"overview-infrastructure/","title":"Infrastructure Overview","text":"<p>File: <code>docs/overview-infrastructure.md</code> Purpose: Simple overview of the two-component infrastructure Target Audience: All users and developers Last Updated: September 22, 2024</p>"},{"location":"overview-infrastructure/#core-architecture","title":"\ud83c\udfd7\ufe0f Core Architecture","text":"<p>Urbalurba Infrastructure has a simple two-component design that runs entirely on your local development machine:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           Your Computer                     \u2502\n\u2502                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 Provision Host   \u2502  \u2502 Kubernetes      \u2502  \u2502\n\u2502  \u2502 Container        \u2502\u2500\u25ba\u2502 Cluster         \u2502  \u2502\n\u2502  \u2502                  \u2502  \u2502                 \u2502  \u2502\n\u2502  \u2502 \u2022 kubectl        \u2502  \u2502 \u2022 Services      \u2502  \u2502\n\u2502  \u2502 \u2022 ansible        \u2502  \u2502 \u2022 Storage       \u2502  \u2502\n\u2502  \u2502 \u2022 scripts        \u2502  \u2502 \u2022 Networking    \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                            \u25b2                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502        Web Browser                     \u2502 \u2502\n\u2502  \u2502  http://service.localhost              \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"overview-infrastructure/#1-provision-host-container","title":"1. Provision Host Container","text":"<p>Purpose of the provision-host is to manage the kubernetes cluster. All sw for doing this is installed in the container so that there is no need to install anything on your computer.</p> <p>A Docker container containing all management tools and scripts: - kubectl, helm, ansible - Infrastructure management tools - Cloud CLIs (az, aws, gcloud) - Cloud provider tools - Orchestration scripts - Automated service deployment - Configuration management - Secrets, manifests, playbooks</p>"},{"location":"overview-infrastructure/#2-kubernetes-cluster","title":"2. Kubernetes Cluster","text":"<p>Purpose is to set up the same services as your cloud provider (Azure) so that you can develop locally.</p> <p>A local Kubernetes cluster running on your machine: - Rancher Desktop (default) - Easy setup with GUI - Services - All applications run as Kubernetes workloads See overview-services.md for list of services and their Azure equivalents</p>"},{"location":"overview-infrastructure/#how-they-work-together","title":"\ud83d\udd04 How They Work Together","text":"<ol> <li>Management: All cluster operations happen from inside the provision-host container</li> <li>Deployment: Scripts in provision-host deploy services to the local Kubernetes cluster</li> <li>Access: Services are accessible via <code>http://service-name.localhost</code> URLs</li> <li>Development: Same environment works identically on any developer machine</li> </ol>"},{"location":"overview-infrastructure/#beyond-local-development","title":"\ud83c\udf10 Beyond Local Development","text":"<p>For production or remote development, the same provision-host can manage: - Azure AKS clusters - Production Kubernetes in the cloud - Azure VMs with MicroK8s - Dedicated remote environments - Multi-cluster setups - Development, staging, production environments</p> <p>The key advantage: same tools, same scripts, same processes whether running locally or in the cloud.</p>"},{"location":"overview-infrastructure/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>System Architecture - Detailed architectural diagrams</li> <li>Installation Guide - Get started in 2 steps</li> <li>Services Overview - List of services and their Azure equivalents</li> <li>Host Types - All deployment options </li> </ul>"},{"location":"overview-installation/","title":"Installation Guide","text":"<p>File: <code>docs/overview-installation.md</code> Purpose: Simple installation guide for Urbalurba Infrastructure Target Audience: New users and developers getting started Last Updated: September 22, 2024</p>"},{"location":"overview-installation/#overview","title":"\ud83d\udccb Overview","text":"<p>Urbalurba Infrastructure provides a complete datacenter environment on your laptop using Kubernetes and Docker. Installation requires just two steps: install Rancher Desktop, then download and run our installation script.</p>"},{"location":"overview-installation/#prerequisites","title":"\ud83d\udee0\ufe0f Prerequisites","text":"<ul> <li>Operating System: macOS, Windows, or Linux</li> <li>Hardware: 8GB+ RAM recommended, 10GB+ free disk space</li> <li>Internet Connection: Required for downloading components</li> </ul>"},{"location":"overview-installation/#step-1-install-rancher-desktop","title":"\ud83d\udce6 Step 1: Install Rancher Desktop","text":"<p>Rancher Desktop provides Kubernetes and Docker for your local development environment.</p> <ol> <li>Download Rancher Desktop from the official website:</li> <li> <p>\ud83c\udf10 https://rancherdesktop.io/</p> </li> <li> <p>Install and configure:</p> </li> <li>Follow the installation instructions for your operating system</li> <li>Start Rancher Desktop and enable Kubernetes</li> <li> <p>Allocate at least 8GB RAM and 4 CPU cores for optimal performance</p> </li> <li> <p>Verify installation:    <pre><code># Check that Kubernetes is running\nkubectl version --client\n\n# Check that Docker is available\ndocker version\n</code></pre></p> </li> </ol> <p>\ud83d\udcdd Note: If you have Docker Desktop installed, uninstall it first as it conflicts with Rancher Desktop.</p>"},{"location":"overview-installation/#step-2-download-urbalurba-infrastructure","title":"\ud83d\ude80 Step 2: Download Urbalurba Infrastructure","text":"<p>Download the latest release ZIP file from GitHub:</p> <ol> <li>Go to releases page: https://github.com/terchris/urbalurba-infrastructure/releases</li> <li>Download the latest release: Click on <code>urbalurba-infrastructure.zip</code></li> <li>Extract the ZIP file to your desired folder</li> </ol> <p>Or use command line:</p> <pre><code># Download latest release\ncurl -L https://github.com/terchris/urbalurba-infrastructure/releases/latest/download/urbalurba-infrastructure.zip -o urbalurba-infrastructure.zip\n\n# Extract\nunzip urbalurba-infrastructure.zip\n\n# Navigate into the folder\ncd urbalurba-infrastructure\n</code></pre>"},{"location":"overview-installation/#what-you-get","title":"What You Get","text":"<p>The infrastructure package contains: - Kubernetes manifests - All service definitions - Provision scripts - Setup and management tools - Documentation - Complete guides and references - Configuration files - Ready-to-use configurations</p>"},{"location":"overview-installation/#verification","title":"\u2705 Verification","text":"<p>After downloading and extracting, verify you have the infrastructure package:</p> <pre><code># Check the main folders are present\nls -la urbalurba-infrastructure/\n</code></pre>"},{"location":"overview-installation/#next-steps-deploy-services","title":"\ud83c\udf10 Next Steps - Deploy Services","text":"<p>After downloading the infrastructure package, you'll need to deploy the services to your Kubernetes cluster:</p> <ol> <li>Follow the Getting Started Guide in the downloaded package for deployment instructions</li> <li>Use the provision scripts to set up and deploy services</li> <li>Access services once deployed at <code>*.localhost</code> domains:</li> </ol>"},{"location":"overview-services/","title":"Services Overview","text":"<p>File: <code>docs/overview-services.md</code> Purpose: Complete table of services available in Urbalurba Infrastructure vs Azure equivalents Target Audience: Architects, developers, and infrastructure engineers Last Updated: September 22, 2024</p>"},{"location":"overview-services/#platform-services-comparison","title":"\ud83d\udccb Platform Services Comparison","text":"<p>This table shows the comprehensive services available in Urbalurba Infrastructure compared to their Azure equivalents:</p> Functionality Azure Service Urbalurba Service Status Container Orchestration Azure Kubernetes Service (AKS) Rancher Desktop / MicroK8s \u2705 Active Reverse Proxy &amp; Load Balancer Azure Application Gateway Traefik \u2705 Active Web Server Azure Static Web Apps NGINX \u2705 Active Primary Database Azure Database for PostgreSQL PostgreSQL (Custom) \u2705 Active Cache &amp; Session Store Azure Cache for Redis Redis \u2705 Active Authentication &amp; SSO Azure Active Directory Authentik \u2705 Active AI Chat Interface Azure OpenAI Service OpenWebUI \u2705 Active LLM Proxy &amp; Router Azure API Management LiteLLM \u2705 Active Document Processing Azure AI Document Intelligence Apache Tika \u2705 Available Vector Database Azure AI Search Qdrant \u2705 Available Monitoring &amp; Dashboards Azure Monitor Grafana \u2705 Available Metrics Collection Azure Monitor Prometheus \u2705 Available Log Aggregation Azure Log Analytics Loki \u2705 Available Distributed Tracing Azure Application Insights Tempo \u2705 Available Observability Azure Application Insights OpenTelemetry \u2705 Available Database Admin Azure Portal pgAdmin \u2705 Available NoSQL Database Azure Cosmos DB MongoDB \u2705 Available Alternative SQL Database Azure Database for MySQL MySQL \u2705 Available Search Engine Azure AI Search Elasticsearch \u2705 Available Message Broker Azure Service Bus RabbitMQ \u2705 Available API Gateway Azure API Management Gravitee \u2705 Available Data Processing Azure Databricks Apache Spark \u2705 Available Notebook Environment Azure Machine Learning JupyterHub \u2705 Available Data Catalog Microsoft Purview Unity Catalog \u274c Container Broken VPN Connectivity Azure VPN Gateway Tailscale \u2705 Available GitOps &amp; CD Azure DevOps ArgoCD \u2705 Available Prometheus Stack Azure Monitor Prometheus + Grafana Stack \u2705 Available Test Services - Whoami (public/protected) \u2705 Active"},{"location":"overview-services/#service-categories","title":"\ud83c\udfaf Service Categories","text":""},{"location":"overview-services/#core-infrastructure-always-active","title":"Core Infrastructure (Always Active)","text":"<ul> <li>Kubernetes - Container orchestration platform</li> <li>Traefik - Reverse proxy and ingress controller</li> <li>NGINX - Web server and static content</li> <li>PostgreSQL - Primary relational database</li> <li>Redis - Cache and session management</li> <li>Authentik - Authentication and single sign-on</li> </ul>"},{"location":"overview-services/#ai-platform-active-by-default","title":"AI Platform (Active by Default)","text":"<ul> <li>OpenWebUI - Modern chat interface for LLMs</li> <li>LiteLLM - Universal LLM proxy and router</li> <li>Tika - Document processing and extraction</li> <li>Qdrant - Vector database for embeddings</li> </ul>"},{"location":"overview-services/#observability-stack-available","title":"Observability Stack (Available)","text":"<ul> <li>Grafana - Monitoring dashboards and visualization</li> <li>Prometheus - Metrics collection and alerting</li> <li>Loki - Log aggregation and analysis</li> <li>Tempo - Distributed tracing</li> <li>OpenTelemetry - Observability instrumentation</li> </ul>"},{"location":"overview-services/#data-platform-available","title":"Data Platform (Available)","text":"<ul> <li>Apache Spark - Distributed data processing</li> <li>JupyterHub - Multi-user notebook environment</li> <li>Unity Catalog - Data governance and cataloging \u274c NOT WORKING (container permission issues)</li> <li>MongoDB - Document database</li> <li>MySQL - Alternative SQL database</li> <li>Elasticsearch - Full-text search engine</li> </ul>"},{"location":"overview-services/#management-tools-available","title":"Management Tools (Available)","text":"<ul> <li>pgAdmin - PostgreSQL administration interface</li> <li>ArgoCD - GitOps continuous delivery</li> <li>Gravitee - API lifecycle management</li> <li>RabbitMQ - Message queuing system</li> <li>Tailscale - Secure VPN connectivity</li> </ul>"},{"location":"overview-services/#status-legend","title":"\ud83d\udcca Status Legend","text":"<ul> <li>\u2705 Active - Deployed by default, ready to use</li> <li>\u2705 Available - Can be deployed on-demand via scripts</li> <li>\u274c Container Broken - Service exists but Docker images have permission/startup issues</li> <li>\ud83d\udd27 Manual - Requires manual configuration</li> <li>\u26a0\ufe0f Beta - Experimental or testing phase</li> </ul>"},{"location":"overview-services/#getting-started","title":"\ud83d\ude80 Getting Started","text":"<ol> <li>Core Services - Automatically deployed with <code>./provision-host/kubernetes/provision-kubernetes.sh</code></li> <li>AI Platform - Deploy with <code>./provision-host/kubernetes/07-ai/01-setup-litellm-openwebui.sh</code></li> <li>Additional Services - Use individual setup scripts in respective folders</li> <li>Access - All services available via <code>http://service-name.localhost</code></li> </ol>"},{"location":"overview-services/#service-access","title":"\ud83c\udf10 Service Access","text":"<p>All services are accessible through consistent localhost URLs:</p> <ul> <li>OpenWebUI: http://openwebui.localhost</li> <li>Authentik: http://authentik.localhost</li> <li>Grafana: http://grafana.localhost</li> <li>pgAdmin: http://pgadmin.localhost</li> <li>ArgoCD: http://argocd.localhost</li> </ul>"},{"location":"overview-services/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>System Architecture - Technical architecture details</li> <li>AI Platform - AI services configuration</li> <li>Authentication - SSO setup and management</li> <li>Hosts - Deployment environments</li> </ul> <p>\ud83d\udca1 Value Proposition: Urbalurba Infrastructure provides 85% of Azure's enterprise capabilities in a local development environment, enabling teams to build, test, and iterate without cloud dependencies or costs.</p>"},{"location":"overview-system-architecture/","title":"Urbalurba Infrastructure System Architecture","text":"<p>File: <code>docs/overview-system-architecture.md</code> Purpose: High-level overview of the two-tier architecture: provision-host + cluster Target Audience: Developers, architects, and anyone needing to understand the overall system design Last Updated: September 20, 2024</p>"},{"location":"overview-system-architecture/#overview","title":"\ud83c\udfd7\ufe0f Overview","text":"<p>The Urbalurba Infrastructure follows a two-tier architecture that separates cluster management from cluster workloads. This design provides a clean separation of concerns, ensuring all management tooling is centralized and the cluster remains focused on running applications.</p> <pre><code>graph TB\n    subgraph \"Host Machine\"\n        HOST[Host Operating System&lt;br/&gt;macOS / Linux / Windows]\n\n        subgraph \"Provision Host Container\"\n            PH[provision-host&lt;br/&gt;Management Environment]\n            TOOLS[All Management Tools&lt;br/&gt;kubectl, helm, ansible&lt;br/&gt;cloud CLIs, etc.]\n            SCRIPTS[Orchestration Scripts&lt;br/&gt;provision-kubernetes.sh&lt;br/&gt;category-based automation]\n            CONFIG[Configuration&lt;br/&gt;playbooks, manifests&lt;br/&gt;secrets, kubeconfig]\n        end\n\n        subgraph \"Kubernetes Cluster\"\n            STORAGE[Storage Systems&lt;br/&gt;PostgreSQL, Redis]\n            NETWORK[Networking&lt;br/&gt;Traefik Ingress]\n            AUTH[Authentication&lt;br/&gt;Authentik SSO]\n            AI[AI Platform&lt;br/&gt;OpenWebUI, LiteLLM]\n            APPS[Other Applications&lt;br/&gt;Custom Services]\n            MGMT[Management Tools&lt;br/&gt;pgAdmin, ArgoCD]\n        end\n    end\n\n    HOST --&gt; PH\n    PH -.-&gt;|\"Manages &amp; Deploys\"| STORAGE\n    PH -.-&gt;|\"Manages &amp; Deploys\"| NETWORK\n    PH -.-&gt;|\"Manages &amp; Deploys\"| AUTH\n    PH -.-&gt;|\"Manages &amp; Deploys\"| AI\n    PH -.-&gt;|\"Manages &amp; Deploys\"| APPS\n    PH -.-&gt;|\"Manages &amp; Deploys\"| MGMT\n</code></pre>"},{"location":"overview-system-architecture/#core-architecture-principles","title":"\ud83c\udfaf Core Architecture Principles","text":""},{"location":"overview-system-architecture/#1-separation-of-management-and-runtime","title":"1. Separation of Management and Runtime","text":"<ul> <li>Provision Host: Contains ALL tools needed to manage the cluster</li> <li>Cluster: Runs ONLY application workloads and services</li> <li>Clean Interface: Management happens through standard Kubernetes APIs</li> </ul>"},{"location":"overview-system-architecture/#2-self-contained-management-environment","title":"2. Self-Contained Management Environment","text":"<ul> <li>Containerized Tooling: All management tools isolated in provision-host container</li> <li>Version Consistency: Same tool versions across all environments</li> <li>Portable: Works identically on any host machine</li> </ul>"},{"location":"overview-system-architecture/#3-declarative-configuration","title":"3. Declarative Configuration","text":"<ul> <li>Infrastructure as Code: All cluster state defined in manifests</li> <li>Reproducible Deployments: Same configuration produces identical results</li> <li>Version Controlled: All configuration tracked in Git</li> </ul>"},{"location":"overview-system-architecture/#tier-1-provision-host-management-layer","title":"\ud83c\udfd7\ufe0f Tier 1: Provision Host (Management Layer)","text":"<p>The provision-host is a containerized management environment that contains ALL tools and scripts needed to manage the Kubernetes cluster.</p>"},{"location":"overview-system-architecture/#whats-inside-the-provision-host","title":"What's Inside the Provision Host","text":"<pre><code>/mnt/urbalurbadisk/               # Mounted working directory\n\u251c\u2500\u2500 provision-host/               # Container setup scripts\n\u2502   \u251c\u2500\u2500 provision-host-00-coresw.sh      # Core software installation\n\u2502   \u251c\u2500\u2500 provision-host-01-cloudproviders.sh  # Cloud provider CLIs\n\u2502   \u251c\u2500\u2500 provision-host-02-kubetools.sh   # Kubernetes tooling\n\u2502   \u251c\u2500\u2500 provision-host-03-net.sh         # Network configuration\n\u2502   \u251c\u2500\u2500 provision-host-04-helmrepo.sh    # Helm repository setup\n\u2502   \u2514\u2500\u2500 kubernetes/               # Orchestration scripts\n\u2502       \u251c\u2500\u2500 provision-kubernetes.sh      # Main orchestration engine\n\u2502       \u251c\u2500\u2500 01-core/                    # Core infrastructure scripts\n\u2502       \u251c\u2500\u2500 02-databases/               # Data service scripts\n\u2502       \u251c\u2500\u2500 07-ai/                      # AI platform scripts\n\u2502       \u2514\u2500\u2500 [nn]-[category]/            # Other service categories\n\u251c\u2500\u2500 ansible/                      # Automation engine\n\u2502   \u251c\u2500\u2500 playbooks/                # Service deployment logic\n\u2502   \u2514\u2500\u2500 inventory/                # Environment configurations\n\u251c\u2500\u2500 manifests/                    # Kubernetes manifests\n\u251c\u2500\u2500 troubleshooting/              # Debug scripts and tools\n\u251c\u2500\u2500 kubeconfig/                   # Kubernetes configuration files\n\u2502   \u251c\u2500\u2500 kubeconf-all              # Merged kubeconfig for all clusters\n\u2502   \u2514\u2500\u2500 rancher-desktop-kubeconf  # Rancher Desktop specific config\n\u251c\u2500\u2500 topsecret/                    # Secrets management\n\u2502   \u251c\u2500\u2500 kubernetes/               # Kubernetes secrets\n\u2502   \u251c\u2500\u2500 secrets-templates/        # Secret templates\n\u2502   \u2514\u2500\u2500 create-kubernetes-secrets.sh  # Secret generation script\n\u251c\u2500\u2500 scripts/                      # Additional utility scripts\n\u251c\u2500\u2500 networking/                   # Network configurations\n\u251c\u2500\u2500 hosts/                        # Host-specific configurations\n\u2514\u2500\u2500 cloud-init/                   # Cloud-init templates\n</code></pre> <p>Pre-installed Tools (via setup scripts): - kubectl, helm - Kubernetes management - ansible - Infrastructure automation - az, aws, gcloud - Cloud provider CLIs - tailscale - VPN tunnel for traffic in to the cluster - cloudlare - VPN tunnel for traffic in to the cluster - jq, yq - JSON/YAML processing - git, curl, wget - Development utilities</p>"},{"location":"overview-system-architecture/#management-capabilities","title":"Management Capabilities","text":"<ul> <li>\ud83d\ude80 Cluster Provisioning: Automated setup of entire infrastructure</li> <li>\ud83d\udce6 Service Deployment: Deploy services using Ansible + Helm</li> <li>\ud83d\udd27 Configuration Management: Manage secrets, configs, manifests</li> <li>\ud83d\udd0d Monitoring &amp; Debugging: Access logs, metrics, troubleshooting tools</li> <li>\u2601\ufe0f Cloud Integration: Deploy to AWS, Azure, GCP from same environment</li> </ul>"},{"location":"overview-system-architecture/#key-benefits","title":"Key Benefits","text":"<ul> <li>\u2705 Tool Consistency: Same versions across all environments</li> <li>\u2705 Reproducible: Identical setup process everywhere</li> <li>\u2705 Isolated: Management tools don't interfere with cluster workloads</li> <li>\u2705 Portable: Works on any machine with Rancher Desktop</li> <li>\u2705 Auditable: All management actions tracked and scripted</li> </ul>"},{"location":"overview-system-architecture/#tier-2-kubernetes-cluster-runtime-layer","title":"\ud83c\udfaa Tier 2: Kubernetes Cluster (Runtime Layer)","text":"<p>The provision-host runs on the host machine and manages the Kubernetes cluster which provides container orchestration for all application services.</p>"},{"location":"overview-system-architecture/#cluster-runtime-options","title":"Cluster Runtime Options","text":"Option Use Case Benefits Rancher Desktop Development, local testing Easy setup, GUI management MicroK8s Production-like local env Lightweight, production features Cloud K8s Production deployment Managed services, high availability"},{"location":"overview-system-architecture/#deployment-flow","title":"\ud83d\udd04 Deployment Flow","text":"<p>Two ways to start the services in the cluster.</p>"},{"location":"overview-system-architecture/#automated-provisioning-process","title":"Automated Provisioning Process","text":"<p>This is run when the cluster is built.  For detailed description see provision-host-kubernetes.md <pre><code>sequenceDiagram\n    participant Dev as Developer\n    participant PH as Provision Host\n    participant K8s as Kubernetes Cluster\n\n    Dev-&gt;&gt;PH: Execute provision-kubernetes.sh\n    PH-&gt;&gt;PH: Discovery: Find numbered directories (01, 02, 07...)\n    PH-&gt;&gt;PH: Discovery: Find numbered scripts in each directory\n\n    loop For each category (01-core, 02-databases, 07-ai...)\n        PH-&gt;&gt;PH: Execute category scripts in sequence\n        PH-&gt;&gt;K8s: Deploy services via Ansible + Helm\n        K8s--&gt;&gt;PH: Report deployment status\n        PH-&gt;&gt;PH: Verify service health\n    end\n\n    PH--&gt;&gt;Dev: Complete provisioning report\n</code></pre></p>"},{"location":"overview-system-architecture/#manual-service-management","title":"Manual Service Management","text":"<p>This is when you log in to the <code>provision-host</code>container and start a service. See the section Starting services in the overview-getting-started.md for an example on how to start a service.</p> <pre><code>sequenceDiagram\n    participant Dev as Developer\n    participant PH as Provision Host\n    participant K8s as Kubernetes Cluster\n\n    Dev-&gt;&gt;PH: Run specific service script\n    PH-&gt;&gt;PH: Execute Ansible playbook\n    PH-&gt;&gt;K8s: Deploy/update service\n    PH-&gt;&gt;K8s: Apply manifests\n    PH-&gt;&gt;K8s: Verify deployment\n    K8s--&gt;&gt;PH: Service status\n    PH--&gt;&gt;Dev: Deployment results\n</code></pre>"},{"location":"overview-system-architecture/#multi-cluster-management","title":"Multi-Cluster Management","text":"<p>The provision-host can manage multiple Kubernetes clusters simultaneously using merged kubeconfig files:</p> <pre><code>graph TB\n    subgraph \"Host Machine\"\n        PH[provision-host&lt;br/&gt;Management Container]\n        CONFIG[kubeconfig/kubeconf-all&lt;br/&gt;Merged Configuration]\n    end\n\n    subgraph \"Development\"\n        RD[rancher-desktop&lt;br/&gt;Local Development]\n    end\n\n    subgraph \"On-Premise\"\n        UMICROK8S[ubuntu-microk8s&lt;br/&gt;Dedicated Hardware]\n    end\n\n    subgraph \"Azure Cloud\"\n        AKS[azure-aks&lt;br/&gt;Production Cluster]\n        AMICROK8S[azure-microk8s&lt;br/&gt;Staging Cluster]\n    end\n\n    subgraph \"Google Cloud\"\n        GKE[google-gke&lt;br/&gt;GKE Cluster]\n    end\n\n    subgraph \"AWS Cloud\"\n        EKS[aws-eks&lt;br/&gt;EKS Cluster]\n    end\n\n    subgraph \"Edge Computing\"\n        RMICROK8S[raspberry-microk8s&lt;br/&gt;IoT/Edge Cluster]\n    end\n\n    PH --&gt; CONFIG\n    CONFIG -.-&gt; RD\n    CONFIG -.-&gt; UMICROK8S\n    CONFIG -.-&gt; AKS\n    CONFIG -.-&gt; AMICROK8S\n    CONFIG -.-&gt; GKE\n    CONFIG -.-&gt; EKS\n    CONFIG -.-&gt; RMICROK8S\n\n    PH -.-&gt;|\"Same tools &amp; scripts\"| RD\n    PH -.-&gt;|\"Same tools &amp; scripts\"| UMICROK8S\n    PH -.-&gt;|\"Same tools &amp; scripts\"| AKS\n    PH -.-&gt;|\"Same tools &amp; scripts\"| AMICROK8S\n    PH -.-&gt;|\"Same tools &amp; scripts\"| GKE\n    PH -.-&gt;|\"Same tools &amp; scripts\"| EKS\n    PH -.-&gt;|\"Same tools &amp; scripts\"| RMICROK8S\n</code></pre> <p>Key Benefits: - \ud83c\udfaf Single Management Point: One provision-host manages all clusters - \ud83d\udd27 Consistent Tooling: Same kubectl, helm, ansible across all environments - \ud83d\udccb Unified Configuration: Merged kubeconfig enables cluster switching with <code>kubectl config use-context</code> - \ud83d\ude80 Identical Deployments: Same manifests and playbooks work everywhere</p> <p>\ud83d\udca1 Key Takeaway: The Urbalurba Infrastructure is fundamentally about separation of concerns - the provision-host handles ALL management complexity, while the cluster focuses purely on running applications reliably and efficiently.</p>"},{"location":"package-ai-environment-management/","title":"AI Package Environment Management Guide","text":"<p>File: <code>docs/package-ai-environment-management.md</code> Purpose: Guide for managing the complete AI infrastructure environment Target Audience: Developers and administrators working with AI infrastructure Last Updated: September 19, 2024</p>"},{"location":"package-ai-environment-management/#overview","title":"\ud83d\udccb Overview","text":"<p>This cluster provides OpenWebUI integrated with LiteLLM proxy for unified model access:</p> <ul> <li><code>openwebui.localhost</code> - Main OpenWebUI environment with Authentik authentication</li> <li>Model Provider: LiteLLM proxy serving multiple model sources</li> <li>Authentication: OAuth2 with Authentik for user management</li> <li>Configuration: External ConfigMap management in <code>topsecret/kubernetes/kubernetes-secrets.yml</code></li> </ul> <p>The environment provides a single, production-ready OpenWebUI instance with enterprise authentication and centralized model management.</p>"},{"location":"package-ai-environment-management/#current-architecture","title":"\ud83c\udfaf Current Architecture","text":"Component Configuration Purpose OpenWebUI StatefulSet with persistent storage Web interface for AI interactions LiteLLM Proxy ConfigMap in topsecret/kubernetes/kubernetes-secrets.yml Unified model provider and API gateway Authentication OAuth2 with Authentik Enterprise user management and SSO Models Multiple sources via LiteLLM Local Ollama + Cloud providers Database Shared PostgreSQL User data, conversations, model configs Document Processing Apache Tika + Qdrant RAG pipeline for knowledge bases"},{"location":"package-ai-environment-management/#quick-start-commands","title":"\ud83d\ude80 Quick Start Commands","text":""},{"location":"package-ai-environment-management/#deploy-complete-ai-infrastructure","title":"Deploy Complete AI Infrastructure","text":""},{"location":"package-ai-environment-management/#automatic-deployment-during-cluster-rebuild","title":"Automatic Deployment (During Cluster Rebuild)","text":"<p>The AI infrastructure is automatically deployed during cluster provisioning via: <pre><code># This runs automatically via provision-kubernetes.sh\nprovision-host/kubernetes/07-ai/01-setup-litellm-openwebui.sh\n</code></pre></p> <p>\u26a0\ufe0f IMPORTANT: During automatic cluster rebuild: - \u2705 USE: <code>01-setup-litellm-openwebui.sh</code> (combined deployment) - \u274c DO NOT USE: Individual scripts (<code>02-setup-open-webui.sh</code>, <code>03-setup-litellm.sh</code>) - These individual scripts are kept in <code>not-in-use/</code> for manual troubleshooting only</p>"},{"location":"package-ai-environment-management/#manual-deployment-options","title":"Manual Deployment Options","text":"<pre><code># Option 1: Deploy using Ansible directly (RECOMMENDED for manual deployment)\ncd /mnt/urbalurbadisk\n\n# Step 1: Deploy LiteLLM first (required dependency)\nansible-playbook ansible/playbooks/210-setup-litellm.yml\n# \u23f3 Wait: ~2-3 minutes for LiteLLM to be verified as working\n\n# Step 2: Deploy OpenWebUI (depends on LiteLLM)\nansible-playbook ansible/playbooks/200-setup-open-webui.yml\n# \u23f3 Wait: ~5-10 minutes for OpenWebUI component setup\n\n# Option 2: Use orchestration script (automated sequencing)\n./scripts/packages/ai.sh\n</code></pre>"},{"location":"package-ai-environment-management/#access-openwebui","title":"Access OpenWebUI","text":"<pre><code># Access the web interface\nopen http://openwebui.localhost\n\n# Create admin account on first login\n# Configure OAuth users via Authentik admin panel\n</code></pre>"},{"location":"package-ai-environment-management/#manage-litellm-models","title":"Manage LiteLLM Models","text":"<pre><code># Edit model configuration\nvim topsecret/kubernetes/kubernetes-secrets.yml\n\n# Apply changes\n./copy2provisionhost.sh\ndocker exec -it provision-host bash -c \"cd /mnt/urbalurbadisk &amp;&amp; kubectl apply -f topsecret/kubernetes/kubernetes-secrets.yml\"\n\n# Restart LiteLLM to reload models\nkubectl rollout restart deployment/litellm -n ai\n</code></pre>"},{"location":"package-ai-environment-management/#configuration-management","title":"\ud83d\udd27 Configuration Management","text":""},{"location":"package-ai-environment-management/#1-litellm-model-configuration","title":"1. LiteLLM Model Configuration","text":"<p>Edit <code>topsecret/kubernetes/kubernetes-secrets.yml</code> to manage models: <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: litellm-config\n  namespace: ai\ndata:\n  config.yaml: |\n    general_settings:\n      master_key: os.environ/LITELLM_PROXY_MASTER_KEY\n    model_list:\n      - model_name: mac-gpt-oss-balanced\n        litellm_params:\n          model: ollama/gpt-oss:20b\n          api_base: \"http://host.lima.internal:11434\"\n          temperature: 0.7\n</code></pre></p>"},{"location":"package-ai-environment-management/#2-required-secrets","title":"2. Required Secrets","text":"<p>Ensure these secrets exist in <code>urbalurba-secrets</code>: - <code>LITELLM_PROXY_MASTER_KEY</code> - LiteLLM API authentication - <code>LITELLM_POSTGRESQL__USER</code> - Database username (<code>litellm</code>) - <code>LITELLM_POSTGRESQL__PASSWORD</code> - Database password - <code>OPENAI_API_KEY</code> - OpenAI API access (if using GPT models) - <code>ANTHROPIC_API_KEY</code> - Anthropic API access (if using Claude) - <code>AZURE_API_KEY</code> - Azure OpenAI access (if using Azure models)</p>"},{"location":"package-ai-environment-management/#developer-workflows","title":"\ud83d\udd04 Developer Workflows","text":""},{"location":"package-ai-environment-management/#workflow-1-model-development-and-testing","title":"Workflow 1: Model Development and Testing","text":"<pre><code># Access OpenWebUI\nopen http://openwebui.localhost\n\n# Login with Authentik OAuth (or create admin account)\n# Test different models from LiteLLM proxy\n# Upload documents for RAG testing\n\n# Benefits:\n# \u2705 Multiple model access via LiteLLM\n# \u2705 Enterprise authentication\n# \u2705 Document processing with Tika + Qdrant\n# \u2705 Persistent conversations and knowledge bases\n</code></pre>"},{"location":"package-ai-environment-management/#workflow-2-model-configuration-changes","title":"Workflow 2: Model Configuration Changes","text":"<pre><code># Edit LiteLLM models\nvim topsecret/kubernetes/kubernetes-secrets.yml\n\n# Apply configuration\n./copy2provisionhost.sh\ndocker exec -it provision-host bash -c \"cd /mnt/urbalurbadisk &amp;&amp; kubectl apply -f topsecret/kubernetes/kubernetes-secrets.yml\"\n\n# Restart LiteLLM to reload models\nkubectl rollout restart deployment/litellm -n ai\n\n# Test new models in OpenWebUI\nopen http://openwebui.localhost\n\n# Benefits:\n# \u2705 Centralized model configuration\n# \u2705 Quick model addition/removal\n# \u2705 Support for multiple providers\n# \u2705 Configuration persistence\n</code></pre>"},{"location":"package-ai-environment-management/#workflow-3-complete-infrastructure-management","title":"Workflow 3: Complete Infrastructure Management","text":"<pre><code># Remove entire AI infrastructure (from provision-host container)\ndocker exec -it provision-host bash -c \"cd /mnt/urbalurbadisk/provision-host/kubernetes/07-ai/not-in-use &amp;&amp; ./01-remove-litellm-openwebui.sh\"\n\n# Redeploy with new configuration\n./scripts/packages/ai.sh\n\n# Benefits:\n# \u2705 Clean slate deployment\n# \u2705 Configuration validation\n# \u2705 Full integration testing\n# \u2705 Infrastructure consistency\n</code></pre>"},{"location":"package-ai-environment-management/#verification-commands","title":"\ud83d\udd0d Verification Commands","text":""},{"location":"package-ai-environment-management/#check-ai-infrastructure-status","title":"Check AI Infrastructure Status","text":"<pre><code># Check all AI pods\nkubectl get pods -n ai\n\n# Check LiteLLM deployment\nkubectl get deployment litellm -n ai\n\n# Check OpenWebUI StatefulSet\nkubectl get statefulset open-webui -n ai\n\n# Check shared PostgreSQL databases\nkubectl exec -n default postgresql-0 -- psql -U postgres -c '\\l' | grep -E '(openwebui|litellm)'\n</code></pre>"},{"location":"package-ai-environment-management/#verify-litellm-configuration","title":"Verify LiteLLM Configuration","text":"<pre><code># Check ConfigMap exists\nkubectl get configmap litellm-config -n ai\n\n# View current model configuration\nkubectl get configmap litellm-config -n ai -o yaml\n\n# Test LiteLLM API\nkubectl port-forward svc/litellm 4000:4000 -n ai &amp;\nMASTER_KEY=$(kubectl get secret urbalurba-secrets -n ai -o jsonpath=\"{.data.LITELLM_PROXY_MASTER_KEY}\" | base64 --decode)\ncurl -X GET http://localhost:4000/v1/models -H \"Authorization: Bearer $MASTER_KEY\"\n</code></pre>"},{"location":"package-ai-environment-management/#check-openwebui-integration","title":"Check OpenWebUI Integration","text":"<pre><code># View OpenWebUI logs\nkubectl logs -f statefulset/open-webui -n ai\n\n# Check OpenWebUI environment variables\nkubectl get statefulset open-webui -n ai -o yaml | grep -A 5 -B 5 LITELLM\n\n# Test OpenWebUI access\nopen http://openwebui.localhost\n</code></pre>"},{"location":"package-ai-environment-management/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"package-ai-environment-management/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"package-ai-environment-management/#issue-openwebui-devlocalhost-shows-404","title":"Issue: openwebui-dev.localhost shows 404","text":"<pre><code># Check if authenticated environment is deployed\nkubectl get ingressroute open-webui-dev-auth -n ai\n\n# If not found, deploy the environment:\nkubectl apply -f manifests/211-openwebui-dev-oauth-secret.yaml\nkubectl apply -f manifests/213-openwebui-dev-statefulset.yaml\nkubectl apply -f manifests/215-openwebui-dev-service.yaml\nkubectl apply -f manifests/212-openwebui-dev-auth-ingress.yaml\n</code></pre>"},{"location":"package-ai-environment-management/#issue-no-continue-with-authentik-button","title":"Issue: No \\\"Continue with authentik\\\" button","text":"<pre><code># Check OAuth environment variables\nkubectl describe statefulset open-webui-dev -n ai | grep -A 10 Environment\n\n# Verify secret is mounted correctly\nkubectl get secret openwebui-dev-oauth -n ai -o yaml\n</code></pre>"},{"location":"package-ai-environment-management/#issue-oauth-redirect-error","title":"Issue: OAuth redirect error","text":"<pre><code># Check Authentik application configuration\n# Redirect URI must be: https://openwebui-dev.localhost/oauth/oidc/callback\n\n# Verify OpenWebUI OIDC configuration\nkubectl logs -n ai -l environment=development --tail=100 | grep -i oauth\n</code></pre>"},{"location":"package-ai-environment-management/#issue-resource-conflicts","title":"Issue: Resource conflicts","text":"<pre><code># If StatefulSet won't start, check for port conflicts\nkubectl describe statefulset open-webui-dev -n ai\n\n# Check if volumes are properly created\nkubectl get pvc -n ai -l environment=development\n</code></pre>"},{"location":"package-ai-environment-management/#additional-resources","title":"\ud83d\udcda Additional Resources","text":""},{"location":"package-ai-environment-management/#official-documentation","title":"Official Documentation","text":"<ul> <li>Authentik OpenWebUI Integration</li> <li>OpenWebUI Documentation</li> <li>Traefik IngressRoute Documentation</li> </ul>"},{"location":"package-ai-environment-management/#related-cluster-documentation","title":"Related Cluster Documentation","text":"<ul> <li>Traefik Ingress Rules: <code>docs/rules-ingress-traefik.md</code></li> <li>Authentik Setup: <code>manifests/075-authentik-config.yaml</code></li> <li>Infrastructure Overview: <code>docs/infrastructure-readme.md</code></li> </ul>"},{"location":"package-ai-environment-management/#summary","title":"\ud83c\udfaf Summary","text":"<p>This AI infrastructure management approach provides:</p>"},{"location":"package-ai-environment-management/#benefits","title":"\u2705 Benefits","text":"<ul> <li>Unified architecture - single production-ready environment with LiteLLM + OpenWebUI</li> <li>Centralized configuration - all model management through ConfigMap in kubernetes-secrets.yml</li> <li>Enterprise authentication - OAuth2 with Authentik for user and group management</li> <li>Multi-provider support - local Ollama + cloud providers through LiteLLM proxy</li> <li>Clean deployment - complete removal and reinstallation capabilities</li> </ul>"},{"location":"package-ai-environment-management/#recommended-workflow","title":"\ud83d\udd04 Recommended Workflow","text":"<ol> <li>Start with complete AI infrastructure deployment via <code>./scripts/packages/ai.sh</code></li> <li>Configure models by editing <code>topsecret/kubernetes/kubernetes-secrets.yml</code></li> <li>Manage user access through OpenWebUI admin panel and Authentik groups</li> <li>Test changes by restarting LiteLLM deployment after configuration updates</li> <li>Clean reinstall when needed using removal scripts + redeploy</li> </ol> <p>This approach provides a production-ready AI platform with enterprise features while maintaining developer-friendly configuration management.</p>"},{"location":"package-ai-litellm-client-key-setup/","title":"LiteLLM Client API Key Setup - Claude Code Integration","text":"<p>File: <code>docs/package-ai-litellm-client-key-setup.md</code> Purpose: Guide for generating and managing LiteLLM client API keys for Claude Code DevContainer integration Target Audience: Developers, DevOps engineers using Claude Code in DevContainers Last Updated: November 23, 2025</p>"},{"location":"package-ai-litellm-client-key-setup/#overview","title":"\ud83d\udccb Overview","text":"<p>LiteLLM uses a two-key system for security and access management:</p> <ul> <li>Master Key: Stored in <code>urbalurba-secrets</code>, used for administrative operations (key generation, management)</li> <li>Client Keys: Generated via API, stored in LiteLLM's PostgreSQL database, scoped to specific models</li> </ul> <p>This guide covers generating client API keys for developers using Claude Code in DevContainers.</p> <p>Why Client Keys: - \u2705 Scoped access to specific models only - \u2705 Usage tracking per developer/client - \u2705 Cost attribution and monitoring - \u2705 Independent key rotation without affecting other users - \u2705 Revocable without impacting master key</p>"},{"location":"package-ai-litellm-client-key-setup/#quick-setup","title":"\ud83d\ude80 Quick Setup","text":""},{"location":"package-ai-litellm-client-key-setup/#step-1-get-the-master-key","title":"Step 1: Get the Master Key","text":"<pre><code>MASTER_KEY=$(kubectl get secret urbalurba-secrets -n ai -o jsonpath=\"{.data.LITELLM_PROXY_MASTER_KEY}\" | base64 --decode)\necho \"Master key retrieved: ${MASTER_KEY:0:10}...\"\n</code></pre>"},{"location":"package-ai-litellm-client-key-setup/#step-2-port-forward-to-litellm","title":"Step 2: Port Forward to LiteLLM","text":"<pre><code>kubectl port-forward -n ai svc/litellm 4000:4000 &amp;\nsleep 3\n</code></pre> <p>Note: Keep this running in the background. You can stop it later with <code>pkill -f \"kubectl port-forward.*litellm\"</code>.</p>"},{"location":"package-ai-litellm-client-key-setup/#step-3-generate-client-api-key","title":"Step 3: Generate Client API Key","text":"<p>IMPORTANT: Replace <code>developer-name</code> with the actual developer's name (e.g., <code>john-doe</code>, <code>jane-smith</code>).</p> <pre><code>DEVELOPER_NAME=\"developer-name\"  # CHANGE THIS\n\ncurl -X POST http://localhost:4000/key/generate \\\n  -H \"Authorization: Bearer $MASTER_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\n    \\\"models\\\": [\\\"claude-sonnet-4-5-20250929\\\", \\\"claude-3-opus-20240229\\\"],\n    \\\"key_alias\\\": \\\"claude-code-${DEVELOPER_NAME}\\\",\n    \\\"duration\\\": null\n  }\"\n</code></pre> <p>Response Example: <pre><code>{\n  \"key\": \"sk-xxxxxxxxxxxxxxxxxx\",\n  \"key_alias\": \"claude-code-developer-name\",\n  \"key_name\": null,\n  \"expires\": null,\n  \"models\": [\"claude-sonnet-4-5-20250929\", \"claude-3-opus-20240229\"],\n  \"created_at\": \"2025-11-23T15:30:00.000000Z\",\n  \"...\": \"...\"\n}\n</code></pre></p> <p>\u26a0\ufe0f SAVE THE <code>key</code> VALUE! This is the only time you'll see it. The developer needs this for their DevContainer.</p>"},{"location":"package-ai-litellm-client-key-setup/#step-4-provide-key-to-developer","title":"Step 4: Provide Key to Developer","text":"<p>Send the developer: 1. The <code>key</code> value (e.g., <code>sk-xxxxxxxxxxxxxxxxxx</code>) 2. Instructions to add it to their DevContainer</p> <p>Developer should run: <pre><code># In their DevContainer\nbash .devcontainer/additions/config-claude-code.sh\n</code></pre></p> <p>When prompted, they enter the client API key you generated.</p>"},{"location":"package-ai-litellm-client-key-setup/#step-5-clean-up","title":"Step 5: Clean Up","text":"<pre><code># Stop the port-forward\npkill -f \"kubectl port-forward.*litellm\"\n</code></pre>"},{"location":"package-ai-litellm-client-key-setup/#verification","title":"\ud83d\udd0d Verification","text":""},{"location":"package-ai-litellm-client-key-setup/#test-the-client-key","title":"Test the Client Key","text":"<p>From cluster (Mac host): <pre><code>CLIENT_KEY=\"sk-xxxxxxxxxxxxxxxxxx\"  # The generated key\n\n# Port-forward if not already running\nkubectl port-forward -n ai svc/litellm 4000:4000 &amp;\n\n# Test models endpoint\ncurl -H \"Authorization: Bearer $CLIENT_KEY\" \\\n  http://localhost:4000/v1/models\n\n# Should return models list including:\n# - claude-sonnet-4-5-20250929\n# - claude-3-opus-20240229\n</code></pre></p> <p>From DevContainer: <pre><code># Test through nginx reverse proxy\ncurl -H \"Authorization: Bearer $CLIENT_KEY\" \\\n  http://localhost:8080/v1/models\n</code></pre></p> <p>Expected Response: <pre><code>{\n  \"data\": [\n    {\"id\": \"claude-sonnet-4-5-20250929\", \"object\": \"model\", ...},\n    {\"id\": \"claude-3-opus-20240229\", \"object\": \"model\", ...}\n  ],\n  \"object\": \"list\"\n}\n</code></pre></p>"},{"location":"package-ai-litellm-client-key-setup/#key-management","title":"\ud83d\udee0\ufe0f Key Management","text":""},{"location":"package-ai-litellm-client-key-setup/#list-all-client-keys","title":"List All Client Keys","text":"<pre><code>curl -H \"Authorization: Bearer $MASTER_KEY\" \\\n  http://localhost:4000/key/info\n</code></pre>"},{"location":"package-ai-litellm-client-key-setup/#view-specific-key-info","title":"View Specific Key Info","text":"<pre><code>CLIENT_KEY_HASH=\"1ffd879a...\"  # From key info response\n\ncurl -H \"Authorization: Bearer $MASTER_KEY\" \\\n  \"http://localhost:4000/key/info?key=${CLIENT_KEY_HASH}\"\n</code></pre>"},{"location":"package-ai-litellm-client-key-setup/#delete-a-client-key","title":"Delete a Client Key","text":"<p>Use when: - Developer leaves the team - Key is compromised - Regular key rotation</p> <pre><code>curl -X DELETE http://localhost:4000/key/delete \\\n  -H \"Authorization: Bearer $MASTER_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"keys\": [\"sk-key-to-delete\"]}'\n</code></pre>"},{"location":"package-ai-litellm-client-key-setup/#rotate-keys","title":"Rotate Keys","text":"<p>Best Practice: Rotate keys quarterly or when a developer changes roles.</p> <pre><code># 1. Generate new key (follow Step 3)\n# 2. Provide new key to developer\n# 3. Developer updates DevContainer configuration\n# 4. Verify new key works\n# 5. Delete old key (optional, for security)\n</code></pre>"},{"location":"package-ai-litellm-client-key-setup/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-ai-litellm-client-key-setup/#error-key-not-allowed-to-access-model","title":"Error: \"Key not allowed to access model\"","text":"<p>Symptom: <pre><code>{\n  \"error\": {\n    \"message\": \"key not allowed to access model. This key can only access models=['claude-sonnet-4.5', 'claude-3-opus']\",\n    ...\n  }\n}\n</code></pre></p> <p>Cause: Key was generated with short model names, but Claude Code requests full names.</p> <p>Solution: Regenerate key with full model names: <pre><code># \u274c Wrong:\n\"models\": [\"claude-sonnet-4.5\", \"claude-3-opus\"]\n\n# \u2705 Correct:\n\"models\": [\"claude-sonnet-4-5-20250929\", \"claude-3-opus-20240229\"]\n</code></pre></p>"},{"location":"package-ai-litellm-client-key-setup/#error-invalid-model-name","title":"Error: \"Invalid model name\"","text":"<p>Symptom: <pre><code>{\n  \"error\": \"completion: Invalid model name passed in model=claude-sonnet-4-5-20250929\"\n}\n</code></pre></p> <p>Cause: LiteLLM ConfigMap doesn't have model name aliases.</p> <p>Solution: Verify ConfigMap has both short and full model names: <pre><code>kubectl get configmap litellm-config -n ai -o yaml | grep \"model_name:\"\n</code></pre></p> <p>Should show: - <code>claude-sonnet-4.5</code> (short name) - <code>claude-sonnet-4-5-20250929</code> (full name)</p> <p>If missing, apply the updated configuration.</p>"},{"location":"package-ai-litellm-client-key-setup/#error-authentication-error-no-api-key-passed-in","title":"Error: \"Authentication Error, No api key passed in\"","text":"<p>Cause: Master key or client key not provided correctly.</p> <p>Solutions: - Verify key copied correctly (no extra spaces/newlines) - Check Authorization header format: <code>Authorization: Bearer sk-xxxxx</code> - Verify port-forward is running: <code>lsof -i :4000</code></p>"},{"location":"package-ai-litellm-client-key-setup/#port-forward-keeps-dying","title":"Port-Forward Keeps Dying","text":"<p>Cause: Network interruption or kubectl timeout.</p> <p>Solution: Run in persistent loop: <pre><code>while true; do\n  kubectl port-forward -n ai svc/litellm 4000:4000\n  echo \"Port forward died, restarting in 5 seconds...\"\n  sleep 5\ndone\n</code></pre></p>"},{"location":"package-ai-litellm-client-key-setup/#security-notes","title":"\ud83d\udd12 Security Notes","text":""},{"location":"package-ai-litellm-client-key-setup/#best-practices","title":"Best Practices","text":"<ul> <li>\u2705 Generate unique keys per developer - Use developer name in alias</li> <li>\u2705 Scope to minimum models needed - Only add models developer will use</li> <li>\u2705 Set expiration for temporary access - Use <code>duration</code> parameter (e.g., <code>\"30d\"</code>)</li> <li>\u2705 Rotate keys quarterly - Regular rotation improves security</li> <li>\u2705 Revoke on developer departure - Delete keys when team members leave</li> <li>\u2705 Monitor usage - Check LiteLLM logs for unusual activity</li> <li>\u2705 Never commit keys to git - Store in <code>.env</code> files (gitignored)</li> </ul>"},{"location":"package-ai-litellm-client-key-setup/#what-not-to-do","title":"What NOT to Do","text":"<ul> <li>\u274c Never share the master key - Only use for admin operations</li> <li>\u274c Don't reuse client keys - Each developer gets unique key</li> <li>\u274c Don't commit keys to repos - Always gitignore <code>.env</code> files</li> <li>\u274c Don't set unlimited duration without reason - Use expiration when possible</li> <li>\u274c Don't grant access to all models - Scope to needed models only</li> </ul>"},{"location":"package-ai-litellm-client-key-setup/#key-storage","title":"Key Storage","text":"<p>Developer DevContainer: - Keys stored in gitignored <code>.env</code> files - Loaded by DevContainer configuration scripts - Never committed to version control</p> <p>LiteLLM Database: - Client keys stored in PostgreSQL - Backed up with standard database backups - Retrievable via master key API calls</p>"},{"location":"package-ai-litellm-client-key-setup/#usage-monitoring","title":"\ud83d\udcca Usage Monitoring","text":""},{"location":"package-ai-litellm-client-key-setup/#track-key-usage","title":"Track Key Usage","text":"<pre><code># Get spending per key\ncurl -H \"Authorization: Bearer $MASTER_KEY\" \\\n  http://localhost:4000/spend/tags\n</code></pre>"},{"location":"package-ai-litellm-client-key-setup/#view-request-logs","title":"View Request Logs","text":"<pre><code># Check LiteLLM logs for specific key\nkubectl logs -n ai deployment/litellm | grep \"claude-code-developer-name\"\n</code></pre> <p>Metrics Tracked: - Request count per model - Token usage (input/output) - Cost per request - Success/failure rates - Timestamp and model used</p>"},{"location":"package-ai-litellm-client-key-setup/#related-documentation","title":"\ud83d\udd17 Related Documentation","text":"<ul> <li>package-ai-litellm.md - Main LiteLLM documentation</li> <li>package-ai-readme.md - AI services overview</li> <li>DevContainer nginx configuration - <code>.devcontainer/additions/nginx/</code></li> </ul>"},{"location":"package-ai-litellm-client-key-setup/#notes","title":"\ud83d\udcdd Notes","text":"<ul> <li>Client keys are independent of cluster rebuilds - Stored in LiteLLM's PostgreSQL database</li> <li>Master key rotation requires regenerating all client keys</li> <li>DevContainer nginx auto-adds <code>Host: litellm.localhost</code> header for Traefik routing</li> <li>Claude Code bug #2182: Host header override workaround via nginx proxy</li> </ul> <p>Generated: 2025-11-23 Maintained by: Platform Engineering Team</p>"},{"location":"package-ai-litellm/","title":"LiteLLM AI Proxy Setup Guide","text":"<p>This document explains how LiteLLM is configured in the Urbalurba infrastructure and how to add/configure AI models.</p>"},{"location":"package-ai-litellm/#overview","title":"Overview","text":"<p>LiteLLM is deployed as a unified AI model proxy that provides OpenAI-compatible API endpoints for multiple model sources including: - Local Ollama instances (in-cluster and external) - Cloud AI providers (OpenAI, Anthropic, Google, etc.) - Custom model endpoints</p>"},{"location":"package-ai-litellm/#architecture","title":"Architecture","text":"<pre><code>Applications \u2192 LiteLLM Proxy \u2192 Model Sources\n                   \u2193\n            Shared PostgreSQL\n</code></pre>"},{"location":"package-ai-litellm/#key-components","title":"Key Components","text":"<ul> <li>LiteLLM Pod: Main proxy service (<code>ai</code> namespace)</li> <li>Shared PostgreSQL: Database for configuration, keys, and usage tracking</li> <li>ConfigMap: Model configuration and routing rules</li> <li>Ingress: External access via <code>http://litellm.localhost</code></li> </ul>"},{"location":"package-ai-litellm/#database-setup","title":"Database Setup","text":"<p>LiteLLM uses a dedicated database on the shared PostgreSQL instance: - Database: <code>litellm</code> - User: <code>litellm</code> - Host: <code>postgresql.default.svc.cluster.local:5432</code></p>"},{"location":"package-ai-litellm/#database-management","title":"Database Management","text":"<p>Create database: <pre><code>cd /mnt/urbalurbadisk\nansible-playbook ansible/playbooks/utility/u10-litellm-create-postgres.yml -e operation=create\n</code></pre></p> <p>Delete database (\u26a0\ufe0f DESTRUCTIVE): <pre><code>ansible-playbook ansible/playbooks/utility/u10-litellm-create-postgres.yml -e operation=delete -e force_delete=true\n</code></pre></p>"},{"location":"package-ai-litellm/#configuration-management","title":"Configuration Management","text":"<p>LiteLLM configuration is managed via external ConfigMap in <code>topsecret/kubernetes/kubernetes-secrets.yml</code>. The Helm chart is configured to use this existing ConfigMap rather than creating its own.</p> <p>Helm Configuration (<code>manifests/220-litellm-config.yaml</code>): <pre><code># Use existing ConfigMap instead of inline config\nconfigMapRef:\n  name: litellm-config\n  key: config.yaml\n\n# Disable Helm-managed ConfigMap creation\nproxyConfigMap:\n  create: false\n  name: litellm-config\n</code></pre></p> <p>ConfigMap Definition (<code>topsecret/kubernetes/kubernetes-secrets.yml</code>): <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: litellm-config\n  namespace: ai\ndata:\n  config.yaml: |\n    general_settings:\n      master_key: os.environ/LITELLM_PROXY_MASTER_KEY\n    model_list:\n      - model_name: mac-gpt-oss-balanced\n        litellm_params:\n          model: ollama/gpt-oss:20b\n          api_base: \"http://host.lima.internal:11434\"\n          temperature: 0.7\n</code></pre></p>"},{"location":"package-ai-litellm/#adding-new-models","title":"Adding New Models","text":""},{"location":"package-ai-litellm/#1-ollama-models-local","title":"1. Ollama Models (Local)","text":"<p>In-cluster Ollama: <pre><code>- model_name: qwen3-0.6b-incluster\n  litellm_params:\n    model: ollama/qwen3:0.6b\n    api_base: \"http://ollama.ai.svc.cluster.local:11434\"\n</code></pre></p> <p>External Ollama (Mac/Host): <pre><code>- model_name: external-llama3\n  litellm_params:\n    model: ollama/llama3:8b\n    api_base: \"http://host.lima.internal:11434\"\n    temperature: 0.7\n</code></pre></p>"},{"location":"package-ai-litellm/#2-cloud-providers","title":"2. Cloud Providers","text":"<p>OpenAI: <pre><code>- model_name: gpt-4o\n  litellm_params:\n    model: gpt-4o\n    api_key: \"os.environ/OPENAI_API_KEY\"\n</code></pre></p> <p>Anthropic Claude: <pre><code>- model_name: claude-3-sonnet\n  litellm_params:\n    model: anthropic/claude-3-sonnet-20240229\n    api_key: \"os.environ/ANTHROPIC_API_KEY\"\n</code></pre></p> <p>Google Gemini: <pre><code>- model_name: gemini-pro\n  litellm_params:\n    model: gemini/gemini-pro\n    api_key: \"os.environ/GOOGLE_API_KEY\"\n</code></pre></p>"},{"location":"package-ai-litellm/#3-model-variants-with-different-temperatures","title":"3. Model Variants with Different Temperatures","text":"<pre><code>- model_name: mac-gpt-oss-creative\n  litellm_params:\n    model: ollama/gpt-oss:20b\n    api_base: \"http://host.lima.internal:11434\"\n    temperature: 0.9\n\n- model_name: mac-gpt-oss-precise\n  litellm_params:\n    model: ollama/gpt-oss:20b\n    api_base: \"http://host.lima.internal:11434\"\n    temperature: 0.3\n</code></pre>"},{"location":"package-ai-litellm/#4-fallback-configuration","title":"4. Fallback Configuration","text":"<pre><code>- model_name: gpt-4-with-fallback\n  litellm_params:\n    model: gpt-4\n    api_key: \"os.environ/OPENAI_API_KEY\"\n    fallbacks:\n      - model: ollama/llama3:8b\n        api_base: \"http://host.lima.internal:11434\"\n</code></pre>"},{"location":"package-ai-litellm/#deployment-process","title":"Deployment Process","text":""},{"location":"package-ai-litellm/#1-update-configuration","title":"1. Update Configuration","text":"<p>Edit the ConfigMap in <code>topsecret/kubernetes/kubernetes-secrets.yml</code></p>"},{"location":"package-ai-litellm/#2-apply-changes","title":"2. Apply Changes","text":"<pre><code># Copy files to provision-host\n./copy2provisionhost.sh\n\n# Apply from provision-host container\ndocker exec -it provision-host bash -c \"cd /mnt/urbalurbadisk &amp;&amp; kubectl apply -f topsecret/kubernetes/kubernetes-secrets.yml\"\n</code></pre>"},{"location":"package-ai-litellm/#3-restart-litellm","title":"3. Restart LiteLLM","text":"<pre><code>kubectl rollout restart deployment/litellm -n ai\n</code></pre>"},{"location":"package-ai-litellm/#4-verify-models","title":"4. Verify Models","text":"<pre><code># Port forward to access API\nkubectl port-forward svc/litellm 4000:4000 -n ai\n\n# Get master key\nMASTER_KEY=$(kubectl get secret urbalurba-secrets -n ai -o jsonpath=\"{.data.LITELLM_PROXY_MASTER_KEY}\" | base64 --decode)\n\n# List available models\ncurl -X GET http://localhost:4000/v1/models -H \"Authorization: Bearer $MASTER_KEY\"\n</code></pre>"},{"location":"package-ai-litellm/#full-installation","title":"Full Installation","text":"<p>Use the Ansible playbook for complete setup:</p> <pre><code>cd /mnt/urbalurbadisk\nansible-playbook ansible/playbooks/210-setup-litellm.yml\n</code></pre> <p>This playbook: 1. Creates the PostgreSQL database 2. Deploys LiteLLM via Helm 3. Applies ingress configuration 4. Verifies installation</p>"},{"location":"package-ai-litellm/#api-usage","title":"API Usage","text":""},{"location":"package-ai-litellm/#authentication","title":"Authentication","text":"<p>All requests require the master key: <pre><code>Authorization: Bearer $MASTER_KEY\n</code></pre></p>"},{"location":"package-ai-litellm/#list-models","title":"List Models","text":"<pre><code>curl -X GET http://localhost:4000/v1/models \\\n  -H \"Authorization: Bearer $MASTER_KEY\"\n</code></pre>"},{"location":"package-ai-litellm/#chat-completion","title":"Chat Completion","text":"<pre><code>curl -X POST http://localhost:4000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $MASTER_KEY\" \\\n  -d '{\n    \"model\": \"mac-gpt-oss-balanced\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n</code></pre>"},{"location":"package-ai-litellm/#environment-variables","title":"Environment Variables","text":"<p>Required secrets in <code>urbalurba-secrets</code>: - <code>LITELLM_PROXY_MASTER_KEY</code>: API authentication (secure random key) - <code>LITELLM_POSTGRESQL__USER</code>: Database username (<code>litellm</code>) - <code>LITELLM_POSTGRESQL__PASSWORD</code>: Database password (secure random password) - <code>OPENAI_API_KEY</code>: OpenAI API access (if using OpenAI models) - <code>ANTHROPIC_API_KEY</code>: Anthropic API access (if using Claude) - <code>GOOGLE_API_KEY</code>: Google API access (if using Gemini)</p>"},{"location":"package-ai-litellm/#troubleshooting","title":"Troubleshooting","text":""},{"location":"package-ai-litellm/#check-pod-status","title":"Check Pod Status","text":"<pre><code>kubectl get pods -n ai\nkubectl logs -f deployment/litellm -n ai\n</code></pre>"},{"location":"package-ai-litellm/#database-connection-issues","title":"Database Connection Issues","text":"<pre><code># Test database connectivity\nkubectl exec -it litellm-xxx -n ai -- psql postgresql://litellm:$DB_PASSWORD@postgresql.default.svc.cluster.local:5432/litellm\n</code></pre>"},{"location":"package-ai-litellm/#model-not-available","title":"Model Not Available","text":"<ol> <li>Verify model configuration in ConfigMap</li> <li>Check API keys for cloud providers</li> <li>Ensure Ollama is running and accessible</li> <li>Review LiteLLM logs for specific errors</li> </ol>"},{"location":"package-ai-litellm/#configuration-reload","title":"Configuration Reload","text":"<pre><code>kubectl rollout restart deployment/litellm -n ai\nkubectl rollout status deployment/litellm -n ai\n</code></pre>"},{"location":"package-ai-litellm/#access-points","title":"Access Points","text":"<ul> <li>Internal: <code>http://litellm.ai.svc.cluster.local:4000</code></li> <li>External: <code>http://litellm.localhost</code> (via Traefik)</li> <li>Port Forward: <code>kubectl port-forward svc/litellm 4000:4000 -n ai</code></li> </ul>"},{"location":"package-ai-litellm/#litellm-admin-ui-access","title":"LiteLLM Admin UI Access","text":"<p>\u26a0\ufe0f IMPORTANT: The LiteLLM Admin UI with authentication is an Enterprise/Premium feature only.</p>"},{"location":"package-ai-litellm/#free-version-current-setup","title":"Free Version (Current Setup):","text":"<ul> <li>\u2705 API Access: Full API functionality available</li> <li>\u2705 Model Management: Via API calls and OpenWebUI interface</li> <li>\u274c Web Admin UI: No authentication available (requires Enterprise license)</li> </ul>"},{"location":"package-ai-litellm/#enterprise-version-paid","title":"Enterprise Version (Paid):","text":"<ul> <li>\u2705 Authenticated Web UI: Username/password protection</li> <li>\u2705 Advanced Features: SSO, RBAC, audit logging</li> <li>\u2705 Dashboard Access: Full web-based management interface</li> </ul>"},{"location":"package-ai-litellm/#alternative-access","title":"Alternative Access:","text":"<p>Since the web UI requires a paid license, use OpenWebUI (<code>http://openwebui.localhost</code>) as your primary interface for: - Model selection and management - Chat interface with all LiteLLM models - User authentication via Authentik integration</p>"},{"location":"package-ai-litellm/#best-practices","title":"Best Practices","text":"<ol> <li>Model Naming: Use descriptive names indicating source and characteristics</li> <li>Temperature Variants: Create separate model entries for different use cases</li> <li>Fallbacks: Configure local models as fallbacks for cloud models</li> <li>API Keys: Store sensitive keys in Kubernetes secrets, reference as <code>os.environ/KEY_NAME</code></li> <li>Testing: Always verify model availability after configuration changes</li> <li>Monitoring: Check logs regularly for authentication and connectivity issues</li> </ol>"},{"location":"package-ai-litellm/#complete-ai-infrastructure-setup","title":"Complete AI Infrastructure Setup","text":""},{"location":"package-ai-litellm/#using-the-orchestration-script","title":"Using the Orchestration Script","text":"<p>For a complete AI infrastructure deployment with both LiteLLM and OpenWebUI:</p> <pre><code># From host machine\nscripts/packages/ai.sh\n\n# This runs the complete orchestration inside provision-host container\n</code></pre> <p>The orchestration performs: 1. LiteLLM Setup: Database creation + Helm deployment + ConfigMap configuration 2. OpenWebUI Setup: Database setup + Tika deployment + OpenWebUI with LiteLLM integration 3. Ingress Configuration: External access via <code>openwebui.localhost</code> and <code>litellm.localhost</code></p>"},{"location":"package-ai-litellm/#manual-component-installation","title":"Manual Component Installation","text":"<p>LiteLLM Only: <pre><code>cd /mnt/urbalurbadisk\nansible-playbook ansible/playbooks/210-setup-litellm.yml\n</code></pre></p> <p>OpenWebUI Only (requires LiteLLM running): <pre><code>cd /mnt/urbalurbadisk\nansible-playbook ansible/playbooks/200-setup-open-webui.yml -e deploy_ollama_incluster=false\n</code></pre></p>"},{"location":"package-ai-litellm/#final-configuration","title":"Final Configuration","text":"<p>After deployment, configure OpenWebUI to use LiteLLM:</p> <ol> <li>Access OpenWebUI: <code>http://openwebui.localhost</code></li> <li>Create Admin User: First login creates admin account</li> <li>Configure LiteLLM Connection:</li> <li>Go to Settings \u2192 Connections</li> <li>URL: <code>http://litellm.ai.svc.cluster.local:4000/v1</code></li> <li>Auth: Bearer</li> <li>API Key: <code>$(kubectl get secret urbalurba-secrets -n ai -o jsonpath=\"{.data.LITELLM_PROXY_MASTER_KEY}\" | base64 --decode)</code></li> <li>Save and Refresh: All LiteLLM models will appear in OpenWebUI</li> </ol>"},{"location":"package-ai-litellm/#integration-with-openwebui","title":"Integration with OpenWebUI","text":"<p>LiteLLM integrates seamlessly with OpenWebUI: 1. OpenWebUI configured to use LiteLLM as OpenAI-compatible backend 2. All LiteLLM models appear in OpenWebUI model dropdown 3. Arena mode available for model comparison 4. Single authentication point for all AI providers 5. Shared PostgreSQL database for both services 6. Unified ingress access via Traefik</p>"},{"location":"package-ai-openwebui-model-access-setup/","title":"OpenWebUI Model Access Setup Guide","text":"<p>This guide covers how to configure model access for users in the OpenWebUI environment that integrates with LiteLLM proxy and Authentik authentication.</p>"},{"location":"package-ai-openwebui-model-access-setup/#litellm-integration-model-access","title":"LiteLLM Integration Model Access","text":""},{"location":"package-ai-openwebui-model-access-setup/#overview","title":"Overview","text":"<p>OpenWebUI is configured to use LiteLLM as the primary model provider, which gives access to multiple model sources through a unified interface. Models are configured in the LiteLLM ConfigMap and automatically discovered by OpenWebUI.</p>"},{"location":"package-ai-openwebui-model-access-setup/#model-sources-available","title":"Model Sources Available:","text":"<ul> <li>Local Ollama models (in-cluster and external)</li> <li>Cloud providers (OpenAI, Anthropic, Azure, Google)</li> <li>Custom model configurations with different parameters</li> </ul>"},{"location":"package-ai-openwebui-model-access-setup/#current-model-configuration","title":"Current Model Configuration:","text":"<p>Available models from LiteLLM ConfigMap (<code>topsecret/kubernetes/kubernetes-secrets.yml</code>): - <code>mac-gpt-oss-balanced</code> - Local Ollama model with balanced temperature - <code>mac-gpt-oss-creative</code> - Local Ollama model with high temperature - <code>mac-gpt-oss-precise</code> - Local Ollama model with low temperature - <code>external-ollama-gemma3</code> - External Ollama Gemma model - <code>gpt-4o</code> - OpenAI GPT-4 Omni - <code>azure-gpt-4</code> - Azure OpenAI GPT-4 - <code>claude-3-opus</code> - Anthropic Claude 3 Opus</p>"},{"location":"package-ai-openwebui-model-access-setup/#oauth-user-model-access-configuration","title":"OAuth User Model Access Configuration","text":""},{"location":"package-ai-openwebui-model-access-setup/#default-behavior","title":"Default Behavior","text":"<p>When OAuth users log in via Authentik, newly discovered models from LiteLLM proxy default to \"Private\" visibility for security reasons.</p>"},{"location":"package-ai-openwebui-model-access-setup/#admin-configuration-steps","title":"Admin Configuration Steps:","text":"<ol> <li>Login as admin user (local account, not OAuth)</li> <li>Navigate to Admin Panel \u2192 Settings \u2192 Models</li> <li>For each LiteLLM model you want OAuth users to access:</li> <li>Find the model in the list (e.g., <code>mac-gpt-oss-balanced</code>)</li> <li>Change \"Visibility\" from \"Private\" to \"Public\"</li> <li>Configure \"Whitelist\" if specific user groups should have access</li> <li>Click \"Save &amp; Update\"</li> <li>Test with OAuth user - they should now see the models in dropdown</li> </ol>"},{"location":"package-ai-openwebui-model-access-setup/#security-recommendations","title":"Security Recommendations:","text":"<ul> <li>\u2705 Local models: Safe to make public (free, no API costs)</li> <li>\u26a0\ufe0f Cloud models: Carefully control access (paid API usage)</li> <li>\ud83d\udd12 Premium models: Keep private or whitelist specific groups</li> <li>\ud83d\udcca Cost tracking: Monitor usage through LiteLLM admin interface</li> </ul>"},{"location":"package-ai-openwebui-model-access-setup/#group-based-access-control","title":"Group-Based Access Control:","text":"<p>Configure model access by Authentik groups: 1. Admin Panel \u2192 Settings \u2192 Models 2. Select model \u2192 Advanced Settings 3. Whitelist specific groups (matches Authentik group names) 4. Apply group restrictions for cost-sensitive models</p>"},{"location":"package-ai-openwebui-model-access-setup/#litellm-model-management","title":"LiteLLM Model Management","text":""},{"location":"package-ai-openwebui-model-access-setup/#adding-new-models","title":"Adding New Models","text":"<p>To add new models to the system:</p> <ol> <li> <p>Edit ConfigMap in <code>topsecret/kubernetes/kubernetes-secrets.yml</code>:    <pre><code>model_list:\n  - model_name: new-model-name\n    litellm_params:\n      model: provider/model-id\n      api_key: \"os.environ/API_KEY_NAME\"\n</code></pre></p> </li> <li> <p>Apply changes:    <pre><code>./copy2provisionhost.sh\ndocker exec -it provision-host bash -c \"cd /mnt/urbalurbadisk &amp;&amp; kubectl apply -f topsecret/kubernetes/kubernetes-secrets.yml\"\n</code></pre></p> </li> <li> <p>Restart LiteLLM:    <pre><code>kubectl rollout restart deployment/litellm -n ai\n</code></pre></p> </li> <li> <p>Configure model visibility in OpenWebUI admin panel</p> </li> </ol>"},{"location":"package-ai-openwebui-model-access-setup/#cost-management","title":"Cost Management","text":"<ul> <li>Free models: Local Ollama models have no ongoing costs</li> <li>Paid models: Cloud provider models charge per token/request</li> <li>Monitoring: Check LiteLLM logs for usage and costs</li> <li>Budget control: Use model whitelisting for expensive models</li> </ul>"},{"location":"package-ai-openwebui-model-access-setup/#troubleshooting","title":"Troubleshooting:","text":"<ul> <li>OAuth user still can't see models? Check that model visibility is set to \"Public\"</li> <li>New models not appearing? They default to \"Private\" - admin must make them \"Public\"</li> <li>Admin can see models but OAuth user cannot? This is expected behavior with \"Private\" models</li> <li>Models not loading from LiteLLM? Check ConfigMap format and restart LiteLLM deployment</li> <li>API errors for cloud models? Verify API keys are set correctly in secrets</li> </ul>"},{"location":"package-ai-readme/","title":"AI Package","text":"<p>The AI package is a comprehensive self-hosted AI platform that enables organizations to build and deploy AI applications with advanced document processing and knowledge management capabilities. This implementation is based on the Open WebUI project, which provides a powerful foundation for building AI applications. We've enhanced and customized it to better suit enterprise needs and specific use cases.</p>"},{"location":"package-ai-readme/#getting-started","title":"Getting started","text":"<p>On your host computer, run the following command to install the AI package:</p> <pre><code>./scripts/packages/ai.sh\n</code></pre> <p>The script will install the AI package and start the Open WebUI frontend. You can then access the Open WebUI frontend at http://openwebui.localhost.</p> <p>The install takes a while to complete as it sets up the LiteLLM proxy and OpenWebUI components. No models are downloaded into the cluster - all models are accessed through the LiteLLM proxy.</p> <p>To use models, you should install Ollama on your host computer and configure the LiteLLM proxy to access it. When running on your host computer, Ollama is able to use the GPU and access more memory for larger, smarter models.</p>"},{"location":"package-ai-readme/#checking-installation-progress","title":"Checking installation progress","text":"<p>You are not supposed to know anything about kubernetes so we have a script that you can run on your host computer to check the progress of the installation.</p> <pre><code>./scripts/manage/k9s.sh \n</code></pre> <p>This will show you a list of whats going on in the cluster. You just need to wait until you see  <code>Running</code> on all items on the list.</p>"},{"location":"package-ai-readme/#installing-ollama-on-your-host-computer","title":"Installing ollama on your host computer","text":"<p>Go to ollama.com and download the ollama binary for your platform.</p> <p>There are documentation on how to install ollama on your platform on the ollama documentation on github.</p> <p>TODO: add more doc here so people can pull a model and run it locally. There must be someone that has written a good guide for this.</p>"},{"location":"package-ai-readme/#technical-stuff","title":"Technical stuff","text":""},{"location":"package-ai-readme/#implementation-differences-from-open-webui","title":"Implementation Differences from Open WebUI","text":"<p>While our implementation is based on Open WebUI, we've made several significant modifications to enhance its capabilities and better suit enterprise needs:</p>"},{"location":"package-ai-readme/#1-vector-database","title":"1. Vector Database","text":"<ul> <li>Original: Uses ChromaDB as the default vector database</li> <li>Our Implementation: Uses PostgreSQL with pgvector extension</li> <li>Leverages existing shared database infrastructure</li> <li>Simplified deployment and maintenance</li> <li>Better integration with enterprise databases</li> <li>Reduced resource requirements</li> </ul>"},{"location":"package-ai-readme/#2-document-processing","title":"2. Document Processing","text":"<ul> <li>Original: Uses embedded Tika server</li> <li>Our Implementation: Deployed standalone Tika server</li> <li>Better resource isolation</li> <li>Improved scalability</li> <li>Independent scaling of document processing</li> <li>Enhanced reliability</li> </ul>"},{"location":"package-ai-readme/#3-llm-integration","title":"3. LLM Integration","text":"<ul> <li>Original: Direct integration with Ollama and OpenAI-compatible APIs</li> <li>Our Implementation: Uses LiteLLM as a central proxy</li> <li>Unified interface for all LLM providers</li> <li>Advanced fallback mechanisms</li> <li>Better cost tracking and monitoring</li> <li>Enhanced rate limiting and access control</li> <li>Support for multiple API providers through a single interface</li> </ul>"},{"location":"package-ai-readme/#4-storage-architecture","title":"4. Storage Architecture","text":"<ul> <li>Original: Uses embedded storage solutions</li> <li>Our Implementation: Kubernetes-native persistent storage</li> <li>Better data persistence</li> <li>Improved backup capabilities</li> <li>Enhanced scalability</li> <li>Better resource management</li> </ul>"},{"location":"package-ai-readme/#5-deployment-architecture","title":"5. Deployment Architecture","text":"<ul> <li>Original: Designed for simpler deployments</li> <li>Our Implementation: Kubernetes-native deployment</li> <li>Better scalability</li> <li>Enhanced reliability</li> <li>Improved resource management</li> <li>Better integration with enterprise infrastructure</li> </ul>"},{"location":"package-ai-readme/#6-security-enhancements","title":"6. Security Enhancements","text":"<ul> <li>Original: Basic security features</li> <li>Our Implementation: Enhanced security features</li> <li>Centralized API key management</li> <li>Advanced access control</li> <li>Better secret management</li> <li>Enhanced audit capabilities</li> </ul>"},{"location":"package-ai-readme/#7-monitoring-and-management","title":"7. Monitoring and Management","text":"<ul> <li>Original: Basic monitoring capabilities</li> <li>Our Implementation: Enhanced monitoring and management</li> <li>Detailed cost tracking</li> <li>Usage analytics</li> <li>Better resource monitoring</li> <li>Enhanced troubleshooting capabilities</li> </ul> <p>Key features include:</p> <ul> <li>Knowledge Base Management: </li> <li>Document ingestion and processing through a RAG (Retrieval-Augmented Generation) pipeline</li> <li>Support for various document formats (PDF, Word, Excel, PowerPoint)</li> <li>Integration with Apache Tika for advanced document extraction</li> <li> <p>Vector database storage using PostgreSQL with pgvector extension</p> </li> <li> <p>Chat Interface:</p> </li> <li>ChatGPT-like interface for querying knowledge bases</li> <li>Support for multiple LLM providers through LiteLLM proxy</li> <li>Local model support via Ollama</li> <li>Markdown and LaTeX support for rich text interactions</li> <li>Code execution capabilities via Pyodide</li> <li> <p>Mermaid diagram rendering for visualizations</p> </li> <li> <p>Collaboration Features:</p> </li> <li>Multi-user support with role-based access control</li> <li>Team-based knowledge sharing</li> <li>User groups and granular permissions</li> <li>Shared workspaces and chat histories</li> <li> <p>Webhook integrations for notifications (Discord, Slack, Teams)</p> </li> <li> <p>Mobile &amp; Accessibility:</p> </li> <li>Progressive Web App (PWA) support for mobile devices</li> <li>Responsive design for desktop and mobile</li> <li>Speech-to-text integration</li> <li> <p>Offline capabilities when hosted locally</p> </li> <li> <p>Security &amp; Administration:</p> </li> <li>Granular user permissions and access control</li> <li>LDAP authentication support</li> <li>API key management</li> <li>Model whitelisting</li> <li>Rate limiting and usage monitoring</li> <li> <p>Toxic content filtering</p> </li> <li> <p>Integration Capabilities:</p> </li> <li>Support for multiple OpenAI-compatible APIs</li> <li>Custom database integration (SQLite, Postgres)</li> <li>External speech-to-text services</li> <li>Web search integration for RAG</li> <li> <p>Custom pipeline support for extended functionality</p> </li> <li> <p>Organizational Features:</p> </li> <li>Centralized API Key Management:<ul> <li>Secure sharing of API keys across departments</li> <li>Support for multiple LLM providers (OpenAI, Anthropic, Azure, etc.)</li> <li>Virtual key management for different teams/projects</li> <li>Rate limiting and usage quotas per department</li> </ul> </li> <li>Cost Management:<ul> <li>Detailed cost tracking per department/user</li> <li>Usage monitoring and analytics</li> <li>Budget management and alerts</li> <li>Integration with logging tools (Lunary, MLflow, Langfuse, Helicone)</li> </ul> </li> <li>LLM Gateway Features:<ul> <li>Unified interface for accessing 100+ LLM models</li> <li>Automatic retry and fallback logic</li> <li>Consistent output format across providers</li> <li>Load balancing across multiple deployments</li> </ul> </li> </ul> <p>The platform is designed to operate entirely offline while maintaining enterprise-grade security and scalability features. It provides organizations with a secure, cost-effective way to leverage multiple LLM providers while maintaining control over usage and costs.</p>"},{"location":"package-ai-readme/#system-architecture","title":"System Architecture","text":"<p>The AI platform uses LiteLLM as a central proxy for all LLM interactions, providing unified access to multiple AI providers through a single interface.</p> <pre><code>graph TD\n    subgraph \"Document Processing\"\n        A[Document Upload] --&gt; B[Tika Server]\n        B --&gt; C[Text Extraction]\n        C --&gt; D[Vector Embeddings]\n    end\n\n    subgraph \"Knowledge Bases (PostgreSQL)\"\n        D --&gt; E[Internal KB]\n        D --&gt; F[Public KB]\n        D --&gt; G[Department KB]\n    end\n\n    subgraph \"LLM Integration\"\n        H[Ollama Local] --&gt; I[LiteLLM Proxy]\n        J[OpenAI] --&gt; I\n        K[Anthropic] --&gt; I\n        L[Azure] --&gt; I\n    end\n\n    subgraph \"User Interface\"\n        M[Open WebUI] --&gt; N[Chat Interface]\n        N --&gt; O[Query Processing]\n        O --&gt; P[Response Generation]\n    end\n\n    E --&gt; I\n    F --&gt; I\n    G --&gt; I\n    I --&gt; O\n</code></pre>"},{"location":"package-ai-readme/#use-cases","title":"Use Cases","text":""},{"location":"package-ai-readme/#1-document-processing-and-knowledge-base-creation","title":"1. Document Processing and Knowledge Base Creation","text":"<pre><code>sequenceDiagram\n    participant User\n    participant OpenWebUI\n    participant Tika\n    participant Qdrant\n    participant LiteLLM\n\n    User-&gt;&gt;OpenWebUI: Upload Document\n    OpenWebUI-&gt;&gt;Tika: Extract Text\n    Tika--&gt;&gt;OpenWebUI: Extracted Text\n    OpenWebUI-&gt;&gt;LiteLLM: Generate Embeddings\n    LiteLLM--&gt;&gt;OpenWebUI: Document Embeddings\n    OpenWebUI-&gt;&gt;PostgreSQL: Store in Knowledge Base\n    PostgreSQL--&gt;&gt;OpenWebUI: Confirmation\n    OpenWebUI--&gt;&gt;User: Document Processed\n</code></pre>"},{"location":"package-ai-readme/#2-querying-knowledge-bases-with-different-llms","title":"2. Querying Knowledge Bases with Different LLMs","text":"<pre><code>graph LR\n    subgraph \"Knowledge Base Types\"\n        A[Internal Documents] --&gt; B[Local LLM]\n        C[Public Documents] --&gt; D[External LLMs]\n        E[Department Docs] --&gt; F[Custom LLM Config]\n    end\n\n    subgraph \"LLM Selection\"\n        B --&gt; G[Ollama]\n        D --&gt; H[ChatGPT]\n        D --&gt; I[Claude]\n        D --&gt; J[Azure]\n        F --&gt; K[Department LLM]\n    end\n\n    subgraph \"Query Flow\"\n        L[User Query] --&gt; M[Knowledge Base Selection]\n        M --&gt; N[LLM Selection]\n        N --&gt; O[Response Generation]\n    end\n</code></pre>"},{"location":"package-ai-readme/#3-multi-user-collaboration","title":"3. Multi-User Collaboration","text":"<pre><code>graph TD\n    subgraph \"User Groups\"\n        A[Department A] --&gt; D[Shared KB]\n        B[Department B] --&gt; D\n        C[Department C] --&gt; D\n    end\n\n    subgraph \"Access Control\"\n        D --&gt; E[Role-Based Access]\n        E --&gt; F[Permissions]\n        F --&gt; G[Model Access]\n    end\n\n    subgraph \"Collaboration\"\n        H[Chat Sharing] --&gt; I[Team Workspaces]\n        I --&gt; J[Shared History]\n        J --&gt; K[Knowledge Sharing]\n    end\n</code></pre> <p>The platform supports flexible LLM configuration per knowledge base: - Internal documents can be configured to use local Ollama models for enhanced privacy - Public documents can leverage external LLMs (OpenAI, Anthropic, Azure) through LiteLLM - Department-specific knowledge bases can have custom LLM configurations - All configurations are managed through the LiteLLM proxy, providing unified access and cost tracking</p>"},{"location":"package-ai-readme/#open-webui-stack-setup","title":"Open WebUI Stack Setup","text":"<p>The AI stack is set up using an Ansible playbook (<code>200-setup-open-webui.yml</code>) that deploys a complete AI infrastructure on Kubernetes. The stack consists of several key components:</p>"},{"location":"package-ai-readme/#core-components","title":"Core Components","text":"<ol> <li>Persistent Storage</li> <li>Provides persistent storage for all AI components</li> <li>Ensures data persistence across pod restarts</li> <li> <p>Kubernetes Persistent Volumes Documentation</p> </li> <li> <p>Apache Tika</p> </li> <li>Document processing and extraction server</li> <li>Used for handling various document formats</li> <li>Helm chart: <code>tika/tika</code></li> <li>Apache Tika Official Website</li> <li> <p>Tika Helm Chart</p> </li> <li> <p>LiteLLM</p> </li> <li>LLM proxy service and gateway</li> <li>Acts as a central dispatcher for all LLM requests</li> <li>Enables integration with various LLM providers</li> <li>Supports OpenAI, Anthropic, Azure, and other providers</li> <li>Provides detailed cost tracking and usage analytics</li> <li>Manages API keys and access control</li> <li>Implements rate limiting and fallback strategies</li> <li>Configuration: Uses external ConfigMap in <code>topsecret/kubernetes/kubernetes-secrets.yml</code></li> <li>Database: Uses shared PostgreSQL (database: <code>litellm</code>, user: <code>litellm</code>)</li> <li>Helm chart: <code>oci://ghcr.io/berriai/litellm-helm</code></li> <li>LiteLLM Official Website</li> <li>LiteLLM Documentation</li> <li> <p>LiteLLM Helm Chart</p> </li> <li> <p>Open WebUI</p> </li> <li>An extensible, feature-rich, and user-friendly self-hosted AI platform</li> <li>Designed to operate entirely offline</li> <li>Supports various LLM runners like Ollama and OpenAI-compatible APIs</li> <li>Features a built-in inference engine for RAG (Retrieval-Augmented Generation)</li> <li>Provides a powerful AI deployment solution with enterprise capabilities</li> <li>Helm chart: <code>open-webui/open-webui</code></li> <li>Open WebUI Official Website</li> <li>Open WebUI Documentation</li> <li>Open WebUI GitHub</li> <li>Open WebUI Helm Chart</li> </ol>"},{"location":"package-ai-readme/#open-webui-custom-configuration","title":"Open WebUI Custom Configuration","text":"<p>The default Open WebUI Helm chart has been customized to better integrate with our AI stack:</p>"},{"location":"package-ai-readme/#disabled-components","title":"Disabled Components","text":"<ul> <li>Embedded Ollama (using LiteLLM proxy instead)</li> <li>Built-in Tika server (using standalone Tika deployment)</li> <li>WebSocket support (not required for our setup)</li> <li>Redis cluster (not required for our setup)</li> </ul>"},{"location":"package-ai-readme/#enabled-features","title":"Enabled Features","text":"<ul> <li>Document processing pipelines</li> <li>Persistent storage using existing PVC</li> <li>Integration with LiteLLM proxy for LLM access</li> <li>PostgreSQL with pgvector for vector database storage</li> <li>Standalone Tika server for document extraction</li> </ul>"},{"location":"package-ai-readme/#resource-configuration","title":"Resource Configuration","text":"<ul> <li>Memory: 768Mi request, 1.5Gi limit</li> <li>CPU: 300m request, 600m limit</li> </ul>"},{"location":"package-ai-readme/#key-integrations","title":"Key Integrations","text":"<ol> <li>LiteLLM Proxy</li> <li>Connected via <code>OPENAI_API_BASE: http://litellm:4000</code></li> <li>Uses master key from Kubernetes secrets</li> <li> <p>Enables access to multiple LLM providers</p> </li> <li> <p>Document Processing</p> </li> <li>Uses standalone Tika server at <code>http://tika:9998</code></li> <li> <p>Configured for document extraction and processing</p> </li> <li> <p>Vector Database</p> </li> <li>Uses PostgreSQL with pgvector extension</li> <li>Database: <code>openwebui</code></li> <li> <p>Connection via shared PostgreSQL service</p> </li> <li> <p>Embedding Model</p> </li> <li>Uses <code>all-MiniLM-L6-v2</code> for RAG embeddings</li> <li>Configured for efficient document processing</li> </ol>"},{"location":"package-ai-readme/#technical-notes","title":"Technical Notes","text":"<ul> <li>Model Access via LiteLLM Proxy:</li> <li>No models are deployed in the cluster</li> <li>All model access is routed through LiteLLM proxy</li> <li>Configure Ollama on your host computer for local model access</li> <li>Cloud models (OpenAI, Anthropic, etc.) are accessed via API keys in LiteLLM configuration</li> </ul>"},{"location":"package-ai-readme/#configuration-and-requirements","title":"Configuration and Requirements","text":"<p>The setup requires: - A Kubernetes cluster - Helm package manager - Required API keys stored in Kubernetes secrets:   - <code>LITELLM_PROXY_MASTER_KEY</code>   - <code>OPENAI_API_KEY</code>   - <code>ANTHROPIC_API_KEY</code>   - <code>AZURE_API_KEY</code>   - <code>AZURE_API_BASE</code>   - PostgreSQL database credentials (automatically configured)</p>"},{"location":"package-ai-readme/#deployment-process","title":"Deployment Process","text":"<ol> <li>Creates an <code>ai</code> namespace in Kubernetes</li> <li>Verifies required secrets exist</li> <li>Sets up persistent storage</li> <li>Adds required Helm repositories</li> <li>Deploys components in sequence:</li> <li>LiteLLM proxy (first - required dependency)</li> <li>Tika server</li> <li>Open WebUI frontend</li> </ol> <p>Each component is deployed with appropriate timeouts and readiness checks to ensure proper initialization.</p>"},{"location":"package-ai-readme/#experiements-and-notes","title":"Experiements and notes","text":""},{"location":"package-ai-readme/#rag-pipeline-notes","title":"RAG pipeline notes","text":""},{"location":"package-ai-readme/#norwegian-bert-models-for-rag","title":"Norwegian BERT Models for RAG","text":"<p>The following Norwegian BERT models are particularly well-suited for use in Retrieval-Augmented Generation (RAG) pipelines, providing strong Norwegian language understanding and generation capabilities:</p> Model Name Developer/Source Main Use Case Notes NorBERT University of Oslo General Norwegian NLP Trained from scratch on Norwegian NorBERT 3 Large Dataloop/NorwAI Advanced NLP tasks in Norwegian Large model, versatile Klinisk NorBERT eHealthResearch Clinical/medical Norwegian text Fine-tuned for healthcare NB-BERT National Library (NB) General Norwegian &amp; Scandinavian NLP Trained on 200 years of text Norwegian BERT Certainly AI General Norwegian NLP Open-source, community-driven <p>These models can be integrated into the RAG pipeline to enhance Norwegian language processing capabilities, particularly useful for: - Document understanding and retrieval in Norwegian - Question answering systems - Text summarization - Information extraction from Norwegian documents</p>"},{"location":"package-auth-authentik-auth10-developer-guide/","title":"Auth10 Developer Guide","text":""},{"location":"package-auth-authentik-auth10-developer-guide/#overview","title":"Overview","text":"<p>This guide explains how to use the Auth10 dynamic service protection system to protect your Kubernetes services with Authentik authentication.</p>"},{"location":"package-auth-authentik-auth10-developer-guide/#quick-start","title":"Quick Start","text":""},{"location":"package-auth-authentik-auth10-developer-guide/#1-protect-a-service","title":"1. Protect a Service","text":"<p>Edit <code>topsecret/kubernetes/kubernetes-secrets.yml</code> and add your service to the <code>protected_services</code> list:</p> <pre><code># In topsecret/kubernetes/kubernetes-secrets.yml\nprotected_services:\n  - name: myapp\n    type: proxy\n    description: \"My awesome application\"\n    domains: [\"localhost\", \"cloudflare\"]\n</code></pre>"},{"location":"package-auth-authentik-auth10-developer-guide/#2-deploy-configuration","title":"2. Deploy Configuration","text":"<pre><code># Deploy the updated configuration using the Ansible playbook\nansible-playbook ansible/playbooks/070-setup-authentik.yml -e kube_context=\"rancher-desktop\"\n</code></pre>"},{"location":"package-auth-authentik-auth10-developer-guide/#3-verify-protection","title":"3. Verify Protection","text":"<pre><code># Check if service is protected\ncurl -I http://myapp.localhost\n# Should redirect to Authentik login\n\n# Both domains now require authentication automatically:\n# - http://myapp.localhost\n# - https://myapp.urbalurba.no\n</code></pre>"},{"location":"package-auth-authentik-auth10-developer-guide/#configuration","title":"Configuration","text":""},{"location":"package-auth-authentik-auth10-developer-guide/#service-types","title":"Service Types","text":""},{"location":"package-auth-authentik-auth10-developer-guide/#proxy-services-forward-auth","title":"Proxy Services (Forward Auth)","text":"<p>For services that use Traefik forward auth middleware:</p> <pre><code>protected_services:\n  - name: whoami\n    type: proxy\n    description: \"Whoami test service\"\n    domains: [\"localhost\", \"tailscale\", \"cloudflare\"]\n    application_slug: \"whoami-app\"\n</code></pre>"},{"location":"package-auth-authentik-auth10-developer-guide/#oauth2-services","title":"OAuth2 Services","text":"<p>For services that use OAuth2/OIDC authentication:</p> <pre><code>protected_services:\n  - name: openwebui\n    type: oauth2\n    description: \"OpenWebUI application\"\n    domains: [\"localhost\", \"tailscale\", \"cloudflare\"]\n    application_slug: \"openwebui-app\"\n    oauth_config:\n      client_id: \"{{ openwebui_oauth_client_id }}\"\n      client_secret: \"{{ openwebui_oauth_client_secret }}\"\n      redirect_uri: \"/oauth/oidc/callback\"\n</code></pre>"},{"location":"package-auth-authentik-auth10-developer-guide/#basic-auth-services","title":"Basic Auth Services","text":"<p>For services that use simple username/password authentication:</p> <pre><code>protected_services:\n  - name: basic-auth-service\n    type: basic\n    description: \"Basic authentication service\"\n    domains: [\"localhost\", \"tailscale\"]\n    basic_auth:\n      username: \"admin\"\n      password: \"{{ BASIC_AUTH_PASSWORD }}\"\n</code></pre>"},{"location":"package-auth-authentik-auth10-developer-guide/#domain-configuration","title":"Domain Configuration","text":""},{"location":"package-auth-authentik-auth10-developer-guide/#available-domains","title":"Available Domains","text":"<ul> <li>localhost: <code>http://service.localhost</code> (development)</li> <li>tailscale: <code>https://service.dog-pence.ts.net</code> (Tailscale MagicDNS)</li> <li>cloudflare: <code>https://service.urbalurba.no</code> (external domain)</li> </ul>"},{"location":"package-auth-authentik-auth10-developer-guide/#domain-selection","title":"Domain Selection","text":"<pre><code># Use all available domains\ndomains: \"auto\"\n\n# Use specific domains\ndomains: [\"localhost\", \"tailscale\"]\n\n# Use only external domains\ndomains: [\"tailscale\", \"cloudflare\"]\n</code></pre>"},{"location":"package-auth-authentik-auth10-developer-guide/#workflow","title":"Workflow","text":""},{"location":"package-auth-authentik-auth10-developer-guide/#adding-a-new-service","title":"Adding a New Service","text":"<ol> <li> <p>Edit Configuration <pre><code># Add service to kubernetes-secrets.yml\nvim topsecret/kubernetes/kubernetes-secrets.yml\n</code></pre></p> </li> <li> <p>Deploy Configuration <pre><code># Run auth script\n./scripts/packages/auth.sh\n</code></pre></p> </li> <li> <p>Verify Protection <pre><code># Check if service is protected\ncurl -I http://whoami.localhost\n# Should redirect to Authentik login\n</code></pre></p> </li> </ol>"},{"location":"package-auth-authentik-auth10-developer-guide/#using-the-system","title":"Using the System","text":"<ol> <li> <p>Edit Configuration <pre><code>vim topsecret/kubernetes/kubernetes-secrets.yml\n</code></pre></p> </li> <li> <p>Deploy Changes <pre><code>./scripts/packages/auth.sh\n</code></pre></p> </li> <li> <p>Access Service</p> </li> <li>Open browser to <code>http://whoami.localhost</code></li> <li>You'll be redirected to Authentik login</li> <li>After login, you'll be redirected back to the service</li> </ol>"},{"location":"package-auth-authentik-auth10-developer-guide/#examples","title":"Examples","text":""},{"location":"package-auth-authentik-auth10-developer-guide/#example-1-simple-web-service","title":"Example 1: Simple Web Service","text":"<pre><code>protected_services:\n  - name: my-app\n    type: proxy\n    description: \"My web application\"\n    domains: [\"localhost\", \"tailscale\"]\n    application_slug: \"my-app\"\n</code></pre>"},{"location":"package-auth-authentik-auth10-developer-guide/#example-2-oauth2-application","title":"Example 2: OAuth2 Application","text":"<pre><code>protected_services:\n  - name: dashboard\n    type: oauth2\n    description: \"Admin dashboard\"\n    domains: [\"localhost\", \"tailscale\", \"cloudflare\"]\n    application_slug: \"dashboard-app\"\n    oauth_config:\n      client_id: \"{{ dashboard_oauth_client_id }}\"\n      client_secret: \"{{ dashboard_oauth_client_secret }}\"\n      redirect_uri: \"/auth/callback\"\n</code></pre>"},{"location":"package-auth-authentik-auth10-developer-guide/#example-3-basic-auth-service","title":"Example 3: Basic Auth Service","text":"<pre><code>protected_services:\n  - name: basic-auth-service\n    type: basic\n    description: \"Basic authentication service\"\n    domains: [\"localhost\", \"tailscale\"]\n    basic_auth:\n      username: \"admin\"\n      password: \"{{ BASIC_AUTH_PASSWORD }}\"\n</code></pre>"},{"location":"package-auth-authentik-auth10-developer-guide/#example-4-development-only","title":"Example 4: Development Only","text":"<pre><code>protected_services:\n  - name: dev-tools\n    type: proxy\n    description: \"Development tools\"\n    domains: [\"localhost\"]  # Only localhost\n    application_slug: \"dev-tools\"\n</code></pre>"},{"location":"package-auth-authentik-auth10-developer-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"package-auth-authentik-auth10-developer-guide/#common-issues","title":"Common Issues","text":""},{"location":"package-auth-authentik-auth10-developer-guide/#service-not-protected","title":"Service Not Protected","text":"<ol> <li>Check if service is in <code>protected_services</code> list</li> <li>Verify domains are enabled</li> <li>Check Ansible playbook logs</li> </ol>"},{"location":"package-auth-authentik-auth10-developer-guide/#authentication-loop","title":"Authentication Loop","text":"<ol> <li>Check CSRF trusted origins</li> <li>Verify provider configuration</li> <li>Check IngressRoute configuration</li> </ol>"},{"location":"package-auth-authentik-auth10-developer-guide/#domain-not-working","title":"Domain Not Working","text":"<ol> <li>Verify domain is enabled in <code>domains</code> section</li> <li>Check DNS resolution</li> <li>Verify IngressRoute configuration</li> </ol>"},{"location":"package-auth-authentik-auth10-developer-guide/#debug-commands","title":"Debug Commands","text":"<pre><code># Check Authentik pods\nkubectl get pods -n authentik\n\n# Check Authentik logs\nkubectl logs -n authentik -l app.kubernetes.io/name=authentik\n\n# Check IngressRoutes\nkubectl get ingressroute -n default\n\n# Check providers in Authentik\nkubectl exec -n authentik deployment/authentik-server -- python manage.py shell\n</code></pre>"},{"location":"package-auth-authentik-auth10-developer-guide/#logs-and-monitoring","title":"Logs and Monitoring","text":"<pre><code># Authentik server logs\nkubectl logs -n authentik deployment/authentik-server\n\n# Authentik worker logs\nkubectl logs -n authentik deployment/authentik-worker\n\n# Traefik logs\nkubectl logs -n traefik deployment/traefik\n</code></pre>"},{"location":"package-auth-authentik-auth10-developer-guide/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"package-auth-authentik-auth10-developer-guide/#custom-domains","title":"Custom Domains","text":"<p>To add a new domain type:</p> <ol> <li> <p>Add to domains section <pre><code>domains:\n  custom:\n    enabled: true\n    base_domain: \"{{ CUSTOM_DOMAIN }}\"\n    protocol: \"https\"\n    description: \"Custom domain\"\n</code></pre></p> </li> <li> <p>Update service configuration <pre><code>protected_services:\n  - name: my-service\n    domains: [\"localhost\", \"tailscale\", \"custom\"]\n</code></pre></p> </li> </ol>"},{"location":"package-auth-authentik-auth10-developer-guide/#custom-oauth2-configuration","title":"Custom OAuth2 Configuration","text":"<pre><code>protected_services:\n  - name: custom-app\n    type: oauth2\n    oauth_config:\n      client_id: \"{{ custom_client_id }}\"\n      client_secret: \"{{ custom_client_secret }}\"\n      redirect_uri: \"/custom/callback\"\n      # Additional OAuth2 settings can be added here\n</code></pre>"},{"location":"package-auth-authentik-auth10-developer-guide/#best-practices","title":"Best Practices","text":"<ol> <li>Use descriptive names for services and applications</li> <li>Test on localhost first before deploying to external domains</li> <li>Keep domains minimal - only enable what you need</li> <li>Use version control for all configuration changes</li> <li>Monitor logs for authentication issues</li> <li>Backup configuration before making changes</li> </ol>"},{"location":"package-auth-authentik-auth10-developer-guide/#application-integration","title":"Application Integration","text":""},{"location":"package-auth-authentik-auth10-developer-guide/#expected-authentication-headers","title":"Expected Authentication Headers","text":"<p>Applications receive these headers for authenticated users: <pre><code>X-authentik-username: myapp-admin\nX-authentik-email: admin@myapp.local\nX-authentik-groups: myapp-admins,myapp-users\nX-authentik-name: myapp Administrator\nX-authentik-uid: unique-user-id\nX-authentik-jwt: jwt-token-here\n</code></pre></p>"},{"location":"package-auth-authentik-auth10-developer-guide/#integration-examples","title":"Integration Examples","text":""},{"location":"package-auth-authentik-auth10-developer-guide/#nextjs-application","title":"Next.js Application","text":"<pre><code>// middleware.js\nexport function middleware(request) {\n  const user = request.headers.get('x-authentik-username')\n  const groups = request.headers.get('x-authentik-groups')?.split(',') || []\n\n  // Check if user has required permissions\n  if (request.nextUrl.pathname.startsWith('/admin')) {\n    if (!groups.includes('myapp-admins')) {\n      return new Response('Forbidden', { status: 403 })\n    }\n  }\n\n  if (request.nextUrl.pathname.startsWith('/dashboard')) {\n    if (!groups.includes('myapp-users')) {\n      return new Response('Forbidden', { status: 403 })\n    }\n  }\n}\n</code></pre>"},{"location":"package-auth-authentik-auth10-developer-guide/#nodejsexpress-application","title":"Node.js/Express Application","text":"<pre><code>// auth-middleware.js\nfunction requireGroup(requiredGroup) {\n  return (req, res, next) =&gt; {\n    const userGroups = req.headers['x-authentik-groups']?.split(',') || []\n\n    if (!userGroups.includes(requiredGroup)) {\n      return res.status(403).json({ error: 'Insufficient permissions' })\n    }\n\n    next()\n  }\n}\n\n// Usage\napp.get('/admin/*', requireGroup('myapp-admins'), adminRoutes)\napp.get('/dashboard/*', requireGroup('myapp-users'), userRoutes)\n</code></pre>"},{"location":"package-auth-authentik-auth10-developer-guide/#multi-environment-testing","title":"Multi-Environment Testing","text":"<p>The system provides two parallel environments for each app:</p> <ol> <li>Authentication Testing: <code>whoami-{app}.localhost</code></li> <li>Points to existing whoami service</li> <li>Uses app-specific authentication</li> <li>Perfect for testing user/group behaviors</li> <li> <p>Independent of actual app development</p> </li> <li> <p>Application Development: <code>{app}.localhost</code></p> </li> <li>Points to developer's actual application</li> <li>Uses same authentication configuration</li> <li>Real-world integration testing</li> <li>Production-ready patterns</li> </ol>"},{"location":"package-auth-authentik-auth10-developer-guide/#system-verification-commands","title":"System Verification Commands","text":"<pre><code># Check deployment status\nkubectl get pods -n authentik                    # Should show 2+ running pods\nkubectl get svc -n authentik                     # Services\nkubectl get ingress -n authentik                 # Ingress configuration\nkubectl get middleware -n default                # Forward auth middleware\n\n# Check system health\nkubectl logs -n authentik deployment/authentik-server --tail=10\nkubectl logs -n authentik deployment/authentik-worker --tail=10\n</code></pre>"},{"location":"package-auth-authentik-auth10-developer-guide/#authentication-flow-testing","title":"Authentication Flow Testing","text":"<pre><code># Test external access to admin interface\ncurl -I http://authentik.localhost/if/admin/     # Should return 200 OK\n\n# Test authentication flow (should redirect to login)\ncurl -L http://whoami.localhost\n\n# Test from within cluster\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never --command -- \\\n  curl -s -H \"Host: authentik.localhost\" \\\n  http://traefik.kube-system.svc.cluster.local\n</code></pre>"},{"location":"package-auth-authentik-auth10-developer-guide/#support","title":"Support","text":"<p>For issues and questions: 1. Check the troubleshooting section 2. Review logs for error messages 3. Check the configuration reference 4. Create an issue in the project repository</p>"},{"location":"package-auth-authentik-auth10/","title":"Authentik Blueprint Creation and Management","text":""},{"location":"package-auth-authentik-auth10/#overview","title":"Overview","text":"<p>This document describes the blueprint architecture for Authentik authentication in the Urbalurba infrastructure, covering both the new Auth10 dynamic blueprint system and the legacy static blueprint approach.</p> <p>The Auth10 system represents a major evolution from manual blueprint creation to fully automated, configuration-driven blueprint generation that supports multi-domain authentication out of the box.</p>"},{"location":"package-auth-authentik-auth10/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Auth10 Dynamic Blueprint System</li> <li>Configuration in kubernetes-secrets.yml</li> <li>Template Architecture</li> <li>Deployment Workflow</li> <li>Adding New Services</li> <li>Multi-Domain Support</li> <li>Legacy Static Blueprints</li> <li>Migration Guide</li> <li>Troubleshooting</li> <li>Advanced Configuration</li> </ol>"},{"location":"package-auth-authentik-auth10/#auth10-dynamic-blueprint-system","title":"Auth10 Dynamic Blueprint System","text":""},{"location":"package-auth-authentik-auth10/#what-is-auth10","title":"What is Auth10?","text":"<p>Auth10 is a template-driven system that automatically generates Authentik blueprints, configurations, and routing rules based on simple YAML configuration. It solves the external domain limitation mentioned in previous documentation by automating the creation of multi-domain authentication providers.</p>"},{"location":"package-auth-authentik-auth10/#key-benefits","title":"Key Benefits","text":"<ul> <li>\ud83c\udfaf Configuration-Driven: Define services once, get multi-domain auth automatically</li> <li>\ud83c\udf10 Multi-Domain Support: Automatic provider creation for localhost, Tailscale, Cloudflare domains</li> <li>\ud83d\udd04 Dynamic Generation: Templates rendered during deployment, always current</li> <li>\ud83d\udee1\ufe0f Outpost Integration: Automatic provider linking to embedded outpost</li> <li>\ud83d\udcdd Zero Manual Steps: No manual provider configuration in Authentik UI required</li> <li>\ud83d\ude80 Scalable: Add new services by updating configuration only</li> </ul>"},{"location":"package-auth-authentik-auth10/#architecture-overview","title":"Architecture Overview","text":"<pre><code>kubernetes-secrets.yml (config)\n    \u2193\nAuth10 Jinja2 Templates (.j2 files)\n    \u2193\nAnsible Template Rendering (during deployment)\n    \u2193\nGenerated Manifests (.yaml files)\n    \u2193\nKubernetes Deployment\n    \u2193\nMulti-Domain Authentication Ready\n</code></pre>"},{"location":"package-auth-authentik-auth10/#configuration-in-kubernetes-secretsyml","title":"Configuration in kubernetes-secrets.yml","text":""},{"location":"package-auth-authentik-auth10/#domain-configuration","title":"Domain Configuration","text":"<p>The <code>domains</code> section defines all available domains for authentication:</p> <pre><code># ================================================================\n# DOMAIN CONFIGURATION FOR AUTH10\n# ================================================================\ndomains:\n  localhost:\n    enabled: true\n    base_domain: \"localhost\"\n    protocol: \"http\"\n    description: \"Local development domain\"\n\n  tailscale:\n    enabled: true\n    base_domain: \"dog-pence.ts.net\"  # Your actual Tailscale domain\n    protocol: \"https\"\n    description: \"Tailscale MagicDNS domain\"\n\n  cloudflare:\n    enabled: true\n    base_domain: \"urbalurba.no\"      # Your actual Cloudflare domain\n    protocol: \"https\"\n    description: \"Cloudflare external domain\"\n</code></pre>"},{"location":"package-auth-authentik-auth10/#domain-fields","title":"Domain Fields","text":"<ul> <li><code>enabled</code>: Boolean - Whether this domain should be processed</li> <li><code>base_domain</code>: String - The base domain (e.g., \"localhost\", \"urbalurba.no\")</li> <li><code>protocol</code>: String - \"http\" or \"https\"</li> <li><code>description</code>: String - Human-readable description for documentation</li> </ul>"},{"location":"package-auth-authentik-auth10/#protected-services-configuration","title":"Protected Services Configuration","text":"<p>The <code>protected_services</code> section defines which services should be protected with authentication:</p> <pre><code># ================================================================\n# PROTECTED SERVICES CONFIGURATION FOR AUTH10\n# ================================================================\nprotected_services:\n  # Example: Basic proxy authentication service\n  - name: whoami\n    type: proxy\n    description: \"Whoami test service\"\n    domains: [\"localhost\", \"tailscale\", \"cloudflare\"]\n    application_slug: \"whoami-app\"\n\n  # Example: OAuth2/OIDC service (for future use)\n  - name: grafana\n    type: oauth2\n    description: \"Grafana monitoring dashboard\"\n    domains: [\"localhost\", \"cloudflare\"]\n    oauth_config:\n      client_id: \"grafana-client-id\"\n      client_secret: \"grafana-client-secret\"\n      redirect_uri: \"/oauth/callback\"\n\n  # Example: Basic auth service\n  - name: private-docs\n    type: basic\n    description: \"Private documentation site\"\n    domains: [\"localhost\", \"tailscale\"]\n</code></pre>"},{"location":"package-auth-authentik-auth10/#service-fields","title":"Service Fields","text":"<ul> <li><code>name</code>: String - Service name (used for provider/application naming)</li> <li><code>type</code>: String - Authentication type: <code>proxy</code>, <code>oauth2</code>, or <code>basic</code></li> <li><code>description</code>: String - Human-readable description</li> <li><code>domains</code>: Array - List of domain keys this service should be available on</li> <li><code>application_slug</code>: String - (Optional) Custom application slug</li> <li><code>oauth_config</code>: Object - (Required for oauth2 type) OAuth configuration</li> </ul>"},{"location":"package-auth-authentik-auth10/#authentication-types","title":"Authentication Types","text":"<ol> <li><code>proxy</code>: Forward authentication via Authentik embedded outpost</li> <li>Best for: Services that don't have built-in OAuth support</li> <li> <p>Flow: Service \u2192 Authentik \u2192 Login \u2192 Service (with headers)</p> </li> <li> <p><code>oauth2</code>: Direct OAuth2/OIDC integration</p> </li> <li>Best for: Services with built-in OAuth support (Grafana, etc.)</li> <li> <p>Flow: Service \u2192 Authentik OAuth \u2192 Login \u2192 Service (with tokens)</p> </li> <li> <p><code>basic</code>: HTTP Basic Authentication</p> </li> <li>Best for: Simple services requiring basic protection</li> <li>Flow: Browser basic auth dialog \u2192 Authentik verification</li> </ol>"},{"location":"package-auth-authentik-auth10/#template-architecture","title":"Template Architecture","text":""},{"location":"package-auth-authentik-auth10/#template-location","title":"Template Location","text":"<p>Auth10 templates are stored in: <pre><code>ansible/templates/auth10/\n\u251c\u2500\u2500 073-authentik-service-protection-blueprint.yaml.j2\n\u251c\u2500\u2500 075-authentik-config.yaml.j2\n\u251c\u2500\u2500 076-authentik-ingressroute.yaml.j2\n\u251c\u2500\u2500 078-service-protection-ingressroute.yaml.j2\n\u2514\u2500\u2500 079-basic-auth-middleware.yaml.j2\n</code></pre></p>"},{"location":"package-auth-authentik-auth10/#generated-files","title":"Generated Files","text":"<p>During deployment, templates generate:</p> <ol> <li>Service Protection Blueprint (<code>073-authentik-service-protection-blueprint.yaml</code>)</li> <li>Creates Authentik providers for each service-domain combination</li> <li>Creates applications linked to providers</li> <li>Links all providers to embedded outpost</li> <li> <p>Creates property mappings for groups/roles</p> </li> <li> <p>Authentik Configuration (<code>075-authentik-config.yaml</code>)</p> </li> <li>Helm values for Authentik deployment</li> <li>Dynamic CSRF trusted origins</li> <li> <p>Blueprint discovery configuration</p> </li> <li> <p>Service Protection IngressRoute (<code>078-service-protection-ingressroute.yaml</code>)</p> </li> <li>Traefik routing rules for protected services</li> <li>Unified routing using HostRegexp patterns</li> <li>Middleware linkage for authentication</li> </ol>"},{"location":"package-auth-authentik-auth10/#template-variables","title":"Template Variables","text":"<p>Templates receive these variables from <code>kubernetes-secrets.yml</code>: - <code>domains</code>: Complete domain configuration - <code>protected_services</code>: Array of services to protect</p>"},{"location":"package-auth-authentik-auth10/#deployment-workflow","title":"Deployment Workflow","text":""},{"location":"package-auth-authentik-auth10/#automatic-deployment","title":"Automatic Deployment","text":"<p>The Auth10 system is integrated into the main Authentik deployment playbook:</p> <pre><code># Deploy complete Authentik infrastructure with Auth10\nansible-playbook ansible/playbooks/070-setup-authentik.yml -e kube_context=\"rancher-desktop\"\n</code></pre>"},{"location":"package-auth-authentik-auth10/#deployment-steps","title":"Deployment Steps","text":"<ol> <li>Configuration Loading (Step 24.1-24.3)</li> <li>Load <code>kubernetes-secrets.yml</code></li> <li>Validate Auth10 configuration</li> <li> <p>Display loaded configuration</p> </li> <li> <p>Template Rendering (Step 24.4-24.6)</p> </li> <li>Render service protection blueprint</li> <li>Render Authentik configuration</li> <li> <p>Render service protection IngressRoute</p> </li> <li> <p>Blueprint Deployment (Step 25)</p> </li> <li>Deploy generated blueprint before Helm</li> <li> <p>Ensure ConfigMap is available for mounting</p> </li> <li> <p>Authentik Deployment (Steps 30+)</p> </li> <li>Deploy Authentik with generated configuration</li> <li>Blueprint discovery and processing</li> <li> <p>Outpost configuration and provider linking</p> </li> <li> <p>Route Deployment (Step 46)</p> </li> <li>Deploy generated IngressRoute for service protection</li> <li>Enable multi-domain routing</li> </ol>"},{"location":"package-auth-authentik-auth10/#adding-new-services","title":"Adding New Services","text":""},{"location":"package-auth-authentik-auth10/#step-1-update-configuration","title":"Step 1: Update Configuration","text":"<p>Add your service to <code>topsecret/kubernetes/kubernetes-secrets.yml</code>:</p> <pre><code>protected_services:\n  # Existing services...\n\n  # New service\n  - name: myapp\n    type: proxy\n    description: \"My awesome application\"\n    domains: [\"localhost\", \"cloudflare\"]  # Choose relevant domains\n</code></pre>"},{"location":"package-auth-authentik-auth10/#step-2-deploy-service","title":"Step 2: Deploy Service","text":"<p>Ensure your service is deployed in Kubernetes:</p> <pre><code># Create your service deployment and service\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: myapp\n        image: my-app:latest\n        ports:\n        - containerPort: 8080\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: myapp\n  namespace: default\nspec:\n  selector:\n    app: myapp\n  ports:\n  - port: 80\n    targetPort: 8080\n</code></pre>"},{"location":"package-auth-authentik-auth10/#step-3-re-run-deployment","title":"Step 3: Re-run Deployment","text":"<pre><code># Re-run the Authentik playbook to generate new configuration\nansible-playbook ansible/playbooks/070-setup-authentik.yml -e kube_context=\"rancher-desktop\"\n</code></pre>"},{"location":"package-auth-authentik-auth10/#step-4-verify","title":"Step 4: Verify","text":"<p>Your service will automatically be available at: - <code>http://myapp.localhost</code> (if localhost domain enabled) - <code>https://myapp.urbalurba.no</code> (if cloudflare domain enabled)</p> <p>All domains will require authentication and redirect to <code>authentik.localhost</code> for login.</p>"},{"location":"package-auth-authentik-auth10/#multi-domain-support","title":"Multi-Domain Support","text":""},{"location":"package-auth-authentik-auth10/#how-it-works","title":"How It Works","text":"<p>Auth10 creates separate Authentik providers for each service-domain combination:</p> <pre><code>Service: whoami\nDomains: [localhost, tailscale, cloudflare]\n\nGenerated Providers:\n\u251c\u2500\u2500 whoami-localhost-provider (http://whoami.localhost)\n\u251c\u2500\u2500 whoami-tailscale-provider (https://whoami.dog-pence.ts.net)\n\u2514\u2500\u2500 whoami-cloudflare-provider (https://whoami.urbalurba.no)\n\nAll providers linked to: authentik Embedded Outpost\n</code></pre>"},{"location":"package-auth-authentik-auth10/#domain-resolution","title":"Domain Resolution","text":"<ul> <li>Development: <code>http://whoami.localhost</code> \u2192 <code>http://authentik.localhost</code></li> <li>Tailscale: <code>https://whoami.dog-pence.ts.net</code> \u2192 <code>http://authentik.localhost</code></li> <li>Cloudflare: <code>https://whoami.urbalurba.no</code> \u2192 <code>http://authentik.localhost</code></li> </ul>"},{"location":"package-auth-authentik-auth10/#adding-new-domains","title":"Adding New Domains","text":"<ol> <li> <p>Add domain configuration: <pre><code>domains:\n  # Existing domains...\n\n  newdomain:\n    enabled: true\n    base_domain: \"example.com\"\n    protocol: \"https\"\n    description: \"New external domain\"\n</code></pre></p> </li> <li> <p>Update services to include the new domain: <pre><code>protected_services:\n  - name: whoami\n    domains: [\"localhost\", \"tailscale\", \"cloudflare\", \"newdomain\"]  # Add newdomain\n</code></pre></p> </li> <li> <p>Re-deploy to generate new providers and routing</p> </li> </ol>"},{"location":"package-auth-authentik-auth10/#legacy-static-blueprints","title":"Legacy Static Blueprints","text":""},{"location":"package-auth-authentik-auth10/#overview_1","title":"Overview","text":"<p>Before Auth10, Authentik blueprints were created manually as static YAML files. This approach required: - Manual creation of each blueprint - Hardcoded domains and service configurations - Manual provider configuration in Authentik UI for external domains - Separate files for each service or configuration</p>"},{"location":"package-auth-authentik-auth10/#static-blueprint-examples","title":"Static Blueprint Examples","text":""},{"location":"package-auth-authentik-auth10/#test-users-and-groups-blueprint","title":"Test Users and Groups Blueprint","text":"<pre><code># manifests/073-authentik-1-test-users-groups-blueprint.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: test-users-groups-blueprint\n  namespace: authentik\n  labels:\n    app.kubernetes.io/name: authentik\n    app.kubernetes.io/component: blueprint\n    blueprints.goauthentik.io/instantiate: \"true\"\ndata:\n  test-users-groups.yaml: |\n    version: 1\n    metadata:\n      name: \"Test Users and Groups\"\n\n    entries:\n      # Create test groups\n      - model: authentik_core.group\n        state: present\n        identifiers:\n          name: \"HQ\"\n        attrs:\n          name: \"HQ\"\n          users: []\n\n      # Create test users\n      - model: authentik_core.user\n        state: present\n        identifiers:\n          username: \"it1@urbalurba.no\"\n        attrs:\n          username: \"it1@urbalurba.no\"\n          email: \"it1@urbalurba.no\"\n          name: \"IT Bruker 1\"\n          groups:\n            - !Find [authentik_core.group, [name, \"HQ\"]]\n</code></pre>"},{"location":"package-auth-authentik-auth10/#openwebui-oauth-blueprint-template-based-legacy","title":"OpenWebUI OAuth Blueprint (Template-based Legacy)","text":"<pre><code># manifests/073-authentik-2-openwebui-blueprint.yaml.j2\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: openwebui-blueprint\n  namespace: authentik\n  labels:\n    app.kubernetes.io/name: authentik\n    app.kubernetes.io/component: blueprint\n    blueprints.goauthentik.io/instantiate: \"true\"\ndata:\n  openwebui.yaml: |\n    version: 1\n    metadata:\n      name: \"OpenWebUI OAuth2/OIDC\"\n\n    entries:\n      # Create OAuth2 provider\n      - model: authentik_providers_oauth2.oauth2provider\n        state: present\n        identifiers:\n          name: openwebui-dev-provider\n        attrs:\n          name: openwebui-dev-provider\n          client_type: confidential\n          client_id: \"{{ openwebui_oauth_client_id }}\"\n          client_secret: \"{{ openwebui_oauth_client_secret }}\"\n          redirect_uris:\n            - \"{{ openwebui_oauth_redirect_uri }}\"\n</code></pre>"},{"location":"package-auth-authentik-auth10/#limitations-of-static-blueprints","title":"Limitations of Static Blueprints","text":"<ol> <li>Manual Provider Creation: Each external domain required manual configuration</li> <li>Hardcoded Values: Domain-specific values embedded in templates</li> <li>Scalability Issues: Adding services required creating new blueprint files</li> <li>External Domain Problem: Providers created via blueprints weren't automatically linked to outposts</li> <li>Maintenance Overhead: Multiple files to maintain for similar configurations</li> </ol>"},{"location":"package-auth-authentik-auth10/#when-to-use-static-blueprints","title":"When to Use Static Blueprints","text":"<p>Static blueprints are still appropriate for: - One-time configuration: Test users, groups, flows that don't change - Complex custom flows: Authentication flows requiring specific customization - Integration-specific config: Service-specific OAuth providers (OpenWebUI, Grafana) - System configuration: Core system settings that don't vary by service</p>"},{"location":"package-auth-authentik-auth10/#migration-guide","title":"Migration Guide","text":""},{"location":"package-auth-authentik-auth10/#from-static-to-auth10","title":"From Static to Auth10","text":"<p>If you have existing static blueprints for service protection:</p> <ol> <li>Identify Protected Services: List services currently using static blueprints</li> <li>Extract Configuration: Identify domains, authentication types, and settings</li> <li>Update kubernetes-secrets.yml: Add services to <code>protected_services</code> section</li> <li>Remove Static Files: Delete old static blueprint files</li> <li>Test Deployment: Run Auth10 deployment and verify functionality</li> </ol>"},{"location":"package-auth-authentik-auth10/#example-migration","title":"Example Migration","text":"<p>Before (Static): <pre><code># manifests/072-authentik-myapp-blueprint.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: myapp-blueprint\ndata:\n  myapp.yaml: |\n    entries:\n      - model: authentik_providers_proxy.proxyprovider\n        identifiers:\n          name: myapp-localhost-provider\n        attrs:\n          external_host: http://myapp.localhost\n          internal_host: http://myapp.default.svc.cluster.local\n</code></pre></p> <p>After (Auth10): <pre><code># topsecret/kubernetes/kubernetes-secrets.yml\nprotected_services:\n  - name: myapp\n    type: proxy\n    description: \"My application\"\n    domains: [\"localhost\", \"cloudflare\"]\n</code></pre></p>"},{"location":"package-auth-authentik-auth10/#troubleshooting","title":"Troubleshooting","text":""},{"location":"package-auth-authentik-auth10/#common-issues","title":"Common Issues","text":""},{"location":"package-auth-authentik-auth10/#1-configuration-not-loading","title":"1. Configuration Not Loading","text":"<pre><code># Check if kubernetes-secrets.yml is valid\nansible-playbook ansible/playbooks/070-setup-authentik.yml --syntax-check\n\n# Verify Auth10 configuration sections exist\ngrep -A 10 \"domains:\" topsecret/kubernetes/kubernetes-secrets.yml\ngrep -A 10 \"protected_services:\" topsecret/kubernetes/kubernetes-secrets.yml\n</code></pre>"},{"location":"package-auth-authentik-auth10/#2-templates-not-rendering","title":"2. Templates Not Rendering","text":"<pre><code># Check template files exist\nls -la ansible/templates/auth10/\n\n# Verify Ansible can access templates\nansible localhost -m template -a \"src=ansible/templates/auth10/073-authentik-service-protection-blueprint.yaml.j2 dest=/tmp/test.yaml\" --extra-vars \"@topsecret/kubernetes/kubernetes-secrets.yml\"\n</code></pre>"},{"location":"package-auth-authentik-auth10/#3-providers-not-created","title":"3. Providers Not Created","text":"<pre><code># Check blueprint application\nkubectl logs -n authentik deployment/authentik-worker | grep -i blueprint\n\n# Verify ConfigMap exists\nkubectl get configmap -n authentik service-protection-blueprint -o yaml\n\n# Check provider creation\nkubectl exec -n authentik deployment/authentik-server -- python manage.py shell -c \"from authentik.providers.proxy.models import ProxyProvider; print([p.name for p in ProxyProvider.objects.all()])\"\n</code></pre>"},{"location":"package-auth-authentik-auth10/#4-external-domains-not-working","title":"4. External Domains Not Working","text":"<pre><code># Verify outpost provider linking\nkubectl logs -n authentik deployment/authentik-worker | grep -i outpost\n\n# Check if providers are linked to outpost\n# (Access Authentik admin UI to verify outpost configuration)\n</code></pre>"},{"location":"package-auth-authentik-auth10/#5-dns-resolution-issues","title":"5. DNS Resolution Issues","text":"<pre><code># Test domain resolution\nnslookup whoami.localhost\ncurl -I http://whoami.localhost\n\n# For external domains, verify tunnel/DNS configuration\ncurl -I https://whoami.urbalurba.no\n</code></pre>"},{"location":"package-auth-authentik-auth10/#deployment-verification","title":"Deployment Verification","text":"<p>After deployment, verify Auth10 is working:</p> <pre><code># 1. Check generated files exist\nls -la manifests/073-authentik-service-protection-blueprint.yaml\nls -la manifests/075-authentik-config.yaml\nls -la manifests/078-service-protection-ingressroute.yaml\n\n# 2. Test localhost authentication\ncurl -I http://whoami.localhost\n# Expected: HTTP 302 redirect to authentik.localhost\n\n# 3. Test external domain (if tunnel active)\ncurl -I https://whoami.dog-pence.ts.net\n# Expected: HTTP 302 redirect to authentik.localhost\n\n# 4. Verify providers in Authentik\n# Access http://authentik.localhost/if/admin/\n# Check Applications \u2192 Providers for generated providers\n</code></pre>"},{"location":"package-auth-authentik-auth10/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"package-auth-authentik-auth10/#custom-oauth2-configuration","title":"Custom OAuth2 Configuration","text":"<p>For services requiring OAuth2/OIDC:</p> <pre><code>protected_services:\n  - name: grafana\n    type: oauth2\n    description: \"Grafana monitoring dashboard\"\n    domains: [\"localhost\", \"cloudflare\"]\n    oauth_config:\n      client_id: \"grafana-oauth-client\"\n      client_secret: \"super-secret-key\"\n      redirect_uri: \"/oauth/callback\"\n      scopes: [\"openid\", \"email\", \"profile\", \"groups\"]\n      token_validity:\n        access_code: \"minutes=1\"\n        access_token: \"hours=1\"\n        refresh_token: \"days=30\"\n</code></pre>"},{"location":"package-auth-authentik-auth10/#basic-authentication-services","title":"Basic Authentication Services","text":"<p>For simple HTTP basic auth:</p> <pre><code>protected_services:\n  - name: private-wiki\n    type: basic\n    description: \"Private documentation wiki\"\n    domains: [\"localhost\", \"tailscale\"]\n    basic_config:\n      realm: \"Private Documentation\"\n      users:\n        - username: \"admin\"\n          password_hash: \"$2b$12$hash...\"\n</code></pre>"},{"location":"package-auth-authentik-auth10/#conditional-domain-assignment","title":"Conditional Domain Assignment","text":"<p>Enable services only on specific domains:</p> <pre><code>protected_services:\n  # Development only\n  - name: debug-app\n    type: proxy\n    domains: [\"localhost\"]\n\n  # Production only\n  - name: prod-dashboard\n    type: proxy\n    domains: [\"cloudflare\"]\n\n  # Internal network only\n  - name: internal-tools\n    type: proxy\n    domains: [\"tailscale\"]\n</code></pre>"},{"location":"package-auth-authentik-auth10/#custom-application-configuration","title":"Custom Application Configuration","text":"<p>Override default application settings:</p> <pre><code>protected_services:\n  - name: special-app\n    type: proxy\n    domains: [\"localhost\", \"cloudflare\"]\n    application_config:\n      slug: \"custom-app-slug\"\n      meta_launch_url: \"https://special-app.urbalurba.no/dashboard\"\n      policy_engine_mode: \"all\"  # Require all policies to pass\n      meta_description: \"Special application with custom settings\"\n</code></pre>"},{"location":"package-auth-authentik-auth10/#quick-reference","title":"Quick Reference","text":""},{"location":"package-auth-authentik-auth10/#add-new-service-most-common","title":"Add New Service (Most Common)","text":"<ol> <li> <p>Edit <code>topsecret/kubernetes/kubernetes-secrets.yml</code>: <pre><code>protected_services:\n  - name: newservice\n    type: proxy\n    description: \"New service description\"\n    domains: [\"localhost\", \"cloudflare\"]\n</code></pre></p> </li> <li> <p>Deploy your service to Kubernetes</p> </li> <li> <p>Run deployment: <pre><code>ansible-playbook ansible/playbooks/070-setup-authentik.yml -e kube_context=\"rancher-desktop\"\n</code></pre></p> </li> <li> <p>Access: <code>http://newservice.localhost</code> (requires authentication)</p> </li> </ol>"},{"location":"package-auth-authentik-auth10/#add-new-domain","title":"Add New Domain","text":"<ol> <li> <p>Edit <code>topsecret/kubernetes/kubernetes-secrets.yml</code>: <pre><code>domains:\n  newdomain:\n    enabled: true\n    base_domain: \"new.example.com\"\n    protocol: \"https\"\n    description: \"New domain\"\n</code></pre></p> </li> <li> <p>Update services to include new domain</p> </li> <li> <p>Re-deploy Auth10 system</p> </li> </ol>"},{"location":"package-auth-authentik-auth10/#configuration-file-locations","title":"Configuration File Locations","text":"<ul> <li>Auth10 Config: <code>topsecret/kubernetes/kubernetes-secrets.yml</code></li> <li>Auth10 Templates: <code>ansible/templates/auth10/</code></li> <li>Generated Manifests: <code>manifests/073-*.yaml</code>, <code>manifests/075-*.yaml</code>, <code>manifests/078-*.yaml</code></li> <li>Deployment Playbook: <code>ansible/playbooks/070-setup-authentik.yml</code></li> </ul>"},{"location":"package-auth-authentik-auth10/#support-resources","title":"Support Resources","text":"<ul> <li>Blueprint Documentation: <code>docs/package-auth-authentik-blueprints-syntax.md</code></li> <li>General Authentication: <code>docs/package-auth-authentik.md</code></li> <li>Test Users: <code>docs/package-auth-authentik-testusers.md</code></li> <li>Authentik Official Docs: https://docs.goauthentik.io/blueprints/</li> </ul> <p>This documentation reflects the Auth10 system as implemented in the Urbalurba infrastructure. For questions or improvements, please refer to the project repository and issue tracker.</p>"},{"location":"package-auth-authentik-blueprints-syntax/","title":"Authentik Blueprints Manual","text":""},{"location":"package-auth-authentik-blueprints-syntax/#overview","title":"Overview","text":"<p>Authentik blueprints provide a way to template, automate, and distribute authentik configuration as code. They allow you to define your authentication infrastructure declaratively using YAML files, enabling version control, reproducible deployments, and automated configuration management.</p> <p>This manual focuses on the Kubernetes ConfigMap pattern for blueprint deployment, which provides automatic discovery and application of blueprints when authentik starts.</p>"},{"location":"package-auth-authentik-blueprints-syntax/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Basic Structure</li> <li>Deployment Patterns</li> <li>Helm Configuration for Blueprints</li> <li>Core Concepts</li> <li>Custom YAML Tags</li> <li>Working Examples</li> <li>Blueprint Development Methodology</li> <li>Common Use Cases</li> <li>Best Practices</li> <li>Troubleshooting</li> <li>Quick Reference</li> </ol>"},{"location":"package-auth-authentik-blueprints-syntax/#basic-structure","title":"Basic Structure","text":""},{"location":"package-auth-authentik-blueprints-syntax/#blueprint-yaml-anatomy","title":"Blueprint YAML Anatomy","text":"<p>Every authentik blueprint follows this basic structure:</p> <pre><code># yaml-language-server: $schema=https://goauthentik.io/blueprints/schema.json\nversion: 1\nmetadata:\n  name: \"Blueprint Name\"\n  labels:\n    blueprints.goauthentik.io/instantiate: \"true\"\n\ncontext: {}\n\nentries:\n  - model: authentik_core.application\n    state: present\n    identifiers:\n      slug: \"my-app\"\n    attrs:\n      name: \"My Application\"\n      # ... other attributes\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#required-components","title":"Required Components","text":"Component Required Description <code>version</code> \u2705 Blueprint format version (currently <code>1</code>) <code>entries</code> \u2705 List of objects to create/manage <code>metadata.name</code> \u2705 Human-readable blueprint name <code>context</code> \u274c Default context variables <code>metadata.labels</code> \u274c Special blueprint configuration labels"},{"location":"package-auth-authentik-blueprints-syntax/#special-labels","title":"Special Labels","text":"Label Purpose <code>blueprints.goauthentik.io/instantiate: \"true\"</code> Auto-apply blueprint on discovery <code>blueprints.goauthentik.io/system: \"true\"</code> Mark as system blueprint <code>blueprints.goauthentik.io/description: \"text\"</code> Blueprint description"},{"location":"package-auth-authentik-blueprints-syntax/#deployment-patterns","title":"Deployment Patterns","text":""},{"location":"package-auth-authentik-blueprints-syntax/#kubernetes-configmap-pattern-recommended","title":"Kubernetes ConfigMap Pattern (Recommended)","text":"<p>This is the pattern used in your working blueprints. It provides automatic discovery and application.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-blueprint\n  namespace: authentik\n  labels:\n    app.kubernetes.io/name: authentik\n    app.kubernetes.io/component: blueprint\n    blueprints.goauthentik.io/instantiate: \"true\"\ndata:\n  blueprint.yaml: |\n    # yaml-language-server: $schema=https://goauthentik.io/blueprints/schema.json\n    version: 1\n    metadata:\n      name: \"My Blueprint\"\n      labels:\n        blueprints.goauthentik.io/instantiate: \"true\"\n\n    context: {}\n\n    entries:\n      # Your blueprint entries here\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#key-benefits","title":"Key Benefits","text":"<ul> <li>\u2705 Automatic Discovery: Authentik automatically detects and applies blueprints</li> <li>\u2705 Kubernetes Native: Deployed using standard Kubernetes resources</li> <li>\u2705 Version Control: Can be managed in Git alongside other manifests</li> <li>\u2705 Monitoring: Changes trigger automatic reapplication (every 60 minutes + file watchers)</li> </ul>"},{"location":"package-auth-authentik-blueprints-syntax/#prerequisites","title":"Prerequisites","text":"<ol> <li>Authentik namespace must exist</li> <li>Blueprint ConfigMaps must be applied BEFORE deploying Authentik with Helm</li> <li>Proper labels must be set for automatic discovery</li> <li>Blueprint names must be listed in Helm values under <code>blueprints.configMaps</code></li> </ol>"},{"location":"package-auth-authentik-blueprints-syntax/#helm-configuration-for-blueprints","title":"Helm Configuration for Blueprints","text":""},{"location":"package-auth-authentik-blueprints-syntax/#connecting-configmaps-to-authentik","title":"Connecting ConfigMaps to Authentik","text":"<p>For blueprint ConfigMaps to be discovered and applied by Authentik, they must be explicitly listed in the Helm values configuration. This is a crucial step that connects your deployed ConfigMaps to the running Authentik instance.</p>"},{"location":"package-auth-authentik-blueprints-syntax/#helm-values-configuration","title":"Helm Values Configuration","text":"<p>Add the following to your Authentik Helm values file (e.g., <code>values.yaml</code> or <code>values-authentik.yaml</code>):</p> <pre><code># Blueprint system configuration\nblueprints:\n  # List of ConfigMaps containing blueprints\n  # Only keys ending with .yaml will be discovered and applied\n  configMaps:\n    - \"whoami-forward-auth-blueprint\"        # Proxy authentication setup\n    - \"openwebui-authentik-blueprint\"         # OAuth2/OIDC application setup\n    - \"users-groups-test-blueprint\"           # Test blueprint for users and groups\n    # Add your blueprint ConfigMap names here\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#complete-deployment-workflow","title":"Complete Deployment Workflow","text":"<pre><code># 1. Deploy blueprint ConfigMaps FIRST (before Authentik)\nkubectl apply -f manifests/073-authentik-whoami-blueprint.yaml\nkubectl apply -f manifests/074-authentik-openwebui-blueprint-hardcoded.yaml\nkubectl apply -f manifests/072-authentik-users-groups-blueprint.yaml\n\n# 2. Verify ConfigMaps are created\nkubectl get configmaps -n authentik -l app.kubernetes.io/component=blueprint\n\n# 3. Deploy/upgrade Authentik with Helm (with blueprint references in values)\nhelm upgrade --install authentik authentik/authentik \\\n  -n authentik \\\n  -f values-authentik.yaml  # Contains the blueprints.configMaps configuration\n\n# 4. Monitor blueprint application\nkubectl logs -n authentik deployment/authentik-server | grep -i blueprint\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#blueprint-discovery-process","title":"Blueprint Discovery Process","text":"<ol> <li>ConfigMap Creation: Blueprint ConfigMaps are deployed to the <code>authentik</code> namespace</li> <li>Helm Reference: ConfigMap names are listed in <code>blueprints.configMaps</code> in Helm values</li> <li>Authentik Startup: When Authentik starts, it reads the configured ConfigMap list</li> <li>Blueprint Loading: Authentik loads and applies blueprints from the referenced ConfigMaps</li> <li>Automatic Reapplication: Changes to ConfigMaps trigger reapplication (monitored every 60 minutes)</li> </ol>"},{"location":"package-auth-authentik-blueprints-syntax/#key-configuration-rules","title":"Key Configuration Rules","text":"Rule Description Example Exact Name Match ConfigMap names in Helm values must exactly match deployed ConfigMap names <code>configMaps: [\"openwebui-authentik-blueprint\"]</code> YAML Files Only Only data keys ending with <code>.yaml</code> are processed <code>data: { \"openwebui.yaml\": \"...\", \"readme.txt\": \"...\" }</code> \u2190 Only <code>openwebui.yaml</code> processed Namespace Consistency ConfigMaps must be in the same namespace as Authentik Both in <code>authentik</code> namespace Deploy Before Helm ConfigMaps must exist before Authentik deployment <code>kubectl apply -f blueprints/</code> then <code>helm install</code>"},{"location":"package-auth-authentik-blueprints-syntax/#example-complete-helm-values","title":"Example Complete Helm Values","text":"<pre><code># values-authentik.yaml\nauthentik:\n  secret_key: \"your-secret-key-here\"\n  postgresql:\n    password: \"your-pg-password\"\n\n# Blueprint system configuration\nblueprints:\n  configMaps:\n    # Application blueprints\n    - \"whoami-forward-auth-blueprint\"          # Forward auth proxy setup\n    - \"openwebui-authentik-blueprint\"           # OAuth2/OIDC for OpenWebUI\n    - \"grafana-saml-blueprint\"                  # SAML setup for Grafana\n\n    # User management blueprints  \n    - \"users-groups-test-blueprint\"             # Test users and departments\n    - \"ldap-import-blueprint\"                   # LDAP user synchronization\n\n    # Flow customization blueprints\n    - \"custom-login-flow-blueprint\"             # Custom authentication flow\n    - \"mfa-enforcement-blueprint\"               # Multi-factor authentication\n\n# Other Authentik configuration...\nimage:\n  tag: \"2024.8.3\"\n\npostgresql:\n  enabled: true\n  auth:\n    postgresPassword: \"your-pg-password\"\n    database: \"authentik\"\n\nredis:\n  enabled: true\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#troubleshooting-helm-configuration","title":"Troubleshooting Helm Configuration","text":""},{"location":"package-auth-authentik-blueprints-syntax/#blueprint-not-loading","title":"Blueprint Not Loading","text":"<p>Symptoms: ConfigMap exists but blueprint not applied</p> <p>Check List: 1. Verify ConfigMap name in Helm values:    <pre><code># Check deployed ConfigMap name\nkubectl get configmaps -n authentik -l app.kubernetes.io/component=blueprint\n\n# Compare with Helm values\nhelm get values authentik -n authentik\n</code></pre></p> <ol> <li> <p>Check ConfigMap data keys:    <pre><code>kubectl describe configmap openwebui-authentik-blueprint -n authentik\n# Look for .yaml files in data section\n</code></pre></p> </li> <li> <p>Verify Authentik can read ConfigMaps:    <pre><code>kubectl logs -n authentik deployment/authentik-server | grep -i \"configmap\\|blueprint\"\n</code></pre></p> </li> </ol>"},{"location":"package-auth-authentik-blueprints-syntax/#common-configuration-mistakes","title":"Common Configuration Mistakes","text":"Issue Wrong Correct Name Mismatch <code>configMaps: [\"openwebui-blueprint\"]</code> <code>configMaps: [\"openwebui-authentik-blueprint\"]</code> Missing Quotes <code>configMaps: [openwebui-authentik-blueprint]</code> <code>configMaps: [\"openwebui-authentik-blueprint\"]</code> Wrong Data Key <code>data: { \"blueprint.yml\": \"...\" }</code> <code>data: { \"blueprint.yaml\": \"...\" }</code> Deploy Order Helm first, then ConfigMaps ConfigMaps first, then Helm"},{"location":"package-auth-authentik-blueprints-syntax/#blueprint-updates-and-redeployment","title":"Blueprint Updates and Redeployment","text":"<p>When updating blueprints:</p> <pre><code># Update blueprint ConfigMaps\nkubectl apply -f manifests/074-authentik-openwebui-blueprint-hardcoded.yaml\n\n# Authentik automatically detects changes (within 60 minutes)\n# Or force immediate reapplication:\nkubectl rollout restart deployment/authentik-server -n authentik\n</code></pre> <p>Note: New blueprints require updating Helm values and redeploying Authentik, but existing blueprint changes are automatically detected.</p>"},{"location":"package-auth-authentik-blueprints-syntax/#core-concepts","title":"Core Concepts","text":""},{"location":"package-auth-authentik-blueprints-syntax/#models","title":"Models","text":"<p>Authentik blueprints work with Django models. Each entry specifies a model to create or modify:</p> Common Models Purpose <code>authentik_core.application</code> Applications <code>authentik_core.user</code> Users <code>authentik_core.group</code> Groups <code>authentik_providers_proxy.proxyprovider</code> Proxy providers <code>authentik_providers_oauth2.oauth2provider</code> OAuth2/OIDC providers <code>authentik_outposts.outpost</code> Outposts <code>authentik_flows.flow</code> Authentication flows"},{"location":"package-auth-authentik-blueprints-syntax/#states","title":"States","text":"State Behavior <code>present</code> (default) Keep object in sync with definition <code>created</code> Create object if it doesn't exist, don't modify if it exists <code>absent</code> Delete the object"},{"location":"package-auth-authentik-blueprints-syntax/#identifiers-vs-attributes","title":"Identifiers vs Attributes","text":"<ul> <li>Identifiers: Unique fields used to find existing objects</li> <li>Attributes: Properties to set on the object</li> </ul> <pre><code>entries:\n  - model: authentik_core.application\n    identifiers:\n      slug: \"my-app\"  # Used to find the object\n    attrs:\n      name: \"My App\"  # Properties to set\n      meta_launch_url: \"https://app.example.com\"\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#object-relationships","title":"Object Relationships","text":"<p>Objects can reference each other using special tags:</p> <pre><code>entries:\n  # Create provider first\n  - model: authentik_providers_proxy.proxyprovider\n    identifiers:\n      name: \"my-provider\"\n    # ...\n\n  # Reference provider in application\n  - model: authentik_core.application\n    identifiers:\n      slug: \"my-app\"\n    attrs:\n      provider: !Find [authentik_providers_proxy.proxyprovider, [name, my-provider]]\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#custom-yaml-tags","title":"Custom YAML Tags","text":""},{"location":"package-auth-authentik-blueprints-syntax/#find-lookup-objects","title":"!Find - Lookup Objects","text":"<p>Find objects by their attributes and return their primary key:</p> <pre><code>provider: !Find [authentik_providers_proxy.proxyprovider, [name, my-provider]]\nflow: !Find [authentik_flows.flow, [slug, default-authentication-flow]]\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#keyof-reference-by-id","title":"!KeyOf - Reference by ID","text":"<p>Reference objects defined in the same blueprint by their <code>id</code>:</p> <pre><code>entries:\n  - model: authentik_flows.flow\n    identifiers:\n      slug: \"my-flow\"\n    id: flow  # Set an ID\n\n  - model: authentik_flows.flowstagebinding\n    attrs:\n      target: !KeyOf flow  # Reference by ID\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#context-use-variables","title":"!Context - Use Variables","text":"<p>Access context variables (useful for parameterized blueprints):</p> <pre><code>context:\n  app_name: \"MyApp\"\n\nentries:\n  - model: authentik_core.application\n    attrs:\n      name: !Context app_name\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#format-string-formatting","title":"!Format - String Formatting","text":"<p>Format strings with variables:</p> <pre><code>name: !Format [\"%s-provider\", !Context app_name]\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#if-conditional-logic","title":"!If - Conditional Logic","text":"<p>Conditionally include configuration:</p> <pre><code>attrs:\n  enabled: !If [!Context production, true, false]\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#env-environment-variables","title":"!Env - Environment Variables","text":"<p>Use environment variables:</p> <pre><code>password: !Env SECRET_PASSWORD\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#file-file-contents","title":"!File - File Contents","text":"<p>Read file contents:</p> <pre><code>certificate: !File /path/to/cert.pem\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#working-examples","title":"Working Examples","text":""},{"location":"package-auth-authentik-blueprints-syntax/#example-1-application-with-proxy-provider","title":"Example 1: Application with Proxy Provider","text":"<p>This example creates a complete forward authentication setup for a whoami application:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: whoami-forward-auth-blueprint\n  namespace: authentik\n  labels:\n    app.kubernetes.io/name: authentik\n    app.kubernetes.io/component: blueprint\n    blueprints.goauthentik.io/instantiate: \"true\"\ndata:\n  whoami-simple.yaml: |\n    version: 1\n    metadata:\n      name: \"Complete Whoami Forward Auth Setup\"\n      labels:\n        blueprints.goauthentik.io/instantiate: \"true\"\n\n    context: {}\n\n    entries:\n      # Create the proxy provider first\n      - model: authentik_providers_proxy.proxyprovider\n        state: present\n        identifiers:\n          name: \"whoami-provider\"\n        attrs:\n          name: \"whoami-provider\"\n          mode: \"forward_single\"\n          external_host: \"http://whoami.localhost\"\n          access_token_validity: \"hours=24\"\n          authorization_flow: !Find [authentik_flows.flow, [slug, default-provider-authorization-implicit-consent]]\n\n      # Create the application\n      - model: authentik_core.application\n        state: present\n        identifiers:\n          slug: \"whoami\"\n        attrs:\n          name: \"whoami\"\n          provider: !Find [authentik_providers_proxy.proxyprovider, [name, whoami-provider]]\n\n      # Assign to outpost\n      - model: authentik_outposts.outpost\n        state: present\n        identifiers:\n          name: \"authentik Embedded Outpost\"\n        attrs:\n          providers:\n            - !Find [authentik_providers_proxy.proxyprovider, [name, whoami-provider]]\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#example-2-oauth2oidc-application-openwebui","title":"Example 2: OAuth2/OIDC Application (OpenWebUI)","text":"<p>This example demonstrates a complete OAuth2/OIDC setup for OpenWebUI, developed using the reverse engineering methodology described later in this document:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: openwebui-authentik-blueprint\n  namespace: authentik\n  labels:\n    app.kubernetes.io/name: authentik\n    app.kubernetes.io/component: blueprint\n    blueprints.goauthentik.io/instantiate: \"true\"\ndata:\n  openwebui.yaml: |\n    # yaml-language-server: $schema=https://goauthentik.io/blueprints/schema.json\n    version: 1\n    metadata:\n      name: \"Complete OpenWebUI OAuth2/OIDC Setup\"\n      labels:\n        blueprints.goauthentik.io/instantiate: \"true\"\n\n    context: {}\n\n    entries:\n      # Create the OAuth2/OIDC provider first (referenced by application)\n      - model: authentik_providers_oauth2.oauth2provider\n        state: present\n        identifiers:\n          name: openwebui-dev-provider\n        attrs:\n          name: openwebui-dev-provider\n          client_type: confidential\n          client_id: 1c37QuM0qm0g2BzdLbhppVwmUwUUrhmB83e9inEe\n          client_secret: BngAuX1zthtYnyAxPePAwyTqDxfVSq09IDRUTAonRcogYmhnfj39eBk709nKF4ej1OT7OMiJWVYIrwOrdizTFiyQxapQUEpDziPNucs5yxIciEx21PkK82IgURILL06h\n          # Correct redirect URIs format matching working config\n          redirect_uris:\n            - matching_mode: strict\n              url: http://openwebui.localhost/oauth/oidc/callback\n          # Token validity settings from working config\n          access_code_validity: minutes=1\n          access_token_validity: minutes=5\n          refresh_token_validity: days=30\n          # Additional settings from working config\n          sub_mode: hashed_user_id\n          issuer_mode: per_provider\n          include_claims_in_id_token: true\n          # Flow references\n          authorization_flow: !Find [authentik_flows.flow, [slug, default-provider-authorization-implicit-consent]]\n          invalidation_flow: !Find [authentik_flows.flow, [slug, default-provider-invalidation-flow]]\n          signing_key: !Find [authentik_crypto.certificatekeypair, [name, authentik Self-signed Certificate]]\n          # Property mappings - only the 3 that are actually used in working config\n          property_mappings:\n            - !Find [authentik_providers_oauth2.scopemapping, [name, \"authentik default OAuth Mapping: OpenID 'openid'\"]]\n            - !Find [authentik_providers_oauth2.scopemapping, [name, \"authentik default OAuth Mapping: OpenID 'email'\"]]\n            - !Find [authentik_providers_oauth2.scopemapping, [name, \"authentik default OAuth Mapping: OpenID 'profile'\"]]\n\n      # Create the application and link it to the provider\n      - model: authentik_core.application\n        state: present\n        identifiers:\n          slug: openwebui-dev\n        attrs:\n          name: openwebui-dev\n          slug: openwebui-dev\n          # Empty launch URL as in working config\n          meta_launch_url: \"\"\n          policy_engine_mode: any\n          provider: !Find [authentik_providers_oauth2.oauth2provider, [name, openwebui-dev-provider]]\n</code></pre> <p>Key Points from Reverse Engineering: - Complex redirect URIs format with <code>matching_mode: strict</code> is required - Multiple token validity settings must be specified exactly - Property mappings should only include the 3 actually used (openid, email, profile) - Additional OAuth2 attributes like <code>sub_mode</code>, <code>issuer_mode</code>, <code>include_claims_in_id_token</code> are crucial - Empty launch URL works better than trying to specify one</p>"},{"location":"package-auth-authentik-blueprints-syntax/#example-3-users-and-groups","title":"Example 3: Users and Groups","text":"<p>This example creates department-based groups and test users:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: users-groups-test-blueprint\n  namespace: authentik\n  labels:\n    app.kubernetes.io/name: authentik\n    app.kubernetes.io/component: blueprint\n    blueprints.goauthentik.io/instantiate: \"true\"\ndata:\n  users-groups-test-setup.yaml: |\n    version: 1\n    metadata:\n      name: \"Test Users and Groups Setup\"\n\n    entries:\n      # Create groups first\n      - model: authentik_core.group\n        state: present\n        identifiers:\n          name: \"IT Department\"\n        attrs:\n          name: \"IT Department\"\n          is_superuser: false\n          attributes:\n            department: \"IT\"\n            type: \"department_group\"\n\n      # Create users and assign to groups\n      - model: authentik_core.user\n        state: present\n        identifiers:\n          username: \"john.doe\"\n        attrs:\n          username: \"john.doe\"\n          name: \"John Doe\"\n          email: \"john.doe@company.com\"\n          password: \"TempPassword123\"\n          is_active: true\n          groups:\n            - !Find [authentik_core.group, [name, \"IT Department\"]]\n</code></pre> <p>Key Points: - Groups must be created before users that reference them - Use <code>!Find</code> to assign users to groups - Custom attributes can be added for additional metadata</p>"},{"location":"package-auth-authentik-blueprints-syntax/#blueprint-development-methodology","title":"Blueprint Development Methodology","text":""},{"location":"package-auth-authentik-blueprints-syntax/#when-blueprint-documentation-is-insufficient","title":"When Blueprint Documentation is Insufficient","text":"<p>Often, authentik's blueprint documentation lacks specific examples for complex configurations. This section describes a proven methodology for reverse engineering working configurations into blueprints.</p>"},{"location":"package-auth-authentik-blueprints-syntax/#the-manual-setup-export-method","title":"The Manual Setup + Export Method","text":"<p>This approach involves: 1. Manual Configuration in the Authentik UI 2. Export the Working Configuration using Django management commands 3. Analyze the Export to understand the correct structure 4. Create the Blueprint based on the working configuration</p>"},{"location":"package-auth-authentik-blueprints-syntax/#step-by-step-process","title":"Step-by-Step Process","text":""},{"location":"package-auth-authentik-blueprints-syntax/#1-create-manual-configuration","title":"1. Create Manual Configuration","text":"<p>Set up the desired configuration manually through the Authentik web UI:</p> <pre><code># Example: OpenWebUI OAuth2 Setup\n# 1. Navigate to authentik.localhost\n# 2. Applications &gt; Applications &gt; Create with Provider\n# 3. Configure Provider Type: OAuth2/OpenID Connect\n# 4. Set all required parameters through the UI\n# 5. Test the configuration works\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#2-export-working-configuration","title":"2. Export Working Configuration","text":"<p>Use Django management commands to export the working configuration:</p> <pre><code># Log into the authentik worker pod\nkubectl exec -it -n authentik deployment/authentik-worker -- /bin/bash\n\n# Export all blueprints to see the structure\npython manage.py export_blueprint &gt; /tmp/all-blueprints.yaml\n\n# Export specific object types\npython manage.py dumpdata authentik_providers_oauth2.oauth2provider --format=yaml &gt; /tmp/oauth2-providers.yaml\npython manage.py dumpdata authentik_core.application --format=yaml &gt; /tmp/applications.yaml\n\n# Find the object ID using Django shell\npython manage.py shell -c \"\nfrom authentik.providers.oauth2.models import OAuth2Provider\nfrom authentik.core.models import Application\n\n# Find your provider\nprovider = OAuth2Provider.objects.get(name='openwebui-dev-provider')\nprint(f'Provider ID: {provider.pk}')\nprint(f'Client Type: {provider.client_type}')\nprint(f'Redirect URIs: {provider.redirect_uris}')\n\n# Find your application\napp = Application.objects.get(slug='openwebui-dev')\nprint(f'Application ID: {app.pk}')\n\"\n\n# Export specific object by ID\npython manage.py dumpdata authentik_providers_oauth2.oauth2provider --format=yaml --pk=4 &gt; /tmp/specific-provider.yaml\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#3-analyze-export-files","title":"3. Analyze Export Files","text":"<p>Copy the export files from the pod and analyze them:</p> <pre><code># Copy exports to local machine\nkubectl cp authentik/deployment/authentik-worker:/tmp/oauth2-providers.yaml ./oauth2-export.yaml\nkubectl cp authentik/deployment/authentik-worker:/tmp/applications.yaml ./app-export.yaml\n\n# Examine the structure\ncat oauth2-export.yaml\ncat app-export.yaml\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#4-identify-key-differences","title":"4. Identify Key Differences","text":"<p>Compare your failed blueprint attempts with the working export:</p> <pre><code># Working export shows:\nredirect_uris:\n  - url: http://openwebui.localhost/oauth/oidc/callback\n    matching_mode: strict\n\n# vs failed blueprint had:\nredirect_uris: \n  - http://openwebui.localhost/oauth/oidc/callback\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#5-create-corrected-blueprint","title":"5. Create Corrected Blueprint","text":"<p>Build the blueprint using the exact working configuration format:</p> <pre><code># Use the complex object format that actually works\nredirect_uris:\n  - matching_mode: strict\n    url: http://openwebui.localhost/oauth/oidc/callback\n\n# Include all the hidden settings from exports\naccess_code_validity: minutes=1\naccess_token_validity: minutes=5  # This was missing!\nrefresh_token_validity: days=30\nsub_mode: hashed_user_id\nissuer_mode: per_provider\ninclude_claims_in_id_token: true\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#export-commands-quick-reference","title":"Export Commands Quick Reference","text":"<pre><code># Essential export commands for blueprint development:\n\n# 1. Get into authentik worker pod\nkubectl exec -it -n authentik deployment/authentik-worker -- /bin/bash\n\n# 2. Export all objects (big file, useful for finding relationships)\npython manage.py export_blueprint &gt; /tmp/all-blueprints.yaml\n\n# 3. Export specific model types\npython manage.py dumpdata authentik_providers_oauth2.oauth2provider --format=yaml &gt; /tmp/oauth2-providers.yaml\npython manage.py dumpdata authentik_providers_proxy.proxyprovider --format=yaml &gt; /tmp/proxy-providers.yaml\npython manage.py dumpdata authentik_core.application --format=yaml &gt; /tmp/applications.yaml\npython manage.py dumpdata authentik_core.user --format=yaml &gt; /tmp/users.yaml\npython manage.py dumpdata authentik_core.group --format=yaml &gt; /tmp/groups.yaml\n\n# 4. Find specific object IDs using Django shell\npython manage.py shell -c \"\nfrom authentik.providers.oauth2.models import OAuth2Provider\n# Replace with your object query\nobj = OAuth2Provider.objects.get(name='your-provider-name')\nprint(f'Object ID: {obj.pk}')\nprint(f'Key attributes: {obj.__dict__}')\n\"\n\n# 5. Export specific object by ID\npython manage.py dumpdata model_name --format=yaml --pk=object_id &gt; /tmp/specific-object.yaml\n\n# 6. Copy files back to local machine\nkubectl cp authentik/deployment/authentik-worker:/tmp/file.yaml ./local-file.yaml\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#common-models-for-export","title":"Common Models for Export","text":"Model Path Purpose Export Command <code>authentik_providers_oauth2.oauth2provider</code> OAuth2/OIDC providers <code>python manage.py dumpdata authentik_providers_oauth2.oauth2provider --format=yaml</code> <code>authentik_providers_proxy.proxyprovider</code> Proxy providers <code>python manage.py dumpdata authentik_providers_proxy.proxyprovider --format=yaml</code> <code>authentik_providers_saml.samlprovider</code> SAML providers <code>python manage.py dumpdata authentik_providers_saml.samlprovider --format=yaml</code> <code>authentik_core.application</code> Applications <code>python manage.py dumpdata authentik_core.application --format=yaml</code> <code>authentik_core.user</code> Users <code>python manage.py dumpdata authentik_core.user --format=yaml</code> <code>authentik_core.group</code> Groups <code>python manage.py dumpdata authentik_core.group --format=yaml</code> <code>authentik_flows.flow</code> Flows <code>python manage.py dumpdata authentik_flows.flow --format=yaml</code>"},{"location":"package-auth-authentik-blueprints-syntax/#success-indicators","title":"Success Indicators","text":"<p>You know your reverse engineering worked when:</p> <ol> <li>Blueprint applies without errors</li> <li>Objects appear in Authentik UI</li> <li>Configuration matches your manual setup exactly</li> <li>Application authentication works as expected</li> </ol>"},{"location":"package-auth-authentik-blueprints-syntax/#documentation-benefits","title":"Documentation Benefits","text":"<p>Always document your reverse engineering process:</p> <pre><code># Include in blueprint comments:\n# This blueprint was corrected based on working manual setup:\n# Export commands used to capture working config:\n# kubectl exec -it -n authentik deployment/authentik-worker -- python manage.py dumpdata authentik_providers_oauth2.oauth2provider --format=yaml --pk=4\n#\n# Manual setup steps for reference:\n# 1. Applications &gt; Applications &gt; Create with Provider\n# 2. Name: \"openwebui-dev\"\n# [... detailed manual steps ...]\n</code></pre> <p>This methodology ensures you can reliably create blueprints for any Authentik configuration, regardless of documentation gaps.</p>"},{"location":"package-auth-authentik-blueprints-syntax/#common-use-cases","title":"Common Use Cases","text":""},{"location":"package-auth-authentik-blueprints-syntax/#1-oidc-application-setup-simple","title":"1. OIDC Application Setup (Simple)","text":"<pre><code>entries:\n  - model: authentik_providers_oauth2.oauth2provider\n    identifiers:\n      name: \"my-oidc-app-provider\"\n    attrs:\n      name: \"my-oidc-app-provider\"\n      client_type: \"confidential\"\n      client_id: \"my-oidc-app\"\n      authorization_flow: !Find [authentik_flows.flow, [slug, default-provider-authorization-implicit-consent]]\n\n  - model: authentik_core.application\n    identifiers:\n      slug: \"my-oidc-app\"\n    attrs:\n      name: \"My OIDC App\"\n      provider: !Find [authentik_providers_oauth2.oauth2provider, [name, my-oidc-app-provider]]\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#2-saml-application-setup","title":"2. SAML Application Setup","text":"<pre><code>entries:\n  - model: authentik_providers_saml.samlprovider\n    identifiers:\n      name: \"my-saml-app-provider\"\n    attrs:\n      name: \"my-saml-app-provider\"\n      acs_url: \"https://app.example.com/saml/acs\"\n      issuer: \"https://auth.example.com\"\n      sp_binding: \"post\"\n      authorization_flow: !Find [authentik_flows.flow, [slug, default-provider-authorization-implicit-consent]]\n\n  - model: authentik_core.application\n    identifiers:\n      slug: \"my-saml-app\"\n    attrs:\n      name: \"My SAML App\"\n      provider: !Find [authentik_providers_saml.samlprovider, [name, my-saml-app-provider]]\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#3-bulk-user-creation","title":"3. Bulk User Creation","text":"<pre><code>entries:\n  - model: authentik_core.group\n    identifiers:\n      name: \"Employees\"\n    attrs:\n      name: \"Employees\"\n\n  # Use context for parameterized user creation\n  - model: authentik_core.user\n    identifiers:\n      username: !Context username\n    attrs:\n      username: !Context username\n      email: !Context email\n      name: !Context full_name\n      is_active: true\n      groups:\n        - !Find [authentik_core.group, [name, \"Employees\"]]\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#4-custom-flow-creation","title":"4. Custom Flow Creation","text":"<pre><code>entries:\n  - model: authentik_flows.flow\n    identifiers:\n      slug: \"custom-auth-flow\"\n    id: custom_flow\n    attrs:\n      name: \"Custom Authentication\"\n      title: \"Custom Login\"\n      designation: \"authentication\"\n\n  - model: authentik_stages_identification.identificationstage\n    identifiers:\n      name: \"custom-identification\"\n    id: identification_stage\n\n  - model: authentik_flows.flowstagebinding\n    identifiers:\n      target: !KeyOf custom_flow\n      stage: !KeyOf identification_stage\n      order: 10\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#best-practices","title":"Best Practices","text":""},{"location":"package-auth-authentik-blueprints-syntax/#file-organization","title":"File Organization","text":"<ol> <li>Use descriptive file names: <code>073-authentik-whoami-blueprint.yaml</code></li> <li>Include comprehensive headers with description, usage, and prerequisites</li> <li>Group related configurations in single blueprints when logical</li> </ol>"},{"location":"package-auth-authentik-blueprints-syntax/#documentation-standards","title":"Documentation Standards","text":"<pre><code># File: manifests/XXX-authentik-[purpose]-blueprint.yaml\n#\n# Description:\n# Brief description of what this blueprint does\n#\n# Usage:\n#   kubectl apply -f XXX-authentik-[purpose]-blueprint.yaml\n#\n# Prerequisites:\n# - List prerequisites here\n#\n# This blueprint replaces these manual UI steps:\n# 1. Step 1\n# 2. Step 2\n# ...\n#\n# Development notes:\n# - Include reverse engineering details if used\n# - Document export commands used\n# - Note any special configuration requirements\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#dependency-management","title":"Dependency Management","text":"<ol> <li>Order matters: Create referenced objects first</li> <li>Use !Find for cross-references: Don't hardcode IDs</li> <li>Consider using !KeyOf for objects in the same blueprint</li> </ol>"},{"location":"package-auth-authentik-blueprints-syntax/#security-considerations","title":"Security Considerations","text":"<ol> <li>Avoid hardcoded secrets in blueprints</li> <li>Use !Env for sensitive data: <code>password: !Env SECRET_PASSWORD</code></li> <li>Mark test blueprints clearly: Include environment indicators</li> <li>Use proper RBAC: Don't give excessive permissions</li> </ol>"},{"location":"package-auth-authentik-blueprints-syntax/#testing-and-validation","title":"Testing and Validation","text":"<ol> <li>Use schema validation: Include schema comment at top of files</li> <li>Test in development first: Use separate namespaces/environments</li> <li>Verify object creation: Check authentik UI after applying</li> <li>Monitor blueprint status: Use kubectl to check ConfigMap status</li> </ol>"},{"location":"package-auth-authentik-blueprints-syntax/#troubleshooting","title":"Troubleshooting","text":""},{"location":"package-auth-authentik-blueprints-syntax/#common-issues","title":"Common Issues","text":""},{"location":"package-auth-authentik-blueprints-syntax/#blueprint-not-applied","title":"Blueprint Not Applied","text":"<p>Symptoms: Blueprint exists but objects aren't created</p> <p>Solutions: 1. Check labels on ConfigMap:    <pre><code>labels:\n  blueprints.goauthentik.io/instantiate: \"true\"\n</code></pre> 2. Verify authentik can read the ConfigMap 3. Check authentik logs for blueprint errors</p>"},{"location":"package-auth-authentik-blueprints-syntax/#object-not-found-errors","title":"Object Not Found Errors","text":"<p>Symptoms: <code>!Find</code> or <code>!KeyOf</code> tags failing</p> <p>Solutions: 1. Ensure referenced objects are created first 2. Check identifiers match exactly 3. Verify object actually exists in authentik</p>"},{"location":"package-auth-authentik-blueprints-syntax/#invalid-blueprint-format","title":"Invalid Blueprint Format","text":"<p>Symptoms: Blueprint validation errors</p> <p>Solutions: 1. Use schema validation in your editor 2. Check YAML syntax 3. Verify all required fields are present 4. Use the reverse engineering method to get correct format</p>"},{"location":"package-auth-authentik-blueprints-syntax/#complex-configuration-failures","title":"Complex Configuration Failures","text":"<p>Symptoms: Blueprint applies but configuration doesn't work</p> <p>Solutions: 1. Apply the reverse engineering methodology:    - Set up manually in UI    - Export working configuration    - Compare with your blueprint    - Update blueprint with exact working values 2. Check for missing required attributes 3. Verify object relationships are correct</p>"},{"location":"package-auth-authentik-blueprints-syntax/#permission-denied","title":"Permission Denied","text":"<p>Symptoms: Blueprint can't create objects</p> <p>Solutions: 1. Check authentik service account permissions 2. Verify RBAC configuration 3. Check namespace permissions</p>"},{"location":"package-auth-authentik-blueprints-syntax/#advanced-debugging","title":"Advanced Debugging","text":""},{"location":"package-auth-authentik-blueprints-syntax/#using-export-commands-for-debugging","title":"Using Export Commands for Debugging","text":"<p>When blueprints fail, use exports to understand the current state:</p> <pre><code># Check what actually got created\nkubectl exec -it -n authentik deployment/authentik-worker -- python manage.py shell -c \"\nfrom authentik.providers.oauth2.models import OAuth2Provider\nfrom authentik.core.models import Application\n\n# List all providers to see what exists\nfor p in OAuth2Provider.objects.all():\n    print(f'Provider: {p.name} (ID: {p.pk})')\n\n# List all applications\nfor a in Application.objects.all():\n    print(f'Application: {a.name} (Slug: {a.slug})')\n\"\n\n# Export failed/partial configuration\npython manage.py dumpdata authentik_providers_oauth2.oauth2provider --format=yaml --pk=problem_id &gt; /tmp/debug.yaml\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#useful-commands","title":"Useful Commands","text":"<pre><code># Check ConfigMap status\nkubectl get configmap -n authentik\n\n# View ConfigMap contents\nkubectl describe configmap whoami-forward-auth-blueprint -n authentik\n\n# Check authentik logs for blueprint processing\nkubectl logs -n authentik deployment/authentik-server | grep -i blueprint\n\n# Check worker logs for Django errors\nkubectl logs -n authentik deployment/authentik-worker\n\n# Validate YAML locally\nyamllint blueprint.yaml\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#quick-reference","title":"Quick Reference","text":""},{"location":"package-auth-authentik-blueprints-syntax/#essential-models","title":"Essential Models","text":"Model Purpose Key Identifiers <code>authentik_core.application</code> Applications <code>slug</code> <code>authentik_core.user</code> Users <code>username</code> <code>authentik_core.group</code> Groups <code>name</code> <code>authentik_providers_proxy.proxyprovider</code> Proxy providers <code>name</code> <code>authentik_providers_oauth2.oauth2provider</code> OIDC providers <code>name</code> <code>authentik_providers_saml.samlprovider</code> SAML providers <code>name</code> <code>authentik_outposts.outpost</code> Outposts <code>name</code> <code>authentik_flows.flow</code> Flows <code>slug</code>"},{"location":"package-auth-authentik-blueprints-syntax/#common-attributes","title":"Common Attributes","text":""},{"location":"package-auth-authentik-blueprints-syntax/#application-authentik_coreapplication","title":"Application (<code>authentik_core.application</code>)","text":"<pre><code>attrs:\n  name: \"App Name\"\n  slug: \"app-slug\"\n  meta_launch_url: \"https://app.example.com\"  # Can be empty \"\"\n  provider: !Find [provider_model, [name, provider-name]]\n  policy_engine_mode: any\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#oauth2-provider-authentik_providers_oauth2oauth2provider","title":"OAuth2 Provider (<code>authentik_providers_oauth2.oauth2provider</code>)","text":"<pre><code>attrs:\n  name: \"provider-name\"\n  client_type: \"confidential\"  # or \"public\"\n  client_id: \"your-client-id\"\n  client_secret: \"your-client-secret\"\n  redirect_uris:\n    - matching_mode: strict  # Important: complex format often required\n      url: \"https://app.example.com/callback\"\n  access_code_validity: \"minutes=1\"\n  access_token_validity: \"hours=1\"  # Or minutes=5 for short-lived\n  refresh_token_validity: \"days=30\"\n  sub_mode: \"hashed_user_id\"\n  issuer_mode: \"per_provider\"\n  include_claims_in_id_token: true\n  authorization_flow: !Find [authentik_flows.flow, [slug, default-provider-authorization-implicit-consent]]\n  invalidation_flow: !Find [authentik_flows.flow, [slug, default-provider-invalidation-flow]]\n  signing_key: !Find [authentik_crypto.certificatekeypair, [name, authentik Self-signed Certificate]]\n  property_mappings:\n    - !Find [authentik_providers_oauth2.scopemapping, [name, \"authentik default OAuth Mapping: OpenID 'openid'\"]]\n    - !Find [authentik_providers_oauth2.scopemapping, [name, \"authentik default OAuth Mapping: OpenID 'email'\"]]\n    - !Find [authentik_providers_oauth2.scopemapping, [name, \"authentik default OAuth Mapping: OpenID 'profile'\"]]\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#proxy-provider-authentik_providers_proxyproxyprovider","title":"Proxy Provider (<code>authentik_providers_proxy.proxyprovider</code>)","text":"<pre><code>attrs:\n  name: \"provider-name\"\n  mode: \"forward_single\"  # or \"forward_domain\", \"proxy\"\n  external_host: \"https://app.example.com\"\n  authorization_flow: !Find [authentik_flows.flow, [slug, flow-slug]]\n  access_token_validity: \"hours=24\"\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#user-authentik_coreuser","title":"User (<code>authentik_core.user</code>)","text":"<pre><code>attrs:\n  username: \"username\"\n  email: \"user@example.com\"\n  name: \"Full Name\"\n  password: \"password\"\n  is_active: true\n  groups:\n    - !Find [authentik_core.group, [name, \"Group Name\"]]\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#group-authentik_coregroup","title":"Group (<code>authentik_core.group</code>)","text":"<pre><code>attrs:\n  name: \"Group Name\"\n  is_superuser: false\n  attributes:\n    custom_field: \"value\"\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#template-checklist","title":"Template Checklist","text":"<ul> <li>[ ] Schema comment at top</li> <li>[ ] Version set to 1</li> <li>[ ] Metadata name defined</li> <li>[ ] Proper instantiate label</li> <li>[ ] All dependencies ordered correctly</li> <li>[ ] Identifiers use unique fields</li> <li>[ ] States defined appropriately</li> <li>[ ] Documentation header complete</li> <li>[ ] Reverse engineering methodology documented if used</li> </ul>"},{"location":"package-auth-authentik-blueprints-syntax/#file-template","title":"File Template","text":"<pre><code># File: manifests/XXX-authentik-[purpose]-blueprint.yaml\n#\n# Description: [What this blueprint does]\n# Usage: kubectl apply -f XXX-authentik-[purpose]-blueprint.yaml\n# Prerequisites: [List prerequisites]\n#\n# Manual setup steps for reference:\n# [Include detailed UI steps for fallback/troubleshooting]\n#\n# Development methodology:\n# [If reverse engineered, include export commands used]\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: [blueprint-name]\n  namespace: authentik\n  labels:\n    app.kubernetes.io/name: authentik\n    app.kubernetes.io/component: blueprint\n    blueprints.goauthentik.io/instantiate: \"true\"\ndata:\n  blueprint.yaml: |\n    # yaml-language-server: $schema=https://goauthentik.io/blueprints/schema.json\n    version: 1\n    metadata:\n      name: \"[Blueprint Name]\"\n      labels:\n        blueprints.goauthentik.io/instantiate: \"true\"\n\n    context: {}\n\n    entries:\n      # Your entries here\n</code></pre>"},{"location":"package-auth-authentik-blueprints-syntax/#resources","title":"Resources","text":"<ul> <li>Official Authentik Blueprint Documentation</li> <li>Blueprint Schema</li> <li>Example Blueprints</li> <li>YAML Custom Tags</li> </ul> <p>Last updated: September 2025</p>"},{"location":"package-auth-authentik-readme/","title":"Authentication Package","text":"<p>The Authentication package is a comprehensive self-hosted identity and access management (IAM) platform that enables organizations to implement enterprise-grade authentication, authorization, and single sign-on (SSO) capabilities. This implementation is based on the Authentik project, enhanced with our Auth10 dynamic authentication system that provides automated multi-domain service protection through configuration-driven templates.</p>"},{"location":"package-auth-authentik-readme/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Getting Started</li> <li>Auth10 Dynamic Authentication System</li> <li>Technical Implementation</li> <li>System Architecture</li> <li>Use Cases</li> <li>Authentik Stack Setup</li> <li>Developer Workflow</li> <li>Security and Compliance</li> <li>Monitoring and Management</li> <li>Troubleshooting</li> <li>Related Documentation</li> <li>Future Enhancements</li> </ol>"},{"location":"package-auth-authentik-readme/#getting-started","title":"Getting Started","text":""},{"location":"package-auth-authentik-readme/#prerequisites","title":"Prerequisites","text":"<p>Before installing the Authentication package, you must configure your services in the <code>kubernetes-secrets.yml</code> file:</p> <ol> <li>Configure Protected Services: Edit <code>topsecret/kubernetes/kubernetes-secrets.yml</code> and add your services to the <code>protected_services</code> section</li> <li>Set Domain Configuration: Ensure your <code>domains</code> section includes the domains you want to use (localhost, tailscale, cloudflare)</li> </ol> <p>For detailed configuration instructions, see: Auth10 Developer Guide - Configuration</p>"},{"location":"package-auth-authentik-readme/#installation","title":"Installation","text":"<p>Once your configuration is ready, run the following command on your host computer:</p> <pre><code>./scripts/packages/auth.sh\n</code></pre> <p>The script will install the Authentication package and start the Authentik frontend. You can then access the Authentik admin interface at http://authentik.localhost/if/admin/.</p> <p>The install takes a while to complete as it sets up the database, creates the initial admin user, and configures all authentication flows. The system is designed to be production-ready from the first deployment.</p>"},{"location":"package-auth-authentik-readme/#default-credentials","title":"Default Credentials","text":"<p>After installation, you can access the system with these default credentials: - Admin URL: http://authentik.localhost/if/admin/ - Username: <code>admin@urbalurba.local</code> - Password: <code>SecretPassword1</code></p> <p>Important: Change these default credentials immediately after first login for security purposes.</p>"},{"location":"package-auth-authentik-readme/#installation-progress-and-troubleshooting","title":"Installation Progress and Troubleshooting","text":"<p>For detailed installation monitoring, testing procedures, and troubleshooting, see: Auth10 Developer Guide</p>"},{"location":"package-auth-authentik-readme/#auth10-dynamic-authentication-system","title":"Auth10 Dynamic Authentication System","text":"<p>Auth10 is our advanced template-driven authentication system that automatically generates multi-domain authentication configurations. It represents a major evolution from manual blueprint creation to fully automated, configuration-driven authentication.</p>"},{"location":"package-auth-authentik-readme/#key-benefits","title":"Key Benefits","text":"<ul> <li>\ud83c\udfaf Configuration-Driven: Define services once in <code>kubernetes-secrets.yml</code>, get multi-domain authentication automatically</li> <li>\ud83c\udf10 Multi-Domain Support: Automatic provider creation for localhost, Tailscale, and Cloudflare domains</li> <li>\ud83d\udd04 Dynamic Generation: Templates rendered during deployment, always current and consistent</li> <li>\ud83d\udee1\ufe0f External Domain Solution: Solves the manual provider configuration problem for external domains</li> <li>\ud83d\udcdd Zero Manual Setup: No manual provider configuration in Authentik UI required</li> <li>\ud83d\ude80 Enterprise-Grade: Production-ready security, compliance features, and scalability</li> <li>\u26a1 Rapid Deployment: Add new services by updating configuration only</li> </ul>"},{"location":"package-auth-authentik-readme/#how-auth10-works","title":"How Auth10 Works","text":"<ol> <li>Configuration: Define domains and protected services in <code>kubernetes-secrets.yml</code></li> <li>Template Rendering: Ansible renders Jinja2 templates during deployment</li> <li>Blueprint Generation: Dynamic blueprints created with multi-domain providers</li> <li>Outpost Linking: All providers automatically linked to embedded outpost</li> <li>Multi-Domain Auth: External domains work automatically without manual setup</li> </ol> <p>For practical usage examples and step-by-step instructions, see: <code>docs/package-auth-authentik-auth10-developer-guide.md</code></p>"},{"location":"package-auth-authentik-readme/#supported-authentication-types","title":"Supported Authentication Types","text":"<ul> <li><code>proxy</code>: Forward authentication (most services)</li> <li><code>oauth2</code>: Direct OAuth2/OIDC integration (Grafana, etc.)</li> <li><code>basic</code>: HTTP Basic Authentication (simple services)</li> </ul> <p>For complete Auth10 documentation, see: <code>docs/package-auth-authentik-auth10.md</code></p>"},{"location":"package-auth-authentik-readme/#technical-implementation","title":"Technical Implementation","text":"<p>Our implementation includes significant enhancements over standard Authentik, including shared database/Redis integration, advanced Traefik routing, enhanced security features, and enterprise-grade monitoring capabilities.</p> <p>For detailed technical implementation information, see: Technical Implementation Guide</p>"},{"location":"package-auth-authentik-readme/#system-architecture","title":"System Architecture","text":"<pre><code>graph TD\n    subgraph \"User Authentication\"\n        A[User Request] --&gt; B[Traefik Ingress]\n        B --&gt; C{Authenticated?}\n        C --&gt;|No| D[Forward Auth Middleware]\n        D --&gt; E[Authentik Embedded Outpost]\n        E --&gt; F[OAuth2 Redirect]\n        F --&gt; G[Authentik Login]\n        G --&gt; H[Authentication Success]\n        H --&gt; I[Redirect to Application]\n    end\n\n    subgraph \"Backend Services\"\n        J[Authentik Server] --&gt; K[PostgreSQL Database]\n        J --&gt; L[Redis Cache]\n        M[Authentik Worker] --&gt; N[Background Tasks]\n    end\n\n    subgraph \"Protected Applications\"\n        O[Whoami App] --&gt; P[Authentication Headers]\n        Q[Custom Apps] --&gt; R[OAuth2/SAML Integration]\n    end\n\n    I --&gt; P\n    I --&gt; R\n    K --&gt; E\n    L --&gt; E\n</code></pre>"},{"location":"package-auth-authentik-readme/#use-cases","title":"Use Cases","text":""},{"location":"package-auth-authentik-readme/#1-forward-authentication-for-applications","title":"1. Forward Authentication for Applications","text":"<pre><code>sequenceDiagram\n    participant User\n    participant App\n    participant Traefik\n    participant Authentik\n    participant Database\n\n    User-&gt;&gt;App: Access Protected Resource\n    App-&gt;&gt;Traefik: Forward Request\n    Traefik-&gt;&gt;Authentik: Check Authentication\n    Authentik-&gt;&gt;Database: Verify User Session\n    Database--&gt;&gt;Authentik: Session Valid\n    Authentik--&gt;&gt;Traefik: User Authenticated\n    Traefik--&gt;&gt;App: Forward with Auth Headers\n    App--&gt;&gt;User: Protected Resource\n</code></pre>"},{"location":"package-auth-authentik-readme/#2-oauth2-provider-for-applications","title":"2. OAuth2 Provider for Applications","text":"<pre><code>graph LR\n    subgraph \"OAuth2 Flow\"\n        A[User] --&gt; B[Application]\n        B --&gt; C[Authentik OAuth2]\n        C --&gt; D[User Login]\n        D --&gt; E[Authorization Grant]\n        E --&gt; F[Access Token]\n        F --&gt; G[API Access]\n    end\n\n    subgraph \"Application Types\"\n        H[Web Applications] --&gt; I[SPA Support]\n        J[Mobile Apps] --&gt; K[Native App Support]\n        L[API Services] --&gt; M[Token Validation]\n    end\n</code></pre>"},{"location":"package-auth-authentik-readme/#3-enterprise-sso-integration","title":"3. Enterprise SSO Integration","text":"<pre><code>graph TD\n    subgraph \"Identity Sources\"\n        A[Local Users] --&gt; D[Central Directory]\n        B[LDAP/AD] --&gt; D\n        C[HR System] --&gt; D\n    end\n\n    subgraph \"SSO Providers\"\n        D --&gt; E[SAML Service Provider]\n        E --&gt; F[Enterprise Applications]\n        D --&gt; G[OAuth2 Provider]\n        G --&gt; H[Modern Web Apps]\n    end\n\n    subgraph \"Access Control\"\n        I[Role-Based Access] --&gt; J[Permission Management]\n        J --&gt; K[Policy Enforcement]\n        K --&gt; L[Audit Logging]\n    end\n</code></pre> <p>The platform supports flexible authentication configuration per application: - Internal applications can use forward authentication for seamless protection - Modern web applications can integrate via OAuth2 for enhanced user experience - Enterprise applications can use SAML for traditional SSO integration - All configurations are managed through a centralized admin interface</p>"},{"location":"package-auth-authentik-readme/#deployment-architecture","title":"Deployment Architecture","text":"<p>The authentication system is deployed using an Ansible playbook that creates a complete Kubernetes-based infrastructure including:</p> <ul> <li>Shared Services: PostgreSQL database and Redis cache shared across applications</li> <li>Authentik Components: Server, worker, and embedded outpost for authentication</li> <li>Traefik Integration: Forward authentication middleware and ingress routing</li> <li>Security Features: TLS encryption, audit logging, and session management</li> </ul> <p>For detailed deployment information, component specifications, and configuration details, see: Technical Implementation Guide</p>"},{"location":"package-auth-authentik-readme/#authentik-working-mechanism","title":"Authentik Working Mechanism","text":""},{"location":"package-auth-authentik-readme/#how-authentik-works-after-cluster-reset","title":"How Authentik Works After Cluster Reset","text":""},{"location":"package-auth-authentik-readme/#post-reset-initialization-process","title":"Post-Reset Initialization Process","text":"<p>After a cluster reset, Authentik goes through a specific initialization sequence:</p> <ol> <li>Helm Deployment</li> <li>Authentik server and worker pods start</li> <li>Database connections established</li> <li> <p>Basic configuration loaded</p> </li> <li> <p>Blueprint Processing</p> </li> <li>Blueprint discovery task enqueued</li> <li>Applications and providers created automatically</li> <li> <p>Outpost configuration stored in database</p> </li> <li> <p>Outpost Initialization</p> </li> <li>Configuration: Outpost settings are correct by default</li> <li>Runtime State: But outpost process is not fully activated</li> <li> <p>Status: Configuration exists but not being applied</p> </li> <li> <p>Activation Trigger</p> </li> <li>Admin Interface Access: Logging into <code>http://authentik.localhost</code></li> <li>Application Viewing: Viewing any application in the admin interface</li> <li>Result: Outpost process \"wakes up\" and applies configuration</li> </ol>"},{"location":"package-auth-authentik-readme/#why-admin-interface-access-triggers-activation","title":"Why Admin Interface Access Triggers Activation","text":"<p>The Authentik admin interface access serves as a configuration activation trigger:</p> <ul> <li>Database Connection: Establishes full database connectivity</li> <li>Configuration Loading: Forces outpost to read its configuration</li> <li>Process Initialization: Completes outpost process initialization</li> <li>Runtime Activation: Applies configuration to running process</li> </ul>"},{"location":"package-auth-authentik-readme/#outpost-configuration-states","title":"Outpost Configuration States","text":"<pre><code># State 1: After Cluster Reset (Inactive)\nConfiguration: \u2705 Correct (authentik_host_browser: \"\")\nRuntime: \u274c Not applied (generates 0.0.0.0:9000 URLs)\n\n# State 2: After Admin Interface Access (Active)\nConfiguration: \u2705 Correct (authentik_host_browser: \"\")\nRuntime: \u2705 Applied (generates correct URLs)\n\n# State 3: After Manual Configuration (Active)\nConfiguration: \u2705 Custom settings\nRuntime: \u2705 Applied (generates custom URLs)\n</code></pre>"},{"location":"package-auth-authentik-readme/#dynamic-host-detection-mechanism","title":"Dynamic Host Detection Mechanism","text":"<p>When <code>authentik_host_browser</code> is empty (the default):</p> <ol> <li>Outpost detects incoming domain from the request</li> <li>Generates redirect URLs using the same domain</li> <li>Automatically adapts to both <code>.localhost</code> and <code>.urbalurba.no</code></li> <li>No manual configuration needed for multi-domain support</li> </ol>"},{"location":"package-auth-authentik-readme/#configuration-persistence","title":"Configuration Persistence","text":"<ul> <li>Outpost settings are stored in the database</li> <li>Survive pod restarts and cluster resets</li> <li>Blueprint recreates applications and providers</li> <li>But outpost configuration needs activation trigger</li> </ul>"},{"location":"package-auth-authentik-readme/#best-practices-for-cluster-resets","title":"Best Practices for Cluster Resets","text":"<ol> <li>Deploy all manifests first</li> <li>Wait for blueprint processing (check logs for completion)</li> <li>Access admin interface to trigger outpost activation</li> <li>Test authentication flow to verify everything works</li> <li>No manual configuration changes needed</li> </ol>"},{"location":"package-auth-authentik-readme/#troubleshooting-flow","title":"Troubleshooting Flow","text":"<pre><code>graph TD\n    A[Cluster Reset] --&gt; B[Deploy Manifests]\n    B --&gt; C[Wait for Blueprint]\n    C --&gt; D[Access Admin Interface]\n    D --&gt; E{Outpost Working?}\n    E --&gt;|Yes| F[\u2705 Success]\n    E --&gt;|No| G[Pod Restart]\n    G --&gt; H{Outpost Working?}\n    H --&gt;|Yes| I[\u2705 Success]\n    H --&gt;|No| J[Manual Configuration]\n    J --&gt; K[\u2705 Success]\n</code></pre>"},{"location":"package-auth-authentik-readme/#developer-resources","title":"Developer Resources","text":"<p>For developers working with the authentication system:</p> <ul> <li>Quick Start Guide: See Auth10 Developer Guide for step-by-step instructions</li> <li>Configuration Examples: Practical examples for all service types and use cases</li> <li>Integration Code: Ready-to-use code examples for popular frameworks</li> <li>Testing Workflows: Multi-environment testing strategies and automation</li> <li>Troubleshooting: Debug commands and common issue resolution</li> </ul>"},{"location":"package-auth-authentik-readme/#enterprise-features","title":"Enterprise Features","text":"<p>The authentication system provides enterprise-grade security, compliance, and management capabilities including:</p> <ul> <li>Security: Multi-factor authentication, brute force protection, secure session management</li> <li>Compliance: GDPR, SOC 2, HIPAA readiness with configurable policies</li> <li>Monitoring: Health checks, audit logging, performance metrics</li> <li>Management: Automated backups, disaster recovery, structured logging</li> </ul> <p>For detailed security specifications, compliance features, monitoring procedures, and troubleshooting guides, see: Technical Implementation Guide</p>"},{"location":"package-auth-authentik-readme/#related-documentation","title":"Related Documentation","text":"Task Documentation Get Started Auth10 Developer Guide Configure Services Auth10 System Guide Technical Details Technical Implementation Guide Blueprint Syntax Blueprint Syntax Reference Test Users Test Users Guide"},{"location":"package-auth-authentik-readme/#summary","title":"Summary","text":"<p>The Authentication package provides enterprise-grade identity and access management with the Auth10 system enabling configuration-driven, multi-domain authentication. It offers production-ready deployment automation, comprehensive security features, and seamless integration for both developers and system administrators.</p> <p>The platform enables rapid development of authenticated applications while maintaining security best practices and providing a clear path to production deployment across localhost, Tailscale, and Cloudflare domains.</p>"},{"location":"package-auth-authentik-technical-implementation/","title":"Authentik Technical Implementation","text":""},{"location":"package-auth-authentik-technical-implementation/#overview","title":"Overview","text":"<p>This document details the technical implementation of the Authentik authentication system in the Urbalurba infrastructure. While our implementation is based on the open-source Authentik project, we've made several significant modifications to enhance its capabilities and better suit enterprise needs in a Kubernetes environment.</p>"},{"location":"package-auth-authentik-technical-implementation/#implementation-differences-from-standard-authentik","title":"Implementation Differences from Standard Authentik","text":""},{"location":"package-auth-authentik-technical-implementation/#1-database-integration","title":"1. Database Integration","text":"<ul> <li>Original: Includes PostgreSQL as part of the Helm chart deployment</li> <li>Our Implementation: Uses shared PostgreSQL instance across multiple services</li> <li>Single PostgreSQL instance serving multiple applications</li> <li>Dedicated <code>authentik</code> database with proper user permissions</li> <li>Shared backup and recovery procedures</li> <li>Centralized database monitoring and management</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#2-session-management","title":"2. Session Management","text":"<ul> <li>Original: Includes Redis as part of the Helm chart deployment</li> <li>Our Implementation: Uses shared Redis instance across multiple services</li> <li>Single Redis instance serving multiple applications</li> <li>Shared session management and caching layer</li> <li>Centralized Redis monitoring and management</li> <li>Enhanced security through shared authentication</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#3-ingress-and-routing","title":"3. Ingress and Routing","text":"<ul> <li>Original: Basic Traefik integration</li> <li>Our Implementation: Advanced Traefik integration with forward authentication</li> <li>Forward authentication middleware for protecting applications</li> <li>Proper ingress class configuration</li> <li>SSL/TLS termination support</li> <li>Catch-all routing for undefined hostnames</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#4-forward-authentication","title":"4. Forward Authentication","text":"<ul> <li>Original: Basic forward authentication capabilities</li> <li>Our Implementation: Enhanced forward authentication with embedded outpost</li> <li>Kubernetes-native deployment</li> <li>Automatic service discovery</li> <li>Enhanced security and isolation</li> <li>Better integration with Traefik</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#5-storage-architecture","title":"5. Storage Architecture","text":"<ul> <li>Original: Uses embedded storage solutions</li> <li>Our Implementation: Kubernetes-native persistent storage</li> <li>Better data persistence</li> <li>Improved backup capabilities</li> <li>Enhanced scalability</li> <li>Better resource management</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#6-deployment-architecture","title":"6. Deployment Architecture","text":"<ul> <li>Original: Designed for simpler deployments</li> <li>Our Implementation: Kubernetes-native deployment</li> <li>Better scalability</li> <li>Enhanced reliability</li> <li>Improved resource management</li> <li>Better integration with enterprise infrastructure</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#7-security-enhancements","title":"7. Security Enhancements","text":"<ul> <li>Original: Basic security features</li> <li>Our Implementation: Enhanced security features</li> <li>Centralized secret management</li> <li>Advanced access control</li> <li>Better audit capabilities</li> <li>Enhanced monitoring and alerting</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#8-monitoring-and-management","title":"8. Monitoring and Management","text":"<ul> <li>Original: Basic monitoring capabilities</li> <li>Our Implementation: Enhanced monitoring and management</li> <li>Comprehensive health checks</li> <li>Detailed logging and audit trails</li> <li>Better resource monitoring</li> <li>Enhanced troubleshooting capabilities</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#core-features","title":"Core Features","text":""},{"location":"package-auth-authentik-technical-implementation/#identity-management","title":"Identity Management","text":"<ul> <li>User and group management with hierarchical organization</li> <li>Role-based access control (RBAC)</li> <li>Multi-factor authentication (MFA) support</li> <li>User provisioning and deprovisioning workflows</li> <li>Password policies and complexity requirements</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#authentication-flows","title":"Authentication Flows","text":"<ul> <li>OAuth2 provider for modern web applications</li> <li>SAML provider for enterprise SSO integration</li> <li>OpenID Connect support</li> <li>Custom authentication flows and policies</li> <li>Social login integration capabilities</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#forward-authentication","title":"Forward Authentication","text":"<ul> <li>Protect any application with authentication</li> <li>Seamless integration with Traefik ingress</li> <li>Automatic redirect to login for unauthenticated users</li> <li>Session management and persistence</li> <li>Support for multiple protected applications</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#application-integration","title":"Application Integration","text":"<ul> <li>OAuth2 client applications</li> <li>SAML service providers</li> <li>API authentication and authorization</li> <li>Custom application integration</li> <li>Webhook and notification support</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#security-compliance","title":"Security &amp; Compliance","text":"<ul> <li>Audit logging and compliance reporting</li> <li>Data encryption at rest and in transit</li> <li>GDPR and privacy compliance features</li> <li>Security event monitoring</li> <li>Backup and disaster recovery</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#enterprise-features","title":"Enterprise Features","text":""},{"location":"package-auth-authentik-technical-implementation/#multi-tenant-support","title":"Multi-tenant Support","text":"<ul> <li>Organization-based user management</li> <li>Department and team structures</li> <li>Granular permission management</li> <li>Custom branding per organization</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#advanced-policies","title":"Advanced Policies","text":"<ul> <li>Conditional access policies</li> <li>Risk-based authentication</li> <li>Geographic access restrictions</li> <li>Time-based access controls</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#integration-capabilities","title":"Integration Capabilities","text":"<ul> <li>LDAP/Active Directory integration</li> <li>HR system synchronization</li> <li>Custom API integrations</li> <li>Webhook notifications</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#design-philosophy","title":"Design Philosophy","text":"<p>The platform is designed to operate entirely offline while maintaining enterprise-grade security and scalability features. It provides organizations with a secure, cost-effective way to implement comprehensive identity and access management while maintaining control over user access and security policies.</p>"},{"location":"package-auth-authentik-technical-implementation/#system-architecture","title":"System Architecture","text":"<pre><code>graph TD\n    subgraph \"User Authentication\"\n        A[User Request] --&gt; B[Traefik Ingress]\n        B --&gt; C{Authenticated?}\n        C --&gt;|No| D[Forward Auth Middleware]\n        D --&gt; E[Authentik Embedded Outpost]\n        E --&gt; F[OAuth2 Redirect]\n        F --&gt; G[Authentik Login]\n        G --&gt; H[Authentication Success]\n        H --&gt; I[Redirect to Application]\n    end\n\n    subgraph \"Backend Services\"\n        J[Authentik Server] --&gt; K[PostgreSQL Database]\n        J --&gt; L[Redis Cache]\n        M[Authentik Worker] --&gt; N[Background Tasks]\n    end\n\n    subgraph \"Protected Applications\"\n        O[Whoami App] --&gt; P[Authentication Headers]\n        Q[Custom Apps] --&gt; R[OAuth2/SAML Integration]\n    end\n\n    I --&gt; P\n    I --&gt; R\n    K --&gt; E\n    L --&gt; E\n</code></pre>"},{"location":"package-auth-authentik-technical-implementation/#authentik-stack-setup","title":"Authentik Stack Setup","text":"<p>The Authentication stack is set up using an Ansible playbook (<code>070-setup-authentik.yml</code>) that deploys a complete authentication infrastructure on Kubernetes. The stack consists of several key components:</p>"},{"location":"package-auth-authentik-technical-implementation/#core-components","title":"Core Components","text":"<ol> <li>Shared PostgreSQL Database</li> <li>Shared database instance serving multiple applications</li> <li>Dedicated <code>authentik</code> database within the shared PostgreSQL instance</li> <li>Stores user accounts, groups, policies, and audit logs</li> <li>Ensures data persistence across pod restarts</li> <li>PostgreSQL Official Website</li> <li> <p>PostgreSQL Helm Chart</p> </li> <li> <p>Shared Redis Cache</p> </li> <li>Shared Redis instance serving multiple applications</li> <li>Session management and caching layer for Authentik</li> <li>Stores user sessions and temporary data</li> <li>Provides distributed session management</li> <li>Redis Official Website</li> <li> <p>Redis Helm Chart</p> </li> <li> <p>Traefik Ingress Controller</p> </li> <li>Kubernetes ingress controller for routing external traffic</li> <li>Handles SSL/TLS termination and routing</li> <li>Integrates with forward authentication middleware</li> <li>Traefik Official Website</li> <li> <p>Traefik Helm Chart</p> </li> <li> <p>Forward Authentication Middleware</p> </li> <li>Traefik middleware for protecting applications</li> <li>Redirects unauthenticated users to Authentik login</li> <li>Manages authentication headers and session validation</li> <li> <p>Traefik Middleware Documentation</p> </li> <li> <p>Authentik Server</p> </li> <li>Core authentication server</li> <li>Handles user authentication and authorization</li> <li>Manages OAuth2 and SAML flows</li> <li>Provides admin interface and API</li> <li>Authentik Official Website</li> <li>Authentik Documentation</li> <li> <p>Authentik Helm Chart</p> </li> <li> <p>Authentik Worker</p> </li> <li>Background task processor</li> <li>Handles user provisioning and deprovisioning</li> <li>Manages scheduled tasks and workflows</li> <li>Processes webhook notifications</li> <li> <p>Authentik Worker Documentation</p> </li> <li> <p>Embedded Outpost</p> </li> <li>Lightweight authentication proxy</li> <li>Integrates with Traefik for forward authentication</li> <li>Manages session validation and user context</li> <li>Provides seamless authentication experience</li> <li>Authentik Outpost Documentation</li> </ol>"},{"location":"package-auth-authentik-technical-implementation/#authentik-custom-configuration","title":"Authentik Custom Configuration","text":"<p>The default Authentik Helm chart has been customized to better integrate with our authentication stack:</p>"},{"location":"package-auth-authentik-technical-implementation/#enabled-components","title":"Enabled Components","text":"<ul> <li>PostgreSQL database integration with proper permissions</li> <li>Redis session management with authentication</li> <li>Traefik ingress integration</li> <li>Forward authentication middleware</li> <li>Embedded outpost for seamless integration</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#disabled-components","title":"Disabled Components","text":"<ul> <li>Built-in PostgreSQL database (using shared PostgreSQL instance)</li> <li>Built-in Redis cache (using shared Redis instance)</li> <li>Basic ingress (using Traefik integration)</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#resource-configuration","title":"Resource Configuration","text":"<ul> <li>Authentik Server: 512Mi request, 1Gi limit</li> <li>Authentik Worker: 256Mi request, 512Mi limit</li> <li>Shared PostgreSQL: Resources managed separately</li> <li>Shared Redis: Resources managed separately</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#key-integrations","title":"Key Integrations","text":"<ol> <li>PostgreSQL Database</li> <li>Connected via <code>AUTHENTIK_POSTGRESQL__HOST: postgresql.default.svc.cluster.local</code></li> <li>Database name: <code>authentik</code></li> <li>User credentials from Kubernetes secrets</li> <li> <p>SSL mode: <code>require</code></p> </li> <li> <p>Redis Cache</p> </li> <li>Connected via <code>AUTHENTIK_REDIS__HOST: redis-master.default.svc.cluster.local</code></li> <li>Port: <code>6379</code></li> <li>Password authentication from Kubernetes secrets</li> <li> <p>Database: <code>0</code></p> </li> <li> <p>Traefik Integration</p> </li> <li>Ingress class: <code>traefik</code></li> <li>Forward authentication middleware</li> <li>SSL redirect configuration</li> <li> <p>Host-based routing</p> </li> <li> <p>Forward Authentication</p> </li> <li>Middleware: <code>default-authentik-forward-auth@kubernetescrd</code></li> <li>Embedded outpost integration</li> <li>Session management and validation</li> <li>Automatic redirect to login</li> </ol>"},{"location":"package-auth-authentik-technical-implementation/#technical-notes","title":"Technical Notes","text":"<ul> <li>Session Management:</li> <li>Sessions are stored in Redis for distributed access</li> <li>Session timeout configurable per application</li> <li>Automatic session cleanup and expiration</li> <li>Support for multiple concurrent sessions per user</li> <li>Database Migrations:</li> <li>Automatic database schema updates</li> <li>Version compatibility checking</li> <li>Rollback capabilities for failed migrations</li> <li>Backup before major schema changes</li> <li>Security Configuration:</li> <li>TLS encryption for all connections</li> <li>Password hashing with modern algorithms</li> <li>Rate limiting for authentication attempts</li> <li>Audit logging for security events</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#configuration-and-requirements","title":"Configuration and Requirements","text":"<p>The setup requires: - A Kubernetes cluster with Traefik ingress controller - Shared PostgreSQL instance with <code>authentik</code> database created - Shared Redis instance for session management - Helm package manager - Required secrets stored in Kubernetes:   - <code>AUTHENTIK_SECRET_KEY</code>: Authentik encryption key   - <code>AUTHENTIK_POSTGRESQL__PASSWORD</code>: Database user password   - <code>AUTHENTIK_REDIS__PASSWORD</code>: Redis authentication password</p>"},{"location":"package-auth-authentik-technical-implementation/#deployment-process","title":"Deployment Process","text":"<ol> <li>Creates an <code>authentik</code> namespace in Kubernetes</li> <li>Verifies required secrets exist</li> <li>Connects to shared PostgreSQL instance and creates <code>authentik</code> database with proper permissions</li> <li>Connects to shared Redis instance for session management</li> <li>Deploys Authentik components in sequence:</li> <li>Database migrations and setup</li> <li>Authentik server and worker pods</li> <li>Forward authentication middleware</li> <li>Ingress configuration</li> <li>Configures whoami test application for verification</li> </ol> <p>Each component is deployed with appropriate timeouts and readiness checks to ensure proper initialization.</p>"},{"location":"package-auth-authentik-technical-implementation/#use-cases-and-applications","title":"Use Cases and Applications","text":""},{"location":"package-auth-authentik-technical-implementation/#1-protecting-internal-applications","title":"1. Protecting Internal Applications","text":"<p>The forward authentication system allows you to protect any internal application without modifying the application code:</p> <pre><code># Example: Protecting a custom application\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-app\n  annotations:\n    traefik.ingress.kubernetes.io/router.middlewares: \"default-authentik-forward-auth@kubernetescrd\"\nspec:\n  rules:\n  - host: myapp.localhost\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: my-app-service\n            port:\n              number: 80\n</code></pre>"},{"location":"package-auth-authentik-technical-implementation/#2-oauth2-integration-for-modern-apps","title":"2. OAuth2 Integration for Modern Apps","text":"<p>Modern web applications can integrate directly with Authentik as an OAuth2 provider:</p> <pre><code>// Example: OAuth2 client configuration\nconst oauthConfig = {\n  clientId: 'your-app-client-id',\n  redirectUri: 'http://localhost:3000/callback',\n  authorizationEndpoint: 'http://authentik.localhost/application/o/authorize/',\n  tokenEndpoint: 'http://authentik.localhost/application/o/token/',\n  scope: 'openid profile email'\n};\n</code></pre>"},{"location":"package-auth-authentik-technical-implementation/#3-saml-integration-for-enterprise-apps","title":"3. SAML Integration for Enterprise Apps","text":"<p>Traditional enterprise applications can use SAML for SSO integration:</p> <pre><code>&lt;!-- Example: SAML Service Provider configuration --&gt;\n&lt;md:EntityDescriptor entityID=\"http://myapp.localhost\"&gt;\n  &lt;md:SPSSODescriptor protocolSupportEnumeration=\"urn:oasis:names:tc:SAML:2.0:protocol\"&gt;\n    &lt;md:AssertionConsumerService\n      Binding=\"urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST\"\n      Location=\"http://myapp.localhost/saml/acs\"/&gt;\n  &lt;/md:SPSSODescriptor&gt;\n&lt;/md:EntityDescriptor&gt;\n</code></pre>"},{"location":"package-auth-authentik-technical-implementation/#security-and-compliance","title":"Security and Compliance","text":""},{"location":"package-auth-authentik-technical-implementation/#authentication-security","title":"Authentication Security","text":"<ul> <li>Multi-Factor Authentication (MFA): Support for TOTP, SMS, and hardware tokens</li> <li>Password Policies: Configurable complexity requirements and expiration</li> <li>Brute Force Protection: Rate limiting and account lockout policies</li> <li>Session Security: Secure session management with configurable timeouts</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#data-protection","title":"Data Protection","text":"<ul> <li>Encryption: All sensitive data encrypted at rest and in transit</li> <li>Audit Logging: Comprehensive logging of all authentication events</li> <li>Data Retention: Configurable data retention policies</li> <li>Backup and Recovery: Automated backup procedures with encryption</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#compliance-features","title":"Compliance Features","text":"<ul> <li>GDPR Compliance: Data portability and deletion capabilities</li> <li>SOC 2 Support: Security controls and monitoring</li> <li>HIPAA Ready: Healthcare compliance features</li> <li>Custom Compliance: Configurable policies for industry-specific requirements</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#monitoring-and-management","title":"Monitoring and Management","text":""},{"location":"package-auth-authentik-technical-implementation/#health-monitoring","title":"Health Monitoring","text":"<ul> <li>Pod Health Checks: Kubernetes-native health monitoring</li> <li>Database Connectivity: Continuous database connection monitoring</li> <li>Redis Health: Cache service health monitoring</li> <li>Traefik Integration: Ingress controller health status</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#logging-and-analytics","title":"Logging and Analytics","text":"<ul> <li>Structured Logging: JSON-formatted logs for easy parsing</li> <li>Audit Trails: Complete authentication event logging</li> <li>Performance Metrics: Response time and throughput monitoring</li> <li>Error Tracking: Comprehensive error logging and alerting</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#backup-and-recovery","title":"Backup and Recovery","text":"<ul> <li>Automated Backups: Scheduled database and configuration backups</li> <li>Point-in-Time Recovery: Database restore capabilities</li> <li>Configuration Backup: Helm values and custom configurations</li> <li>Disaster Recovery: Multi-region backup and restore procedures</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"package-auth-authentik-technical-implementation/#common-issues","title":"Common Issues","text":"<ol> <li>Authentication Failures</li> <li>Check database connectivity</li> <li>Verify Redis connection</li> <li>Check user credentials and permissions</li> <li> <p>Review authentication logs</p> </li> <li> <p>Forward Authentication Issues</p> </li> <li>Verify Traefik middleware configuration</li> <li>Check outpost connectivity</li> <li>Review ingress annotations</li> <li> <p>Test middleware functionality</p> </li> <li> <p>Database Connection Problems</p> </li> <li>Check PostgreSQL pod status</li> <li>Verify network connectivity</li> <li>Review database logs</li> <li>Check secret configuration</li> </ol>"},{"location":"package-auth-authentik-technical-implementation/#debug-commands","title":"Debug Commands","text":"<pre><code># Check pod status\nkubectl get pods -n authentik\n\n# View logs\nkubectl logs -n authentik deployment/authentik-server\nkubectl logs -n authentik deployment/authentik-worker\n\n# Check services\nkubectl get svc -n authentik\n\n# Verify ingress configuration\nkubectl get ingress -n authentik\nkubectl get middleware -n default\n\n# Test connectivity\nkubectl run test-curl --image=curlimages/curl --rm -i --restart=Never --command -- curl -I http://authentik.localhost/if/admin/\n</code></pre>"},{"location":"package-auth-authentik-technical-implementation/#reset-and-recovery","title":"Reset and Recovery","text":"<pre><code># Remove specific app authentication (if using developer workflow)\n./scripts/auth/remove-app-auth.sh myapp\n\n# Complete Authentik reset (careful!)\nkubectl delete namespace authentik\nrm -rf ~/authentik-blueprints/*\n# Then redeploy using setup script\n\n# Database reset (if needed)\nkubectl exec -it deployment/postgresql -n default -- psql -U authentik -c \"DROP DATABASE authentik;\"\nkubectl exec -it deployment/postgresql -n default -- psql -U authentik -c \"CREATE DATABASE authentik;\"\n</code></pre>"},{"location":"package-auth-authentik-technical-implementation/#future-enhancements","title":"Future Enhancements","text":""},{"location":"package-auth-authentik-technical-implementation/#planned-features","title":"Planned Features","text":""},{"location":"package-auth-authentik-technical-implementation/#auth10-enhancements","title":"Auth10 Enhancements","text":"<ul> <li>Advanced Service Types: Support for SAML and custom authentication protocols</li> <li>Conditional Domain Assignment: Dynamic domain selection based on service requirements</li> <li>Template Customization: User-configurable template modifications</li> <li>Auth10 GUI: Web interface for Auth10 configuration management</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#platform-features","title":"Platform Features","text":"<ul> <li>Advanced Policy Engine: Conditional access policies and risk-based authentication</li> <li>API Gateway Integration: Enhanced API protection and rate limiting</li> <li>Multi-Region Support: Geographic distribution and failover</li> <li>Enhanced Monitoring: Prometheus metrics and Grafana dashboards</li> <li>Mobile App Support: Native mobile authentication SDKs</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#integration-roadmap","title":"Integration Roadmap","text":"<ul> <li>LDAP/Active Directory: Enhanced directory integration with Auth10 automation</li> <li>HR System Sync: Automated user provisioning with Auth10 service assignment</li> <li>SIEM Integration: Security information and event management</li> <li>Multi-Cloud Auth10: Extended Auth10 support for AWS, Azure, GCP domains</li> <li>Compliance Reporting: Automated compliance documentation</li> <li>Custom Workflows: User-defined authentication flows</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#developer-workflow-enhancements","title":"Developer Workflow Enhancements","text":"<ul> <li>Auth10 Configuration GUI: Visual interface for managing protected services</li> <li>Testing Automation: Automated testing of authentication flows across all domains</li> <li>Performance Monitoring: Application-specific authentication metrics with Auth10 context</li> <li>Advanced User Scenarios: Complex testing scenarios and role simulation</li> <li>Integration Templates: Pre-built Auth10 configurations for popular frameworks</li> <li>Multi-Domain Testing: Automated testing across localhost, Tailscale, and Cloudflare domains</li> </ul>"},{"location":"package-auth-authentik-technical-implementation/#related-documentation","title":"Related Documentation","text":"<ul> <li>Main Authentication Guide - Overview and getting started</li> <li>Auth10 System Guide - Configuration-driven authentication</li> <li>Developer Guide - Practical implementation</li> <li>Blueprint Syntax - Blueprint reference</li> <li>Test Users Guide - User management</li> </ul> <p>This document focuses on the technical implementation details. For practical usage and configuration, see the related documentation above.</p>"},{"location":"package-auth-authentik-testusers/","title":"Urbalurba Authentik Test Users Package","text":""},{"location":"package-auth-authentik-testusers/#overview","title":"Overview","text":"<p>This document provides a complete guide for creating and managing test users in Authentik that mirror the organizational structure found in production Okta. The goal is to provide developers with a realistic test environment that includes all Okta fields and organizational relationships while maintaining data anonymity.</p> <p>This package includes: - Complete user specification with all Okta fields - Organizational structure mirroring organizational departments - Blueprint conversion methodology for automated deployment - Implementation guide for Authentik integration</p>"},{"location":"package-auth-authentik-testusers/#table-of-contents","title":"Table of Contents","text":"<ol> <li>User Specification</li> <li>Okta Field Mapping</li> <li>Organizational Structure</li> <li>Blueprint Implementation</li> <li>Deployment Guide</li> <li>Testing and Validation</li> <li>Maintenance and Updates</li> </ol>"},{"location":"package-auth-authentik-testusers/#user-specification","title":"User Specification","text":""},{"location":"package-auth-authentik-testusers/#user-distribution","title":"User Distribution","text":"<p>Total Users: 11 - HQ Users: 9 - Covering all major departments from the national office - District Users: 2 - Representing regional/district level operations</p>"},{"location":"package-auth-authentik-testusers/#complete-user-table","title":"Complete User Table","text":"Username Email Name Department vi_department vi_departmentID vi_unitname vi_unitID costCenter vi_Locale vi_StateProvince vi_position vi_employeeform employeeNumber samAccountName managerId manager ManagerdisplayName isEmployee isVolunteer u_start_date u_end_date mobilePhone secondEmail bankid_birthdate bankid_nnin_altsub bankid_altsub bankid_verification_timestamp bankid_user_verified bankid_sub bankid_firstname bankid_lastname AgressoDomainUser RelationNumber Azure_lastNonInteractiveSignInDateTime ServiceNowManagerIdExternalID extensionAttribute7 u_crm_guid streetAddress city zipCode state deliveryOffice division title ok1 ok1@urbalurba.no Ola Nordmann \u00d8konomi og administrasjon \u00d8konomi og administrasjon N750 \u00d8konomi og administrasjon #5421000#N750#N750# N750 Oslo Nasjonalkontoret \u00d8konomi- og administrasjonsmedarbeider 1 Fast ansatt 25001 105010OK1 105010MGR1 manager1@urbalurba.no Manager Person true false 01/01/2020 +4790012345 ola.nordmann@example.no 80-01-15 15018012345 Pass 2020-01-01T09:00:00 true 12345678-1234-1234-1234-123456789012 Ola Nordmann Z94\\105010OK1 105010OK1 2024-01-15T08:30:00Z d1b50bf41ba56690825da711604bcbf2 5207a9ec-a6bf-ec11-8117-001dd8b74416 bb0a469d-905c-ef11-bfe3-0022489bed2b Storgata 1 Oslo 0155 Nasjonalkontoret RK Hovedkontor 105010-001 - RK Hovedkontor \u00d8konomi- og administrasjonsmedarbeider re1 re1@urbalurba.no Kari Hansen \u00d8konomi og administrasjon Regnskap og rapportering N760 Regnskap og rapportering #5421000#N760#N760# N760 Oslo Nasjonalkontoret Regnskapsmedarbeider 1 Fast ansatt 25002 105010RE1 105010OK1 ola.nordmann@urbalurba.no Ola Nordmann true false 01/02/2020 +4790012346 kari.hansen@example.no 85-03-22 22038512345 Pass 2020-02-01T09:00:00 true 12345678-1234-1234-1234-123456789013 Kari Hansen Z94\\105010RE1 105010RE1 2024-01-15T08:30:00Z d1b50bf41ba56690825da711604bcbf3 5207a9ec-a6bf-ec11-8117-001dd8b74417 bb0a469d-905c-ef11-bfe3-0022489bed2c Storgata 2 Oslo 0155 Nasjonalkontoret RK Hovedkontor 105010-001 - RK Hovedkontor Regnskapsmedarbeider it1 it1@urbalurba.no Erik Larsen \u00d8konomi og administrasjon IT N770 IT #5421000#N770#N770# N770 Oslo Nasjonalkontoret IT Specialist 1 Fast ansatt 25003 105010IT1 105010OK1 ola.nordmann@urbalurba.no Ola Nordmann true false 01/03/2020 +4790012347 erik.larsen@example.no 82-07-10 10078212345 Pass 2020-03-01T09:00:00 true 12345678-1234-1234-1234-123456789014 Erik Larsen Z94\\105010IT1 105010IT1 2024-01-15T08:30:00Z d1b50bf41ba56690825da711604bcbf4 5207a9ec-a6bf-ec11-8117-001dd8b74418 bb0a469d-905c-ef11-bfe3-0022489bed2d Storgata 3 Oslo 0155 Nasjonalkontoret RK Hovedkontor 105010-001 - RK Hovedkontor IT Specialist hr1 hr1@urbalurba.no Anna Olsen HR og organisasjonsutvikling HR N780 HR #5421000#N780#N780# N780 Oslo Nasjonalkontoret HR Medarbeider 1 Fast ansatt 25004 105010HR1 105010OK1 ola.nordmann@urbalurba.no Ola Nordmann true false 01/04/2020 +4790012348 anna.olsen@example.no 88-11-05 05118812345 Pass 2020-04-01T09:00:00 true 12345678-1234-1234-1234-123456789015 Anna Olsen Z94\\105010HR1 105010HR1 2024-01-15T08:30:00Z d1b50bf41ba56690825da711604bcbf5 5207a9ec-a6bf-ec11-8117-001dd8b74419 bb0a469d-905c-ef11-bfe3-0022489bed2e Storgata 4 Oslo 0155 Nasjonalkontoret RK Hovedkontor 105010-001 - RK Hovedkontor HR Medarbeider ko1 ko1@urbalurba.no Lars Andersen Kommunikasjon og samfunn Kommunikasjon N790 Kommunikasjon #5421000#N790#N790# N790 Oslo Nasjonalkontoret Kommunikasjonsmedarbeider 1 Fast ansatt 25005 105010KO1 105010OK1 ola.nordmann@urbalurba.no Ola Nordmann true false 01/05/2020 +4790012349 lars.andersen@example.no 79-09-18 18097912345 Pass 2020-05-01T09:00:00 true 12345678-1234-1234-1234-123456789016 Lars Andersen Z94\\105010KO1 105010KO1 2024-01-15T08:30:00Z d1b50bf41ba56690825da711604bcbf6 5207a9ec-a6bf-ec11-8117-001dd8b74420 bb0a469d-905c-ef11-bfe3-0022489bed2f Storgata 5 Oslo 0155 Nasjonalkontoret RK Hovedkontor 105010-001 - RK Hovedkontor Kommunikasjonsmedarbeider in1 in1@urbalurba.no Maria Johansen Inntekter Inntekter N800 Inntekter #5421000#N800#N800# N800 Oslo Nasjonalkontoret Inntektsmedarbeider 1 Fast ansatt 25006 105010IN1 105010OK1 ola.nordmann@urbalurba.no Ola Nordmann true false 01/06/2020 +4790012350 maria.johansen@example.no 83-12-03 03128312345 Pass 2020-06-01T09:00:00 true 12345678-1234-1234-1234-123456789017 Maria Johansen Z94\\105010IN1 105010IN1 2024-01-15T08:30:00Z d1b50bf41ba56690825da711604bcbf7 5207a9ec-a6bf-ec11-8117-001dd8b74421 bb0a469d-905c-ef11-bfe3-0022489bed30 Storgata 6 Oslo 0155 Nasjonalkontoret RK Hovedkontor 105010-001 - RK Hovedkontor Inntektsmedarbeider sr1 sr1@urbalurba.no Thomas Pedersen Nasjonale programmer og beredskap S\u00f8k og redning N810 S\u00f8k og redning #5421000#N810#N810# N810 Oslo Nasjonalkontoret Beredskapsmedarbeider 1 Fast ansatt 25007 105010SR1 105010OK1 ola.nordmann@urbalurba.no Ola Nordmann true false 01/07/2020 +4790012351 thomas.pedersen@example.no 81-04-14 14048112345 Pass 2020-07-01T09:00:00 true 12345678-1234-1234-1234-123456789018 Thomas Pedersen Z94\\105010SR1 105010SR1 2024-01-15T08:30:00Z d1b50bf41ba56690825da711604bcbf8 5207a9ec-a6bf-ec11-8117-001dd8b74422 bb0a469d-905c-ef11-bfe3-0022489bed31 Storgata 7 Oslo 0155 Nasjonalkontoret RK Hovedkontor 105010-001 - RK Hovedkontor Beredskapsmedarbeider ip1 ip1@urbalurba.no Ingrid Svendsen Internasjonale programmer og beredskap Technical Unit N820 Technical Unit #5421000#N820#N820# N820 Oslo Nasjonalkontoret Teknisk r\u00e5dgiver 1 Fast ansatt 25008 105010IP1 105010OK1 ola.nordmann@urbalurba.no Ola Nordmann true false 01/08/2020 +4790012352 ingrid.svendsen@example.no 86-06-27 27068612345 Pass 2020-08-01T09:00:00 true 12345678-1234-1234-1234-123456789019 Ingrid Svendsen Z94\\105010IP1 105010IP1 2024-01-15T08:30:00Z d1b50bf41ba56690825da711604bcbf9 5207a9ec-a6bf-ec11-8117-001dd8b74423 bb0a469d-905c-ef11-bfe3-0022489bed32 Storgata 8 Oslo 0155 Nasjonalkontoret RK Hovedkontor 105010-001 - RK Hovedkontor Teknisk r\u00e5dgiver di1 di1@urbalurba.no Per Kristiansen \u00d8konomi og administrasjon Digital innovasjon N830 Digital innovasjon #5421000#N830#N830# N830 Oslo Nasjonalkontoret Digital innovasjonsr\u00e5dgiver 1 Fast ansatt 25009 105010DI1 105010OK1 ola.nordmann@urbalurba.no Ola Nordmann true false 01/09/2020 +4790012353 per.kristiansen@example.no 84-02-11 11028412345 Pass 2020-09-01T09:00:00 true 12345678-1234-1234-1234-123456789020 Per Kristiansen Z94\\105010DI1 105010DI1 2024-01-15T08:30:00Z d1b50bf41ba56690825da711604bcbfa 5207a9ec-a6bf-ec11-8117-001dd8b74424 bb0a469d-905c-ef11-bfe3-0022489bed33 Storgata 9 Oslo 0155 Nasjonalkontoret RK Hovedkontor 105010-001 - RK Hovedkontor Digital innovasjonsr\u00e5dgiver dist1 dist1@urbalurba.no Bj\u00f8rn Nilsen Distriktskontor Buskerud RK D006 Buskerud RK #5421000#D006#D006# D006 Drammen Distrikt Distriktsmedarbeider 1 Fast ansatt 25010 105010DIST1 105010OK1 ola.nordmann@urbalurba.no Ola Nordmann true false 01/10/2020 +4790012354 bjorn.nilsen@example.no 87-08-25 25088712345 Pass 2020-10-01T09:00:00 true 12345678-1234-1234-1234-123456789021 Bj\u00f8rn Nilsen Z94\\105010DIST1 105010DIST1 2024-01-15T08:30:00Z d1b50bf41ba56690825da711604bcbfb 5207a9ec-a6bf-ec11-8117-001dd8b74425 bb0a469d-905c-ef11-bfe3-0022489bed34 Drammensveien 1 Drammen 3015 Distrikt Buskerud RK 105010-002 - Buskerud RK Distriktsmedarbeider dist2 dist2@urbalurba.no Solveig Berg Distriktskontor Hordaland RK D010 Hordaland RK #5421000#D010#D010# D010 Bergen Distrikt Distriktsmedarbeider 1 Fast ansatt 25011 105010DIST2 105010OK1 ola.nordmann@urbalurba.no Ola Nordmann true false 01/11/2020 +4790012355 solveig.berg@example.no 89-01-08 08018912345 Pass 2020-11-01T09:00:00 true 12345678-1234-1234-1234-123456789022 Solveig Berg Z94\\105010DIST2 105010DIST2 2024-01-15T08:30:00Z d1b50bf41ba56690825da711604bcbfc 5207a9ec-a6bf-ec11-8117-001dd8b74426 bb0a469d-905c-ef11-bfe3-0022489bed35 Bryggen 1 Bergen 5003 Distrikt Hordaland RK 105010-003 - Hordaland RK Distriktsmedarbeider"},{"location":"package-auth-authentik-testusers/#okta-field-mapping","title":"Okta Field Mapping","text":""},{"location":"package-auth-authentik-testusers/#field-categories","title":"Field Categories","text":"<p>The user specification includes all fields found in production Okta, organized into these categories:</p>"},{"location":"package-auth-authentik-testusers/#core-identity-fields","title":"Core Identity Fields","text":"<ul> <li>username: Unique identifier for each user (ok1, re1, it1, etc.)</li> <li>email: Primary email address using @urbalurba.no domain</li> <li>name: Full name using generic Norwegian names</li> <li>login: Same as email (standard Okta pattern)</li> </ul>"},{"location":"package-auth-authentik-testusers/#organizational-fields","title":"Organizational Fields","text":"<ul> <li>department: Main department from organizational chart</li> <li>vi_department: Visma integration department field</li> <li>vi_departmentID: Unique department identifier (N750-N830 for HQ, D006/D010 for districts)</li> <li>vi_unitname: Unit name within department</li> <li>vi_unitID: Unique unit identifier with organizational hierarchy</li> <li>costCenter: Cost center code matching department</li> <li>vi_Locale: Geographic location (Oslo for HQ, Drammen/Bergen for districts)</li> <li>vi_StateProvince: Organizational level (Nasjonalkontoret/Distrikt)</li> </ul>"},{"location":"package-auth-authentik-testusers/#employment-fields","title":"Employment Fields","text":"<ul> <li>vi_position: Job title/position</li> <li>vi_employeeform: Employment type (1 Fast ansatt = Full-time employee)</li> <li>employeeNumber: Sequential employee number (25001-25011)</li> <li>samAccountName: Active Directory account name</li> <li>managerId: Manager's samAccountName</li> <li>manager: Manager's email address</li> <li>ManagerdisplayName: Manager's full name</li> <li>isEmployee: Always true for all users</li> <li>isVolunteer: Always false for all users</li> <li>u_start_date: Employment start date (DD/MM/YYYY format)</li> <li>u_end_date: Employment end date (empty for active employees)</li> </ul>"},{"location":"package-auth-authentik-testusers/#contact-fields","title":"Contact Fields","text":"<ul> <li>mobilePhone: Norwegian mobile phone number (+47XXXXXXXX)</li> <li>secondEmail: Personal email address using @example.no domain</li> <li>streetAddress: Generic Norwegian street address</li> <li>city: City name (Oslo, Drammen, Bergen)</li> <li>zipCode: Norwegian postal code</li> <li>state: Organizational state (Nasjonalkontoret/Distrikt)</li> <li>deliveryOffice: Office location name (RK Hovedkontor, Buskerud RK, Hordaland RK)</li> <li>division: Full division name with code (105010-001 - RK Hovedkontor, etc.)</li> </ul>"},{"location":"package-auth-authentik-testusers/#bankid-fields-norwegian-national-id","title":"BankID Fields (Norwegian National ID)","text":"<ul> <li>bankid_birthdate: Birth date in YY-MM-DD format</li> <li>bankid_nnin_altsub: Norwegian national identification number (fake)</li> <li>bankid_altsub: BankID alternative subject (Pass)</li> <li>bankid_verification_timestamp: When BankID was verified</li> <li>bankid_user_verified: Whether user is BankID verified (true)</li> <li>bankid_sub: BankID subject identifier (UUID format)</li> <li>bankid_firstname: First name from BankID</li> <li>bankid_lastname: Last name from BankID</li> </ul>"},{"location":"package-auth-authentik-testusers/#system-integration-fields","title":"System Integration Fields","text":"<ul> <li>AgressoDomainUser: Agresso system domain user (Z94\\samAccountName)</li> <li>RelationNumber: Relation number (same as samAccountName)</li> <li>Azure_lastNonInteractiveSignInDateTime: Last Azure sign-in timestamp</li> <li>ServiceNowManagerIdExternalID: ServiceNow manager external ID</li> <li>extensionAttribute7: Extension attribute (UUID format)</li> <li>u_crm_guid: CRM system GUID (UUID format)</li> </ul>"},{"location":"package-auth-authentik-testusers/#data-anonymization-strategy","title":"Data Anonymization Strategy","text":""},{"location":"package-auth-authentik-testusers/#personal-information","title":"Personal Information","text":"<ul> <li>Names: Generic Norwegian names (Ola Nordmann, Kari Hansen, etc.)</li> <li>Emails: @urbalurba.no for work, @example.no for personal</li> <li>Phone Numbers: Norwegian format but fake numbers</li> <li>Addresses: Generic Norwegian addresses</li> <li>BankID Data: Valid Norwegian format but invalid numbers</li> </ul>"},{"location":"package-auth-authentik-testusers/#organizational-data","title":"Organizational Data","text":"<ul> <li>Department Names: Real department names (not sensitive)</li> <li>Cost Centers: Realistic codes but generic</li> <li>Employee IDs: Sequential numbers starting from 25001</li> <li>Manager Relationships: Realistic hierarchy for testing</li> </ul>"},{"location":"package-auth-authentik-testusers/#organizational-structure","title":"Organizational Structure","text":""},{"location":"package-auth-authentik-testusers/#manager-relationships","title":"Manager Relationships","text":"<ul> <li>ok1 (Ola Nordmann) is the manager for all other users</li> <li>All users report to ok1 via managerId and manager fields</li> <li>This creates a realistic organizational hierarchy for testing</li> </ul>"},{"location":"package-auth-authentik-testusers/#department-structure","title":"Department Structure","text":"<ul> <li>HQ Departments: 9 departments covering all major functions</li> <li>District Offices: 2 districts representing regional operations</li> <li>Cost Centers: Unique codes for each department/unit</li> <li>Geographic Distribution: Oslo (HQ), Drammen (Buskerud), Bergen (Hordaland)</li> </ul>"},{"location":"package-auth-authentik-testusers/#group-assignment-logic","title":"Group Assignment Logic","text":"<ul> <li>HQ Group: Users with <code>state</code> or <code>vi_StateProvince</code> = \"Nasjonalkontoret\"</li> <li>Distrikt Group: Users with <code>state</code> or <code>vi_StateProvince</code> = \"Distrikt\"</li> <li>Alternative Logic: Infer from <code>vi_departmentID</code> prefix: <code>N...</code> \u2192 <code>HQ</code>, <code>D...</code> \u2192 <code>Distrikt</code></li> </ul>"},{"location":"package-auth-authentik-testusers/#blueprint-implementation","title":"Blueprint Implementation","text":""},{"location":"package-auth-authentik-testusers/#blueprint-structure","title":"Blueprint Structure","text":"<p>The Authentik blueprint follows this structure:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: users-groups-test-blueprint\n  namespace: authentik\n  labels:\n    app.kubernetes.io/name: authentik\n    app.kubernetes.io/component: blueprint\n    blueprints.goauthentik.io/instantiate: \"true\"\ndata:\n  users-groups-test-setup.yaml: |\n    # yaml-language-server: $schema=https://goauthentik.io/blueprints/schema.json\n    version: 1\n    metadata:\n      name: \"Complete Okta-Compatible Test Users - Urbalurba Development Environment\"\n      labels:\n        blueprints.goauthentik.io/instantiate: \"true\"\n\n    context: {}\n\n    entries:\n      # Groups first\n      - model: authentik_core.group\n        state: present\n        identifiers:\n          name: \"HQ\"\n        attrs:\n          name: \"HQ\"\n          is_superuser: false\n          attributes:\n            type: \"org_group\"\n            scope: \"hq\"\n\n      - model: authentik_core.group\n        state: present\n        identifiers:\n          name: \"Distrikt\"\n        attrs:\n          name: \"Distrikt\"\n          is_superuser: false\n          attributes:\n            type: \"org_group\"\n            scope: \"district\"\n\n      # User entries go here\n</code></pre>"},{"location":"package-auth-authentik-testusers/#field-mapping-rules","title":"Field Mapping Rules","text":""},{"location":"package-auth-authentik-testusers/#core-authentik-fields-direct-mapping","title":"Core Authentik Fields (Direct Mapping)","text":"<ul> <li><code>username</code> \u2192 <code>username</code></li> <li><code>email</code> \u2192 <code>email</code></li> <li><code>name</code> \u2192 <code>name</code></li> <li><code>password</code> \u2192 <code>password</code> (always \"Password123\")</li> <li><code>is_active</code> \u2192 <code>is_active</code> (always true)</li> </ul>"},{"location":"package-auth-authentik-testusers/#custom-attributes-all-other-fields","title":"Custom Attributes (All Other Fields)","text":"<p>All other fields from the specification go into the <code>attributes</code> section: - <code>department</code> \u2192 <code>attributes.department</code> - <code>vi_department</code> \u2192 <code>attributes.vi_department</code> - <code>employeeNumber</code> \u2192 <code>attributes.employeeNumber</code> - <code>samAccountName</code> \u2192 <code>attributes.samAccountName</code> - <code>managerId</code> \u2192 <code>attributes.managerId</code> - <code>manager</code> \u2192 <code>attributes.manager</code> - <code>ManagerdisplayName</code> \u2192 <code>attributes.ManagerdisplayName</code> - <code>isEmployee</code> \u2192 <code>attributes.isEmployee</code> - <code>isVolunteer</code> \u2192 <code>attributes.isVolunteer</code> - <code>u_start_date</code> \u2192 <code>attributes.u_start_date</code> - <code>u_end_date</code> \u2192 <code>attributes.u_end_date</code> - <code>mobilePhone</code> \u2192 <code>attributes.mobilePhone</code> - <code>secondEmail</code> \u2192 <code>attributes.secondEmail</code> - <code>bankid_birthdate</code> \u2192 <code>attributes.bankid_birthdate</code> - <code>bankid_nnin_altsub</code> \u2192 <code>attributes.bankid_nnin_altsub</code> - <code>bankid_altsub</code> \u2192 <code>attributes.bankid_altsub</code> - <code>bankid_verification_timestamp</code> \u2192 <code>attributes.bankid_verification_timestamp</code> - <code>bankid_user_verified</code> \u2192 <code>attributes.bankid_user_verified</code> - <code>bankid_sub</code> \u2192 <code>attributes.bankid_sub</code> - <code>bankid_firstname</code> \u2192 <code>attributes.bankid_firstname</code> - <code>bankid_lastname</code> \u2192 <code>attributes.bankid_lastname</code> - <code>AgressoDomainUser</code> \u2192 <code>attributes.AgressoDomainUser</code> - <code>RelationNumber</code> \u2192 <code>attributes.RelationNumber</code> - <code>Azure_lastNonInteractiveSignInDateTime</code> \u2192 <code>attributes.Azure_lastNonInteractiveSignInDateTime</code> - <code>ServiceNowManagerIdExternalID</code> \u2192 <code>attributes.ServiceNowManagerIdExternalID</code> - <code>extensionAttribute7</code> \u2192 <code>attributes.extensionAttribute7</code> - <code>u_crm_guid</code> \u2192 <code>attributes.u_crm_guid</code> - <code>streetAddress</code> \u2192 <code>attributes.streetAddress</code> - <code>city</code> \u2192 <code>attributes.city</code> - <code>zipCode</code> \u2192 <code>attributes.zipCode</code> - <code>state</code> \u2192 <code>attributes.state</code> - <code>deliveryOffice</code> \u2192 <code>attributes.deliveryOffice</code> - <code>division</code> \u2192 <code>attributes.division</code> - <code>title</code> \u2192 <code>attributes.title</code> - <code>vi_employeeform</code> \u2192 <code>attributes.vi_employeeform</code> - <code>vi_departmentID</code> \u2192 <code>attributes.vi_departmentID</code> - <code>vi_unitname</code> \u2192 <code>attributes.vi_unitname</code> - <code>vi_unitID</code> \u2192 <code>attributes.vi_unitID</code> - <code>costCenter</code> \u2192 <code>attributes.costCenter</code> - <code>vi_Locale</code> \u2192 <code>attributes.vi_Locale</code> - <code>vi_StateProvince</code> \u2192 <code>attributes.vi_StateProvince</code> - <code>vi_position</code> \u2192 <code>attributes.vi_position</code></p>"},{"location":"package-auth-authentik-testusers/#complete-user-entry-example","title":"Complete User Entry Example","text":"<p>Here's a complete example for user <code>ok1</code>:</p> <pre><code>- model: authentik_core.user\n  state: present\n  identifiers:\n    username: \"ok1\"\n  attrs:\n    username: \"ok1\"\n    name: \"Ola Nordmann\"\n    email: \"ok1@urbalurba.no\"\n    password: \"Password123\"\n    is_active: true\n    attributes:\n      department: \"\u00d8konomi og administrasjon\"\n      vi_department: \"\u00d8konomi og administrasjon\"\n      vi_departmentID: \"N750\"\n      vi_unitname: \"\u00d8konomi og administrasjon\"\n      vi_unitID: \"#5421000#N750#N750#\"\n      costCenter: \"N750\"\n      vi_Locale: \"Oslo\"\n      vi_StateProvince: \"Nasjonalkontoret\"\n      vi_position: \"\u00d8konomi- og administrasjonsmedarbeider\"\n      vi_employeeform: \"1 Fast ansatt\"\n      employeeNumber: \"25001\"\n      samAccountName: \"105010OK1\"\n      managerId: \"105010MGR1\"\n      manager: \"manager1@urbalurba.no\"\n      ManagerdisplayName: \"Manager Person\"\n      isEmployee: \"true\"\n      isVolunteer: \"false\"\n      u_start_date: \"01/01/2020\"\n      u_end_date: \"\"\n      mobilePhone: \"+4790012345\"\n      secondEmail: \"ola.nordmann@example.no\"\n      bankid_birthdate: \"80-01-15\"\n      bankid_nnin_altsub: \"15018012345\"\n      bankid_altsub: \"Pass\"\n      bankid_verification_timestamp: \"2020-01-01T09:00:00\"\n      bankid_user_verified: \"true\"\n      bankid_sub: \"12345678-1234-1234-1234-123456789012\"\n      bankid_firstname: \"Ola\"\n      bankid_lastname: \"Nordmann\"\n      AgressoDomainUser: \"Z94\\\\105010OK1\"\n      RelationNumber: \"105010OK1\"\n      Azure_lastNonInteractiveSignInDateTime: \"2024-01-15T08:30:00Z\"\n      ServiceNowManagerIdExternalID: \"d1b50bf41ba56690825da711604bcbf2\"\n      extensionAttribute7: \"5207a9ec-a6bf-ec11-8117-001dd8b74416\"\n      u_crm_guid: \"bb0a469d-905c-ef11-bfe3-0022489bed2b\"\n      streetAddress: \"Storgata 1\"\n      city: \"Oslo\"\n      zipCode: \"0155\"\n      state: \"Nasjonalkontoret\"\n      deliveryOffice: \"RK Hovedkontor\"\n      division: \"105010-001 - RK Hovedkontor\"\n      title: \"\u00d8konomi- og administrasjonsmedarbeider\"\n    groups:\n      - !Find [authentik_core.group, [name, \"HQ\"]]\n</code></pre>"},{"location":"package-auth-authentik-testusers/#data-type-handling","title":"Data Type Handling","text":""},{"location":"package-auth-authentik-testusers/#string-values","title":"String Values","text":"<p>Most fields are strings and should be quoted: <pre><code>attributes:\n  department: \"\u00d8konomi og administrasjon\"\n  employeeNumber: \"25001\"\n  isEmployee: \"true\"\n</code></pre></p>"},{"location":"package-auth-authentik-testusers/#boolean-values","title":"Boolean Values","text":"<p>Boolean fields should be strings in Authentik: <pre><code>attributes:\n  isEmployee: \"true\"\n  isVolunteer: \"false\"\n  bankid_user_verified: \"true\"\n</code></pre></p>"},{"location":"package-auth-authentik-testusers/#empty-values","title":"Empty Values","text":"<p>For empty fields, use empty strings: <pre><code>attributes:\n  u_end_date: \"\"\n</code></pre></p>"},{"location":"package-auth-authentik-testusers/#deployment-guide","title":"Deployment Guide","text":""},{"location":"package-auth-authentik-testusers/#prerequisites","title":"Prerequisites","text":"<ol> <li>Authentik namespace must exist</li> <li>Blueprint ConfigMaps must be applied BEFORE deploying Authentik with Helm</li> <li>Proper labels must be set for automatic discovery</li> <li>Blueprint names must be listed in Helm values under <code>blueprints.configMaps</code></li> </ol>"},{"location":"package-auth-authentik-testusers/#helm-configuration","title":"Helm Configuration","text":"<p>Add the following to your Authentik Helm values file:</p> <pre><code># Blueprint system configuration\nblueprints:\n  # List of ConfigMaps containing blueprints\n  # Only keys ending with .yaml will be discovered and applied\n  configMaps:\n    - \"whoami-forward-auth-blueprint\"        # Proxy authentication setup\n    - \"openwebui-authentik-blueprint\"         # OAuth2/OIDC application setup\n    - \"users-groups-test-blueprint\"           # Test blueprint for users and groups\n    # Add your blueprint ConfigMap names here\n</code></pre>"},{"location":"package-auth-authentik-testusers/#complete-deployment-workflow","title":"Complete Deployment Workflow","text":"<pre><code># 1. Deploy blueprint ConfigMaps FIRST (before Authentik)\nkubectl apply -f manifests/074-authentik-users-groups-blueprint.yaml\n\n# 2. Verify ConfigMaps are created\nkubectl get configmaps -n authentik -l app.kubernetes.io/component=blueprint\n\n# 3. Deploy/upgrade Authentik with Helm (with blueprint references in values)\nhelm upgrade --install authentik authentik/authentik \\\n  -n authentik \\\n  -f values-authentik.yaml  # Contains the blueprints.configMaps configuration\n\n# 4. Monitor blueprint application\nkubectl logs -n authentik deployment/authentik-server | grep -i blueprint\n</code></pre>"},{"location":"package-auth-authentik-testusers/#blueprint-discovery-process","title":"Blueprint Discovery Process","text":"<ol> <li>ConfigMap Creation: Blueprint ConfigMaps are deployed to the <code>authentik</code> namespace</li> <li>Helm Reference: ConfigMap names are listed in <code>blueprints.configMaps</code> in Helm values</li> <li>Authentik Startup: When Authentik starts, it reads the configured ConfigMap list</li> <li>Blueprint Loading: Authentik loads and applies blueprints from the referenced ConfigMaps</li> <li>Automatic Reapplication: Changes to ConfigMaps trigger reapplication (monitored every 60 minutes)</li> </ol>"},{"location":"package-auth-authentik-testusers/#testing-and-validation","title":"Testing and Validation","text":""},{"location":"package-auth-authentik-testusers/#validation-checklist","title":"Validation Checklist","text":"<p>Before finalizing the blueprint, verify: - [ ] All 11 users are included - [ ] All fields from specification are mapped - [ ] YAML syntax is valid - [ ] Authentik blueprint format is correct - [ ] All string values are properly quoted - [ ] Boolean values are strings (\"true\"/\"false\") - [ ] Empty fields use empty strings (\"\") - [ ] Metadata and labels are correct - [ ] Blueprint instantiation label is present</p>"},{"location":"package-auth-authentik-testusers/#testing-the-blueprint","title":"Testing the Blueprint","text":"<p>After creating the blueprint:</p> <ol> <li>Apply to cluster: <code>kubectl apply -f manifests/074-authentik-users-groups-blueprint.yaml</code></li> <li>Restart Authentik (to pick up changes immediately): <code>kubectl rollout restart deployment/authentik-server -n authentik</code></li> <li>Check Authentik: Verify users appear in Authentik admin interface</li> <li>Test authentication: Try logging in with test credentials</li> <li>Verify fields: Check that all custom attributes are present</li> <li>Test applications: Verify integration with OpenWebUI and other apps</li> </ol> <p>Note: Authentik automatically detects blueprint changes within 60 minutes, but restarting ensures immediate application of changes.</p>"},{"location":"package-auth-authentik-testusers/#usage-notes","title":"Usage Notes","text":""},{"location":"package-auth-authentik-testusers/#for-developers","title":"For Developers","text":"<ul> <li>All users have password \"Password123\" for easy testing</li> <li>Users cover all major organizational scenarios</li> <li>Field values are realistic but anonymous</li> <li>Easy to modify for specific test scenarios</li> </ul>"},{"location":"package-auth-authentik-testusers/#for-testing","title":"For Testing","text":"<ul> <li>Authentication: All users can log in with their credentials</li> <li>Authorization: Manager relationships can be tested</li> <li>Integration: All Okta fields are present for app testing</li> <li>Edge Cases: Some fields are empty to test null handling</li> </ul>"},{"location":"package-auth-authentik-testusers/#for-maintenance","title":"For Maintenance","text":"<ul> <li>Users are created once when Authentik starts</li> <li>No updates needed after initial deployment</li> <li>Easy to add more users following the same pattern</li> <li>Clear documentation for future modifications</li> </ul>"},{"location":"package-auth-authentik-testusers/#maintenance-and-updates","title":"Maintenance and Updates","text":""},{"location":"package-auth-authentik-testusers/#adding-new-users","title":"Adding New Users","text":"<ol> <li>Add user to specification table</li> <li>Follow conversion process for new user</li> <li>Add user entry to blueprint</li> <li>Reapply blueprint to cluster</li> </ol>"},{"location":"package-auth-authentik-testusers/#modifying-existing-users","title":"Modifying Existing Users","text":"<ol> <li>Update user data in specification</li> <li>Regenerate user entry in blueprint</li> <li>Reapply blueprint to cluster</li> </ol>"},{"location":"package-auth-authentik-testusers/#field-changes","title":"Field Changes","text":"<ol> <li>Update field mapping rules if needed</li> <li>Regenerate entire blueprint</li> <li>Reapply blueprint to cluster</li> </ol>"},{"location":"package-auth-authentik-testusers/#blueprint-updates-and-redeployment","title":"Blueprint Updates and Redeployment","text":"<p>When updating blueprints:</p> <pre><code># Update blueprint ConfigMaps\nkubectl apply -f manifests/074-authentik-users-groups-blueprint.yaml\n\n# Authentik automatically detects changes (within 60 minutes)\n# Or force immediate reapplication:\nkubectl rollout restart deployment/authentik-server -n authentik\n</code></pre> <p>Note: New blueprints require updating Helm values and redeploying Authentik, but existing blueprint changes are automatically detected.</p>"},{"location":"package-auth-authentik-testusers/#implementation-notes","title":"Implementation Notes","text":"<p>This specification will be used to create the Authentik blueprint file <code>manifests/074-authentik-users-groups-blueprint.yaml</code> which will:</p> <ol> <li>Create all 11 users with complete field mappings</li> <li>Set up realistic organizational relationships with manager hierarchies</li> <li>Provide comprehensive test data for development</li> <li>Maintain data anonymity while preserving structure</li> <li>Support all applications that depend on Okta field structure</li> </ol> <p>The blueprint ensures that the user specification is accurately converted into a functional Authentik configuration that provides comprehensive test data for development environments. The structured approach guarantees consistency and completeness while maintaining the flexibility to modify and extend the test data as needed.</p>"},{"location":"package-auth-authentik-testusers/#resources","title":"Resources","text":"<ul> <li>Authentik Blueprints Manual</li> <li>Okta Fields Reference</li> <li>User Table Specification</li> <li>Blueprint Conversion Guide</li> <li>Official Authentik Blueprint Documentation</li> <li>Blueprint Schema</li> </ul> <p>Last updated: January 2025</p>"},{"location":"package-core-nginx/","title":"Nginx Web Server - Core Infrastructure","text":"<p>File: <code>docs/package-core-nginx.md</code> Purpose: Complete guide to Nginx deployment and configuration in Urbalurba infrastructure Target Audience: Infrastructure engineers, developers working with web services Last Updated: September 22, 2024</p>"},{"location":"package-core-nginx/#overview","title":"\ud83d\udccb Overview","text":"<p>Nginx serves as the catch-all web server in the Urbalurba infrastructure. It acts as the default fallback for any hostname or path not specifically defined in Traefik IngressRoutes, displaying \"Hello World\" content to users who access undefined routes.</p> <p>Key Function: When someone accesses a hostname or path that doesn't match any specific service route (like <code>whoami.localhost</code> or <code>authentik.localhost</code>), Traefik routes the request to nginx, which serves the default content from <code>testdata/website/</code>.</p> <p>Catch-All Behavior: - Priority 1 (lowest) in Traefik routing - ensures all other routes are checked first - Serves as safety net for undefined routes and testing - Displays \"Hello World\" page from <code>testdata/website/index.html</code> - Prevents 404 errors for cluster domain access</p> <p>For detailed ingress routing rules, see rules-ingress-traefik.md.</p>"},{"location":"package-core-nginx/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"package-core-nginx/#deployment-components","title":"Deployment Components","text":"<pre><code>Nginx Service Stack:\n\u251c\u2500\u2500 Helm Release (bitnami/nginx)\n\u251c\u2500\u2500 Persistent Volume (nginx-content-pvc)\n\u251c\u2500\u2500 ConfigMap (nginx configuration)\n\u251c\u2500\u2500 Service (ClusterIP)\n\u251c\u2500\u2500 IngressRoute (external access)\n\u2514\u2500\u2500 Pod (nginx container)\n</code></pre>"},{"location":"package-core-nginx/#file-structure","title":"File Structure","text":"<pre><code>01-core/\n\u251c\u2500\u2500 020-setup-nginx.sh          # Main deployment script\n\u2514\u2500\u2500 not-in-use/\n    \u2514\u2500\u2500 020-remove-nginx.sh     # Removal script\n\nmanifests/\n\u251c\u2500\u2500 020-nginx-config.yaml       # Nginx configuration\n\u2514\u2500\u2500 020-nginx-root-ingress.yaml # Ingress routing\n\nansible/playbooks/\n\u251c\u2500\u2500 020-setup-nginx.yml         # Main deployment logic\n\u251c\u2500\u2500 020-setup-web-files.yml     # Content preparation\n\u2514\u2500\u2500 020-remove-nginx.yml        # Removal logic\n</code></pre>"},{"location":"package-core-nginx/#catch-all-routing-configuration","title":"\u2699\ufe0f Catch-All Routing Configuration","text":"<p>Nginx uses Traefik IngressRoute for catch-all routing:</p> <pre><code># manifests/020-nginx-root-ingress.yaml\napiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: nginx-root-catch-all\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: PathPrefix(`/`)\n      kind: Rule\n      priority: 1  # Lowest priority - ensures all other routes checked first\n      services:\n        - name: nginx\n          port: 80\n</code></pre> <p>How It Works: 1. User accesses undefined hostname (e.g., <code>random.localhost</code>) 2. Traefik checks all IngressRoutes with higher priorities first 3. No specific route matches, so nginx catch-all rule (priority 1) is used 4. Request routed to nginx service 5. Nginx serves \"Hello World\" content from PVC</p>"},{"location":"package-core-nginx/#content-management","title":"\ud83d\udcc1 Content Management","text":""},{"location":"package-core-nginx/#web-content-structure","title":"Web Content Structure","text":"<pre><code>nginx-content-pvc/\n\u251c\u2500\u2500 index.html              # \"Hello World\" homepage (from testdata/website/)\n\u2514\u2500\u2500 urbalurba-test.html     # Generated test page for verification\n</code></pre> <p>Content Source: Files are copied from <code>testdata/website/</code> directory, which contains: <pre><code>testdata/website/\n\u2514\u2500\u2500 index.html              # Simple \"Hello World\" page\n</code></pre></p>"},{"location":"package-core-nginx/#content-deployment-process","title":"Content Deployment Process","text":"<ol> <li>Source Content: Copies files from <code>testdata/website/</code> directory (contains <code>index.html</code> with \"Hello World\")</li> <li>PVC Creation: <code>020-setup-web-files.yml</code> creates PVC and uploads content</li> <li>Nginx Deployment: Helm chart deploys with volume mounted to serve the content</li> <li>Catch-All Routing: IngressRoute configured with priority 1 for fallback routing</li> </ol>"},{"location":"package-core-nginx/#adding-custom-content","title":"Adding Custom Content","text":"<p>TODO: test these commands <pre><code># Copy files to nginx content volume\nkubectl cp ./my-website.html nginx-pod:/usr/share/nginx/html/\n\n# Or update via ConfigMap for smaller files\nkubectl create configmap nginx-content --from-file=./content/\n</code></pre></p>"},{"location":"package-core-nginx/#catch-all-testing","title":"Catch-All Testing","text":"<pre><code># Test catch-all behavior with undefined hostnames\ncurl http://undefined.localhost        # Should show \"Hello World\"\ncurl http://random-name.localhost      # Should show \"Hello World\"\ncurl http://localhost/test             # Should show \"Hello World\"\n\n# Test specific content\ncurl http://localhost                  # Shows \"Hello World\" from index.html\ncurl http://localhost/urbalurba-test.html  # Shows generated test page\n\n# Compare with defined routes (these should NOT go to nginx)\ncurl http://whoami.localhost           # Goes to whoami service, not nginx\ncurl http://authentik.localhost        # Goes to authentik service, not nginx\n</code></pre> <p>\ud83d\udca1 Key Insight: Nginx serves as the catch-all safety net for undefined routes in your cluster. When users access hostnames or paths not configured in Traefik, they see a friendly \"Hello World\" instead of an error page.</p>"},{"location":"package-core-readme/","title":"Core Infrastructure Package","text":"<p>File: <code>docs/package-core-readme.md</code> Purpose: Overview of core infrastructure services and deployment patterns Target Audience: Infrastructure engineers, developers deploying core services Last Updated: September 22, 2024</p>"},{"location":"package-core-readme/#overview","title":"\ud83d\udccb Overview","text":"<p>The Core Infrastructure Package provides essential foundation services required for any Kubernetes cluster deployment. These services are deployed first during cluster provisioning to establish the basic infrastructure layer that all other services depend on.</p> <p>Core Services Include: - Web Servers: Nginx for reverse proxy and static content - Storage Systems: Persistent volumes and storage classes - Network Services: DNS, load balancing, and ingress - Basic Monitoring: Health checks and readiness probes</p>"},{"location":"package-core-readme/#service-organization","title":"\ud83d\uddc2\ufe0f Service Organization","text":""},{"location":"package-core-readme/#directory-structure","title":"Directory Structure","text":"<pre><code>provision-host/kubernetes/01-core/\n\u251c\u2500\u2500 020-setup-nginx.sh          # Nginx web server deployment\n\u2514\u2500\u2500 not-in-use/                 # Inactive core services\n    \u2514\u2500\u2500 020-remove-nginx.sh     # Nginx removal script\n</code></pre>"},{"location":"package-core-readme/#deployment-workflow","title":"\ud83d\ude80 Deployment Workflow","text":""},{"location":"package-core-readme/#automatic-deployment","title":"Automatic Deployment","text":"<p>Core services deploy automatically during cluster provisioning:</p> <pre><code># Full cluster provisioning (includes core services)\n./provision-kubernetes.sh rancher-desktop\n</code></pre>"},{"location":"package-core-readme/#manual-core-service-management","title":"Manual Core Service Management","text":"<pre><code># Deploy specific core service\ncd provision-host/kubernetes/01-core/\n./020-setup-nginx.sh rancher-desktop\n\n# Remove specific core service (from not-in-use folder)\ncd provision-host/kubernetes/01-core/not-in-use/\n./020-remove-nginx.sh rancher-desktop\n</code></pre> <p>\ud83d\udca1 Remember: Core services are the foundation of your cluster. Ensure they're stable and well-tested before deploying other services that depend on them.</p>"},{"location":"package-databases-mongodb/","title":"MongoDB Database - Optional NoSQL Database Service","text":"<p>File: <code>docs/package-databases-mongodb.md</code> Purpose: Complete guide to MongoDB deployment and configuration in Urbalurba infrastructure Target Audience: Database administrators, developers working with MongoDB and NoSQL applications Last Updated: September 22, 2024</p>"},{"location":"package-databases-mongodb/#overview","title":"\ud83d\udccb Overview","text":"<p>MongoDB provides a NoSQL document database option in the Urbalurba infrastructure. It's designed as an optional service (located in <code>not-in-use/</code> folder) that can be activated when needed for applications requiring document-based storage, particularly for the Gravitee API Management platform.</p> <p>Key Features: - Document Database: Full MongoDB 8.0.5 compatibility with ARM64 support - Manifest-Based Deployment: Uses direct Kubernetes manifests with StatefulSet - Secret Management: Integrates with urbalurba-secrets for secure authentication - Gravitee Integration: Pre-configured for Gravitee API Management platform - Persistent Storage: 8GB persistent volume with automatic user initialization - Easy Activation: Move script from <code>not-in-use/</code> to activate service</p> <p>TODO: mongodb was set up for gravitee. Change it so it is not coupled tight to gravitee</p>"},{"location":"package-databases-mongodb/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"package-databases-mongodb/#deployment-components","title":"Deployment Components","text":"<pre><code>MongoDB Service Stack:\n\u251c\u2500\u2500 StatefulSet (mongo:8.0.5)\n\u251c\u2500\u2500 ConfigMap (mongod.conf configuration)\n\u251c\u2500\u2500 ConfigMap (user initialization script)\n\u251c\u2500\u2500 Service (ClusterIP on port 27017)\n\u251c\u2500\u2500 PersistentVolumeClaim (8GB storage)\n\u251c\u2500\u2500 urbalurba-secrets (authentication credentials)\n\u2514\u2500\u2500 Pod (mongodb container)\n</code></pre>"},{"location":"package-databases-mongodb/#file-structure","title":"File Structure","text":"<pre><code>02-databases/\n\u2514\u2500\u2500 not-in-use/                 # Inactive by default\n    \u251c\u2500\u2500 04-setup-mongodb.sh     # Main deployment script\n    \u2514\u2500\u2500 04-remove-mongodb.sh    # Removal script\n\nmanifests/\n\u2514\u2500\u2500 040-mongodb-config.yaml     # Complete MongoDB configuration\n\nansible/playbooks/\n\u251c\u2500\u2500 040-setup-mongodb.yml       # Main deployment logic\n\u2514\u2500\u2500 040-remove-database-mongodb.yml  # Removal logic\n</code></pre>"},{"location":"package-databases-mongodb/#deployment","title":"\ud83d\ude80 Deployment","text":""},{"location":"package-databases-mongodb/#service-activation","title":"Service Activation","text":"<p>MongoDB is inactive by default. To activate:</p> <pre><code># Move script from not-in-use to activate\ncd provision-host/kubernetes/02-databases/\nmv not-in-use/04-setup-mongodb.sh ./\n\n# Deploy MongoDB\n./04-setup-mongodb.sh rancher-desktop\n</code></pre>"},{"location":"package-databases-mongodb/#manual-deployment","title":"Manual Deployment","text":"<pre><code># Deploy to specific Kubernetes context\n./04-setup-mongodb.sh multipass-microk8s\n./04-setup-mongodb.sh azure-aks\n</code></pre>"},{"location":"package-databases-mongodb/#prerequisites","title":"Prerequisites","text":"<p>Before deploying MongoDB, ensure the required secrets are configured in <code>urbalurba-secrets</code>:</p> <ul> <li><code>MONGODB_ROOT_USER</code>: Root username for MongoDB admin</li> <li><code>MONGODB_ROOT_PASSWORD</code>: Root password for MongoDB admin</li> <li><code>GRAVITEE_MONGODB_DATABASE_USER</code>: Application user for Gravitee</li> <li><code>GRAVITEE_MONGODB_DATABASE_PASSWORD</code>: Application password for Gravitee</li> <li><code>GRAVITEE_MONGODB_DATABASE_NAME</code>: Database name for Gravitee (typically 'graviteedb')</li> </ul>"},{"location":"package-databases-mongodb/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"package-databases-mongodb/#statefulset-configuration","title":"StatefulSet Configuration","text":"<p>MongoDB uses a StatefulSet deployment with ARM64-compatible image:</p> <pre><code># From manifests/040-mongodb-config.yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongodb\nspec:\n  serviceName: mongodb\n  replicas: 1\n  template:\n    spec:\n      containers:\n      - name: mongodb\n        image: mongo:8.0.5\n        ports:\n        - containerPort: 27017\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"200m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n</code></pre>"},{"location":"package-databases-mongodb/#service-configuration","title":"Service Configuration","text":"<pre><code># MongoDB service\napiVersion: v1\nkind: Service\nmetadata:\n  name: mongodb\nspec:\n  ports:\n  - port: 27017\n    targetPort: 27017\n    protocol: TCP\n  selector:\n    app: mongodb\n  type: ClusterIP\n</code></pre>"},{"location":"package-databases-mongodb/#custom-mongodb-configuration","title":"Custom MongoDB Configuration","text":"<pre><code># Custom mongod.conf configuration\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mongodb-config\ndata:\n  mongod.conf: |\n    storage:\n      dbPath: /data/db\n    systemLog:\n      destination: file\n      path: /data/log/mongod.log\n      logAppend: true\n    net:\n      port: 27017\n      bindIp: 0.0.0.0\n    security:\n      authorization: enabled\n</code></pre>"},{"location":"package-databases-mongodb/#user-initialization-script","title":"User Initialization Script","text":"<p>MongoDB automatically creates Gravitee users during startup:</p> <pre><code>// From init script ConfigMap\ndb = db.getSiblingDB('admin');\n\nconst dbName = process.env.GRAVITEE_MONGODB_DATABASE_NAME || 'graviteedb';\nconst username = process.env.GRAVITEE_MONGODB_DATABASE_USER || 'gravitee_user';\nconst password = process.env.GRAVITEE_MONGODB_DATABASE_PASSWORD || 'gravitee';\n\ndb.createUser({\n  user: username,\n  pwd: password,\n  roles: [\n    { role: 'readWrite', db: dbName },\n    { role: 'dbAdmin', db: dbName },\n    { role: 'readWrite', db: 'admin' }\n  ]\n});\n</code></pre>"},{"location":"package-databases-mongodb/#monitoring-verification","title":"\ud83d\udd0d Monitoring &amp; Verification","text":""},{"location":"package-databases-mongodb/#health-checks","title":"Health Checks","text":"<pre><code># Check pod status\nkubectl get pods -l app=mongodb\n\n# Check StatefulSet status\nkubectl get statefulset mongodb\n\n# Check service status\nkubectl get svc mongodb\n\n# View MongoDB logs\nkubectl logs -l app=mongodb\n</code></pre>"},{"location":"package-databases-mongodb/#database-connection-testing","title":"Database Connection Testing","text":"<pre><code># Test connection from within cluster using mongosh\nkubectl run mongodb-client --image=mongo:8.0.5 --rm -it --restart=Never -- \\\n  mongosh --host mongodb.default.svc.cluster.local --port 27017\n\n# Check if MongoDB is ready (using root credentials)\nkubectl exec -it mongodb-0 -- mongosh --quiet --eval \"db.adminCommand('ping')\"\n\n# Test with authentication\nkubectl exec -it mongodb-0 -- mongosh --username root --password --authenticationDatabase admin\n</code></pre>"},{"location":"package-databases-mongodb/#automated-verification","title":"Automated Verification","text":"<p>The deployment includes automated testing of database functionality:</p> <pre><code># Run verification manually (check playbook for details)\ncd /mnt/urbalurbadisk/ansible\nansible-playbook playbooks/040-setup-mongodb.yml -e kube_context=rancher-desktop\n</code></pre> <p>Verification Process: 1. Connects to MongoDB server using root credentials 2. Verifies Gravitee user creation and permissions 3. Tests database operations and connectivity 4. Validates initialization script execution 5. Creates test documents to verify functionality</p>"},{"location":"package-databases-mongodb/#management-operations","title":"\ud83d\udee0\ufe0f Management Operations","text":""},{"location":"package-databases-mongodb/#database-administration","title":"Database Administration","text":"<pre><code># Access MongoDB shell with authentication\nkubectl exec -it mongodb-0 -- mongosh --username root --password --authenticationDatabase admin\n\n# Switch to Gravitee database\nkubectl exec -it mongodb-0 -- mongosh --username gravitee_user --password --authenticationDatabase admin graviteedb\n\n# Show databases\nkubectl exec -it mongodb-0 -- mongosh --username root --password --authenticationDatabase admin --eval \"show dbs\"\n\n# Show collections in graviteedb\nkubectl exec -it mongodb-0 -- mongosh --username gravitee_user --password --authenticationDatabase admin graviteedb --eval \"show collections\"\n</code></pre>"},{"location":"package-databases-mongodb/#backup-operations","title":"Backup Operations","text":"<pre><code># Create database backup using mongodump\nkubectl exec mongodb-0 -- mongodump --authenticationDatabase admin --username root --password --out /tmp/backup\n\n# Copy backup from pod to local machine\nkubectl cp mongodb-0:/tmp/backup ./mongodb-backup\n\n# Restore from backup using mongorestore\nkubectl cp ./mongodb-backup mongodb-0:/tmp/restore\nkubectl exec mongodb-0 -- mongorestore --authenticationDatabase admin --username root --password /tmp/restore\n</code></pre>"},{"location":"package-databases-mongodb/#service-removal","title":"Service Removal","text":"<pre><code># Remove MongoDB service\ncd provision-host/kubernetes/02-databases/not-in-use/\n./04-remove-mongodb.sh rancher-desktop\n</code></pre> <p>Removal Process: - Removes MongoDB StatefulSet and pods - Deletes MongoDB service - Removes persistent volume claims and data - Preserves urbalurba-secrets and namespace structure - Provides complete cleanup verification</p>"},{"location":"package-databases-mongodb/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-databases-mongodb/#common-issues","title":"Common Issues","text":"<p>Pod Won't Start: <pre><code># Check pod events and logs\nkubectl describe pod mongodb-0\nkubectl logs mongodb-0\n\n# Check StatefulSet status\nkubectl describe statefulset mongodb\n\n# Verify PVC is bound\nkubectl get pvc mongodb-data-mongodb-0\nkubectl describe pvc mongodb-data-mongodb-0\n</code></pre></p> <p>Connection Issues: <pre><code># Verify service endpoints\nkubectl describe svc mongodb\nkubectl get endpoints mongodb\n\n# Test DNS resolution\nkubectl run test-pod --image=busybox --rm -it -- \\\n  nslookup mongodb.default.svc.cluster.local\n\n# Check MongoDB logs for connection errors\nkubectl logs mongodb-0 | grep -i error\n</code></pre></p> <p>Authentication Problems: <pre><code># Check if secrets are properly configured\nkubectl get secret urbalurba-secrets -o yaml\n\n# Verify secret keys exist\nkubectl get secret urbalurba-secrets -o jsonpath='{.data}' | jq 'keys'\n\n# Test root user authentication\nkubectl exec -it mongodb-0 -- mongosh --username root --password --authenticationDatabase admin --eval \"db.runCommand({connectionStatus:1})\"\n</code></pre></p> <p>Initialization Issues: <pre><code># Check if init script was executed\nkubectl logs mongodb-0 | grep \"initialization\"\n\n# Verify Gravitee user was created\nkubectl exec -it mongodb-0 -- mongosh --username root --password --authenticationDatabase admin --eval \"db.getUsers()\"\n\n# Check if test collection exists\nkubectl exec -it mongodb-0 -- mongosh --username gravitee_user --password --authenticationDatabase admin graviteedb --eval \"db.test.find({})\"\n</code></pre></p> <p>Performance Issues: <pre><code># Check resource usage\nkubectl top pod mongodb-0\n\n# View MongoDB process status\nkubectl exec -it mongodb-0 -- mongosh --username root --password --authenticationDatabase admin --eval \"db.serverStatus()\"\n\n# Check slow operations\nkubectl exec -it mongodb-0 -- mongosh --username root --password --authenticationDatabase admin --eval \"db.currentOp()\"\n</code></pre></p>"},{"location":"package-databases-mongodb/#maintenance","title":"\ud83d\udccb Maintenance","text":""},{"location":"package-databases-mongodb/#regular-tasks","title":"Regular Tasks","text":"<ol> <li>Health Monitoring: Check pod and service status daily</li> <li>Backup Schedule: Implement regular database backups using mongodump</li> <li>Log Monitoring: Monitor MongoDB logs for errors and performance issues</li> <li>Security Updates: Update MongoDB image tags regularly</li> </ol>"},{"location":"package-databases-mongodb/#backup-procedures","title":"Backup Procedures","text":"<pre><code># Full backup of all databases\nkubectl exec mongodb-0 -- mongodump --authenticationDatabase admin --username root --password --out /tmp/full-backup\n\n# Backup specific database (Gravitee)\nkubectl exec mongodb-0 -- mongodump --authenticationDatabase admin --username gravitee_user --password --db graviteedb --out /tmp/gravitee-backup\n\n# Backup with compression\nkubectl exec mongodb-0 -- mongodump --authenticationDatabase admin --username root --password --gzip --out /tmp/compressed-backup\n</code></pre>"},{"location":"package-databases-mongodb/#disaster-recovery","title":"Disaster Recovery","text":"<pre><code># Restore full backup\nkubectl exec mongodb-0 -- mongorestore --authenticationDatabase admin --username root --password /tmp/full-backup\n\n# Restore specific database\nkubectl exec mongodb-0 -- mongorestore --authenticationDatabase admin --username root --password --db graviteedb /tmp/gravitee-backup/graviteedb\n\n# Restore from compressed backup\nkubectl exec mongodb-0 -- mongorestore --authenticationDatabase admin --username root --password --gzip /tmp/compressed-backup\n</code></pre>"},{"location":"package-databases-mongodb/#gravitee-integration","title":"Gravitee Integration","text":"<ol> <li>User Permissions: Ensure Gravitee user has proper read/write access to graviteedb</li> <li>Connection String: Use the format: <code>mongodb://gravitee_user:password@mongodb.default.svc.cluster.local:27017/graviteedb?authSource=admin</code></li> <li>Database Monitoring: Monitor Gravitee-specific collections for growth and performance</li> <li>Backup Coordination: Coordinate MongoDB backups with Gravitee maintenance windows</li> </ol> <p>\ud83d\udca1 Key Insight: MongoDB serves as the document database backend for Gravitee API Management and other NoSQL applications in the Urbalurba infrastructure. It provides ARM64 compatibility and automated user provisioning for seamless integration with the platform.</p>"},{"location":"package-databases-mysql/","title":"MySQL Database - Optional Database Service","text":"<p>File: <code>docs/package-databases-mysql.md</code> Purpose: Complete guide to MySQL deployment and configuration in Urbalurba infrastructure Target Audience: Database administrators, developers working with MySQL Last Updated: September 22, 2024</p>"},{"location":"package-databases-mysql/#overview","title":"\ud83d\udccb Overview","text":"<p>MySQL provides an alternative relational database option in the Urbalurba infrastructure. It's designed as an optional service (located in <code>not-in-use/</code> folder) that can be activated when needed for applications requiring MySQL-specific features or compatibility.</p> <p>Key Features: - Standard SQL Database: Full MySQL 8.x compatibility - Helm-Based Deployment: Uses Bitnami MySQL chart for reliable deployment - Secret Management: Integrates with urbalurba-secrets for secure authentication - Automated Testing: Includes CRUD verification and health checks - Easy Activation: Move script from <code>not-in-use/</code> to activate service</p>"},{"location":"package-databases-mysql/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"package-databases-mysql/#deployment-components","title":"Deployment Components","text":"<pre><code>MySQL Service Stack:\n\u251c\u2500\u2500 Helm Release (bitnami/mysql)\n\u251c\u2500\u2500 ConfigMap (custom MySQL configuration)\n\u251c\u2500\u2500 Service (ClusterIP on port 3306)\n\u251c\u2500\u2500 urbalurba-secrets (authentication credentials)\n\u2514\u2500\u2500 Pod (mysql container)\n</code></pre>"},{"location":"package-databases-mysql/#file-structure","title":"File Structure","text":"<pre><code>02-databases/\n\u2514\u2500\u2500 not-in-use/                 # Inactive by default\n    \u251c\u2500\u2500 06-setup-mysql.sh       # Main deployment script\n    \u2514\u2500\u2500 06-remove-mysql.sh      # Removal script\n\nmanifests/\n\u2514\u2500\u2500 043-database-mysql-config.yaml  # MySQL service and configuration\n\nansible/playbooks/\n\u251c\u2500\u2500 040-database-mysql.yml      # Main deployment logic\n\u251c\u2500\u2500 040-remove-database-mysql.yml   # Removal logic\n\u2514\u2500\u2500 utility/\n    \u2514\u2500\u2500 u08-verify-mysql.yml    # CRUD testing and verification\n</code></pre>"},{"location":"package-databases-mysql/#deployment","title":"\ud83d\ude80 Deployment","text":""},{"location":"package-databases-mysql/#service-activation","title":"Service Activation","text":"<p>MySQL is inactive by default. To activate:</p> <pre><code># Move script from not-in-use to activate\ncd provision-host/kubernetes/02-databases/\nmv not-in-use/06-setup-mysql.sh ./\n\n# Deploy MySQL\n./06-setup-mysql.sh rancher-desktop\n</code></pre>"},{"location":"package-databases-mysql/#manual-deployment","title":"Manual Deployment","text":"<pre><code># Deploy to specific Kubernetes context\n./06-setup-mysql.sh multipass-microk8s\n./06-setup-mysql.sh azure-aks\n</code></pre>"},{"location":"package-databases-mysql/#prerequisites","title":"Prerequisites","text":"<p>Before deploying MySQL, ensure the required secrets are configured in <code>urbalurba-secrets</code>:</p> <ul> <li><code>MYSQL_ROOT_PASSWORD</code>: Root user password</li> <li><code>MYSQL_USER</code>: Application user name</li> <li><code>MYSQL_PASSWORD</code>: Application user password</li> <li><code>MYSQL_DATABASE</code>: Default database name</li> <li><code>MYSQL_HOST</code>: Database host (typically service name)</li> </ul>"},{"location":"package-databases-mysql/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"package-databases-mysql/#helm-configuration","title":"Helm Configuration","text":"<p>MySQL uses the Bitnami Helm chart with the following setup:</p> <pre><code># Deployment command (from Ansible playbook)\nhelm install mysql bitnami/mysql \\\n  --namespace default \\\n  -f manifests/043-database-mysql-config.yaml \\\n  --set auth.rootPassword=\"$MYSQL_ROOT_PASSWORD\" \\\n  --set auth.username=\"$MYSQL_USER\" \\\n  --set auth.password=\"$MYSQL_PASSWORD\" \\\n  --set auth.database=\"$MYSQL_DATABASE\"\n</code></pre>"},{"location":"package-databases-mysql/#service-configuration","title":"Service Configuration","text":"<pre><code># manifests/043-database-mysql-config.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\n  namespace: default\nspec:\n  ports:\n    - port: 3306\n      targetPort: 3306\n  selector:\n    app.kubernetes.io/name: mysql\n</code></pre>"},{"location":"package-databases-mysql/#custom-mysql-configuration","title":"Custom MySQL Configuration","text":"<pre><code># Optional custom configuration in ConfigMap\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mysql-custom-config\ndata:\n  my.cnf: |\n    [mysqld]\n    max_connections=200\n    sql_mode=STRICT_ALL_TABLES\n</code></pre>"},{"location":"package-databases-mysql/#database-connection-testing","title":"Database Connection Testing","text":"<pre><code># Test connection from within cluster\nkubectl run mysql-client --image=mysql:8.0 --rm -it --restart=Never -- \\\n  mysql -h mysql.default.svc.cluster.local -u root -p\n\n# Check if MySQL is ready\nkubectl exec -it mysql-pod -- mysqladmin ping -uroot -p\n</code></pre>"},{"location":"package-databases-mysql/#automated-verification","title":"Automated Verification","text":"<p>The deployment includes automated CRUD testing:</p> <pre><code># Run verification playbook manually\ncd /mnt/urbalurbadisk/ansible\nansible-playbook playbooks/utility/u08-verify-mysql.yml\n</code></pre> <p>Verification Process: 1. Connects to MySQL server using root credentials 2. Creates test database and table 3. Inserts and retrieves test data 4. Verifies data integrity 5. Cleans up test database</p>"},{"location":"package-databases-mysql/#management-operations","title":"\ud83d\udee0\ufe0f Management Operations","text":""},{"location":"package-databases-mysql/#database-administration","title":"Database Administration","text":"<pre><code># Access MySQL shell\nkubectl exec -it mysql-pod -- mysql -uroot -p\n\n# Create new database\nkubectl exec -it mysql-pod -- mysql -uroot -p -e \"CREATE DATABASE myapp;\"\n\n# Show databases\nkubectl exec -it mysql-pod -- mysql -uroot -p -e \"SHOW DATABASES;\"\n\n# Run SQL script\nkubectl cp script.sql mysql-pod:/tmp/\nkubectl exec -it mysql-pod -- mysql -uroot -p &lt; /tmp/script.sql\n</code></pre>"},{"location":"package-databases-mysql/#backup-operations","title":"Backup Operations","text":"<pre><code># Create database backup\nkubectl exec mysql-pod -- mysqldump -uroot -p myapp &gt; backup.sql\n\n# Restore from backup\nkubectl exec -i mysql-pod -- mysql -uroot -p myapp &lt; backup.sql\n</code></pre>"},{"location":"package-databases-mysql/#service-removal","title":"Service Removal","text":"<pre><code># Remove MySQL service\ncd provision-host/kubernetes/02-databases/not-in-use/\n./06-remove-mysql.sh rancher-desktop\n</code></pre> <p>Removal Process: - Uninstalls MySQL Helm release - Waits for pods to terminate - Preserves urbalurba-secrets and namespace structure - Does not remove persistent data (if configured)</p>"},{"location":"package-databases-mysql/#backup-procedures","title":"Backup Procedures","text":"<pre><code># Full backup of all databases\nkubectl exec mysql-pod -- mysqldump -uroot -p --all-databases &gt; full-backup.sql\n\n# Backup specific database\nkubectl exec mysql-pod -- mysqldump -uroot -p myapp &gt; myapp-backup.sql\n\n# Backup with compression\nkubectl exec mysql-pod -- mysqldump -uroot -p myapp | gzip &gt; myapp-backup.sql.gz\n</code></pre>"},{"location":"package-databases-mysql/#disaster-recovery","title":"Disaster Recovery","text":"<pre><code># Restore full backup\nkubectl exec -i mysql-pod -- mysql -uroot -p &lt; full-backup.sql\n\n# Restore specific database\nkubectl exec -i mysql-pod -- mysql -uroot -p myapp &lt; myapp-backup.sql\n\n# Restore from compressed backup\ngunzip -c myapp-backup.sql.gz | kubectl exec -i mysql-pod -- mysql -uroot -p myapp\n</code></pre> <p>\ud83d\udca1 Key Insight: MySQL serves as an optional alternative to PostgreSQL in the Urbalurba infrastructure. Activate it when you need MySQL-specific features or have applications that require MySQL compatibility.</p>"},{"location":"package-databases-postgresql-container/","title":"PostgreSQL Container for Urbalurba Infrastructure","text":"<p>File: <code>docs/package-postgresql-container.md</code> Purpose: Complete documentation for custom PostgreSQL container with AI/ML extensions and CI/CD automation Target Audience: Infrastructure engineers, DevOps teams, and developers working with PostgreSQL extensions Last Updated: September 22, 2024</p>"},{"location":"package-databases-postgresql-container/#overview","title":"\ud83d\udccb Overview","text":"<p>This document covers the custom PostgreSQL container (<code>containers/postgresql/</code>) that extends the official Bitnami PostgreSQL image with additional extensions required for the Urbalurba platform. This custom container provides enhanced functionality for modern applications requiring vector search, geospatial data, and hierarchical data structures.</p> <p>The container is automatically built and published via GitHub Actions CI/CD to GitHub Container Registry with full multi-architecture support.</p>"},{"location":"package-databases-postgresql-container/#why-custom-container","title":"Why Custom Container?","text":"<p>The standard Bitnami PostgreSQL image doesn't include certain extensions that are crucial for modern AI and data-intensive applications:</p> <ul> <li>pgvector: Essential for vector search and AI embeddings</li> <li>PostGIS: Required for geospatial data types and queries</li> <li>hstore: Enables key-value storage within PostgreSQL columns</li> <li>ltree: Supports hierarchical/tree-like data structures</li> </ul>"},{"location":"package-databases-postgresql-container/#extensions-included","title":"Extensions Included","text":"Extension Version Purpose pgvector Latest Vector similarity search and AI embeddings PostGIS 3.x Geospatial data types, functions, and indexing hstore Built-in Key-value pairs in single values ltree Built-in Hierarchical tree-like data uuid-ossp Built-in UUID generation functions pg_trgm Built-in Trigram matching for fuzzy text search btree_gin Built-in Additional indexing methods pgcrypto Built-in Cryptographic functions"},{"location":"package-databases-postgresql-container/#container-details","title":"Container Details","text":"<ul> <li>Base Image: <code>bitnami/postgresql:16</code></li> <li>Registry: <code>ghcr.io/terchris/urbalurba-postgresql</code></li> <li>Architectures: <code>linux/amd64</code>, <code>linux/arm64</code></li> <li>Security: Runs as non-root user (UID 1001)</li> <li>Optimization: Multi-stage build for minimal size</li> <li>Package Source: PostgreSQL official repository (PGDG) for latest packages</li> <li>Build Dependencies: wget, ca-certificates, gnupg, lsb-release (cleaned up after build)</li> </ul>"},{"location":"package-databases-postgresql-container/#development-workflow","title":"Development Workflow","text":""},{"location":"package-databases-postgresql-container/#optimal-multi-architecture-development","title":"Optimal Multi-Architecture Development","text":"<p>Take advantage of your local hardware for comprehensive testing:</p> <pre><code># 1. Local Development &amp; Testing (Your Mac = Native ARM64)\ncd containers/postgresql\n./build.sh --single-arch\n# \u2705 Native ARM64 build and testing\n# \u2705 All 8 extensions validated\n# \u2705 Performance testing without emulation\n\n# 2. Commit and Push Changes\ngit add .\ngit commit -m \"PostgreSQL container improvements\"\ngit push\n\n# 3. CI/CD Automatically Handles:\n# \u2705 AMD64: Full functional testing (GitHub Actions)\n# \u2705 ARM64: Build verification (GitHub Actions)\n# \u2705 Multi-arch: Registry publishing (GitHub Actions)\n# \u2705 Security: Vulnerability scanning (GitHub Actions)\n\n# 4. Result: Both architectures fully validated!\n</code></pre>"},{"location":"package-databases-postgresql-container/#why-this-works-perfectly","title":"Why This Works Perfectly","text":"<ul> <li>Your Mac: Native ARM64 testing (real performance, all features)</li> <li>GitHub Actions: Native AMD64 testing (most common deployment)</li> <li>Combined: Complete multi-architecture confidence</li> <li>No Emulation: Fast, reliable, production-representative testing</li> </ul>"},{"location":"package-databases-postgresql-container/#local-development","title":"Local Development","text":""},{"location":"package-databases-postgresql-container/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Desktop with BuildKit enabled</li> <li>For multi-arch builds: <code>docker buildx create --use</code></li> <li>Internet access for PGDG repository during build</li> <li>Sufficient disk space for multi-stage build (PostgreSQL 16 + pgvector images)</li> </ul>"},{"location":"package-databases-postgresql-container/#build-locally","title":"Build Locally","text":"<pre><code>cd containers/postgresql\n\n# Build single architecture (recommended for local testing)\n./build.sh --single-arch\n\n# On Apple Silicon Macs: This builds and tests ARM64 natively!\n# On Intel/AMD64: This builds and tests AMD64 natively!\n\n# Build multi-architecture (requires push to registry)\n./build.sh --push\n\n# Build specific version\n./build.sh --version v1.2.0 --push\n\n# Show all options\n./build.sh --help\n</code></pre>"},{"location":"package-databases-postgresql-container/#build-options","title":"Build Options","text":"<ul> <li><code>--single-arch</code>: Build for current architecture only (faster, good for local testing)</li> <li><code>--push</code>: Push to registry (required for multi-arch builds)</li> <li><code>--version VERSION</code>: Set custom version tag</li> <li><code>--platform PLATFORMS</code>: Set target platforms (default: linux/amd64,linux/arm64)</li> <li><code>--help</code>: Show detailed usage information</li> </ul>"},{"location":"package-databases-postgresql-container/#test-locally","title":"Test Locally","text":"<pre><code># Run the container\ndocker run -d --name postgres-test \\\n  -e POSTGRESQL_PASSWORD=testpass123 \\\n  -e POSTGRESQL_POSTGRES_PASSWORD=testpass123 \\\n  -e POSTGRESQL_DATABASE=testdb \\\n  -p 5432:5432 \\\n  ghcr.io/terchris/urbalurba-postgresql:latest\n\n# Wait for PostgreSQL to be ready\ndocker exec postgres-test pg_isready -U postgres\n\n# Test all extensions (with authentication)\ndocker exec -e PGPASSWORD=testpass123 postgres-test psql -U postgres -d testdb -c \"\n  CREATE EXTENSION IF NOT EXISTS vector;\n  CREATE EXTENSION IF NOT EXISTS hstore;\n  CREATE EXTENSION IF NOT EXISTS ltree;\n  CREATE EXTENSION IF NOT EXISTS postgis;\n  CREATE EXTENSION IF NOT EXISTS \\\"uuid-ossp\\\";\n  CREATE EXTENSION IF NOT EXISTS pg_trgm;\n  CREATE EXTENSION IF NOT EXISTS btree_gin;\n  CREATE EXTENSION IF NOT EXISTS pgcrypto;\n\"\n\n# Verify extensions are installed\ndocker exec -e PGPASSWORD=testpass123 postgres-test psql -U postgres -d testdb -c \\\n  \"SELECT extname FROM pg_extension WHERE extname NOT IN ('plpgsql') ORDER BY extname;\"\n\n# Cleanup\ndocker stop postgres-test &amp;&amp; docker rm postgres-test\n</code></pre>"},{"location":"package-databases-postgresql-container/#automated-testing","title":"Automated Testing","text":"<p>The build script includes comprehensive automated testing: - Verifies all 8 extensions install correctly - Tests basic functionality of each extension - Validates vector operations, geospatial queries, and hierarchical data - Automatically cleans up test containers</p>"},{"location":"package-databases-postgresql-container/#integration-with-urbalurba-infrastructure","title":"Integration with Urbalurba Infrastructure","text":""},{"location":"package-databases-postgresql-container/#in-kubernetes-manifests","title":"In Kubernetes Manifests","text":"<p>Update your <code>manifests/042-database-postgresql-config.yaml</code>:</p> <pre><code>spec:\n  template:\n    spec:\n      containers:\n      - name: postgresql\n        image: ghcr.io/terchris/urbalurba-postgresql:latest\n</code></pre>"},{"location":"package-databases-postgresql-container/#in-ansible-playbooks","title":"In Ansible Playbooks","text":"<p>Reference in <code>ansible/playbooks/040-database-postgresql.yml</code>:</p> <pre><code>- name: Deploy custom PostgreSQL\n  kubernetes.core.k8s:\n    definition:\n      spec:\n        template:\n          spec:\n            containers:\n            - image: ghcr.io/terchris/urbalurba-postgresql:latest\n</code></pre>"},{"location":"package-databases-postgresql-container/#cicd-pipeline-with-github-actions","title":"\ud83d\udd04 CI/CD Pipeline with GitHub Actions","text":"<p>The container is automatically built and published via GitHub Actions using the workflow at <code>.github/workflows/build-postgresql-container.yml</code>. This provides comprehensive automation for building, testing, and publishing the PostgreSQL container.</p>"},{"location":"package-databases-postgresql-container/#workflow-overview","title":"Workflow Overview","text":"<p>File: <code>.github/workflows/build-postgresql-container.yml</code> Purpose: Automated multi-architecture container builds with security scanning and testing</p>"},{"location":"package-databases-postgresql-container/#workflow-triggers","title":"Workflow Triggers","text":"<p>The workflow runs automatically on: - Push to main - When PostgreSQL container files change (<code>containers/postgresql/**</code>) - Pull requests - For testing changes before merge - Release tags - For versioned releases - Manual dispatch - With custom version parameters</p>"},{"location":"package-databases-postgresql-container/#multi-job-architecture","title":"Multi-Job Architecture","text":""},{"location":"package-databases-postgresql-container/#1-build-job","title":"1. Build Job \ud83d\ude80","text":"<ul> <li>Platform: Ubuntu (GitHub Actions)</li> <li>Duration: ~15 minutes</li> <li>Architectures: <code>linux/amd64</code>, <code>linux/arm64</code></li> <li>Registry: GitHub Container Registry (<code>ghcr.io</code>)</li> </ul> <p>Key Features: - Multi-architecture builds using Docker Buildx - Automatic tagging based on trigger type - BuildKit caching for faster subsequent builds - Metadata generation with OCI labels</p> <p>Generated Tags: - <code>latest</code> - Main branch pushes - <code>v1.0.0</code> - Release tags - <code>pr-123</code> - Pull request builds - <code>main-abc1234-20241201</code> - SHA-based unique tags</p>"},{"location":"package-databases-postgresql-container/#2-security-scan-job","title":"2. Security Scan Job \ud83d\udee1\ufe0f","text":"<ul> <li>Platform: Ubuntu (GitHub Actions)</li> <li>Duration: ~5 minutes</li> <li>Tool: Trivy vulnerability scanner</li> <li>Scope: CRITICAL and HIGH severity issues</li> </ul> <p>Security Features: - SARIF report generation for GitHub Security tab - Human-readable vulnerability summaries - Automatic upload to GitHub Security dashboard - Failure tolerance (workflow continues even with vulnerabilities)</p>"},{"location":"package-databases-postgresql-container/#3-test-job","title":"3. Test Job \ud83e\uddea","text":"<ul> <li>Platform: Ubuntu (GitHub Actions)</li> <li>Duration: ~10 minutes</li> <li>Architecture: AMD64 (native testing)</li> <li>Extensions Tested: All 8 extensions validated</li> </ul> <p>Testing Strategy: - AMD64: Full functional testing on native GitHub runners - PostgreSQL Client: Uses official PostgreSQL client tools - Extension Validation: Creates and tests all extensions - Functional Tests: Vector operations, geospatial queries, hierarchical data - Health Checks: Built-in container health monitoring</p> <p>Why AMD64 Only for Testing: - GitHub Actions runners are AMD64-only (no native ARM64) - QEMU emulation for ARM64 testing is unreliable for complex containers - ARM64 build verification happens separately (see ARM64 Verification job)</p>"},{"location":"package-databases-postgresql-container/#4-arm64-verification-job","title":"4. ARM64 Verification Job \ud83d\udd0d","text":"<ul> <li>Platform: Ubuntu (GitHub Actions)</li> <li>Duration: ~3 minutes</li> <li>Purpose: Verify ARM64 images build and can be pulled</li> </ul> <p>Verification Process: - Manifest inspection for ARM64 variant - Image pull test for ARM64 architecture - Basic image metadata validation - No emulated runtime testing (by design for reliability)</p>"},{"location":"package-databases-postgresql-container/#workflow-permissions","title":"Workflow Permissions","text":"<pre><code>permissions:\n  contents: read          # Read repository contents\n  packages: write         # Push to GitHub Container Registry\n  security-events: write  # Upload security scan results\n</code></pre>"},{"location":"package-databases-postgresql-container/#environment-variables","title":"Environment Variables","text":"<pre><code>env:\n  REGISTRY: ghcr.io\n  IMAGE_NAME: ghcr.io/${{ github.repository_owner }}/urbalurba-postgresql\n</code></pre>"},{"location":"package-databases-postgresql-container/#manual-workflow-dispatch","title":"Manual Workflow Dispatch","text":"<p>The workflow supports manual triggering with custom parameters:</p> <pre><code># Via GitHub UI or gh CLI:\ngh workflow run build-postgresql-container.yml \\\n  -f version=v1.3.0 \\\n  -f push_image=true\n</code></pre> <p>Parameters: - <code>version</code>: Custom container version tag - <code>push_image</code>: Whether to push to registry (default: true)</p>"},{"location":"package-databases-postgresql-container/#workflow-outputs","title":"Workflow Outputs","text":"<p>Each workflow run generates: - Build artifacts: Multi-architecture container images - Security reports: Vulnerability scan results in GitHub Security tab - Test results: Extension functionality validation - Workflow summary: Human-readable results with tags, digests, and test status</p>"},{"location":"package-databases-postgresql-container/#registry-integration","title":"Registry Integration","text":"<ul> <li>Registry: GitHub Container Registry (<code>ghcr.io</code>)</li> <li>Authentication: Automatic via <code>GITHUB_TOKEN</code></li> <li>Visibility: Public (linked to repository)</li> <li>Retention: Follows GitHub's container retention policies</li> </ul>"},{"location":"package-databases-postgresql-container/#cicd-best-practices-implemented","title":"CI/CD Best Practices Implemented","text":"<ol> <li>Security First:</li> <li>No secrets in code or logs</li> <li>Vulnerability scanning on every build</li> <li> <p>Read-only permissions where possible</p> </li> <li> <p>Multi-Architecture Support:</p> </li> <li>Native AMD64 testing for reliability</li> <li>ARM64 build verification for compatibility</li> <li> <p>Manifest inspection for architecture validation</p> </li> <li> <p>Caching Strategy:</p> </li> <li>GitHub Actions cache for Docker builds</li> <li>Layer caching for faster subsequent builds</li> <li> <p>Maximum cache mode for optimal performance</p> </li> <li> <p>Error Handling:</p> </li> <li>Retry logic for container startup</li> <li>Comprehensive debugging output</li> <li> <p>Graceful failure handling</p> </li> <li> <p>Observability:</p> </li> <li>Detailed workflow summaries</li> <li>Test result reporting</li> <li>Build artifact tracking</li> </ol>"},{"location":"package-databases-postgresql-container/#image-tags","title":"Image Tags","text":"<ul> <li><code>ghcr.io/terchris/urbalurba-postgresql:latest</code> - Latest stable build</li> <li><code>ghcr.io/terchris/urbalurba-postgresql:v1.0.0</code> - Specific version</li> <li><code>ghcr.io/terchris/urbalurba-postgresql:pr-123</code> - Pull request builds</li> </ul>"},{"location":"package-databases-postgresql-container/#multi-architecture-support","title":"Multi-Architecture Support","text":""},{"location":"package-databases-postgresql-container/#cicd-testing-strategy","title":"CI/CD Testing Strategy","text":"<p>Our testing approach balances reliability with multi-architecture support:</p> <p>AMD64 Testing (Full): - \u2705 Native testing on GitHub Actions runners - \u2705 Complete functional tests with all 8 extensions - \u2705 Performance validation and integration testing</p> <p>ARM64 Verification (Build-Only): - \u2705 Multi-architecture build verification - \u2705 Image availability and pull testing - \u2705 Manifest inspection and architecture validation - \u26a0\ufe0f No emulated runtime testing (avoided due to QEMU reliability issues)</p>"},{"location":"package-databases-postgresql-container/#why-this-approach","title":"Why This Approach?","text":"<p>GitHub Actions Limitation: Standard GitHub-hosted runners are AMD64-only. ARM64 container testing requires QEMU emulation, which: - Is significantly slower (5-10x overhead) - Has reliability issues with complex containers - Can produce false failures due to timing/emulation problems - Doesn't represent real ARM64 performance</p> <p>Production Confidence:  - AMD64 gets full testing (most common deployment target) - ARM64 build process is verified (image exists and is pullable) - Multi-architecture manifest is validated - Production ARM64 deployments can be validated separately</p>"},{"location":"package-databases-postgresql-container/#arm64-testing-apple-silicon-mac","title":"ARM64 Testing (Apple Silicon Mac)","text":"<p>If you're on Apple Silicon (M1/M2/M3), you can test ARM64 natively:</p> <pre><code># Native ARM64 testing on Apple Silicon\n./build.sh --single-arch\n\n# This provides:\n# \u2705 Native ARM64 performance (no emulation)\n# \u2705 Complete functional testing\n# \u2705 All 8 extensions validated\n# \u2705 Real-world ARM64 confidence\n</code></pre>"},{"location":"package-databases-postgresql-container/#arm64-production-validation","title":"ARM64 Production Validation","text":"<p>For ARM64 production deployments, validate manually:</p> <pre><code># On ARM64 hardware (Apple Silicon, AWS Graviton, etc.)\ndocker run --rm \\\n  -e POSTGRESQL_PASSWORD=testpass \\\n  -e POSTGRESQL_DATABASE=testdb \\\n  ghcr.io/terchris/urbalurba-postgresql:latest \\\n  psql -U postgres -d testdb -c \"CREATE EXTENSION vector; SELECT version();\"\n</code></pre>"},{"location":"package-databases-postgresql-container/#usage-examples","title":"Usage Examples","text":""},{"location":"package-databases-postgresql-container/#basic-postgresql-with-extensions","title":"Basic PostgreSQL with Extensions","text":"<pre><code>-- Connect to database\n\\c your_database\n\n-- Create vector search table\nCREATE TABLE documents (\n  id SERIAL PRIMARY KEY,\n  content TEXT,\n  embedding vector(1536),\n  metadata hstore\n);\n\n-- Create spatial data table\nCREATE TABLE locations (\n  id SERIAL PRIMARY KEY,\n  name TEXT,\n  coordinates GEOMETRY(POINT, 4326)\n);\n\n-- Create hierarchical data\nCREATE TABLE categories (\n  id SERIAL PRIMARY KEY,\n  path ltree\n);\n</code></pre>"},{"location":"package-databases-postgresql-container/#performance-indexes","title":"Performance Indexes","text":"<pre><code>-- Vector similarity index\nCREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops);\n\n-- Spatial index\nCREATE INDEX ON locations USING gist(coordinates);\n\n-- Hierarchical index\nCREATE INDEX ON categories USING gist(path);\n</code></pre>"},{"location":"package-databases-postgresql-container/#troubleshooting","title":"Troubleshooting","text":""},{"location":"package-databases-postgresql-container/#common-issues","title":"Common Issues","text":"<ol> <li>Extension not available: Verify the container is using the custom image</li> <li>Permission denied: Extensions require superuser privileges during installation</li> <li>Build failures: Check Docker BuildKit is enabled</li> <li>Package not found errors: The build now automatically adds PostgreSQL official repository (PGDG)</li> <li>Architecture testing: Use <code>./build.sh --single-arch</code> on your Mac for native ARM64 testing</li> </ol>"},{"location":"package-databases-postgresql-container/#fixed-issues-v120","title":"Fixed Issues (v1.2.0+)","text":"<p>PostgreSQL 16 package availability:  - Problem: <code>postgresql-16-postgis-3</code> and <code>postgresql-contrib-16</code> packages not found in Debian repositories - Solution: Automatically adds PostgreSQL official repository (PGDG) during build - Impact: All PostgreSQL 16 packages now available</p> <p>Authentication in tests: - Problem: psql commands failing with \"no password supplied\" error and container startup failing with \"POSTGRESQL_PASSWORD environment variable is empty\" - Solution: Added both <code>POSTGRESQL_PASSWORD</code> and <code>POSTGRESQL_POSTGRES_PASSWORD</code> environment variables, plus <code>PGPASSWORD</code> for test commands - Impact: Container startup and automated testing now work reliably</p> <p>SQL syntax in extension verification: - Problem: <code>ORDER BY</code> clause error in <code>string_agg()</code> queries - Solution: Moved <code>ORDER BY</code> inside the aggregate function - Impact: Extension verification queries now work correctly</p> <p>Container cleanup conflicts: - Problem: Test containers with same name causing build failures - Solution: Added pre-test cleanup and robust error handling - Impact: Builds can run repeatedly without manual cleanup</p>"},{"location":"package-databases-postgresql-container/#debug-commands","title":"Debug Commands","text":"<pre><code># Check running container\nkubectl exec -it postgresql-pod -- psql -U postgres -c \"SELECT extname, extversion FROM pg_extension;\"\n\n# Verify image\nkubectl describe pod postgresql-pod | grep Image:\n\n# Check logs\nkubectl logs postgresql-pod\n\n# Local debug - check extension files\ndocker exec postgres-test ls -la /opt/bitnami/postgresql/lib/vector.so\ndocker exec postgres-test ls -la /opt/bitnami/postgresql/share/extension/vector*\n\n# Local debug - verify PGDG repository was added\ndocker exec postgres-test cat /etc/apt/sources.list.d/pgdg.list\n</code></pre>"},{"location":"package-databases-postgresql-container/#maintenance","title":"Maintenance","text":""},{"location":"package-databases-postgresql-container/#updating-base-image","title":"Updating Base Image","text":"<ol> <li>Update <code>FROM bitnami/postgresql:16</code> to newer version in Dockerfile</li> <li>Test locally with <code>./build.sh</code></li> <li>Create pull request</li> <li>GitHub Actions will build and test</li> <li>Merge triggers automatic deployment</li> </ol>"},{"location":"package-databases-postgresql-container/#adding-new-extensions","title":"Adding New Extensions","text":"<ol> <li>Add installation commands to Dockerfile</li> <li>Update this README documentation</li> <li>Test locally</li> <li>Submit pull request</li> </ol>"},{"location":"package-databases-postgresql-container/#security","title":"Security","text":"<ul> <li>Container runs as non-root user (1001:1001)</li> <li>Regular vulnerability scanning via Trivy</li> <li>Base image updated automatically via dependabot</li> <li>No secrets or credentials in container image</li> <li>Network isolation through Kubernetes policies</li> <li>PGDG Repository: Uses official PostgreSQL repository with verified GPG signatures</li> <li>Build dependencies: Temporary packages (wget, gnupg) removed after build to minimize attack surface</li> </ul>"},{"location":"package-databases-postgresql-container/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Optimized for development and medium-scale production</li> <li>Default settings suitable for 1-4GB RAM</li> <li>For high-performance workloads, tune PostgreSQL configuration via ConfigMaps</li> <li>Monitor with your existing monitoring stack</li> </ul>"},{"location":"package-databases-postgresql-container/#support","title":"Support","text":"<p>For issues related to: - Container build: Check GitHub Actions logs - Extension functionality: Refer to upstream documentation - Urbalurba integration: See main infrastructure documentation - Performance tuning: Consult PostgreSQL documentation</p>"},{"location":"package-databases-postgresql-container/#version-history","title":"Version History","text":"<ul> <li>v1.0.0: Initial release with pgvector, PostGIS, hstore, ltree</li> <li>v1.1.0: Added btree_gin and pgcrypto extensions</li> <li>v1.2.0: Major improvements and fixes:</li> <li>Updated to PostgreSQL 16.x base with PGDG repository</li> <li>Fixed package availability issues for PostgreSQL 16</li> <li>Enhanced build script with comprehensive testing</li> <li>Fixed authentication issues in automated tests</li> <li>Added robust container cleanup and error handling</li> <li>Improved CI/CD pipeline with realistic multi-architecture testing</li> <li>Optimized testing strategy: Native AMD64 (CI) + Native ARM64 (local Mac)</li> <li>Added detailed troubleshooting documentation</li> <li>Eliminated unreliable emulated testing for faster, more reliable builds</li> </ul>"},{"location":"package-databases-postgresql/","title":"PostgreSQL Database - Primary Database Service","text":"<p>Pre-Built Extensions: Vector Search (pgvector) \u2022 Geospatial (PostGIS) \u2022 Key-Value (hstore) \u2022 Hierarchical (ltree) \u2022 UUID Generation \u2022 Fuzzy Search \u2022 Advanced Indexing \u2022 Cryptography</p> <p>File: <code>docs/package-databases-postgresql.md</code> Purpose: Complete guide to PostgreSQL deployment and configuration in Urbalurba infrastructure Target Audience: Database administrators, developers working with PostgreSQL, AI/ML developers Last Updated: September 22, 2024</p>"},{"location":"package-databases-postgresql/#overview","title":"\ud83d\udccb Overview","text":"<p>PostgreSQL serves as the primary database service in the Urbalurba infrastructure. It's designed as an active service that provides a powerful, production-ready relational database with advanced extensions for AI, geospatial, and modern data-intensive applications.</p> <p>\ud83d\udd27 IMPORTANT: This PostgreSQL deployment uses a custom container with pre-built AI and geospatial extensions. For detailed information about the custom container, its extensions, and CI/CD pipeline, see package-databases-postgresql-container.md.</p> <p>Key Features: - Advanced SQL Database: Full PostgreSQL 16 compatibility with 8 pre-built extensions - Custom Container: Uses <code>ghcr.io/terchris/urbalurba-postgresql</code> with AI/ML and geospatial extensions - Helm-Based Deployment: Uses Bitnami PostgreSQL chart with custom image override - Secret Management: Integrates with urbalurba-secrets for secure authentication - Automated Testing: Includes comprehensive CRUD and extension verification - AI-Ready: Pre-configured with pgvector for vector search and embeddings</p>"},{"location":"package-databases-postgresql/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"package-databases-postgresql/#deployment-components","title":"Deployment Components","text":"<pre><code>PostgreSQL Service Stack:\n\u251c\u2500\u2500 Helm Release (bitnami/postgresql with custom image)\n\u251c\u2500\u2500 StatefulSet (custom urbalurba-postgresql container)\n\u251c\u2500\u2500 ConfigMap (PostgreSQL configuration)\n\u251c\u2500\u2500 Service (ClusterIP on port 5432)\n\u251c\u2500\u2500 PersistentVolumeClaim (8GB storage)\n\u251c\u2500\u2500 urbalurba-secrets (authentication credentials)\n\u2514\u2500\u2500 Pod (postgresql container with 8 extensions)\n</code></pre>"},{"location":"package-databases-postgresql/#file-structure","title":"File Structure","text":"<pre><code>02-databases/\n\u251c\u2500\u2500 05-setup-postgres.sh        # Main deployment script (active)\n\u2514\u2500\u2500 not-in-use/\n    \u2514\u2500\u2500 05-remove-postgres.sh   # Removal script\n\nmanifests/\n\u2514\u2500\u2500 042-database-postgresql-config.yaml  # PostgreSQL Helm configuration\n\nansible/playbooks/\n\u251c\u2500\u2500 040-database-postgresql.yml     # Main deployment logic\n\u251c\u2500\u2500 040-remove-database-postgresql.yml  # Removal logic\n\u2514\u2500\u2500 utility/\n    \u2514\u2500\u2500 u02-verify-postgres.yml     # Extension and CRUD testing\n</code></pre>"},{"location":"package-databases-postgresql/#deployment","title":"\ud83d\ude80 Deployment","text":""},{"location":"package-databases-postgresql/#automatic-deployment","title":"Automatic Deployment","text":"<p>PostgreSQL deploys automatically during cluster provisioning as it's the primary database:</p> <pre><code># Full cluster provisioning (includes PostgreSQL)\n./provision-kubernetes.sh rancher-desktop\n</code></pre>"},{"location":"package-databases-postgresql/#manual-deployment","title":"Manual Deployment","text":"<pre><code># Deploy PostgreSQL with default settings\ncd provision-host/kubernetes/02-databases/\n./05-setup-postgres.sh rancher-desktop\n\n# Deploy to specific Kubernetes context\n./05-setup-postgres.sh multipass-microk8s\n./05-setup-postgres.sh azure-aks\n</code></pre>"},{"location":"package-databases-postgresql/#prerequisites","title":"Prerequisites","text":"<p>Before deploying PostgreSQL, ensure the required secrets are configured in <code>urbalurba-secrets</code>:</p> <ul> <li><code>PGPASSWORD</code>: PostgreSQL admin password</li> <li><code>PGHOST</code>: PostgreSQL service hostname (typically <code>postgresql.default.svc.cluster.local</code>)</li> </ul>"},{"location":"package-databases-postgresql/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"package-databases-postgresql/#custom-container-configuration","title":"Custom Container Configuration","text":"<p>PostgreSQL uses a custom container with pre-built extensions:</p> <pre><code># From manifests/042-database-postgresql-config.yaml\nimage:\n  registry: ghcr.io\n  repository: terchris/urbalurba-postgresql\n  tag: latest\n  pullPolicy: Always\n\n# Enable insecure images for custom container\nglobal:\n  security:\n    allowInsecureImages: true\n</code></pre>"},{"location":"package-databases-postgresql/#pre-built-extensions","title":"Pre-Built Extensions","text":"<p>The custom container includes 8 pre-built extensions automatically enabled:</p> <pre><code>-- AI and Vector Search Extensions\nCREATE EXTENSION IF NOT EXISTS vector;        -- Vector similarity search\n\n-- Geospatial Extensions\nCREATE EXTENSION IF NOT EXISTS postgis;       -- Geospatial data types\n\n-- Advanced Data Type Extensions\nCREATE EXTENSION IF NOT EXISTS hstore;        -- Key-value pairs\nCREATE EXTENSION IF NOT EXISTS ltree;         -- Hierarchical data\n\n-- Utility Extensions\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";   -- UUID generation\nCREATE EXTENSION IF NOT EXISTS pg_trgm;       -- Fuzzy text search\nCREATE EXTENSION IF NOT EXISTS btree_gin;     -- Additional indexing\nCREATE EXTENSION IF NOT EXISTS pgcrypto;      -- Cryptographic functions\n</code></pre>"},{"location":"package-databases-postgresql/#helm-configuration","title":"Helm Configuration","text":"<pre><code># Deployment command (from Ansible playbook)\nhelm install postgresql bitnami/postgresql \\\n  --namespace default \\\n  -f manifests/042-database-postgresql-config.yaml \\\n  --set auth.postgresPassword=\"$PGPASSWORD\"\n</code></pre>"},{"location":"package-databases-postgresql/#resource-configuration","title":"Resource Configuration","text":"<pre><code># Resource limits and requests\nresources:\n  requests:\n    memory: 240Mi\n    cpu: 250m\n  limits:\n    memory: 512Mi\n    cpu: 500m\n\n# Storage configuration\nprimary:\n  persistence:\n    enabled: true\n    size: 8Gi\n</code></pre>"},{"location":"package-databases-postgresql/#monitoring-verification","title":"\ud83d\udd0d Monitoring &amp; Verification","text":""},{"location":"package-databases-postgresql/#health-checks","title":"Health Checks","text":"<pre><code># Check pod status\nkubectl get pods -l app.kubernetes.io/name=postgresql\n\n# Check StatefulSet status\nkubectl get statefulset postgresql\n\n# Check service status\nkubectl get svc postgresql\n\n# View PostgreSQL logs\nkubectl logs -l app.kubernetes.io/name=postgresql\n</code></pre>"},{"location":"package-databases-postgresql/#database-connection-testing","title":"Database Connection Testing","text":"<pre><code># Test connection from within cluster\nkubectl run postgresql-client --image=postgres:16 --rm -it --restart=Never -- \\\n  psql postgresql://postgres:password@postgresql.default.svc.cluster.local:5432/postgres\n\n# Check if PostgreSQL is ready\nkubectl exec -it postgresql-0 -- pg_isready -U postgres\n\n# Test with authentication\nkubectl exec -it postgresql-0 -- psql -U postgres\n</code></pre>"},{"location":"package-databases-postgresql/#extension-verification","title":"Extension Verification","text":"<pre><code># List all installed extensions\nkubectl exec -it postgresql-0 -- psql -U postgres -c \\\n  \"SELECT extname, extversion FROM pg_extension ORDER BY extname;\"\n\n# Test vector extension (pgvector)\nkubectl exec -it postgresql-0 -- psql -U postgres -c \\\n  \"SELECT '[1,2,3]'::vector;\"\n\n# Test geospatial extension (PostGIS)\nkubectl exec -it postgresql-0 -- psql -U postgres -c \\\n  \"SELECT ST_Point(1, 2);\"\n</code></pre>"},{"location":"package-databases-postgresql/#automated-verification","title":"Automated Verification","text":"<p>The deployment includes comprehensive testing of all extensions:</p> <pre><code># Run verification playbook manually\ncd /mnt/urbalurbadisk/ansible\nansible-playbook playbooks/utility/u02-verify-postgres.yml\n</code></pre> <p>Verification Process: 1. Connects to PostgreSQL server using admin credentials 2. Tests all 8 pre-built extensions 3. Performs CRUD operations and data integrity checks 4. Validates vector search, geospatial, and NoSQL capabilities 5. Verifies performance and connection pooling</p>"},{"location":"package-databases-postgresql/#management-operations","title":"\ud83d\udee0\ufe0f Management Operations","text":""},{"location":"package-databases-postgresql/#database-administration","title":"Database Administration","text":"<pre><code># Access PostgreSQL shell\nkubectl exec -it postgresql-0 -- psql -U postgres\n\n# Create new database with extensions\nkubectl exec -it postgresql-0 -- psql -U postgres -c \"CREATE DATABASE myapp;\"\nkubectl exec -it postgresql-0 -- psql -U postgres -d myapp -c \"CREATE EXTENSION vector;\"\n\n# Show databases\nkubectl exec -it postgresql-0 -- psql -U postgres -c \"\\l\"\n\n# Show extensions in database\nkubectl exec -it postgresql-0 -- psql -U postgres -c \"\\dx\"\n</code></pre>"},{"location":"package-databases-postgresql/#advanced-operations","title":"Advanced Operations","text":"<pre><code># Create vector search table\nkubectl exec -it postgresql-0 -- psql -U postgres -c \"\nCREATE TABLE documents (\n  id SERIAL PRIMARY KEY,\n  content TEXT,\n  embedding vector(1536)\n);\nCREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops);\"\n\n# Create geospatial table\nkubectl exec -it postgresql-0 -- psql -U postgres -c \"\nCREATE TABLE locations (\n  id SERIAL PRIMARY KEY,\n  name TEXT,\n  coordinates GEOMETRY(POINT, 4326)\n);\nCREATE INDEX ON locations USING gist(coordinates);\"\n</code></pre>"},{"location":"package-databases-postgresql/#backup-operations","title":"Backup Operations","text":"<pre><code># Create database backup using pg_dump\nkubectl exec postgresql-0 -- pg_dump -U postgres myapp &gt; backup.sql\n\n# Restore from backup\nkubectl exec -i postgresql-0 -- psql -U postgres myapp &lt; backup.sql\n\n# Backup all databases\nkubectl exec postgresql-0 -- pg_dumpall -U postgres &gt; full-backup.sql\n</code></pre>"},{"location":"package-databases-postgresql/#service-removal","title":"Service Removal","text":"<pre><code># Remove PostgreSQL service (preserves data by default)\ncd provision-host/kubernetes/02-databases/not-in-use/\n./05-remove-postgres.sh rancher-desktop\n\n# Completely remove including data\nansible-playbook ansible/playbooks/040-remove-database-postgresql.yml \\\n  -e target_host=rancher-desktop -e remove_pvc=true\n</code></pre> <p>Removal Process: - Uninstalls PostgreSQL Helm release - Waits for pods to terminate - Optionally removes persistent volume claims - Preserves urbalurba-secrets and namespace structure - Provides data retention options and recovery instructions</p>"},{"location":"package-databases-postgresql/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-databases-postgresql/#common-issues","title":"Common Issues","text":"<p>Pod Won't Start: <pre><code># Check pod events and logs\nkubectl describe pod -l app.kubernetes.io/name=postgresql\nkubectl logs -l app.kubernetes.io/name=postgresql\n\n# Check custom image pull\nkubectl describe pod postgresql-0 | grep -A 5 \"Events:\"\n</code></pre></p> <p>Custom Image Issues: <pre><code># Verify custom image is accessible\nkubectl run test-pg --image=ghcr.io/terchris/urbalurba-postgresql:latest --rm -it -- \\\n  psql --version\n\n# Check image pull policy\nkubectl get pod postgresql-0 -o yaml | grep -A 3 \"image:\"\n</code></pre></p> <p>Extension Problems: <pre><code># Check if extensions are installed\nkubectl exec -it postgresql-0 -- psql -U postgres -c \"\\dx\"\n\n# Test specific extension\nkubectl exec -it postgresql-0 -- psql -U postgres -c \\\n  \"SELECT * FROM pg_available_extensions WHERE name='vector';\"\n\n# Reinstall extension if needed\nkubectl exec -it postgresql-0 -- psql -U postgres -c \\\n  \"DROP EXTENSION IF EXISTS vector; CREATE EXTENSION vector;\"\n</code></pre></p> <p>Connection Issues: <pre><code># Verify service endpoints\nkubectl describe svc postgresql\nkubectl get endpoints postgresql\n\n# Test DNS resolution\nkubectl run test-pod --image=busybox --rm -it -- \\\n  nslookup postgresql.default.svc.cluster.local\n\n# Check PostgreSQL configuration\nkubectl exec -it postgresql-0 -- psql -U postgres -c \"SHOW all;\"\n</code></pre></p> <p>Performance Issues: <pre><code># Check resource usage\nkubectl top pod postgresql-0\n\n# View PostgreSQL statistics\nkubectl exec -it postgresql-0 -- psql -U postgres -c \\\n  \"SELECT * FROM pg_stat_activity;\"\n\n# Check slow queries\nkubectl exec -it postgresql-0 -- psql -U postgres -c \\\n  \"SELECT query, calls, total_time FROM pg_stat_statements ORDER BY total_time DESC LIMIT 10;\"\n</code></pre></p>"},{"location":"package-databases-postgresql/#maintenance","title":"\ud83d\udccb Maintenance","text":""},{"location":"package-databases-postgresql/#regular-tasks","title":"Regular Tasks","text":"<ol> <li>Health Monitoring: Check pod and service status daily</li> <li>Extension Updates: Monitor custom container updates for new extension versions</li> <li>Backup Schedule: Implement regular database backups using pg_dump</li> <li>Performance Monitoring: Monitor query performance and resource usage</li> </ol>"},{"location":"package-databases-postgresql/#backup-procedures","title":"Backup Procedures","text":"<pre><code># Full backup of all databases\nkubectl exec postgresql-0 -- pg_dumpall -U postgres &gt; full-backup.sql\n\n# Backup specific database\nkubectl exec postgresql-0 -- pg_dump -U postgres myapp &gt; myapp-backup.sql\n\n# Backup with compression\nkubectl exec postgresql-0 -- pg_dump -U postgres -Fc myapp &gt; myapp-backup.dump\n\n# Schema-only backup\nkubectl exec postgresql-0 -- pg_dump -U postgres --schema-only myapp &gt; schema-backup.sql\n</code></pre>"},{"location":"package-databases-postgresql/#disaster-recovery","title":"Disaster Recovery","text":"<pre><code># Restore full backup\nkubectl exec -i postgresql-0 -- psql -U postgres &lt; full-backup.sql\n\n# Restore specific database\nkubectl exec -i postgresql-0 -- psql -U postgres myapp &lt; myapp-backup.sql\n\n# Restore from compressed backup\nkubectl exec -i postgresql-0 -- pg_restore -U postgres -d myapp myapp-backup.dump\n</code></pre>"},{"location":"package-databases-postgresql/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>package-databases-postgresql-container.md - Custom container details, extensions, and CI/CD</li> </ul> <p>\ud83d\udca1 Key Insight: PostgreSQL serves as the primary database service with advanced AI and geospatial capabilities through a custom container. For production AI applications requiring vector search, this setup provides enterprise-grade performance with pgvector and other modern extensions pre-configured and ready to use.</p>"},{"location":"package-databases-qdrant/","title":"Qdrant - Vector Database for AI/ML Applications","text":"<p>Key Features: Vector Search \u2022 Semantic Similarity \u2022 Embeddings Storage \u2022 API Authentication \u2022 Persistent Storage \u2022 High Performance \u2022 Python SDK</p> <p>File: <code>docs/package-databases-qdrant.md</code> Purpose: Complete guide to Qdrant vector database deployment and configuration in Urbalurba infrastructure Target Audience: AI/ML engineers, developers working with embeddings, data scientists building vector search applications Last Updated: September 24, 2025</p>"},{"location":"package-databases-qdrant/#overview","title":"\ud83d\udccb Overview","text":"<p>Qdrant serves as a high-performance vector database in the Urbalurba infrastructure, designed for AI/ML applications that require fast similarity search over high-dimensional vectors. It provides advanced vector search capabilities with semantic similarity matching for embeddings and machine learning workloads.</p> <p>Key Features: - Vector Search Engine: Optimized for high-dimensional vector similarity search - Embeddings Storage: Store and retrieve text, image, and other embeddings efficiently - API Authentication: Secure access with API key-based authentication - Helm-Based Deployment: Uses official Qdrant chart with production-ready configuration - Persistent Storage: Data and snapshots preserved across pod restarts - Secret Management: Integrates with urbalurba-secrets for secure API key management - Comprehensive Testing: Includes connectivity verification and API validation</p>"},{"location":"package-databases-qdrant/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"package-databases-qdrant/#deployment-components","title":"Deployment Components","text":"<pre><code>Qdrant Vector Database Stack:\n\u251c\u2500\u2500 Helm Release (qdrant/qdrant)\n\u251c\u2500\u2500 Deployment (qdrant:latest container)\n\u251c\u2500\u2500 ConfigMap (Qdrant configuration)\n\u251c\u2500\u2500 Service (ClusterIP on port 6333 HTTP, 6334 gRPC)\n\u251c\u2500\u2500 PersistentVolumeClaims (12GB data + 5GB snapshots)\n\u251c\u2500\u2500 urbalurba-secrets (API key authentication)\n\u2514\u2500\u2500 Pod (qdrant container with vector search engine)\n</code></pre>"},{"location":"package-databases-qdrant/#file-structure","title":"File Structure","text":"<pre><code>02-databases/\n\u251c\u2500\u2500 not-in-use/\n    \u251c\u2500\u2500 07-setup-qdrant.sh        # Main deployment script\n    \u2514\u2500\u2500 07-remove-qdrant.sh       # Removal script\n\nmanifests/\n\u2514\u2500\u2500 044-qdrant-config.yaml       # Qdrant Helm configuration\n\nansible/playbooks/\n\u251c\u2500\u2500 044-setup-qdrant.yml         # Main deployment logic\n\u2514\u2500\u2500 044-remove-qdrant.yml        # Removal logic\n</code></pre>"},{"location":"package-databases-qdrant/#deployment","title":"\ud83d\ude80 Deployment","text":""},{"location":"package-databases-qdrant/#manual-deployment","title":"Manual Deployment","text":"<p>Qdrant is available in the <code>02-databases/not-in-use</code> category and can be deployed manually:</p> <pre><code># Deploy Qdrant with default settings\ncd provision-host/kubernetes/02-databases/not-in-use/\n./07-setup-qdrant.sh rancher-desktop\n\n# Deploy to specific Kubernetes context\n./07-setup-qdrant.sh multipass-microk8s\n./07-setup-qdrant.sh azure-aks\n</code></pre>"},{"location":"package-databases-qdrant/#prerequisites","title":"Prerequisites","text":"<p>Before deploying Qdrant, ensure the required secrets are configured in <code>urbalurba-secrets</code>:</p> <ul> <li><code>QDRANT_API_KEY</code>: API key for secure access to Qdrant endpoints</li> </ul> <p>The API key should be a strong random string that will be used to authenticate all requests to the Qdrant API.</p>"},{"location":"package-databases-qdrant/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"package-databases-qdrant/#qdrant-configuration","title":"Qdrant Configuration","text":"<p>Qdrant uses the official Qdrant image with authentication and persistent storage enabled:</p> <pre><code># From manifests/044-qdrant-config.yaml\nservice:\n  type: ClusterIP\n\napiConfig:\n  enable: true\n\nauth:\n  apiKey: # Set by Ansible playbook from urbalurba-secrets\n\nstorage:\n  # Persistent storage for vector data\n  data:\n    size: 12Gi\n    storageClass: local-path\n  # Separate storage for snapshots\n  snapshots:\n    size: 5Gi\n    storageClass: local-path\n</code></pre>"},{"location":"package-databases-qdrant/#resource-configuration","title":"Resource Configuration","text":"<pre><code># Resource limits and requests\nresources:\n  requests:\n    memory: \"512Mi\"\n    cpu: \"200m\"\n  limits:\n    memory: \"1Gi\"\n    cpu: \"500m\"\n\n# Replica configuration\nreplicaCount: 1  # Single instance for development\n</code></pre>"},{"location":"package-databases-qdrant/#security-configuration","title":"Security Configuration","text":"<pre><code># API authentication\napiConfig:\n  enable: true\n\n# Environment variables from secrets\nenvFrom:\n  - secretRef:\n      name: urbalurba-secrets\n\n# Security context\nsecurityContext:\n  runAsNonRoot: true\n  runAsUser: 1000\n</code></pre>"},{"location":"package-databases-qdrant/#monitoring-verification","title":"\ud83d\udd0d Monitoring &amp; Verification","text":""},{"location":"package-databases-qdrant/#health-checks","title":"Health Checks","text":"<pre><code># Check pod status\nkubectl get pods -l app.kubernetes.io/name=qdrant\n\n# Check deployment status\nkubectl get deployment qdrant\n\n# Check service status\nkubectl get svc qdrant\n\n# View Qdrant logs\nkubectl logs -l app.kubernetes.io/name=qdrant\n</code></pre>"},{"location":"package-databases-qdrant/#service-verification","title":"Service Verification","text":"<pre><code># Check service endpoints\nkubectl get endpoints qdrant\n\n# Test DNS resolution\nkubectl run test-pod --image=busybox --rm -it -- \\\n  nslookup qdrant.default.svc.cluster.local\n\n# Check persistent volumes\nkubectl get pvc -l app.kubernetes.io/name=qdrant\n</code></pre>"},{"location":"package-databases-qdrant/#qdrant-api-testing","title":"Qdrant API Testing","text":"<pre><code># Port forward for external access\nkubectl port-forward svc/qdrant 6333:6333\n\n# Test API connectivity (replace YOUR_API_KEY)\ncurl -H \"api-key: YOUR_API_KEY\" http://localhost:6333/collections\n\n# Check cluster info\ncurl -H \"api-key: YOUR_API_KEY\" http://localhost:6333/cluster\n\n# Test health endpoint\ncurl http://localhost:6333/health\n</code></pre>"},{"location":"package-databases-qdrant/#automated-verification","title":"Automated Verification","text":"<p>The deployment includes comprehensive testing of Qdrant functionality with 15 automated verification tests:</p> <p>Complete Verification Process: 1. API Authentication: Validates API key from urbalurba-secrets 2. Collection Management: Tests collection creation and deletion 3. Vector Operations: Verifies vector insertion and retrieval 4. Similarity Search: Tests vector search functionality with exact matches 5. Data Persistence: Confirms vectors persist correctly in storage 6. API Endpoints: Tests all core Qdrant API endpoints 7. Cleanup Verification: Ensures proper data cleanup and collection removal</p> <p>Verification Tests Include: - \u2705 API connectivity and authentication (HTTP 200 responses) - \u2705 Collection creation and management (create, list, delete operations) - \u2705 Vector data insertion and persistence (points API with payload) - \u2705 Point retrieval by ID (validates data integrity) - \u2705 Vector similarity search functionality (semantic search with scoring) - \u2705 Data cleanup and collection deletion (proper resource management)</p> <p>The verification runs automatically when using <code>./07-setup-qdrant.sh</code> and provides comprehensive testing of all core Qdrant vector database capabilities.</p>"},{"location":"package-databases-qdrant/#management-operations","title":"\ud83d\udee0\ufe0f Management Operations","text":""},{"location":"package-databases-qdrant/#qdrant-administration","title":"Qdrant Administration","text":"<pre><code># Access Qdrant container\nkubectl exec -it $(kubectl get pod -l app.kubernetes.io/name=qdrant -o name) -- bash\n\n# Check Qdrant process\nkubectl exec -it $(kubectl get pod -l app.kubernetes.io/name=qdrant -o name) -- ps aux | grep qdrant\n\n# View configuration\nkubectl exec -it $(kubectl get pod -l app.kubernetes.io/name=qdrant -o name) -- cat /qdrant/config/production.yaml\n</code></pre>"},{"location":"package-databases-qdrant/#collection-management","title":"Collection Management","text":"<pre><code># List all collections\ncurl -H \"api-key: YOUR_API_KEY\" http://localhost:6333/collections\n\n# Create a collection\ncurl -X PUT -H \"api-key: YOUR_API_KEY\" -H \"Content-Type: application/json\" \\\n  --data '{\n    \"vectors\": {\n      \"size\": 384,\n      \"distance\": \"Cosine\"\n    }\n  }' \\\n  http://localhost:6333/collections/my_collection\n\n# Get collection info\ncurl -H \"api-key: YOUR_API_KEY\" http://localhost:6333/collections/my_collection\n\n# Delete a collection\ncurl -X DELETE -H \"api-key: YOUR_API_KEY\" http://localhost:6333/collections/my_collection\n</code></pre>"},{"location":"package-databases-qdrant/#vector-operations","title":"Vector Operations","text":"<pre><code># Insert vectors into collection\ncurl -X PUT -H \"api-key: YOUR_API_KEY\" -H \"Content-Type: application/json\" \\\n  --data '{\n    \"points\": [\n      {\n        \"id\": 1,\n        \"vector\": [0.1, 0.2, 0.3, /* ... 384 dimensions */],\n        \"payload\": {\"text\": \"sample document\", \"category\": \"example\"}\n      }\n    ]\n  }' \\\n  http://localhost:6333/collections/my_collection/points\n\n# Search similar vectors\ncurl -X POST -H \"api-key: YOUR_API_KEY\" -H \"Content-Type: application/json\" \\\n  --data '{\n    \"vector\": [0.1, 0.2, 0.3, /* query vector */],\n    \"limit\": 5,\n    \"with_payload\": true\n  }' \\\n  http://localhost:6333/collections/my_collection/points/search\n\n# Get specific point\ncurl -H \"api-key: YOUR_API_KEY\" http://localhost:6333/collections/my_collection/points/1\n</code></pre>"},{"location":"package-databases-qdrant/#service-removal","title":"Service Removal","text":"<pre><code># Remove Qdrant service (preserves data by default)\ncd provision-host/kubernetes/02-databases/not-in-use/\n./07-remove-qdrant.sh rancher-desktop\n\n# Complete removal including data\nkubectl delete pvc qdrant-data qdrant-snapshots\n</code></pre> <p>Removal Process: - Uninstalls Qdrant Helm release - Waits for pods to terminate - Preserves PVCs by default for data safety - Maintains urbalurba-secrets - Provides complete cleanup options if needed</p>"},{"location":"package-databases-qdrant/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-databases-qdrant/#common-issues","title":"Common Issues","text":"<p>Pod Won't Start: <pre><code># Check pod events and logs\nkubectl describe pod -l app.kubernetes.io/name=qdrant\nkubectl logs -l app.kubernetes.io/name=qdrant\n\n# Check persistent volume claims\nkubectl describe pvc qdrant-data qdrant-snapshots\n</code></pre></p> <p>Authentication Issues: <pre><code># Check API key in secrets\nkubectl get secret urbalurba-secrets -o jsonpath=\"{.data.QDRANT_API_KEY}\" | base64 -d\n\n# Test authentication\ncurl -v -H \"api-key: $(kubectl get secret urbalurba-secrets -o jsonpath='{.data.QDRANT_API_KEY}' | base64 -d)\" \\\n  http://localhost:6333/collections\n\n# Check environment variables in pod\nkubectl exec -it $(kubectl get pod -l app.kubernetes.io/name=qdrant -o name) -- env | grep -i qdrant\n</code></pre></p> <p>Connection Issues: <pre><code># Verify service endpoints\nkubectl describe svc qdrant\nkubectl get endpoints qdrant\n\n# Check port connectivity\nkubectl run test-pod --image=curlimages/curl --rm -it -- \\\n  curl -v http://qdrant:6333/health\n\n# Test gRPC port (6334)\nkubectl run test-pod --image=busybox --rm -it -- \\\n  nc -zv qdrant 6334\n</code></pre></p> <p>Performance Issues: <pre><code># Check resource usage\nkubectl top pod -l app.kubernetes.io/name=qdrant\n\n# Monitor disk usage\nkubectl exec -it $(kubectl get pod -l app.kubernetes.io/name=qdrant -o name) -- df -h\n\n# Check memory usage\nkubectl exec -it $(kubectl get pod -l app.kubernetes.io/name=qdrant -o name) -- free -h\n\n# View Qdrant metrics\ncurl -H \"api-key: YOUR_API_KEY\" http://localhost:6333/metrics\n</code></pre></p>"},{"location":"package-databases-qdrant/#maintenance","title":"\ud83d\udccb Maintenance","text":""},{"location":"package-databases-qdrant/#regular-tasks","title":"Regular Tasks","text":"<ol> <li>Health Monitoring: Check pod and service status daily</li> <li>Storage Monitoring: Monitor disk usage for vector data and snapshots</li> <li>Performance Monitoring: Track query response times and throughput</li> <li>Collection Monitoring: Review collection sizes and vector counts</li> </ol>"},{"location":"package-databases-qdrant/#backup-procedures","title":"Backup Procedures","text":"<pre><code># Create collection snapshot\ncurl -X POST -H \"api-key: YOUR_API_KEY\" -H \"Content-Type: application/json\" \\\n  --data '{\"collection_name\": \"my_collection\"}' \\\n  http://localhost:6333/collections/my_collection/snapshots\n\n# List snapshots\ncurl -H \"api-key: YOUR_API_KEY\" http://localhost:6333/collections/my_collection/snapshots\n\n# Download snapshot\ncurl -H \"api-key: YOUR_API_KEY\" \\\n  http://localhost:6333/collections/my_collection/snapshots/snapshot_name \\\n  -o collection_backup.snapshot\n\n# Copy data from PVC\nkubectl cp $(kubectl get pod -l app.kubernetes.io/name=qdrant -o name):/qdrant/storage \\\n  ./qdrant-backup-$(date +%Y%m%d)/\n</code></pre>"},{"location":"package-databases-qdrant/#disaster-recovery","title":"Disaster Recovery","text":"<pre><code># Restore from snapshot\ncurl -X PUT -H \"api-key: YOUR_API_KEY\" -H \"Content-Type: application/json\" \\\n  --data-binary @collection_backup.snapshot \\\n  http://localhost:6333/collections/my_collection/snapshots/restore\n\n# Restore from PVC backup (requires pod restart)\n# 1. Restore PVC data to persistent volume\n# 2. Delete pod to force restart with restored data\nkubectl delete pod -l app.kubernetes.io/name=qdrant\n</code></pre>"},{"location":"package-databases-qdrant/#use-cases","title":"\ud83d\ude80 Use Cases","text":""},{"location":"package-databases-qdrant/#semantic-search","title":"Semantic Search","text":"<pre><code># Create text embeddings collection\ncurl -X PUT -H \"api-key: YOUR_API_KEY\" -H \"Content-Type: application/json\" \\\n  --data '{\n    \"vectors\": {\n      \"size\": 384,\n      \"distance\": \"Cosine\"\n    }\n  }' \\\n  http://localhost:6333/collections/documents\n\n# Insert document embeddings\ncurl -X PUT -H \"api-key: YOUR_API_KEY\" -H \"Content-Type: application/json\" \\\n  --data '{\n    \"points\": [\n      {\n        \"id\": 1,\n        \"vector\": [/* sentence embedding from model */],\n        \"payload\": {\n          \"text\": \"Machine learning enables computers to learn without explicit programming\",\n          \"source\": \"article_1\",\n          \"timestamp\": \"2025-09-24\"\n        }\n      }\n    ]\n  }' \\\n  http://localhost:6333/collections/documents/points\n\n# Search similar documents\ncurl -X POST -H \"api-key: YOUR_API_KEY\" -H \"Content-Type: application/json\" \\\n  --data '{\n    \"vector\": [/* query embedding */],\n    \"limit\": 5,\n    \"with_payload\": true,\n    \"score_threshold\": 0.8\n  }' \\\n  http://localhost:6333/collections/documents/points/search\n</code></pre>"},{"location":"package-databases-qdrant/#recommendation-system","title":"Recommendation System","text":"<pre><code># Create user preferences collection\ncurl -X PUT -H \"api-key: YOUR_API_KEY\" -H \"Content-Type: application/json\" \\\n  --data '{\n    \"vectors\": {\n      \"size\": 256,\n      \"distance\": \"Dot\"\n    }\n  }' \\\n  http://localhost:6333/collections/user_preferences\n\n# Store user behavior vectors\ncurl -X PUT -H \"api-key: YOUR_API_KEY\" -H \"Content-Type: application/json\" \\\n  --data '{\n    \"points\": [\n      {\n        \"id\": 12345,\n        \"vector\": [/* user preference embedding */],\n        \"payload\": {\n          \"user_id\": 12345,\n          \"categories\": [\"tech\", \"ai\", \"programming\"],\n          \"last_active\": \"2025-09-24\"\n        }\n      }\n    ]\n  }' \\\n  http://localhost:6333/collections/user_preferences/points\n\n# Find similar users\ncurl -X POST -H \"api-key: YOUR_API_KEY\" -H \"Content-Type: application/json\" \\\n  --data '{\n    \"vector\": [/* target user vector */],\n    \"limit\": 10,\n    \"with_payload\": true\n  }' \\\n  http://localhost:6333/collections/user_preferences/points/search\n</code></pre>"},{"location":"package-databases-qdrant/#image-similarity-search","title":"Image Similarity Search","text":"<pre><code># Create image embeddings collection\ncurl -X PUT -H \"api-key: YOUR_API_KEY\" -H \"Content-Type: application/json\" \\\n  --data '{\n    \"vectors\": {\n      \"size\": 2048,\n      \"distance\": \"Cosine\"\n    }\n  }' \\\n  http://localhost:6333/collections/images\n\n# Store image feature vectors\ncurl -X PUT -H \"api-key: YOUR_API_KEY\" -H \"Content-Type: application/json\" \\\n  --data '{\n    \"points\": [\n      {\n        \"id\": \"img_001\",\n        \"vector\": [/* CNN feature extraction */],\n        \"payload\": {\n          \"filename\": \"photo1.jpg\",\n          \"tags\": [\"nature\", \"landscape\", \"mountains\"],\n          \"upload_date\": \"2025-09-24\"\n        }\n      }\n    ]\n  }' \\\n  http://localhost:6333/collections/images/points\n\n# Find visually similar images\ncurl -X POST -H \"api-key: YOUR_API_KEY\" -H \"Content-Type: application/json\" \\\n  --data '{\n    \"vector\": [/* query image features */],\n    \"limit\": 8,\n    \"with_payload\": true\n  }' \\\n  http://localhost:6333/collections/images/points/search\n</code></pre>"},{"location":"package-databases-qdrant/#multi-vector-collections","title":"Multi-Vector Collections","text":"<pre><code># Create collection with named vectors\ncurl -X PUT -H \"api-key: YOUR_API_KEY\" -H \"Content-Type: application/json\" \\\n  --data '{\n    \"vectors\": {\n      \"text\": {\"size\": 384, \"distance\": \"Cosine\"},\n      \"image\": {\"size\": 2048, \"distance\": \"Cosine\"}\n    }\n  }' \\\n  http://localhost:6333/collections/multimodal\n\n# Insert multi-modal data\ncurl -X PUT -H \"api-key: YOUR_API_KEY\" -H \"Content-Type: application/json\" \\\n  --data '{\n    \"points\": [\n      {\n        \"id\": 1,\n        \"vector\": {\n          \"text\": [/* text embedding */],\n          \"image\": [/* image embedding */]\n        },\n        \"payload\": {\n          \"title\": \"Product description\",\n          \"category\": \"electronics\"\n        }\n      }\n    ]\n  }' \\\n  http://localhost:6333/collections/multimodal/points\n\n# Search by text similarity\ncurl -X POST -H \"api-key: YOUR_API_KEY\" -H \"Content-Type: application/json\" \\\n  --data '{\n    \"vector\": {\n      \"name\": \"text\",\n      \"vector\": [/* query text embedding */]\n    },\n    \"limit\": 5\n  }' \\\n  http://localhost:6333/collections/multimodal/points/search\n</code></pre> <p>\ud83d\udca1 Key Insight: Qdrant provides essential vector database capabilities that enable advanced AI/ML applications with semantic search, recommendation systems, and similarity matching. Use Qdrant for storing embeddings, building recommendation engines, implementing semantic search, and creating AI applications that require fast similarity queries over high-dimensional data.</p>"},{"location":"package-databases-readme/","title":"Database Services - Complete Data Layer","text":"<p>File: <code>docs/package-databases-readme.md</code> Purpose: Overview of all database services in Urbalurba infrastructure Target Audience: Database administrators, developers, architects Last Updated: September 24, 2025</p>"},{"location":"package-databases-readme/#overview","title":"\ud83d\udccb Overview","text":"<p>Urbalurba infrastructure provides a comprehensive suite of database services supporting various data models and use cases. From traditional relational databases to NoSQL document stores, the platform offers production-ready database solutions with automated deployment, backup capabilities, and monitoring.</p> <p>Available Database Services: - PostgreSQL: Primary SQL database with AI/ML and geospatial extensions - MySQL: Alternative SQL database for traditional relational workloads - MongoDB: Document-based NoSQL database for flexible schemas - Qdrant: Vector database for AI/ML embeddings and similarity search</p>"},{"location":"package-databases-readme/#database-services","title":"\ud83d\uddc4\ufe0f Database Services","text":""},{"location":"package-databases-readme/#postgresql-primary-database-service","title":"PostgreSQL - Primary Database Service \ud83e\udd47","text":"<p>Status: Active | Port: 5432 | Type: SQL Relational</p> <p>Pre-Built Extensions: Vector Search (pgvector) \u2022 Geospatial (PostGIS) \u2022 Key-Value (hstore) \u2022 Hierarchical (ltree) \u2022 UUID Generation \u2022 Fuzzy Search \u2022 Advanced Indexing \u2022 Cryptography</p> <p>PostgreSQL serves as the primary database service with enterprise-grade features and AI/ML capabilities. Uses a custom container with 8 pre-built extensions for advanced data processing, vector search, and geospatial operations.</p> <p>Key Features: - AI-Ready: pgvector for embeddings and vector similarity search - Geospatial: PostGIS for location-based applications - Custom Container: Pre-built extensions for immediate use - Production-Ready: Bitnami Helm chart with persistent storage</p> <p>\ud83d\udcda Complete Documentation \u2192 \ud83d\udc33 Custom Container Details \u2192</p>"},{"location":"package-databases-readme/#mysql-alternative-sql-database","title":"MySQL - Alternative SQL Database \ud83d\udd04","text":"<p>Status: Optional | Port: 3306 | Type: SQL Relational</p> <p>Traditional MySQL database service for applications requiring MySQL-specific features or legacy compatibility. Provides reliable relational database capabilities with standard MySQL functionality.</p> <p>Key Features: - Standard MySQL: Full MySQL 8.0 compatibility - Helm Deployment: Bitnami MySQL chart - Persistent Storage: 8GB storage with automatic backups - Easy Migration: Standard MySQL tools and procedures</p> <p>\ud83d\udcda Complete Documentation \u2192</p>"},{"location":"package-databases-readme/#mongodb-nosql-document-database","title":"MongoDB - NoSQL Document Database \ud83d\udcc4","text":"<p>Status: Optional (not-in-use) | Port: 27017 | Type: NoSQL Document</p> <p>Document-based NoSQL database for applications requiring flexible schemas and JSON-like data structures. Pre-configured for Gravitee API Management platform but suitable for any document-based applications.</p> <p>Key Features: - Document Storage: JSON-like documents with flexible schemas - MongoDB 8.0: Latest version with ARM64 compatibility - User Management: Automated user provisioning and permissions - Gravitee Integration: Pre-configured for API management platform</p> <p>\ud83d\udcda Complete Documentation \u2192</p>"},{"location":"package-databases-readme/#qdrant-vector-database-for-aiml","title":"Qdrant - Vector Database for AI/ML \ud83c\udfaf","text":"<p>Status: Optional (not-in-use) | Port: 6333 HTTP, 6334 gRPC | Type: Vector Database</p> <p>High-performance vector database designed for AI/ML applications requiring fast similarity search over high-dimensional vectors. Optimized for embeddings storage and semantic search capabilities.</p> <p>Key Features: - Vector Search: Optimized similarity search for embeddings - AI/ML Integration: Perfect for recommendation systems and semantic search - API Authentication: Secure access with API key-based authentication - Persistent Storage: Separate storage for vector data and snapshots</p> <p>\ud83d\udcda Complete Documentation \u2192</p>"},{"location":"package-databases-readme/#deployment-architecture","title":"\ud83c\udfd7\ufe0f Deployment Architecture","text":""},{"location":"package-databases-readme/#service-activation","title":"Service Activation","text":"<pre><code>Database Deployment Status:\n\u251c\u2500\u2500 PostgreSQL (ACTIVE) - Primary database service\n\u251c\u2500\u2500 MySQL (OPTIONAL) - Available for activation\n\u251c\u2500\u2500 MongoDB (INACTIVE) - Located in not-in-use/ folder\n\u2514\u2500\u2500 Qdrant (INACTIVE) - Located in not-in-use/ folder\n</code></pre>"},{"location":"package-databases-readme/#storage-persistence","title":"Storage &amp; Persistence","text":"<p>All database services use Kubernetes PersistentVolumeClaims for data persistence: - PostgreSQL: 8GB persistent storage - MySQL: 8GB persistent storage - MongoDB: 8GB persistent storage - Qdrant: 12GB data storage + 5GB snapshots</p>"},{"location":"package-databases-readme/#secret-management","title":"Secret Management","text":"<p>Database authentication managed through <code>urbalurba-secrets</code>: <pre><code>Database Credentials:\n\u251c\u2500\u2500 PGPASSWORD / PGHOST (PostgreSQL)\n\u251c\u2500\u2500 MYSQL_ROOT_PASSWORD / MYSQL_PASSWORD (MySQL)\n\u251c\u2500\u2500 MONGODB_ROOT_PASSWORD / GRAVITEE_MONGODB_* (MongoDB)\n\u2514\u2500\u2500 QDRANT_API_KEY (Qdrant)\n</code></pre></p>"},{"location":"package-databases-readme/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"package-databases-readme/#deploy-primary-database-postgresql","title":"Deploy Primary Database (PostgreSQL)","text":"<pre><code># Automatic deployment during cluster provisioning\n./provision-kubernetes.sh rancher-desktop\n\n# Manual deployment\ncd provision-host/kubernetes/02-databases/\n./05-setup-postgres.sh rancher-desktop\n</code></pre>"},{"location":"package-databases-readme/#activate-optional-databases","title":"Activate Optional Databases","text":"<pre><code># Activate MySQL (move from not-in-use if needed)\ncd provision-host/kubernetes/02-databases/\n./06-setup-mysql.sh rancher-desktop\n\n# Activate MongoDB\nmv not-in-use/04-setup-mongodb.sh ./\n./04-setup-mongodb.sh rancher-desktop\n\n# Activate Qdrant Vector Database\ncd provision-host/kubernetes/02-databases/not-in-use/\n./07-setup-qdrant.sh rancher-desktop\n</code></pre>"},{"location":"package-databases-readme/#database-selection-guide","title":"\ud83d\udd0d Database Selection Guide","text":""},{"location":"package-databases-readme/#when-to-use-postgresql","title":"When to Use PostgreSQL \u2705","text":"<ul> <li>Primary choice for new applications</li> <li>AI/ML applications requiring vector search</li> <li>Geospatial applications with location data</li> <li>Applications needing advanced SQL features</li> <li>Need for JSONB, arrays, or custom data types</li> </ul>"},{"location":"package-databases-readme/#when-to-use-mysql","title":"When to Use MySQL \ud83d\udd04","text":"<ul> <li>Legacy applications built for MySQL</li> <li>Applications requiring MySQL-specific features</li> <li>Teams with existing MySQL expertise</li> <li>Applications using MySQL-only tools</li> </ul>"},{"location":"package-databases-readme/#when-to-use-mongodb","title":"When to Use MongoDB \ud83d\udcc4","text":"<ul> <li>Applications with frequently changing schemas</li> <li>Rapid prototyping and development</li> <li>Applications storing JSON-like documents</li> <li>Gravitee API Management platform</li> <li>Applications requiring horizontal scaling</li> </ul>"},{"location":"package-databases-readme/#when-to-use-qdrant","title":"When to Use Qdrant \ud83c\udfaf","text":"<ul> <li>AI/ML applications with embeddings and vector search</li> <li>Semantic search and similarity matching</li> <li>Recommendation systems and content discovery</li> <li>Image similarity and visual search applications</li> <li>Building RAG (Retrieval-Augmented Generation) systems</li> </ul>"},{"location":"package-databases-readme/#management-operations","title":"\ud83d\udee0\ufe0f Management Operations","text":""},{"location":"package-databases-readme/#common-operations","title":"Common Operations","text":"<pre><code># Check database status\nkubectl get pods -l app.kubernetes.io/component=database\nkubectl get pods -l app.kubernetes.io/name=qdrant\n\n# View database logs\nkubectl logs -l app.kubernetes.io/name=postgresql\nkubectl logs -l app.kubernetes.io/name=mysql\nkubectl logs -l app=mongodb\nkubectl logs -l app.kubernetes.io/name=qdrant\n\n# Connect to databases\nkubectl exec -it postgresql-0 -- psql -U postgres\nkubectl exec -it mysql-0 -- mysql -u root -p\nkubectl exec -it mongodb-0 -- mongosh --username root --password\n# Qdrant uses HTTP API - test with: curl -H \"api-key: KEY\" http://localhost:6333/collections\n</code></pre>"},{"location":"package-databases-readme/#backup-procedures","title":"Backup Procedures","text":"<pre><code># PostgreSQL backup\nkubectl exec postgresql-0 -- pg_dumpall -U postgres &gt; backup.sql\n\n# MySQL backup\nkubectl exec mysql-0 -- mysqldump -u root -p --all-databases &gt; backup.sql\n\n# MongoDB backup\nkubectl exec mongodb-0 -- mongodump --authenticationDatabase admin --username root --password\n\n# Qdrant backup (snapshots)\ncurl -X POST -H \"api-key: YOUR_API_KEY\" \\\n  http://localhost:6333/collections/COLLECTION_NAME/snapshots\n</code></pre>"},{"location":"package-databases-readme/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-databases-readme/#common-issues","title":"Common Issues","text":"<ul> <li>Pod Won't Start: Check PVC binding and resource limits</li> <li>Connection Refused: Verify service endpoints and DNS resolution</li> <li>Authentication Failed: Check urbalurba-secrets configuration</li> <li>Storage Issues: Verify PVC status and node storage capacity</li> </ul>"},{"location":"package-databases-readme/#diagnostic-commands","title":"Diagnostic Commands","text":"<pre><code># Check service endpoints\nkubectl get endpoints postgresql mysql mongodb qdrant\n\n# Verify storage\nkubectl get pvc -l app.kubernetes.io/component=database\nkubectl get pvc -l app.kubernetes.io/name=qdrant\n\n# Test connectivity\nkubectl run test-pod --image=postgres:16 --rm -it -- \\\n  psql postgresql://postgres:password@postgresql:5432/postgres\n\n# Test Qdrant connectivity\nkubectl run test-pod --image=curlimages/curl --rm -it -- \\\n  curl -H \"api-key: YOUR_API_KEY\" http://qdrant:6333/collections\n</code></pre>"},{"location":"package-databases-readme/#maintenance","title":"\ud83d\udccb Maintenance","text":""},{"location":"package-databases-readme/#regular-tasks","title":"Regular Tasks","text":"<ol> <li>Monitor Storage: Check PVC usage and growth trends</li> <li>Backup Schedule: Implement automated backup procedures</li> <li>Security Updates: Update container images regularly</li> <li>Performance Monitoring: Monitor query performance and resource usage</li> <li>Extension Updates: Monitor PostgreSQL custom container updates</li> </ol>"},{"location":"package-databases-readme/#service-removal","title":"Service Removal","text":"<pre><code># Remove services (preserves data by default)\ncd provision-host/kubernetes/02-databases/not-in-use/\n./05-remove-postgres.sh rancher-desktop\n./06-remove-mysql.sh rancher-desktop\n./04-remove-mongodb.sh rancher-desktop\n./07-remove-qdrant.sh rancher-desktop\n</code></pre>"},{"location":"package-databases-readme/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>PostgreSQL Documentation - Primary database service</li> <li>PostgreSQL Container - Custom container details</li> <li>MySQL Documentation - Alternative SQL database</li> <li>MongoDB Documentation - NoSQL document database</li> <li>Qdrant Documentation - Vector database for AI/ML applications</li> <li>Secrets Management - Database credential configuration</li> <li>Troubleshooting Guide - Database troubleshooting procedures</li> </ul> <p>\ud83d\udca1 Key Insight: The database layer provides comprehensive data storage solutions with PostgreSQL as the primary choice due to its advanced features, AI/ML capabilities, and extensive extension ecosystem. MySQL, MongoDB, and Qdrant serve as specialized alternatives for specific use cases - traditional SQL, document storage, and vector search respectively.</p>"},{"location":"package-datascience-jupyterhub/","title":"JupyterHub - Interactive Notebook Environment for Data Science","text":"<p>Key Features: Interactive Notebooks \u2022 PySpark Integration \u2022 Web-based Interface \u2022 Multi-user Support \u2022 Kubernetes-native \u2022 Secret Authentication \u2022 Distributed Computing</p> <p>File: <code>docs/package-datascience-jupyterhub.md</code> Purpose: Complete guide to JupyterHub deployment and configuration for data science workflows in Urbalurba infrastructure Target Audience: Data scientists, ML engineers, developers working with notebooks and distributed computing Last Updated: September 23, 2025</p>"},{"location":"package-datascience-jupyterhub/#overview","title":"\ud83d\udccb Overview","text":"<p>JupyterHub serves as the interactive notebook environment in the Urbalurba infrastructure, providing a Databricks replacement for data science and machine learning workflows. It offers web-based Jupyter notebooks with PySpark integration for distributed data processing.</p> <p>Key Features: - Interactive Notebooks: Web-based Jupyter interface with Python, Scala, and SQL support - PySpark Integration: Built-in Apache Spark connectivity for distributed data processing - Multi-user Environment: Secure isolated user sessions with persistent storage - Helm-Based Deployment: Uses official JupyterHub chart with custom PySpark configuration - Secret Management: Integrates with urbalurba-secrets for secure authentication - Automated Testing: Includes comprehensive readiness and connectivity verification - Databricks Replacement: Phase 2 of complete Databricks alternative solution</p>"},{"location":"package-datascience-jupyterhub/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"package-datascience-jupyterhub/#deployment-components","title":"Deployment Components","text":"<pre><code>JupyterHub Service Stack:\n\u251c\u2500\u2500 Helm Release (jupyterhub/jupyterhub)\n\u251c\u2500\u2500 Hub Pod (quay.io/jupyterhub/k8s-hub:4.2.0)\n\u251c\u2500\u2500 Proxy Pod (configurable-http-proxy)\n\u251c\u2500\u2500 User Scheduler Pods (2 replicas for HA)\n\u251c\u2500\u2500 Continuous Image Puller (pre-loads notebook images)\n\u251c\u2500\u2500 Service (proxy-public on port 80)\n\u251c\u2500\u2500 PersistentVolumeClaim (user data storage)\n\u251c\u2500\u2500 urbalurba-secrets (authentication credentials)\n\u2514\u2500\u2500 Singleuser Notebook Pods (jupyter/pyspark-notebook:spark-3.5.0)\n</code></pre>"},{"location":"package-datascience-jupyterhub/#file-structure","title":"File Structure","text":"<pre><code>10-datascience/\n\u251c\u2500\u2500 not-in-use/\n    \u251c\u2500\u2500 05-setup-jupyterhub.sh         # Main deployment script\n    \u2514\u2500\u2500 05-remove-jupyterhub.sh        # Removal script\n\nmanifests/\n\u251c\u2500\u2500 310-jupyterhub-config.yaml        # JupyterHub Helm configuration\n\u2514\u2500\u2500 311-jupyterhub-ingress.yaml       # Ingress routing configuration\n\nansible/playbooks/\n\u251c\u2500\u2500 350-setup-jupyterhub.yml          # Main deployment logic\n\u2514\u2500\u2500 350-remove-jupyterhub.yml         # Removal logic\n</code></pre>"},{"location":"package-datascience-jupyterhub/#databricks-replacement-architecture","title":"Databricks Replacement Architecture","text":"<pre><code>Phase 1: Processing Engine\n\u251c\u2500\u2500 Spark Kubernetes Operator (330-setup-spark.yml)\n\u2514\u2500\u2500 Distributed job execution and resource management\n\nPhase 2: Notebook Interface \u2190 THIS COMPONENT\n\u251c\u2500\u2500 JupyterHub (350-setup-jupyterhub.yml)\n\u251c\u2500\u2500 Web-based notebook environment\n\u251c\u2500\u2500 PySpark integration with Phase 1\n\u2514\u2500\u2500 Multi-user collaborative workspace\n</code></pre>"},{"location":"package-datascience-jupyterhub/#deployment","title":"\ud83d\ude80 Deployment","text":""},{"location":"package-datascience-jupyterhub/#manual-deployment","title":"Manual Deployment","text":"<p>JupyterHub is currently in the <code>10-datascience/not-in-use</code> category and can be deployed manually:</p> <pre><code># Deploy JupyterHub with default settings\ncd provision-host/kubernetes/10-datascience/not-in-use/\n./05-setup-jupyterhub.sh rancher-desktop\n\n# Deploy to specific Kubernetes context\n./05-setup-jupyterhub.sh multipass-microk8s\n./05-setup-jupyterhub.sh azure-aks\n</code></pre>"},{"location":"package-datascience-jupyterhub/#prerequisites","title":"Prerequisites","text":"<p>Before deploying JupyterHub, ensure the required secrets are configured in <code>urbalurba-secrets</code>:</p> <ul> <li><code>JUPYTERHUB_AUTH_PASSWORD</code>: JupyterHub admin authentication password</li> </ul> <p>Secrets Generation (following rules-secrets-management.md): <pre><code># 1. Update user config with base template\ncd /mnt/urbalurbadisk/topsecret\ncp secrets-templates/00-master-secrets.yml.template secrets-config/00-master-secrets.yml.template\n\n# 2. Generate and apply secrets\n./create-kubernetes-secrets.sh\nkubectl apply -f kubernetes/kubernetes-secrets.yml\n</code></pre></p>"},{"location":"package-datascience-jupyterhub/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"package-datascience-jupyterhub/#jupyterhub-configuration","title":"JupyterHub Configuration","text":"<p>JupyterHub uses the official JupyterHub Helm chart with PySpark-enabled notebook images:</p> <pre><code># From manifests/310-jupyterhub-config.yaml\nhub:\n  extraEnv:\n    JUPYTERHUB_AUTH_PASSWORD:\n      valueFrom:\n        secretKeyRef:\n          name: urbalurba-secrets\n          key: JUPYTERHUB_AUTH_PASSWORD\n\n  extraConfig:\n    dummy-auth-config: |\n      import os\n      c.DummyAuthenticator.password = os.environ.get('JUPYTERHUB_AUTH_PASSWORD', 'fallback-password')\n\n  config:\n    JupyterHub:\n      authenticator_class: \"dummy\"\n\nsingleuser:\n  image:\n    name: jupyter/pyspark-notebook\n    tag: \"spark-3.5.0\"\n</code></pre>"},{"location":"package-datascience-jupyterhub/#pyspark-integration","title":"PySpark Integration","text":"<pre><code># Notebook container configuration\nsingleuser:\n  lifecycleHooks:\n    postStart:\n      exec:\n        command:\n          - \"bash\"\n          - \"-c\"\n          - |\n            pip install --user pyspark==3.5.0 findspark plotly seaborn scikit-learn\n            echo \"\u2705 PySpark installed successfully\"\n\n  extraEnv:\n    PYSPARK_PYTHON: /opt/conda/bin/python\n    PYSPARK_DRIVER_PYTHON: /opt/conda/bin/python\n</code></pre>"},{"location":"package-datascience-jupyterhub/#helm-configuration","title":"Helm Configuration","text":"<pre><code># Deployment command (from Ansible playbook)\nhelm upgrade --install jupyterhub jupyterhub/jupyterhub \\\n  -f manifests/310-jupyterhub-config.yaml \\\n  --namespace jupyterhub \\\n  --timeout 300s\n</code></pre>"},{"location":"package-datascience-jupyterhub/#resource-configuration","title":"Resource Configuration","text":"<pre><code># Hub resources\nhub:\n  resources:\n    requests:\n      cpu: \"200m\"\n      memory: \"512Mi\"\n    limits:\n      cpu: \"2\"\n      memory: \"1Gi\"\n\n# Proxy resources\nproxy:\n  chp:\n    resources:\n      requests:\n        cpu: \"200m\"\n        memory: \"512Mi\"\n      limits:\n        cpu: \"1\"\n        memory: \"1Gi\"\n\n# User notebook resources\nsingleuser:\n  cpu:\n    limit: 2\n    guarantee: 0.1\n  memory:\n    limit: \"2G\"\n    guarantee: \"512M\"\n</code></pre>"},{"location":"package-datascience-jupyterhub/#storage-configuration","title":"Storage Configuration","text":"<pre><code># User persistent storage\nsingleuser:\n  storage:\n    dynamic:\n      storageClass: local-path\n    capacity: 10Gi\n    homeMountPath: /home/jovyan\n</code></pre>"},{"location":"package-datascience-jupyterhub/#authentication-configuration","title":"Authentication Configuration","text":"<pre><code># DummyAuthenticator for development\n# Username: admin (or any username)\n# Password: from JUPYTERHUB_AUTH_PASSWORD secret\nhub:\n  config:\n    JupyterHub:\n      authenticator_class: \"dummy\"\n\n    DummyAuthenticator:\n      # Password set via extraConfig from environment variable\n      password: # Loaded from urbalurba-secrets\n</code></pre>"},{"location":"package-datascience-jupyterhub/#monitoring-verification","title":"\ud83d\udd0d Monitoring &amp; Verification","text":""},{"location":"package-datascience-jupyterhub/#health-checks","title":"Health Checks","text":"<pre><code># Check pod status\nkubectl get pods -n jupyterhub\n\n# Check hub pod specifically\nkubectl get pods -n jupyterhub -l component=hub\n\n# Check proxy pod\nkubectl get pods -n jupyterhub -l component=proxy\n\n# Check user scheduler\nkubectl get pods -n jupyterhub -l component=user-scheduler\n\n# View JupyterHub logs\nkubectl logs -n jupyterhub -l component=hub\nkubectl logs -n jupyterhub -l component=proxy\n</code></pre>"},{"location":"package-datascience-jupyterhub/#service-verification","title":"Service Verification","text":"<pre><code># Check JupyterHub service\nkubectl get svc -n jupyterhub proxy-public\n\n# Check service endpoints\nkubectl get endpoints -n jupyterhub proxy-public\n\n# Check ingress status\nkubectl get ingress -n jupyterhub jupyterhub\n\n# Verify ingress configuration\nkubectl describe ingress -n jupyterhub jupyterhub\n</code></pre>"},{"location":"package-datascience-jupyterhub/#jupyterhub-access-testing","title":"JupyterHub Access Testing","text":"<pre><code># Test cluster-internal connectivity\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never -n jupyterhub -- \\\n  curl -s -w \"HTTP_CODE:%{http_code}\" http://proxy-public:80\n\n# Test authentication endpoint\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never -n jupyterhub -- \\\n  curl -s -w \"HTTP_CODE:%{http_code}\" http://proxy-public:80/hub/login\n</code></pre>"},{"location":"package-datascience-jupyterhub/#web-interface-access","title":"Web Interface Access","text":"<p>Primary Method - Cluster Ingress (Recommended): <pre><code># Access via cluster ingress (no port-forward needed)\n# URL: http://jupyterhub.localhost\n# Username: admin (or any username with DummyAuthenticator)\n# Password: SecretPassword2 (from urbalurba-secrets)\n</code></pre></p> <p>Alternative Method - Port Forward: <pre><code># Port forward for local access\nkubectl port-forward -n jupyterhub svc/proxy-public 8888:80\n\n# Access via browser\n# URL: http://localhost:8888\n# Username: admin\n# Password: SecretPassword2\n</code></pre></p> <p>External Access (when configured): - URL: <code>https://jupyterhub.urbalurba.no</code> (via Cloudflare tunnel) - Same credentials as internal access</p>"},{"location":"package-datascience-jupyterhub/#automated-verification","title":"Automated Verification","text":"<p>The deployment includes comprehensive testing of JupyterHub functionality:</p> <p>Verification Process: 1. Namespace and secrets creation: Ensures proper environment setup 2. Helm repository management: Adds and updates JupyterHub chart repository 3. Two-stage pod readiness: Waits for hub and proxy pods to be Running and Ready 4. Service connectivity: Verifies internal cluster communication 5. Ingress configuration: Applies and validates routing rules 6. Authentication validation: Confirms secret-based password authentication works</p>"},{"location":"package-datascience-jupyterhub/#management-operations","title":"\ud83d\udee0\ufe0f Management Operations","text":""},{"location":"package-datascience-jupyterhub/#jupyterhub-administration","title":"JupyterHub Administration","text":"<pre><code># Access JupyterHub admin panel\n# Navigate to: http://jupyterhub.localhost/hub/admin\n# Login with admin credentials\n\n# Check hub configuration\nkubectl exec -n jupyterhub deployment/hub -- jupyterhub --help-all\n\n# Check active users\nkubectl exec -n jupyterhub deployment/hub -- \\\n  python3 -c \"\nimport requests\nr = requests.get('http://localhost:8081/hub/api/users')\nprint(r.json())\n\"\n\n# Restart hub (if needed)\nkubectl rollout restart -n jupyterhub deployment/hub\n</code></pre>"},{"location":"package-datascience-jupyterhub/#user-management","title":"User Management","text":"<pre><code># List active user pods\nkubectl get pods -n jupyterhub -l component=singleuser-server\n\n# Check user session status\nkubectl exec -n jupyterhub deployment/hub -- \\\n  python3 -c \"\nimport requests\nr = requests.get('http://localhost:8081/hub/api/users')\nfor user in r.json():\n    print(f'User: {user[\\\"name\\\"]}, Server: {user.get(\\\"server\\\", \\\"Not running\\\")}')\n\"\n\n# Stop user server\nkubectl exec -n jupyterhub deployment/hub -- \\\n  python3 -c \"\nimport requests\nrequests.delete('http://localhost:8081/hub/api/users/username/server')\n\"\n\n# Clean up terminated user pods\nkubectl delete pods -n jupyterhub -l component=singleuser-server --field-selector=status.phase=Succeeded\n</code></pre>"},{"location":"package-datascience-jupyterhub/#notebook-environment-management","title":"Notebook Environment Management","text":"<pre><code># Check available notebook images\nkubectl get pods -n jupyterhub continuous-image-puller -o yaml | grep image:\n\n# Update notebook image\n# Edit manifests/310-jupyterhub-config.yaml:\n# singleuser.image.name: jupyter/pyspark-notebook\n# singleuser.image.tag: \"new-version\"\n\n# Apply configuration update\nhelm upgrade jupyterhub jupyterhub/jupyterhub \\\n  -f manifests/310-jupyterhub-config.yaml \\\n  -n jupyterhub\n\n# Force pull new images\nkubectl delete pods -n jupyterhub -l app=jupyterhub,component=continuous-image-puller\n</code></pre>"},{"location":"package-datascience-jupyterhub/#pyspark-integration-management","title":"PySpark Integration Management","text":"<pre><code># Check PySpark installation in user pod\nkubectl exec -n jupyterhub &lt;user-pod-name&gt; -- python3 -c \"import pyspark; print(pyspark.__version__)\"\n\n# Test Spark session creation\nkubectl exec -n jupyterhub &lt;user-pod-name&gt; -- python3 -c \"\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('test').getOrCreate()\nprint('\u2705 Spark session created successfully')\nspark.stop()\n\"\n\n# Check available Python packages\nkubectl exec -n jupyterhub &lt;user-pod-name&gt; -- pip list | grep -E 'pyspark|findspark|plotly|seaborn|scikit-learn'\n</code></pre>"},{"location":"package-datascience-jupyterhub/#service-removal","title":"Service Removal","text":"<pre><code># Remove JupyterHub service (preserves user data by default)\ncd provision-host/kubernetes/10-datascience/not-in-use/\n./05-remove-jupyterhub.sh rancher-desktop\n\n# Completely remove including user data\nansible-playbook ansible/playbooks/350-remove-jupyterhub.yml \\\n  -e target_host=rancher-desktop\n</code></pre> <p>Removal Process: - Terminates all active user sessions - Uninstalls JupyterHub Helm release - Waits for all pods to terminate - Removes ingress configuration - Preserves urbalurba-secrets and namespace structure - Provides user data retention options and recovery instructions</p>"},{"location":"package-datascience-jupyterhub/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-datascience-jupyterhub/#common-issues","title":"Common Issues","text":"<p>Hub Pod Won't Start: <pre><code># Check pod events and logs\nkubectl describe pod -n jupyterhub -l component=hub\nkubectl logs -n jupyterhub -l component=hub\n\n# Check secret availability\nkubectl get secret -n jupyterhub urbalurba-secrets\nkubectl get secret -n jupyterhub urbalurba-secrets -o jsonpath='{.data.JUPYTERHUB_AUTH_PASSWORD}' | base64 -d\n\n# Check hub configuration\nkubectl get configmap -n jupyterhub hub -o yaml\n</code></pre></p> <p>Authentication Issues: <pre><code># Check JupyterHub credentials in secrets\nkubectl get secret -n jupyterhub urbalurba-secrets -o jsonpath=\"{.data.JUPYTERHUB_AUTH_PASSWORD}\" | base64 -d\n\n# Test authentication via hub API\nkubectl exec -n jupyterhub deployment/hub -- \\\n  curl -X POST http://localhost:8081/hub/login \\\n  -d \"username=admin&amp;password=SecretPassword2\"\n\n# Check authenticator configuration\nkubectl logs -n jupyterhub -l component=hub | grep -i auth\n</code></pre></p> <p>User Server Startup Issues: <pre><code># Check user pod status\nkubectl get pods -n jupyterhub -l component=singleuser-server\nkubectl describe pod -n jupyterhub &lt;user-pod-name&gt;\n\n# Check user server logs\nkubectl logs -n jupyterhub &lt;user-pod-name&gt;\n\n# Check image pull status\nkubectl get events -n jupyterhub --field-selector involvedObject.kind=Pod\n\n# Check storage availability\nkubectl get pvc -n jupyterhub\nkubectl describe pvc -n jupyterhub &lt;user-pvc-name&gt;\n</code></pre></p> <p>Ingress and Connectivity Issues: <pre><code># Verify ingress configuration\nkubectl describe ingress -n jupyterhub jupyterhub\nkubectl get ingress -n jupyterhub jupyterhub -o yaml\n\n# Test service connectivity\nkubectl run test-pod --image=busybox --rm -it -n jupyterhub -- \\\n  wget -qO- http://proxy-public:80\n\n# Check Traefik ingress controller\nkubectl get pods -n kube-system -l app.kubernetes.io/name=traefik\n\n# Test DNS resolution\nkubectl run test-pod --image=busybox --rm -it -- \\\n  nslookup proxy-public.jupyterhub.svc.cluster.local\n</code></pre></p> <p>PySpark Integration Issues: <pre><code># Check PySpark installation\nkubectl exec -n jupyterhub &lt;user-pod-name&gt; -- python3 -c \"\ntry:\n    import pyspark\n    print(f'\u2705 PySpark {pyspark.__version__} installed')\nexcept ImportError as e:\n    print(f'\u274c PySpark not available: {e}')\n\"\n\n# Check Spark driver configuration\nkubectl exec -n jupyterhub &lt;user-pod-name&gt; -- python3 -c \"\nimport os\nprint(f'PYSPARK_PYTHON: {os.environ.get(\\\"PYSPARK_PYTHON\\\", \\\"Not set\\\")}')\nprint(f'PYSPARK_DRIVER_PYTHON: {os.environ.get(\\\"PYSPARK_DRIVER_PYTHON\\\", \\\"Not set\\\")}')\n\"\n\n# Test Spark cluster connectivity (if Spark Operator deployed)\nkubectl exec -n jupyterhub &lt;user-pod-name&gt; -- python3 -c \"\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n    .appName('connectivity-test') \\\n    .config('spark.kubernetes.container.image', 'jupyter/pyspark-notebook:spark-3.5.0') \\\n    .getOrCreate()\nprint('\u2705 Spark session with Kubernetes backend created')\nspark.stop()\n\"\n</code></pre></p> <p>Performance Issues: <pre><code># Check resource usage\nkubectl top pod -n jupyterhub\n\n# Monitor hub performance\nkubectl logs -n jupyterhub -l component=hub --tail=100 | grep -E 'ERROR|WARNING|memory|cpu'\n\n# Check user pod resource limits\nkubectl describe pod -n jupyterhub &lt;user-pod-name&gt; | grep -A 5 -B 5 Resources\n\n# Monitor active sessions\nkubectl exec -n jupyterhub deployment/hub -- \\\n  python3 -c \"\nimport requests\nr = requests.get('http://localhost:8081/hub/api/users')\nactive_users = [u for u in r.json() if u.get('server')]\nprint(f'Active sessions: {len(active_users)}')\nfor user in active_users:\n    print(f'- {user[\\\"name\\\"]}: {user[\\\"server\\\"][\\\"state\\\"]}')\n\"\n</code></pre></p>"},{"location":"package-datascience-jupyterhub/#maintenance","title":"\ud83d\udccb Maintenance","text":""},{"location":"package-datascience-jupyterhub/#regular-tasks","title":"Regular Tasks","text":"<ol> <li>Health Monitoring: Check pod and service status daily</li> <li>User Session Monitoring: Monitor active sessions and resource usage</li> <li>Storage Monitoring: Monitor user storage usage and PVC capacity</li> <li>Image Updates: Keep notebook images updated with latest packages</li> <li>Secret Rotation: Follow urbalurba-secrets rotation procedures</li> </ol>"},{"location":"package-datascience-jupyterhub/#backup-procedures","title":"Backup Procedures","text":"<pre><code># Export user data (requires access to persistent volumes)\nkubectl get pvc -n jupyterhub\nfor pvc in $(kubectl get pvc -n jupyterhub -o name); do\n  echo \"Backing up $pvc\"\n  kubectl cp -n jupyterhub $(kubectl get pod -n jupyterhub -o name | head -1):$(kubectl get $pvc -o jsonpath='{.spec.volumeName}') \\\n    ./jupyterhub-backup-$(date +%Y%m%d)/$pvc/\ndone\n\n# Export JupyterHub configuration\nkubectl get configmap -n jupyterhub hub -o yaml &gt; jupyterhub-config-backup-$(date +%Y%m%d).yaml\n\n# Export user database (if applicable)\nkubectl exec -n jupyterhub deployment/hub -- \\\n  python3 -c \"\nimport sqlite3\nimport shutil\nshutil.copy('/srv/jupyterhub/jupyterhub.sqlite', '/tmp/jupyterhub-backup.sqlite')\n\"\nkubectl cp -n jupyterhub deployment/hub:/tmp/jupyterhub-backup.sqlite ./jupyterhub-db-backup-$(date +%Y%m%d).sqlite\n</code></pre>"},{"location":"package-datascience-jupyterhub/#disaster-recovery","title":"Disaster Recovery","text":"<pre><code># Restore from PVC backup\n# (Requires recreation of PVCs and pod restart)\nkubectl delete pvc -n jupyterhub &lt;user-pvc-name&gt;\nkubectl apply -f &lt;restored-pvc-manifest&gt;\n\n# Restore JupyterHub configuration\nkubectl apply -f jupyterhub-config-backup.yaml\n\n# Restart JupyterHub components\nkubectl rollout restart -n jupyterhub deployment/hub\nkubectl rollout restart -n jupyterhub deployment/proxy\n</code></pre>"},{"location":"package-datascience-jupyterhub/#use-cases","title":"\ud83d\ude80 Use Cases","text":""},{"location":"package-datascience-jupyterhub/#data-science-workflow","title":"Data Science Workflow","text":"<pre><code># In JupyterHub notebook\nimport pandas as pd\nimport numpy as np\nfrom pyspark.sql import SparkSession\n\n# Create Spark session\nspark = SparkSession.builder \\\n    .appName(\"DataScienceWorkflow\") \\\n    .getOrCreate()\n\n# Load data\ndf = spark.read.csv(\"/path/to/data.csv\", header=True, inferSchema=True)\n\n# Data processing\ndf_processed = df.filter(df.value &gt; 100) \\\n    .groupBy(\"category\") \\\n    .agg({\"value\": \"avg\"}) \\\n    .orderBy(\"category\")\n\n# Convert to Pandas for visualization\npandas_df = df_processed.toPandas()\n\n# Visualization with plotly\nimport plotly.express as px\nfig = px.bar(pandas_df, x='category', y='avg(value)')\nfig.show()\n\nspark.stop()\n</code></pre>"},{"location":"package-datascience-jupyterhub/#machine-learning-pipeline","title":"Machine Learning Pipeline","text":"<pre><code># In JupyterHub notebook\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Load and prepare data\ndf = spark.read.parquet(\"/path/to/ml_data.parquet\")\n\n# Feature engineering\nassembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\", \"feature3\"], outputCol=\"features\")\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\nrf = RandomForestClassifier(featuresCol=\"scaled_features\", labelCol=\"label\")\n\n# Create pipeline\npipeline = Pipeline(stages=[assembler, scaler, rf])\n\n# Train model\ntrain_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\nmodel = pipeline.fit(train_data)\n\n# Evaluate model\npredictions = model.transform(test_data)\nevaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\nauc = evaluator.evaluate(predictions)\nprint(f\"AUC: {auc}\")\n</code></pre>"},{"location":"package-datascience-jupyterhub/#distributed-data-processing","title":"Distributed Data Processing","text":"<pre><code># In JupyterHub notebook\nfrom pyspark.sql.functions import col, count, avg, max, min\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\n\n# Process large dataset with Spark\nlarge_df = spark.read.option(\"multiline\", \"true\") \\\n    .json(\"/path/to/large_dataset.json\")\n\n# Distributed aggregations\nsummary = large_df.groupBy(\"region\") \\\n    .agg(\n        count(\"*\").alias(\"total_records\"),\n        avg(\"sales\").alias(\"avg_sales\"),\n        max(\"sales\").alias(\"max_sales\"),\n        min(\"sales\").alias(\"min_sales\")\n    )\n\n# Write results to distributed storage\nsummary.coalesce(1) \\\n    .write \\\n    .mode(\"overwrite\") \\\n    .option(\"header\", \"true\") \\\n    .csv(\"/path/to/output/summary\")\n\n# Show results\nsummary.show()\n</code></pre>"},{"location":"package-datascience-jupyterhub/#collaborative-data-exploration","title":"Collaborative Data Exploration","text":"<pre><code># Shared notebook accessible by multiple users\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load shared dataset\nshared_df = spark.read.table(\"shared_catalog.analysis_data\")\n\n# Convert to Pandas for detailed analysis\npandas_df = shared_df.sample(fraction=0.1).toPandas()\n\n# Create visualizations\nplt.figure(figsize=(12, 8))\nsns.pairplot(pandas_df, hue='category')\nplt.title('Data Exploration - Shared Analysis')\nplt.savefig('/shared/analysis_results.png')\nplt.show()\n\n# Save insights for team\ninsights = pandas_df.describe()\ninsights.to_csv('/shared/dataset_insights.csv')\n</code></pre> <p>\ud83d\udca1 Key Insight: JupyterHub provides an essential web-based notebook environment that enables data scientists and ML engineers to perform interactive data analysis, build machine learning models, and execute distributed data processing workflows. As Phase 2 of the Databricks replacement project, it integrates seamlessly with Spark Kubernetes Operator to provide a complete alternative to Databricks workspace functionality.</p>"},{"location":"package-datascience-spark/","title":"Spark Kubernetes Operator - Distributed Data Processing Engine","text":"<p>Key Features: Distributed Computing \u2022 SparkApplication CRDs \u2022 Kubernetes Native \u2022 ARM64 Support \u2022 Resource Management \u2022 Job Scheduling \u2022 Multi-Language Support</p> <p>File: <code>docs/package-datascience-spark.md</code> Purpose: Complete guide to Apache Spark Kubernetes Operator deployment and configuration in Urbalurba infrastructure Target Audience: Data engineers, data scientists, platform architects, developers building distributed data processing applications Last Updated: September 24, 2025</p>"},{"location":"package-datascience-spark/#overview","title":"\ud83d\udccb Overview","text":"<p>The Spark Kubernetes Operator serves as the distributed data processing engine in the Urbalurba infrastructure, providing native Kubernetes integration for Apache Spark workloads. It enables declarative submission and management of Spark applications through custom Kubernetes resources, replacing traditional Spark cluster managers.</p> <p>This implementation provides 100% Databricks compatibility for PySpark, Spark SQL, and DataFrame operations, allowing seamless migration of existing Databricks workloads to a self-managed Kubernetes environment. The operator manages the complete lifecycle of Spark applications, from submission to cleanup, with automatic resource management and scaling.</p> <p>Key Features: - Kubernetes Native: Uses SparkApplication Custom Resource Definitions (CRDs) for declarative job management - Multi-Language Support: Python (PySpark), Scala, Java, and R support with identical APIs to standard Spark - Resource Management: Automatic pod scheduling, resource allocation, and cleanup - ARM64 Compatibility: Native support for Apple Silicon and ARM-based infrastructure - JupyterHub Integration: Seamless integration with notebook environments for interactive development - Databricks Compatibility: 100% API compatibility for easy workload migration</p>"},{"location":"package-datascience-spark/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"package-datascience-spark/#deployment-components","title":"Deployment Components","text":"<pre><code>Spark Kubernetes Operator Stack:\n\u251c\u2500\u2500 Helm Release (spark-kubernetes-operator/spark-kubernetes-operator)\n\u251c\u2500\u2500 Namespace (spark-operator)\n\u251c\u2500\u2500 Deployment (spark-kubernetes-operator pod)\n\u251c\u2500\u2500 Custom Resource Definitions (SparkApplication, ScheduledSparkApplication)\n\u251c\u2500\u2500 Service Account (spark-operator with RBAC permissions)\n\u251c\u2500\u2500 ClusterRole (spark-operator-role for pod management)\n\u2514\u2500\u2500 Driver/Executor Pods (created dynamically per job)\n</code></pre>"},{"location":"package-datascience-spark/#file-structure","title":"File Structure","text":"<pre><code>10-datascience/not-in-use/\n\u251c\u2500\u2500 03-setup-spark.sh              # Main deployment script\n\u2514\u2500\u2500 03-remove-spark.sh             # Removal script\n\nansible/playbooks/\n\u251c\u2500\u2500 330-setup-spark.yml            # Main deployment logic\n\u2514\u2500\u2500 330-remove-spark.yml           # Removal logic\n\nmanifests/\n\u2514\u2500\u2500 331-sample-data-sparkapplication.yaml  # Example Spark job\n</code></pre>"},{"location":"package-datascience-spark/#deployment","title":"\ud83d\ude80 Deployment","text":""},{"location":"package-datascience-spark/#manual-deployment","title":"Manual Deployment","text":"<p>The Spark Kubernetes Operator is available in the data science package and can be deployed individually:</p> <pre><code># Deploy Spark Operator with verification\ncd provision-host/kubernetes/10-datascience/not-in-use/\n./03-setup-spark.sh rancher-desktop\n\n# Deploy to specific Kubernetes context\n./03-setup-spark.sh multipass-microk8s\n./03-setup-spark.sh azure-aks\n</code></pre>"},{"location":"package-datascience-spark/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes Cluster: 1.19+ with sufficient resources (2+ CPU cores, 4+ GB RAM recommended)</li> <li>kubectl: Configured for target cluster access</li> <li>Helm 3.x: Required for operator installation</li> <li>Container Runtime: Docker or containerd with proper networking</li> </ul>"},{"location":"package-datascience-spark/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"package-datascience-spark/#spark-operator-configuration","title":"Spark Operator Configuration","text":"<p>The operator is deployed with production-ready settings optimized for Kubernetes environments:</p> <pre><code># Helm values configuration\nwebhook:\n  enable: false  # Disabled for simplified deployment\n\nimage:\n  repository: spark-kubernetes-operator/spark-operator\n  tag: latest  # Uses stable release with ARM64 support\n\nrbac:\n  create: true  # Creates necessary service accounts and roles\n\nserviceAccounts:\n  spark:\n    create: true\n    name: spark\n  sparkoperator:\n    create: true\n    name: spark-operator\n</code></pre>"},{"location":"package-datascience-spark/#resource-configuration","title":"Resource Configuration","text":"<pre><code># Operator pod resources\nresources:\n  requests:\n    cpu: 100m\n    memory: 300Mi\n  limits:\n    cpu: 200m\n    memory: 512Mi\n\n# Default Spark application resources (customizable per job)\nspark:\n  driver:\n    cores: 1\n    memory: 1g\n  executor:\n    cores: 1\n    memory: 1g\n    instances: 2\n</code></pre>"},{"location":"package-datascience-spark/#security-configuration","title":"Security Configuration","text":"<pre><code># RBAC Configuration\nclusterRole:\n  create: true  # Allows operator to manage pods cluster-wide\n\nsecurityContext:\n  runAsNonRoot: true\n  runAsUser: 1000\n  fsGroup: 1000\n\n# Network policies (if needed)\nnetworkPolicy:\n  enabled: false  # Can be enabled for additional network security\n</code></pre>"},{"location":"package-datascience-spark/#monitoring-verification","title":"\ud83d\udd0d Monitoring &amp; Verification","text":""},{"location":"package-datascience-spark/#health-checks","title":"Health Checks","text":"<pre><code># Check operator pod status\nkubectl get pods -n spark-operator\n\n# Verify operator deployment\nkubectl get deployment -n spark-operator spark-kubernetes-operator\n\n# Check custom resource definitions\nkubectl get crd | grep spark\n\n# View operator logs\nkubectl logs -n spark-operator deployment/spark-kubernetes-operator\n</code></pre>"},{"location":"package-datascience-spark/#service-verification","title":"Service Verification","text":"<pre><code># Test SparkApplication CRD availability\nkubectl api-resources | grep sparkapplications\n\n# Check RBAC permissions\nkubectl auth can-i create pods --as=system:serviceaccount:spark-operator:spark\n\n# Verify webhook configuration (if enabled)\nkubectl get validatingwebhookconfigurations | grep spark\n</code></pre>"},{"location":"package-datascience-spark/#spark-application-testing","title":"Spark Application Testing","text":"<pre><code># Submit sample Spark application\nkubectl apply -f manifests/331-sample-data-sparkapplication.yaml\n\n# Monitor application status\nkubectl get sparkapplications -A\n\n# Check driver pod logs\nkubectl logs -n spark-operator spark-pi-driver\n\n# View application details\nkubectl describe sparkapplication spark-pi -n spark-operator\n</code></pre>"},{"location":"package-datascience-spark/#automated-verification","title":"Automated Verification","text":"<p>The deployment includes automatic verification of: - \u2705 Operator Readiness: Confirms operator pod is running and ready - \u2705 CRD Registration: Validates SparkApplication custom resources are available - \u2705 RBAC Configuration: Ensures proper service account permissions - \u2705 Helm Deployment: Verifies successful chart installation</p>"},{"location":"package-datascience-spark/#management-operations","title":"\ud83d\udee0\ufe0f Management Operations","text":""},{"location":"package-datascience-spark/#spark-application-administration","title":"Spark Application Administration","text":"<pre><code># List all Spark applications\nkubectl get sparkapplications -A\n\n# Get application status\nkubectl describe sparkapplication &lt;app-name&gt; -n &lt;namespace&gt;\n\n# View driver logs\nkubectl logs &lt;driver-pod-name&gt; -n &lt;namespace&gt;\n\n# Monitor executor pods\nkubectl get pods -l spark-role=executor -n &lt;namespace&gt;\n\n# Delete application\nkubectl delete sparkapplication &lt;app-name&gt; -n &lt;namespace&gt;\n</code></pre>"},{"location":"package-datascience-spark/#operator-management","title":"Operator Management","text":"<pre><code># Scale operator (if needed)\nkubectl scale deployment spark-kubernetes-operator -n spark-operator --replicas=1\n\n# Update operator configuration\nhelm upgrade spark-operator spark-kubernetes-operator/spark-kubernetes-operator -n spark-operator\n\n# Check operator metrics (if enabled)\nkubectl port-forward -n spark-operator svc/spark-operator-webhook 8080:443\n</code></pre>"},{"location":"package-datascience-spark/#service-removal","title":"Service Removal","text":"<pre><code># Remove Spark Operator and all applications\ncd provision-host/kubernetes/10-datascience/not-in-use/\n./03-remove-spark.sh rancher-desktop\n\n# Manual cleanup if needed\nhelm uninstall spark-operator -n spark-operator\nkubectl delete namespace spark-operator\n</code></pre>"},{"location":"package-datascience-spark/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-datascience-spark/#common-issues","title":"Common Issues","text":"<p>Operator Pod Won't Start: <pre><code># Check pod events and logs\nkubectl describe pod -n spark-operator -l app.kubernetes.io/name=spark-operator\nkubectl logs -n spark-operator deployment/spark-kubernetes-operator\n\n# Verify RBAC permissions\nkubectl auth can-i create pods --as=system:serviceaccount:spark-operator:spark-operator\n</code></pre></p> <p>SparkApplication Stuck in Pending: <pre><code># Check resource availability\nkubectl describe nodes\nkubectl get pods -A | grep Pending\n\n# Verify service account permissions\nkubectl get serviceaccount spark -n spark-operator\nkubectl describe clusterrole spark-operator-role\n</code></pre></p> <p>Driver Pod Failures: <pre><code># Check driver pod logs\nkubectl logs &lt;driver-pod&gt; -n &lt;namespace&gt;\n\n# Verify image pull and resources\nkubectl describe pod &lt;driver-pod&gt; -n &lt;namespace&gt;\n\n# Check network connectivity\nkubectl exec &lt;driver-pod&gt; -n &lt;namespace&gt; -- nslookup kubernetes.default.svc.cluster.local\n</code></pre></p> <p>Executor Connection Issues: <pre><code># Check executor pod logs\nkubectl logs &lt;executor-pod&gt; -n &lt;namespace&gt;\n\n# Verify driver service\nkubectl get svc -n &lt;namespace&gt; | grep &lt;driver-svc&gt;\n\n# Test connectivity\nkubectl exec &lt;executor-pod&gt; -n &lt;namespace&gt; -- telnet &lt;driver-svc&gt; 7077\n</code></pre></p>"},{"location":"package-datascience-spark/#maintenance","title":"\ud83d\udccb Maintenance","text":""},{"location":"package-datascience-spark/#regular-tasks","title":"Regular Tasks","text":"<ol> <li>Monitor Resource Usage: Check CPU/memory consumption of running Spark applications</li> <li>Clean up Completed Jobs: Remove old SparkApplication resources</li> <li>Update Operator: Keep operator version current for security and features</li> <li>Review Logs: Monitor operator logs for errors or performance issues</li> </ol>"},{"location":"package-datascience-spark/#backup-procedures","title":"Backup Procedures","text":"<pre><code># Backup SparkApplication definitions\nkubectl get sparkapplications -A -o yaml &gt; spark-applications-backup.yaml\n\n# Backup operator configuration\nhelm get values spark-operator -n spark-operator &gt; spark-operator-values.yaml\n\n# Export custom configurations\nkubectl get configmap -n spark-operator -o yaml &gt; spark-configs-backup.yaml\n</code></pre>"},{"location":"package-datascience-spark/#disaster-recovery","title":"Disaster Recovery","text":"<pre><code># Restore operator\nhelm install spark-operator spark-kubernetes-operator/spark-kubernetes-operator -n spark-operator -f spark-operator-values.yaml\n\n# Restore applications\nkubectl apply -f spark-applications-backup.yaml\n\n# Verify restoration\nkubectl get sparkapplications -A\nkubectl get pods -n spark-operator\n</code></pre>"},{"location":"package-datascience-spark/#use-cases","title":"\ud83d\ude80 Use Cases","text":""},{"location":"package-datascience-spark/#batch-data-processing","title":"Batch Data Processing","text":"<pre><code>apiVersion: \"sparkoperator.k8s.io/v1beta2\"\nkind: SparkApplication\nmetadata:\n  name: data-processing-job\n  namespace: spark-operator\nspec:\n  type: Python\n  mode: cluster\n  image: \"apache/spark-py:v3.5.0\"\n  imagePullPolicy: Always\n  mainApplicationFile: \"s3a://data-bucket/jobs/process_data.py\"\n  arguments:\n    - \"--input-path\"\n    - \"s3a://data-bucket/raw/2025/09/24/\"\n    - \"--output-path\"\n    - \"s3a://data-bucket/processed/2025/09/24/\"\n  sparkVersion: \"3.5.0\"\n  driver:\n    cores: 2\n    memory: \"2g\"\n    serviceAccount: spark\n  executor:\n    cores: 2\n    memory: \"2g\"\n    instances: 4\n</code></pre>"},{"location":"package-datascience-spark/#scheduled-etl-pipeline","title":"Scheduled ETL Pipeline","text":"<pre><code>apiVersion: \"sparkoperator.k8s.io/v1beta2\"\nkind: ScheduledSparkApplication\nmetadata:\n  name: daily-etl-pipeline\n  namespace: spark-operator\nspec:\n  schedule: \"0 2 * * *\"  # Daily at 2 AM\n  template:\n    type: Python\n    mode: cluster\n    image: \"apache/spark-py:v3.5.0\"\n    mainApplicationFile: \"local:///opt/spark/work-dir/etl_pipeline.py\"\n    sparkVersion: \"3.5.0\"\n    driver:\n      cores: 1\n      memory: \"1g\"\n      serviceAccount: spark\n    executor:\n      cores: 1\n      memory: \"1g\"\n      instances: 3\n</code></pre>"},{"location":"package-datascience-spark/#interactive-pyspark-in-jupyterhub","title":"Interactive PySpark in JupyterHub","text":"<pre><code># In JupyterHub notebook\nfrom pyspark.sql import SparkSession\n\n# Create Spark session (automatically connects to operator-managed cluster)\nspark = SparkSession.builder \\\n    .appName(\"InteractiveAnalysis\") \\\n    .config(\"spark.kubernetes.container.image\", \"apache/spark-py:v3.5.0\") \\\n    .config(\"spark.executor.instances\", \"2\") \\\n    .config(\"spark.executor.memory\", \"1g\") \\\n    .config(\"spark.executor.cores\", \"1\") \\\n    .getOrCreate()\n\n# Load and process data\ndf = spark.read.parquet(\"s3a://data-bucket/dataset.parquet\")\nresult = df.groupBy(\"category\").count().orderBy(\"count\", ascending=False)\nresult.show()\n\n# SQL operations\ndf.createOrReplaceTempView(\"data\")\nspark.sql(\"SELECT category, AVG(value) as avg_value FROM data GROUP BY category\").show()\n</code></pre>"},{"location":"package-datascience-spark/#ml-pipeline-with-spark-mllib","title":"ML Pipeline with Spark MLlib","text":"<pre><code># Machine learning pipeline\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\n\n# Prepare features\nindexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\nassembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\", \"categoryIndex\"], outputCol=\"features\")\nrf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n\n# Create and fit pipeline\npipeline = Pipeline(stages=[indexer, assembler, rf])\nmodel = pipeline.fit(training_data)\n\n# Make predictions\npredictions = model.transform(test_data)\npredictions.select(\"features\", \"label\", \"prediction\").show()\n</code></pre> <p>\ud83d\udca1 Key Insight: The Spark Kubernetes Operator provides enterprise-grade distributed computing capabilities that fully replace Databricks' processing engine. Combined with JupyterHub, it delivers 100% API compatibility for PySpark workloads while providing superior resource management through Kubernetes-native orchestration. This enables seamless migration of existing Databricks applications to a self-managed, cloud-independent infrastructure.</p>"},{"location":"package-datascience-unitycatalog/","title":"Unity Catalog - Data Governance and Cataloging \u274c","text":"<p>Key Features: Data Governance \u2022 Metadata Management \u2022 Access Control \u2022 Schema Registry \u2022 Data Lineage \u2022 Three-Level Namespace \u2022 NOT WORKING</p> <p>File: <code>docs/package-datascience-unitycatalog.md</code> Purpose: Documentation of Unity Catalog deployment issues and alternatives for data governance Target Audience: Data engineers, platform architects, developers working with data catalogs Last Updated: September 24, 2025</p>"},{"location":"package-datascience-unitycatalog/#critical-issue-service-not-functional","title":"\u274c CRITICAL ISSUE: SERVICE NOT FUNCTIONAL","text":"<p>Unity Catalog is currently NOT WORKING due to universal container image issues affecting all available Docker images.</p>"},{"location":"package-datascience-unitycatalog/#overview","title":"\ud83d\udccb Overview","text":"<p>Unity Catalog is designed to serve as an enterprise data governance solution providing centralized metadata management, access control, and data lineage tracking. However, all available Docker container images are broken due to fundamental permission issues that prevent the service from starting.</p> <p>Intended Key Features (Not Available): - Data Governance: Centralized data access policies and permissions - Metadata Management: Schema registry and table discovery - Access Control: Fine-grained permissions on databases, schemas, and tables - Three-Level Namespace: catalog.schema.table hierarchical organization - Data Lineage: Track data flow and transformations - Spark Integration: Native support for Spark SQL and DataFrame operations</p>"},{"location":"package-datascience-unitycatalog/#root-cause-analysis","title":"\ud83d\udeab Root Cause Analysis","text":""},{"location":"package-datascience-unitycatalog/#universal-container-issue","title":"Universal Container Issue","text":"<p>ALL available Unity Catalog Docker images suffer from the same critical flaw:</p> <pre><code>Error: failed to start container \"unity-catalog-server\":\nexec: \"bin/start-uc-server\": stat bin/start-uc-server: permission denied\n</code></pre>"},{"location":"package-datascience-unitycatalog/#affected-images","title":"Affected Images","text":"<ul> <li>\u274c <code>unitycatalog/unitycatalog:latest</code> (Official image)</li> <li>\u274c <code>godatadriven/unity-catalog:latest</code> (Community image, 1.1GB)</li> <li>\u274c <code>datacatering/unitycatalog</code> (Alternative image)</li> </ul>"},{"location":"package-datascience-unitycatalog/#technical-details","title":"Technical Details","text":"<ol> <li>Permission Problem: The <code>bin/start-uc-server</code> shell script lacks execute permissions</li> <li>Build Process Bug: All Dockerfiles fail to include <code>RUN chmod +x /app/bin/start-uc-server</code></li> <li>Universal Failure: This affects the entire Unity Catalog container ecosystem</li> <li>Unfixable in Kubernetes: Cannot fix permissions post-deployment due to security policies</li> </ol>"},{"location":"package-datascience-unitycatalog/#architecture-intended-but-non-functional","title":"\ud83c\udfd7\ufe0f Architecture (Intended but Non-Functional)","text":""},{"location":"package-datascience-unitycatalog/#deployment-components-all-broken","title":"Deployment Components (All Broken)","text":"<pre><code>Unity Catalog Stack (BROKEN):\n\u251c\u2500\u2500 Helm Release (FAILS - container won't start)\n\u251c\u2500\u2500 Deployment (FAILS - permission denied)\n\u251c\u2500\u2500 ConfigMap (Working - but unused due to startup failure)\n\u251c\u2500\u2500 Service (Working - but no backend)\n\u251c\u2500\u2500 PersistentVolumeClaims (Working - but unused)\n\u251c\u2500\u2500 urbalurba-secrets (Working - all secrets configured correctly)\n\u2514\u2500\u2500 Pod (FAILS - CrashLoopBackOff due to exec permission denied)\n</code></pre>"},{"location":"package-datascience-unitycatalog/#file-structure","title":"File Structure","text":"<pre><code>10-datascience/not-in-use/\n\u251c\u2500\u2500 07-setup-unity-catalog.sh        # Deployment script (fails due to container)\n\u2514\u2500\u2500 07-remove-unity-catalog.sh       # Removal script (works)\n\nmanifests/\n\u251c\u2500\u2500 320-unity-catalog-deployment.yaml # Kubernetes manifests (correct but ineffective)\n\u2514\u2500\u2500 321-unity-catalog-ingress.yaml    # Ingress configuration (unused)\n\nansible/playbooks/\n\u251c\u2500\u2500 320-setup-unity-catalog.yml       # Ansible playbook (infrastructure works, container fails)\n\u2514\u2500\u2500 utility/u07-setup-unity-catalog-database.yml # Database setup (works correctly)\n</code></pre>"},{"location":"package-datascience-unitycatalog/#verification-results","title":"\ud83d\udd0d Verification Results","text":""},{"location":"package-datascience-unitycatalog/#whats-working","title":"What's Working \u2705","text":"<ul> <li>Database: PostgreSQL <code>unity_catalog</code> database created successfully</li> <li>User: <code>unity_catalog_user</code> with proper permissions</li> <li>Secrets: All Unity Catalog secrets correctly configured</li> <li>Kubernetes Resources: Deployments, services, ingresses all apply correctly</li> <li>Infrastructure: Complete Kubernetes setup is functional</li> </ul>"},{"location":"package-datascience-unitycatalog/#whats-broken","title":"What's Broken \u274c","text":"<ul> <li>Container Startup: All Unity Catalog images fail with permission denied</li> <li>Service Availability: No working endpoint due to container failure</li> <li>API Access: No functional REST API for catalog operations</li> <li>Spark Integration: Cannot connect to non-existent service</li> </ul>"},{"location":"package-datascience-unitycatalog/#attempted-solutions","title":"\ud83d\udee0\ufe0f Attempted Solutions","text":""},{"location":"package-datascience-unitycatalog/#tried-and-failed","title":"Tried and Failed","text":"<ol> <li>Permission Workaround: <code>chmod +x ./bin/start-uc-server</code> fails (permission denied on chmod itself)</li> <li>Alternative Images: All community images have same issue</li> <li>Direct Java Execution: JAR files not at expected locations in containers</li> <li>Security Context Changes: Running as root still fails due to script permissions</li> </ol>"},{"location":"package-datascience-unitycatalog/#why-solutions-dont-work","title":"Why Solutions Don't Work","text":"<ul> <li>Read-Only Filesystem: Container filesystems prevent chmod operations</li> <li>Universal Bug: All Unity Catalog container builds have same flaw</li> <li>Kubernetes Security: Cannot override container entrypoint permissions</li> </ul>"},{"location":"package-datascience-unitycatalog/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-datascience-unitycatalog/#error-patterns","title":"Error Patterns","text":"<pre><code># Container logs show:\nexec: \"bin/start-uc-server\": permission denied\n\n# Pod status shows:\nCrashLoopBackOff   3 (45s ago)\n\n# Events show:\nError: failed to start container \"unity-catalog-server\"\n</code></pre>"},{"location":"package-datascience-unitycatalog/#diagnostic-commands","title":"Diagnostic Commands","text":"<pre><code># Check pod status (will show CrashLoopBackOff)\nkubectl get pods -n unity-catalog\n\n# View error logs (will be empty due to startup failure)\nkubectl logs -n unity-catalog -l app=unity-catalog\n\n# Check pod events (shows permission denied error)\nkubectl describe pod -n unity-catalog -l app=unity-catalog\n\n# Verify infrastructure (database, secrets work)\nkubectl get secret urbalurba-secrets -n unity-catalog -o jsonpath='{.data}' | jq 'keys'\n</code></pre>"},{"location":"package-datascience-unitycatalog/#alternative-solutions","title":"\ud83d\udd04 Alternative Solutions","text":"<p>Since Unity Catalog is non-functional, consider these working alternatives:</p>"},{"location":"package-datascience-unitycatalog/#apache-hive-metastore","title":"Apache Hive Metastore \u2705","text":"<ul> <li>Status: Available and working</li> <li>Features: Schema registry, metadata management</li> <li>Integration: Native Spark support</li> <li>Deployment: Standard Helm charts work correctly</li> </ul>"},{"location":"package-datascience-unitycatalog/#apache-atlas","title":"Apache Atlas \u2705","text":"<ul> <li>Status: Available and working</li> <li>Features: Data governance, lineage tracking</li> <li>Integration: Spark, Kafka, HBase support</li> <li>Deployment: Stable container images available</li> </ul>"},{"location":"package-datascience-unitycatalog/#datahub","title":"DataHub \u2705","text":"<ul> <li>Status: Available and working</li> <li>Features: Modern metadata platform</li> <li>Integration: GraphQL API, web UI</li> <li>Deployment: Well-maintained Docker images</li> </ul>"},{"location":"package-datascience-unitycatalog/#postgresql-custom-metadata-tables","title":"PostgreSQL + Custom Metadata Tables \u2705","text":"<ul> <li>Status: Always works (uses existing PostgreSQL)</li> <li>Features: Custom schema registry</li> <li>Integration: Direct SQL access from Spark</li> <li>Deployment: No additional containers needed</li> </ul>"},{"location":"package-datascience-unitycatalog/#current-recommendations","title":"\ud83d\udccb Current Recommendations","text":""},{"location":"package-datascience-unitycatalog/#immediate-actions","title":"Immediate Actions","text":"<ol> <li>Do NOT attempt to deploy Unity Catalog - it will fail universally</li> <li>Use Apache Hive Metastore for immediate data catalog needs</li> <li>Consider DataHub for modern metadata platform features</li> <li>Monitor Unity Catalog project for container image fixes</li> </ol>"},{"location":"package-datascience-unitycatalog/#long-term-strategy","title":"Long-Term Strategy","text":"<ol> <li>Wait for Upstream Fix: Unity Catalog team needs to fix container builds</li> <li>Custom Image Build: Build corrected Docker image with proper permissions</li> <li>Alternative Implementation: Use working data catalog solutions</li> <li>Hybrid Approach: PostgreSQL metadata + custom tools</li> </ol>"},{"location":"package-datascience-unitycatalog/#status-timeline","title":"\ud83d\ude80 Status Timeline","text":"<ul> <li>September 2025: Unity Catalog containers discovered broken</li> <li>Issue Reported: Community aware of permission problems</li> <li>Expected Fix: Unknown - depends on Unity Catalog team</li> <li>Current Status: ALL IMAGES BROKEN - AVOID DEPLOYMENT</li> </ul>"},{"location":"package-datascience-unitycatalog/#key-insight","title":"\ud83d\udca1 Key Insight","text":"<p>Unity Catalog represents an excellent data governance solution in theory, but all container implementations are fundamentally broken. The issue is not with the Urbalurba infrastructure, Kubernetes configuration, or secrets management - all of those work perfectly. The problem is with the Unity Catalog project's Docker build process, which fails to set proper executable permissions on critical startup scripts.</p> <p>Recommendation: Use working alternatives (Hive Metastore, Atlas, DataHub) until Unity Catalog fixes their container images.</p> <p>\u26a0\ufe0f WARNING: Do not attempt to deploy Unity Catalog until container permission issues are resolved upstream. Deployment will always fail with permission denied errors regardless of configuration changes.</p>"},{"location":"package-datascience/","title":"Databricks Replacement - Data Science Package","text":"<p>A production-ready, on-premises Databricks replacement using open-source components on Kubernetes. This system provides 85% of Databricks functionality with no cloud dependencies, built as a contingency plan for Azure unavailability.</p>"},{"location":"package-datascience/#project-status-production-ready","title":"\ud83c\udfaf Project Status: PRODUCTION READY","text":"<p>Current Achievement: 85% Databricks functionality with full notebook interface and distributed computing capabilities.</p> Databricks Feature Our Implementation Status Compatibility Notebook Interface JupyterHub \u2705 Production Ready 95% identical PySpark Computing Spark 4.0 + Kubernetes \u2705 Production Ready 100% compatible SQL Operations <code>spark.sql()</code> \u2705 Production Ready 100% compatible DataFrame API Native PySpark \u2705 Production Ready 100% compatible Multi-user Workspace JupyterHub auth \u2705 Production Ready 90% feature parity Resource Management Kubernetes \u2705 Production Ready Superior to Databricks Job Execution Spark Operator \u2705 Production Ready Production ready Data Analytics Full PySpark API \u2705 Production Ready 100% compatible"},{"location":"package-datascience/#architecture-overview","title":"\ud83c\udfd7\ufe0f Architecture Overview","text":""},{"location":"package-datascience/#phase-1-processing-engine-notebook-interface-complete","title":"Phase 1: Processing Engine + Notebook Interface \u2705 COMPLETE","text":""},{"location":"package-datascience/#phase-2-business-intelligence-data-catalog-partial-unity-catalog-broken","title":"Phase 2: Business Intelligence + Data Catalog \u26a0\ufe0f PARTIAL (Unity Catalog broken)","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           Complete Analytics Platform Replacement           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Metabase (Business Intelligence) - PLANNED                \u2502\n\u2502  \u251c\u2500\u2500 Self-service dashboards                               \u2502\n\u2502  \u251c\u2500\u2500 Drag-and-drop chart builder                          \u2502\n\u2502  \u251c\u2500\u2500 SQL editor for business analysts                      \u2502\n\u2502  \u251c\u2500\u2500 Automated reports and alerts                          \u2502\n\u2502  \u2514\u2500\u2500 Direct Spark data connectivity                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  JupyterHub (Notebook Interface) - PRODUCTION READY        \u2502\n\u2502  \u251c\u2500\u2500 Python/Scala notebooks                                \u2502\n\u2502  \u251c\u2500\u2500 PySpark 3.5.0 integration                            \u2502\n\u2502  \u251c\u2500\u2500 SQL operations via spark.sql()                        \u2502\n\u2502  \u2514\u2500\u2500 Multi-user authentication                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Apache Spark Kubernetes Operator - PRODUCTION READY       \u2502\n\u2502  \u251c\u2500\u2500 Distributed Spark 4.0 jobs                           \u2502\n\u2502  \u251c\u2500\u2500 SparkApplication CRDs                                 \u2502\n\u2502  \u251c\u2500\u2500 Automatic resource management                         \u2502\n\u2502  \u2514\u2500\u2500 ARM64 compatibility                                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Unity Catalog (Data Catalog) - BROKEN (container issues)  \u2502\n\u2502  \u251c\u2500\u2500 Centralized metadata management                       \u2502\n\u2502  \u251c\u2500\u2500 Table discovery and schema management                 \u2502\n\u2502  \u251c\u2500\u2500 Integration with Spark and Metabase                   \u2502\n\u2502  \u2514\u2500\u2500 Data lineage tracking                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Kubernetes Infrastructure - PRODUCTION READY              \u2502\n\u2502  \u251c\u2500\u2500 K3s cluster (Rancher Desktop)                        \u2502\n\u2502  \u251c\u2500\u2500 Persistent storage (local-path)                       \u2502\n\u2502  \u251c\u2500\u2500 RBAC configuration                                    \u2502\n\u2502  \u2514\u2500\u2500 Traefik ingress controller                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"package-datascience/#core-components","title":"\ud83d\ude80 Core Components","text":""},{"location":"package-datascience/#apache-spark-kubernetes-operator-processing-engine","title":"Apache Spark Kubernetes Operator (Processing Engine)","text":"<ul> <li>Purpose: Replaces Databricks compute clusters and job execution</li> <li>Implementation: Official Apache Foundation project (launched May 2025)</li> <li>Features:</li> <li>Distributed Spark 4.0 processing</li> <li>SparkApplication CRDs for declarative job submission</li> <li>Automatic resource allocation and cleanup</li> <li>ARM64 native support for Apple Silicon</li> <li>Production-ready governance and sustainability</li> </ul>"},{"location":"package-datascience/#jupyterhub-notebook-interface","title":"JupyterHub (Notebook Interface)","text":"<ul> <li>Purpose: Replaces Databricks workspace notebooks</li> <li>Implementation: Official JupyterHub with PySpark integration</li> <li>Features:</li> <li>Web-based Python/Scala notebooks</li> <li>PySpark 3.5.0 fully integrated</li> <li>SQL operations via <code>spark.sql()</code></li> <li>Multi-user authentication and collaboration</li> <li>Persistent notebook storage</li> </ul>"},{"location":"package-datascience/#metabase-business-intelligence-platform","title":"Metabase (Business Intelligence Platform)","text":"<ul> <li>Purpose: Replaces Tableau/Power BI with open-source BI</li> <li>Implementation: Metabase with direct Spark integration</li> <li>Features:</li> <li>Self-service business intelligence for non-technical users</li> <li>Drag-and-drop dashboard creation</li> <li>SQL editor for advanced analytics</li> <li>Automated reporting and email alerts</li> <li>Real-time data visualization</li> <li>Role-based access control and permissions</li> </ul>"},{"location":"package-datascience/#kubernetes-infrastructure","title":"Kubernetes Infrastructure","text":"<ul> <li>Platform: Rancher Desktop with K3s</li> <li>Storage: Local-path persistent volumes</li> <li>Networking: Traefik ingress for web access</li> <li>Security: Complete RBAC configuration</li> <li>Scalability: Dynamic resource allocation</li> </ul>"},{"location":"package-datascience/#what-your-teams-get","title":"\ud83d\udcca What Your Teams Get","text":""},{"location":"package-datascience/#current-capabilities-phase-1-available-now","title":"Current Capabilities (Phase 1 - Available Now)","text":"<ol> <li>Access JupyterHub: <code>http://jupyterhub.localhost</code></li> <li>Create PySpark notebooks with identical syntax to Databricks</li> <li>Run distributed Spark jobs on Kubernetes</li> <li>Execute SQL queries using <code>spark.sql()</code></li> <li>Perform data analytics and machine learning</li> <li>Work collaboratively with multi-user support</li> <li>Offline operation - zero cloud dependencies</li> </ol>"},{"location":"package-datascience/#complete-platform-capabilities-after-phase-2","title":"Complete Platform Capabilities (After Phase 2)","text":"<ol> <li>Self-service BI dashboards via Metabase</li> <li>Drag-and-drop chart creation for business analysts</li> <li>Automated reports and alerts</li> <li>Role-based dashboard sharing</li> <li>Real-time data visualization</li> <li>SQL editor for business users</li> <li>Complete Tableau/Power BI replacement</li> </ol>"},{"location":"package-datascience/#example-usage-identical-to-databricks","title":"Example Usage (Identical to Databricks)","text":"<pre><code>import findspark\nfindspark.init()\n\nfrom pyspark.sql import SparkSession\n\n# Create Spark session (identical to Databricks)\nspark = SparkSession.builder \\\n    .appName(\"DataAnalysis\") \\\n    .getOrCreate()\n\n# DataFrames work exactly like Databricks\ndata = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\ndf = spark.createDataFrame(data, [\"name\", \"age\"])\ndf.show()\n\n# SQL operations work identically\ndf.createOrReplaceTempView(\"people\")\nresult = spark.sql(\"\"\"\n    SELECT name, age \n    FROM people \n    WHERE age &gt; 25\n    ORDER BY age DESC\n\"\"\")\nresult.show()\n\n# Advanced analytics\nspark.sql(\"\"\"\n    SELECT \n        department,\n        COUNT(*) as employee_count,\n        AVG(age) as avg_age\n    FROM employees \n    GROUP BY department\n\"\"\").show()\n</code></pre>"},{"location":"package-datascience/#installation-deployment","title":"\ud83d\udd04 Installation &amp; Deployment","text":""},{"location":"package-datascience/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster (6+ CPUs, 8+ GB RAM)</li> <li>Rancher Desktop or equivalent</li> <li>Helm 3.x</li> <li>kubectl configured</li> </ul>"},{"location":"package-datascience/#quick-installation","title":"Quick Installation","text":"<pre><code># 1. Setup Data Science Stack\n./02-setup-data-science.sh rancher-desktop\n\n# 2. Access JupyterHub\n# Web Interface: http://jupyterhub.localhost\n# Login: admin / [password from urbalurba-secrets]\n</code></pre>"},{"location":"package-datascience/#complete-removal-preserves-secrets-for-quick-reinstall","title":"Complete Removal (Preserves secrets for quick reinstall)","text":"<pre><code>./02-remove-data-science.sh rancher-desktop\n</code></pre>"},{"location":"package-datascience/#roadmap-to-95-databricks-tableaupower-bi-functionality","title":"\ud83d\udcc8 Roadmap to 95% Databricks + Tableau/Power BI Functionality","text":""},{"location":"package-datascience/#phase-2-business-intelligence-visualization-next-priority","title":"Phase 2: Business Intelligence &amp; Visualization (Next Priority)","text":"<ul> <li>Component: Metabase</li> <li>Purpose: Replace Tableau/Power BI with open-source BI platform</li> <li>Timeline: 1-2 sessions</li> <li>Features:</li> <li>Self-service business intelligence</li> <li>Drag-and-drop dashboard creation</li> <li>SQL editor for advanced users</li> <li>Automated reporting and alerts</li> <li>Direct Spark/Postgres connectivity</li> <li>User-friendly interface for business analysts</li> </ul>"},{"location":"package-datascience/#phase-3-data-catalog-medium-priority","title":"Phase 3: Data Catalog (Medium Priority)","text":"<ul> <li>Component: Apache Hive Metastore</li> <li>Purpose: Centralized metadata management</li> <li>Timeline: 1 session</li> <li>Features:</li> <li>Table discovery and schema management</li> <li>Integration with Spark and Metabase</li> <li>Data lineage tracking</li> <li>Schema versioning</li> </ul>"},{"location":"package-datascience/#why-this-approach","title":"\ud83d\udca1 Why This Approach","text":""},{"location":"package-datascience/#advantages-over-commercial-solutions","title":"Advantages Over Commercial Solutions","text":"<ul> <li>Production-ready: Uses enterprise-grade Apache Foundation projects</li> <li>100% Databricks compatible: PySpark API identical, no learning curve</li> <li>Complete BI replacement: Metabase provides Tableau/Power BI functionality</li> <li>Scalable: Kubernetes-native with automatic resource management</li> <li>Maintainable: Official projects with long-term support</li> <li>Cost-effective: No licensing fees (Databricks, Tableau, Power BI)</li> </ul>"},{"location":"package-datascience/#why-metabase-for-business-intelligence","title":"Why Metabase for Business Intelligence","text":"Feature Metabase Tableau Power BI Cost Free &amp; Open Source $70+/user/month $10-$20/user/month Deployment Self-hosted on Kubernetes Cloud/Server Cloud/On-premise Data Sources Direct Spark/Postgres connection Complex connectors Microsoft ecosystem User Experience Simple, intuitive interface Complex, feature-heavy Microsoft-centric Customization Full source code access Limited Limited Maintenance Community + internal team Vendor dependency Vendor dependency"},{"location":"package-datascience/#integration-benefits","title":"Integration Benefits","text":"<ul> <li>Direct Spark connectivity: No data movement or ETL required</li> <li>Same security model: Kubernetes RBAC applies to all components</li> <li>Unified access: Single sign-on across JupyterHub and Metabase</li> <li>Shared infrastructure: Leverages existing Kubernetes cluster</li> </ul>"},{"location":"package-datascience/#technical-details","title":"\ud83d\udd27 Technical Details","text":""},{"location":"package-datascience/#resource-requirements","title":"Resource Requirements","text":"<ul> <li>Current Usage: 6 CPUs, ~7.7Gi RAM</li> <li>JupyterHub: ~500Mi memory, 1 CPU</li> <li>Spark Operator: ~250Mi memory, 1 CPU</li> <li>Available for Workloads: ~6Gi memory, 4+ CPUs</li> <li>Performance: Excellent for development/testing/production</li> </ul>"},{"location":"package-datascience/#arm64-compatibility","title":"ARM64 Compatibility","text":"<ul> <li>Native Apple Silicon support</li> <li>Spark 4.0 with <code>aarch64</code> architecture</li> <li>All components tested on ARM64</li> </ul>"},{"location":"package-datascience/#security-rbac","title":"Security &amp; RBAC","text":"<ul> <li>Complete service account configuration</li> <li>Proper Kubernetes RBAC</li> <li>Secret-based authentication</li> <li>Namespace isolation</li> </ul>"},{"location":"package-datascience/#success-metrics","title":"\ud83c\udf89 Success Metrics","text":""},{"location":"package-datascience/#phase-1-achieved","title":"Phase 1 Achieved","text":"<ul> <li>\u2705 85% Databricks functionality operational</li> <li>\u2705 Production-ready for data science teams</li> <li>\u2705 Zero cloud dependencies</li> <li>\u2705 100% PySpark compatibility</li> <li>\u2705 Multi-user collaborative environment</li> <li>\u2705 Automatic resource management</li> </ul>"},{"location":"package-datascience/#complete-platform-target-phase-2","title":"Complete Platform Target (Phase 2)","text":"<ul> <li>\ud83c\udfaf 95% Databricks + BI functionality</li> <li>\ud83c\udfaf Complete Tableau/Power BI replacement</li> <li>\ud83c\udfaf Self-service analytics for business users</li> <li>\ud83c\udfaf Unified data platform (notebooks + dashboards + catalog)</li> </ul>"},{"location":"package-datascience/#business-impact","title":"Business Impact","text":"<ul> <li>Cost Savings: No Databricks, Tableau, or Power BI licensing</li> <li>Risk Mitigation: Complete cloud independence</li> <li>Development Efficiency: Local development environment</li> <li>Team Productivity: Familiar interfaces for all user types</li> <li>Operational Excellence: Single Kubernetes platform to maintain</li> </ul>"},{"location":"package-datascience/#getting-started","title":"\ud83d\ude80 Getting Started","text":"<ol> <li>Current State: Deploy the existing stack for 85% functionality (data science)</li> <li>Next Phase: Add Metabase for complete analytics platform (95% functionality)</li> <li>Final Phase: Add Hive Metastore for enterprise data catalog</li> <li>Production Migration: Scale to full enterprise deployment</li> </ol> <p>Your complete analytics platform replacement - Databricks + Tableau/Power BI - is within reach!</p>"},{"location":"package-datascience/#mysql-setup-documentation","title":"MySQL Setup Documentation","text":""},{"location":"package-datascience/#overview","title":"Overview","text":"<p>This document describes the process and files involved in setting up MySQL in the urbalurba infrastructure using Ansible, Helm, and Kubernetes. The setup is modeled after the PostgreSQL workflow for consistency and maintainability.</p>"},{"location":"package-datascience/#files-involved","title":"Files Involved","text":""},{"location":"package-datascience/#1-provision-hostkubernetes02-databases06-setup-mysqlsh","title":"1. provision-host/kubernetes/02-databases/06-setup-mysql.sh","text":"<ul> <li>Purpose: Orchestrates the setup of MySQL on the target Kubernetes cluster.</li> <li>Usage: <pre><code>./06-setup-mysql.sh [target-host]\n</code></pre></li> <li>If no target host is provided, defaults to <code>rancher-desktop</code>.</li> <li>Runs the Ansible playbooks to deploy and verify MySQL.</li> </ul>"},{"location":"package-datascience/#2-ansibleplaybooks040-database-mysqlyml","title":"2. ansible/playbooks/040-database-mysql.yml","text":"<ul> <li>Purpose: Ansible playbook to deploy MySQL using the Bitnami Helm chart.</li> <li>Details:</li> <li>Installs the Bitnami MySQL Helm chart in the <code>default</code> namespace.</li> <li>Configures MySQL to use credentials from the <code>urbalurba-secrets</code> Kubernetes secret.</li> <li>Ensures compatibility with ARM64 (Apple Silicon) and other architectures supported by Bitnami.</li> </ul>"},{"location":"package-datascience/#3-ansibleplaybooksutilityu08-verify-mysqlyml","title":"3. ansible/playbooks/utility/u08-verify-mysql.yml","text":"<ul> <li>Purpose: Ansible playbook to verify the MySQL deployment.</li> <li>Details:</li> <li>Checks that the MySQL pod is running and ready.</li> <li>Executes a test query (<code>SHOW DATABASES;</code>) inside the MySQL pod to confirm connectivity and basic functionality.</li> </ul>"},{"location":"package-datascience/#4-manifests043-database-mysql-configyaml","title":"4. manifests/043-database-mysql-config.yaml","text":"<ul> <li>Purpose: Kubernetes manifest for MySQL configuration.</li> <li>Details:</li> <li>Defines a <code>Service</code> for MySQL to expose port 3306 within the cluster.</li> <li>Optionally includes a <code>ConfigMap</code> for custom MySQL configuration (e.g., <code>my.cnf</code>).</li> </ul>"},{"location":"package-datascience/#5-topsecretkuberneteskubernetes-secretsyml","title":"5. topsecret/kubernetes/kubernetes-secrets.yml","text":"<ul> <li>Purpose: Stores sensitive credentials for MySQL and other services.</li> <li>Details:</li> <li>Add the following keys for MySQL:     <pre><code>MYSQL_ROOT_PASSWORD: SecretPassword1\nMYSQL_DATABASE: mydatabase\nMYSQL_USER: myuser\nMYSQL_PASSWORD: SecretPassword1\nMYSQL_HOST: mysql.default\n</code></pre></li> <li>These are referenced by the Helm chart and Ansible playbooks for secure configuration.</li> </ul>"},{"location":"package-datascience/#references","title":"References","text":"<ul> <li>Bitnami MySQL Helm Chart Documentation</li> </ul>"},{"location":"package-datascience/#see-also","title":"See Also","text":"<ul> <li>PostgreSQL setup documentation for a similar workflow.</li> <li>Other database and service setup guides in this documentation folder.</li> </ul>"},{"location":"package-development-argocd/","title":"ArgoCD - GitOps Continuous Delivery","text":"<p>ArgoCD is a declarative, GitOps continuous delivery tool for Kubernetes. It automates the deployment of applications from Git repositories to your Kubernetes cluster, ensuring that the cluster state matches the desired state defined in Git.</p>"},{"location":"package-development-argocd/#overview","title":"Overview","text":"<p>ArgoCD provides: - GitOps Workflow: Git as the single source of truth for deployments - Automated Sync: Automatically sync applications when Git changes - Rollback Support: Easy rollback to any previous Git commit - Multi-Cluster Support: Manage applications across multiple clusters - Health Monitoring: Real-time application health status - Web UI &amp; CLI: Both GUI and command-line interfaces</p>"},{"location":"package-development-argocd/#access-information","title":"Access Information","text":""},{"location":"package-development-argocd/#web-interface","title":"Web Interface","text":"<ul> <li>Primary Access: http://argocd.localhost</li> <li>Port Forward Access:   <pre><code>kubectl port-forward svc/argocd-server -n argocd 8080:80\n# Then access: http://localhost:8080\n</code></pre></li> <li>External Access: https://argocd.urbalurba.no (when Cloudflare tunnel configured)</li> </ul>"},{"location":"package-development-argocd/#login-credentials","title":"Login Credentials","text":"<ul> <li>Username: <code>admin</code></li> <li>Password: <code>SecretPassword2</code> (same as DEFAULT_ADMIN_PASSWORD)</li> <li>Quick Check:   <pre><code>kubectl get secret urbalurba-secrets -n argocd -o jsonpath='{.data.ARGOCD_ADMIN_PASSWORD}' | base64 -d\n</code></pre></li> </ul>"},{"location":"package-development-argocd/#deployment-scripts","title":"Deployment Scripts","text":""},{"location":"package-development-argocd/#setup-argocd","title":"Setup ArgoCD","text":"<pre><code>cd provision-host/kubernetes/08-development/not-in-use\n./02-setup-argocd.sh\n</code></pre>"},{"location":"package-development-argocd/#remove-argocd","title":"Remove ArgoCD","text":"<pre><code>cd provision-host/kubernetes/08-development/not-in-use\n./02-remove-argocd.sh\n</code></pre>"},{"location":"package-development-argocd/#application-management","title":"Application Management","text":"<p>ArgoCD integrates with the development templates from the urbalurba-dev-templates repository.</p>"},{"location":"package-development-argocd/#register-an-application","title":"Register an Application","text":"<p>Register a GitHub repository with ArgoCD for automatic deployment:</p> <pre><code># From provision-host container\ncd /mnt/urbalurbadisk/scripts/argocd\n\nGITHUB_USERNAME=your_username \\\nREPO_NAME=your_repo \\\nGITHUB_PAT=your_token \\\n./argocd-register-app.sh\n</code></pre> <p>This will: 1. Create a namespace with the repository name 2. Store GitHub credentials securely 3. Create an ArgoCD Application resource 4. Automatically sync and deploy the application 5. Monitor health status</p>"},{"location":"package-development-argocd/#remove-an-application","title":"Remove an Application","text":"<p>Remove an application and clean up resources:</p> <pre><code># From provision-host container\ncd /mnt/urbalurbadisk/scripts/argocd\n\nREPO_NAME=your_repo \\\n./argocd-remove-app.sh\n</code></pre> <p>This will: 1. Delete the ArgoCD Application 2. Remove GitHub credentials 3. Delete the namespace and all resources</p>"},{"location":"package-development-argocd/#gitops-workflow","title":"GitOps Workflow","text":""},{"location":"package-development-argocd/#how-it-works","title":"How It Works","text":"<ol> <li>Code Push: Developer pushes code to GitHub</li> <li>CI/CD Pipeline: GitHub Actions builds and pushes container image</li> <li>ArgoCD Sync: ArgoCD detects changes in the Git repository</li> <li>Deployment: ArgoCD applies the Kubernetes manifests to the cluster</li> <li>Health Check: ArgoCD monitors application health</li> </ol>"},{"location":"package-development-argocd/#application-structure","title":"Application Structure","text":"<p>Your repository should contain Kubernetes manifests in one of these locations: - Root directory - <code>/k8s</code> directory - <code>/manifests</code> directory - <code>/deploy</code> directory</p> <p>Example structure: <pre><code>your-repo/\n\u251c\u2500\u2500 src/           # Application source code\n\u251c\u2500\u2500 Dockerfile     # Container definition\n\u251c\u2500\u2500 k8s/           # Kubernetes manifests\n\u2502   \u251c\u2500\u2500 deployment.yaml\n\u2502   \u251c\u2500\u2500 service.yaml\n\u2502   \u2514\u2500\u2500 ingress.yaml\n\u2514\u2500\u2500 .github/\n    \u2514\u2500\u2500 workflows/\n        \u2514\u2500\u2500 deploy.yml  # GitHub Actions workflow\n</code></pre></p>"},{"location":"package-development-argocd/#integration-with-development-templates","title":"Integration with Development Templates","text":"<p>The development templates in urbalurba-dev-templates are pre-configured for ArgoCD deployment:</p> <ol> <li>Select Template: Use <code>.devcontainer/dev/dev-template.sh</code> to choose a template</li> <li>Develop: Write your code using the devcontainer toolbox</li> <li>Push: Commit and push to GitHub</li> <li>Deploy: ArgoCD automatically deploys your application</li> </ol> <p>Supported languages: - TypeScript/Node.js - Python - Java - C# - Go - PHP</p>"},{"location":"package-development-argocd/#architecture","title":"Architecture","text":""},{"location":"package-development-argocd/#components","title":"Components","text":"<ul> <li>ArgoCD Server: API server and web UI</li> <li>ArgoCD Repo Server: Manages Git repositories</li> <li>ArgoCD Application Controller: Monitors applications and syncs state</li> <li>ArgoCD Redis: Cache layer for improved performance</li> <li>ArgoCD ApplicationSet Controller: Manages multiple applications</li> </ul>"},{"location":"package-development-argocd/#security","title":"Security","text":"<ul> <li>Standardized Credentials: Uses urbalurba-secrets for consistency</li> <li>GitHub PAT Storage: Secure storage of GitHub credentials</li> <li>RBAC: Role-based access control for multi-user environments</li> <li>TLS: HTTPS support for external access</li> </ul>"},{"location":"package-development-argocd/#troubleshooting","title":"Troubleshooting","text":""},{"location":"package-development-argocd/#check-argocd-status","title":"Check ArgoCD Status","text":"<pre><code># Check pods\nkubectl get pods -n argocd\n\n# Check services\nkubectl get svc -n argocd\n\n# View logs\nkubectl logs -f deployment/argocd-server -n argocd\n</code></pre>"},{"location":"package-development-argocd/#common-issues","title":"Common Issues","text":"<p>Login Failed - Clear browser cache/cookies - Use incognito/private window - Verify password: <code>kubectl get secret urbalurba-secrets -n argocd -o jsonpath='{.data.ARGOCD_ADMIN_PASSWORD}' | base64 -d</code></p> <p>Application Not Syncing - Check GitHub credentials are correct - Verify repository structure has Kubernetes manifests - Check ArgoCD Application status in web UI</p> <p>Port Forward Not Working - Ensure no other process is using the port - Try a different port: <code>kubectl port-forward svc/argocd-server -n argocd 8081:80</code></p>"},{"location":"package-development-argocd/#reset-admin-password","title":"Reset Admin Password","text":"<p>If you need to reset the admin password:</p> <pre><code># Generate new bcrypt hash\necho \"YourNewPassword\" | htpasswd -niBC 10 admin | cut -d ':' -f 2\n\n# Update secret (create a file with the hash)\nkubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: argocd-secret\n  namespace: argocd\ntype: Opaque\nstringData:\n  admin.password: \"YOUR_BCRYPT_HASH_HERE\"\n  admin.passwordMtime: \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"\nEOF\n\n# Restart server\nkubectl rollout restart deployment/argocd-server -n argocd\n</code></pre>"},{"location":"package-development-argocd/#related-documentation","title":"Related Documentation","text":"<ul> <li>Development Templates - Pre-configured application templates</li> <li>DevContainer Toolbox - Development environment setup</li> <li>ArgoCD Official Docs - Comprehensive ArgoCD documentation</li> </ul>"},{"location":"package-development-argocd/#summary","title":"Summary","text":"<p>ArgoCD provides GitOps-based continuous delivery for the Urbalurba Infrastructure, enabling: - Automated deployments from Git repositories - Consistent application state across environments - Easy rollback and version control - Integration with development templates for rapid prototyping - Standardized credentials for novice-friendly access</p> <p>The combination of ArgoCD with the development templates creates a complete CI/CD pipeline from development to deployment on your local Kubernetes cluster.</p>"},{"location":"package-development-readme/","title":"Development Services - Complete Development and Testing Environment","text":"<p>File: <code>docs/package-development-readme.md</code> Purpose: Overview of all development and testing services in Urbalurba infrastructure Target Audience: Software developers, DevOps engineers, QA engineers Last Updated: December 19, 2024</p>"},{"location":"package-development-readme/#overview","title":"\ud83d\udccb Overview","text":"<p>Urbalurba infrastructure provides a comprehensive development and testing environment that enables rapid software development, testing, and deployment. The development stack is designed to mirror production environments using open source software, providing a complete CI/CD pipeline from development to deployment.</p> <p>Available Development Services: - ArgoCD: GitOps continuous delivery for automated deployments - Development Templates: Pre-configured application templates for multiple languages - DevContainer Toolbox: Integrated development environment with language support</p>"},{"location":"package-development-readme/#development-services","title":"\ud83d\udda5\ufe0f Development Services","text":""},{"location":"package-development-readme/#argocd-gitops-continuous-delivery","title":"ArgoCD - GitOps Continuous Delivery \ud83d\ude80","text":"<p>Status: Optional (not-in-use) | Port: 80 | Type: Continuous Delivery</p> <p>GitOps Workflow: Git as Single Source of Truth \u2022 Automated Sync \u2022 Multi-Cluster Support \u2022 Health Monitoring</p> <p>ArgoCD provides a declarative GitOps continuous delivery tool for Kubernetes that automates application deployment from Git repositories, ensuring cluster state matches the desired state defined in Git.</p> <p>Key Features: - GitOps Workflow: Git as the single source of truth for deployments - Automated Sync: Automatically sync applications when Git changes - Rollback Support: Easy rollback to any previous Git commit - Multi-Cluster Support: Manage applications across multiple clusters - Health Monitoring: Real-time application health status - Web UI &amp; CLI: Both GUI and command-line interfaces</p> <p>Access Information: - Web Interface: <code>http://argocd.localhost</code> - Username: <code>admin</code> - Password: <code>SecretPassword2</code> (from urbalurba-secrets) - External Access: <code>https://argocd.urbalurba.no</code> (when Cloudflare tunnel configured)</p> <p>\ud83d\udcda Complete Documentation \u2192</p>"},{"location":"package-development-readme/#development-templates-multi-language-application-templates","title":"Development Templates - Multi-Language Application Templates \ud83d\udee0\ufe0f","text":"<p>Status: Active | Type: Development Framework</p> <p>Multi-Language Support: TypeScript \u2022 Python \u2022 Java \u2022 C# \u2022 Go \u2022 PHP \u2022 Pre-configured CI/CD</p> <p>The development templates provide pre-configured application templates for rapid software development across multiple programming languages, integrated with the devcontainer-toolbox project.</p> <p>Key Features: - Multi-Language Support: TypeScript, Python, Java, C#, Go, PHP - Pre-configured CI/CD: GitHub Actions workflows for automated building and deployment - DevContainer Integration: Seamless integration with devcontainer-toolbox - Kubernetes Ready: Pre-configured Kubernetes manifests for deployment - Database Integration: Built-in support for MySQL and other databases - Template Selection: Easy template selection via <code>.devcontainer/dev/dev-template.sh</code></p> <p>Supported Languages: - TypeScript/Node.js: Modern web development with Express.js - Python: FastAPI and Flask web applications - Java: Spring Boot enterprise applications - C#: .NET Core web applications - Go: High-performance web services - PHP: Laravel and Symfony web applications</p> <p>\ud83d\udcda Complete Documentation \u2192</p>"},{"location":"package-development-readme/#devcontainer-toolbox-integrated-development-environment","title":"DevContainer Toolbox - Integrated Development Environment \ud83e\uddf0","text":"<p>Status: Active | Type: Development Environment</p> <p>Language Support: Multiple Programming Languages \u2022 Integrated Tools \u2022 Containerized Development</p> <p>The devcontainer-toolbox provides a comprehensive development environment with support for multiple programming languages and frameworks, enabling consistent development experiences across different projects.</p> <p>Key Features: - Multi-Language Support: Python, Node.js, Java, C#, Go, PHP - Containerized Development: Consistent development environment - Integrated Tools: Pre-configured development tools and extensions - Template Integration: Seamless integration with development templates - Rapid Setup: Quick project initialization and configuration</p>"},{"location":"package-development-readme/#deployment-architecture","title":"\ud83c\udfd7\ufe0f Deployment Architecture","text":""},{"location":"package-development-readme/#service-activation","title":"Service Activation","text":"<pre><code>Development Service Status:\n\u251c\u2500\u2500 ArgoCD (OPTIONAL) - Located in not-in-use/ folder\n\u251c\u2500\u2500 Development Templates (ACTIVE) - Available via GitHub repository\n\u2514\u2500\u2500 DevContainer Toolbox (ACTIVE) - Integrated development environment\n</code></pre>"},{"location":"package-development-readme/#access-methods","title":"Access Methods","text":"<p>All development interfaces use Traefik IngressRoute for DNS-based routing: - ArgoCD: <code>http://argocd.localhost</code> - Development Templates: Available via GitHub repository - DevContainer Toolbox: Integrated in development environment</p>"},{"location":"package-development-readme/#authentication-models","title":"Authentication Models","text":"<pre><code>Authentication Approaches:\n\u251c\u2500\u2500 ArgoCD: urbalurba-secrets (ARGOCD_ADMIN_PASSWORD)\n\u251c\u2500\u2500 Development Templates: GitHub repository access\n\u2514\u2500\u2500 DevContainer Toolbox: Integrated authentication\n</code></pre>"},{"location":"package-development-readme/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"package-development-readme/#activate-gitops-continuous-delivery-argocd","title":"Activate GitOps Continuous Delivery (ArgoCD)","text":"<pre><code># Move from not-in-use to activate\ncd provision-host/kubernetes/08-development/not-in-use/\nmv 02-setup-argocd.sh ./\n\n# Deploy ArgoCD\n./02-setup-argocd.sh\n\n# Access via browser\nopen http://argocd.localhost\n# Login: admin / SecretPassword2\n</code></pre>"},{"location":"package-development-readme/#start-development-with-templates","title":"Start Development with Templates","text":"<pre><code># Clone development templates\ngit clone https://github.com/terchris/urbalurba-dev-templates.git\ncd urbalurba-dev-templates\n\n# Select a template\n.devcontainer/dev/dev-template.sh\n\n# Choose your language and framework\n# Start developing and push to GitHub\n# ArgoCD will automatically deploy your application\n</code></pre>"},{"location":"package-development-readme/#access-devcontainer-toolbox","title":"Access DevContainer Toolbox","text":"<pre><code># The devcontainer-toolbox is integrated with development templates\n# No separate installation required\n# Available when using development templates\n</code></pre>"},{"location":"package-development-readme/#development-service-selection-guide","title":"\ud83d\udd0d Development Service Selection Guide","text":""},{"location":"package-development-readme/#when-to-use-argocd","title":"When to Use ArgoCD \u2705","text":"<ul> <li>GitOps Workflow: Prefer Git as single source of truth</li> <li>Automated Deployments: Need automatic sync when Git changes</li> <li>Multi-Environment: Managing applications across multiple clusters</li> <li>Rollback Requirements: Need easy rollback to previous versions</li> <li>Team Collaboration: Multiple developers working on same project</li> </ul>"},{"location":"package-development-readme/#when-to-use-development-templates","title":"When to Use Development Templates \ud83d\udee0\ufe0f","text":"<ul> <li>Rapid Prototyping: Quick application development and testing</li> <li>Multi-Language Development: Working with different programming languages</li> <li>CI/CD Integration: Need automated building and deployment</li> <li>Learning: Understanding different frameworks and patterns</li> <li>Standardization: Consistent project structure across teams</li> </ul>"},{"location":"package-development-readme/#when-to-use-devcontainer-toolbox","title":"When to Use DevContainer Toolbox \ud83e\uddf0","text":"<ul> <li>Consistent Environment: Need reproducible development setup</li> <li>Language Support: Working with multiple programming languages</li> <li>Integrated Tools: Pre-configured development tools and extensions</li> <li>Containerized Development: Prefer containerized development approach</li> <li>Template Integration: Using development templates effectively</li> </ul>"},{"location":"package-development-readme/#development-operations","title":"\ud83d\udee0\ufe0f Development Operations","text":""},{"location":"package-development-readme/#common-access-patterns","title":"Common Access Patterns","text":"<pre><code># Check ArgoCD status\nkubectl get pods -n argocd\nkubectl get svc -n argocd\n\n# Verify ArgoCD accessibility\ncurl -H \"Host: argocd.localhost\" http://localhost/\n\n# Check application deployments\nkubectl get applications -n argocd\nkubectl get apps -n argocd\n</code></pre>"},{"location":"package-development-readme/#application-management","title":"Application Management","text":"<pre><code># Register a new application with ArgoCD\ncd /mnt/urbalurbadisk/scripts/argocd\n\nGITHUB_USERNAME=your_username \\\nREPO_NAME=your_repo \\\nGITHUB_PAT=your_token \\\n./argocd-register-app.sh\n\n# Remove an application\nREPO_NAME=your_repo \\\n./argocd-remove-app.sh\n</code></pre>"},{"location":"package-development-readme/#development-workflow","title":"Development Workflow","text":"<pre><code># 1. Select development template\n.devcontainer/dev/dev-template.sh\n\n# 2. Develop your application\n# Write code using the devcontainer toolbox\n\n# 3. Push to GitHub\ngit add .\ngit commit -m \"Initial commit\"\ngit push origin main\n\n# 4. ArgoCD automatically deploys\n# Check status at http://argocd.localhost\n\n# 5. Access your application\n# Check ingress routes for your application URL\n</code></pre>"},{"location":"package-development-readme/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-development-readme/#common-issues","title":"Common Issues","text":"<p>ArgoCD Won't Load: <pre><code># Check pod status\nkubectl describe pod -l app.kubernetes.io/name=argocd-server -n argocd\n\n# Verify service endpoints\nkubectl get endpoints argocd-server -n argocd\n\n# Check IngressRoute configuration\nkubectl get ingressroute argocd -n argocd\n</code></pre></p> <p>Application Not Syncing: <pre><code># Check GitHub credentials\nkubectl get secret -n argocd\n\n# Verify repository structure has Kubernetes manifests\n# Check ArgoCD Application status in web UI\n\n# View ArgoCD logs\nkubectl logs -f deployment/argocd-server -n argocd\n</code></pre></p> <p>Development Template Issues: <pre><code># Verify GitHub repository access\ngit clone https://github.com/terchris/urbalurba-dev-templates.git\n\n# Check devcontainer configuration\nls -la .devcontainer/\n\n# Verify template selection script\n.devcontainer/dev/dev-template.sh\n</code></pre></p>"},{"location":"package-development-readme/#service-specific-troubleshooting","title":"Service-Specific Troubleshooting","text":"<p>ArgoCD Issues: - Login failures \u2192 Verify ARGOCD_ADMIN_PASSWORD in secrets - Application sync problems \u2192 Check GitHub credentials and repository structure - Port forward issues \u2192 Ensure no other process is using the port</p> <p>Development Template Issues: - Template selection problems \u2192 Check devcontainer-toolbox installation - CI/CD failures \u2192 Verify GitHub Actions configuration - Deployment issues \u2192 Check Kubernetes manifests and ArgoCD configuration</p> <p>DevContainer Toolbox Issues: - Language support missing \u2192 Check devcontainer configuration - Tool integration problems \u2192 Verify extension installation - Container issues \u2192 Check Docker and devcontainer setup</p>"},{"location":"package-development-readme/#maintenance","title":"\ud83d\udccb Maintenance","text":""},{"location":"package-development-readme/#regular-tasks","title":"Regular Tasks","text":"<ol> <li>Health Monitoring: Check ArgoCD pod and service status regularly</li> <li>Application Monitoring: Monitor deployed applications and their health</li> <li>Template Updates: Keep development templates up to date</li> <li>Security Updates: Update container images and configurations</li> <li>Credential Management: Rotate GitHub tokens and ArgoCD passwords</li> </ol>"},{"location":"package-development-readme/#backup-procedures","title":"Backup Procedures","text":"<pre><code># ArgoCD configuration backup\nkubectl exec -it deployment/argocd-server -n argocd -- \\\n  tar -czf /tmp/argocd-backup.tar.gz /app/config\nkubectl cp deployment/argocd-server:/tmp/argocd-backup.tar.gz ./argocd-backup.tar.gz\n\n# Application definitions backup\nkubectl get applications -n argocd -o yaml &gt; argocd-applications-backup.yaml\n\n# Development templates backup\ngit clone https://github.com/terchris/urbalurba-dev-templates.git\ntar -czf urbalurba-dev-templates-backup.tar.gz urbalurba-dev-templates/\n</code></pre>"},{"location":"package-development-readme/#service-removal","title":"Service Removal","text":"<pre><code># Remove ArgoCD\ncd provision-host/kubernetes/08-development/not-in-use/\n./02-remove-argocd.sh\n\n# Remove applications\ncd /mnt/urbalurbadisk/scripts/argocd\nREPO_NAME=your_repo ./argocd-remove-app.sh\n\n# Development templates and devcontainer-toolbox are external dependencies\n# No removal needed from the infrastructure\n</code></pre>"},{"location":"package-development-readme/#integration-points","title":"\ud83d\udd17 Integration Points","text":""},{"location":"package-development-readme/#database-integration","title":"Database Integration","text":"<ul> <li>MySQL Setup: See MySQL Setup Documentation</li> <li>PostgreSQL: Integrated with management services</li> <li>Redis: Available for caching and session storage</li> </ul>"},{"location":"package-development-readme/#external-dependencies","title":"External Dependencies","text":"<ul> <li>GitHub: Required for repository access and CI/CD</li> <li>Docker Hub: Container image registry</li> <li>DevContainer Toolbox: devcontainer-toolbox project</li> <li>Development Templates: urbalurba-dev-templates</li> </ul> <p>\ud83d\udca1 Key Insight: The development layer provides a complete CI/CD pipeline from code development to deployment, with ArgoCD handling GitOps-based continuous delivery, development templates offering multi-language application frameworks, and devcontainer-toolbox providing integrated development environments. This combination enables rapid prototyping, testing, and deployment of software applications with minimal configuration overhead.</p>"},{"location":"package-development-templates/","title":"Dev and Test Package","text":"<p>This package contains the tools and services for developing and testing software. The dev and test stack is set up to be as close to the production stack as possible using open source software.</p> <p>The goal is to provide a complete development environment for building, testing, and deploying software.</p> <p>The stack includes ArgoCD for managing the deployment of the software.</p> <p>The dev and test pacage provides the deployment and the services (databases +++) to support the development of the software.</p> <p>Enabling rapid development, testing and deployment of the software.</p>"},{"location":"package-development-templates/#development-environment","title":"Development environment","text":"<p>The development environment builds on the devcontainer-toolbox project.</p> <p>The devcontainer-toolbox project provides a set of tools and services to develop sw using a vide selection of programming languages and frameworks:</p> <ul> <li>Python</li> <li>Node.js</li> <li>Java</li> <li>C#</li> <li>Go</li> <li>PHP</li> </ul>"},{"location":"package-development-templates/#development-templates","title":"Development templates","text":"<p>The urbalurba-dev-templates repository provides a set of templates for developing software using the devcontainer-toolbox project.</p> <p>Here you will find sample programs that demonstrates the development of backend and frontend applications. All examples are implemented in TypeScript, Python, Java, C#, Go and PHP.</p> <p>The templates are designed to be used with the devcontainer-toolbox project.</p> <p>Just type \u00b4.devcontainer/dev/dev-template.sh\u00b4 in the devcontainer-toolbox and select a template. And you are ready to go.</p>"},{"location":"package-development-templates/#deployment","title":"Deployment","text":"<p>The templates are designed so that when you push to GitHub your code will be built, containerized and deployed to the kubernetes cluster on your local machine.</p> <p>The deployment is done using ArgoCD. And once deployed you can open your browser and test the application.</p>"},{"location":"package-development-templates/#database-setup","title":"Database Setup","text":"<ul> <li>See MySQL Setup Documentation for details on deploying and verifying MySQL in the cluster.</li> </ul>"},{"location":"package-management-pgadmin/","title":"pgAdmin - PostgreSQL Database Administration Interface","text":"<p>Web Interface: PostgreSQL Management \u2022 Multi-Database Support \u2022 SQL Editor \u2022 Visual Query Builder \u2022 User Management \u2022 Backup/Restore \u2022 Performance Monitoring</p> <p>File: <code>docs/package-management-pgadmin.md</code> Purpose: Complete guide to pgAdmin deployment and configuration in Urbalurba infrastructure Target Audience: Database administrators, developers needing database management tools, PostgreSQL users Last Updated: September 23, 2024</p>"},{"location":"package-management-pgadmin/#overview","title":"\ud83d\udccb Overview","text":"<p>pgAdmin provides a comprehensive web-based administration interface for PostgreSQL databases in the Urbalurba infrastructure. It's designed as an optional management service that offers full database administration capabilities through a modern web interface.</p> <p>Key Features: - Web-Based Interface: Full-featured PostgreSQL administration via browser - Auto-Connected to PostgreSQL: Pre-configured connection to cluster PostgreSQL instance - Helm-Based Deployment: Uses runix/pgadmin4 chart for reliable deployment - Secret Management: Integrates with urbalurba-secrets for secure authentication - DNS-Based Routing: Accessible via <code>pgadmin.localhost</code> with multi-domain support - 10GB Storage: Persistent storage for query history, preferences, and configurations - Production Ready: Includes proper security context and resource limits</p>"},{"location":"package-management-pgadmin/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"package-management-pgadmin/#deployment-components","title":"Deployment Components","text":"<pre><code>pgAdmin Service Stack:\n\u251c\u2500\u2500 Helm Release (runix/pgadmin4 with custom configuration)\n\u251c\u2500\u2500 Deployment (pgadmin container with security context)\n\u251c\u2500\u2500 Service (ClusterIP on port 80)\n\u251c\u2500\u2500 IngressRoute (Traefik routing for pgadmin.localhost)\n\u251c\u2500\u2500 ConfigMap (PostgreSQL server definitions)\n\u251c\u2500\u2500 PersistentVolumeClaim (10GB storage)\n\u251c\u2500\u2500 urbalurba-secrets (authentication credentials)\n\u2514\u2500\u2500 Pod (pgadmin4 container with auto-configured PostgreSQL connection)\n</code></pre>"},{"location":"package-management-pgadmin/#file-structure","title":"File Structure","text":"<pre><code>06-management/\n\u2514\u2500\u2500 not-in-use/                    # Inactive by default\n    \u251c\u2500\u2500 03-setup-pgadmin.sh        # Main deployment script\n    \u2514\u2500\u2500 03-remove-pgadmin.sh       # Removal script\n\nmanifests/\n\u251c\u2500\u2500 641-adm-pgadmin.yaml           # pgAdmin Helm configuration\n\u2514\u2500\u2500 741-pgadmin-ingressroute.yaml  # Traefik routing configuration\n\nansible/playbooks/\n\u251c\u2500\u2500 641-adm-pgadmin.yml            # Main deployment logic\n\u2514\u2500\u2500 641-remove-pgadmin.yml         # Removal logic\n</code></pre>"},{"location":"package-management-pgadmin/#deployment","title":"\ud83d\ude80 Deployment","text":""},{"location":"package-management-pgadmin/#service-activation","title":"Service Activation","text":"<p>pgAdmin is inactive by default. To activate and deploy:</p> <pre><code># Move script from not-in-use to activate\ncd provision-host/kubernetes/06-management/\nmv not-in-use/03-setup-pgadmin.sh ./\n\n# Deploy pgAdmin\n./03-setup-pgadmin.sh rancher-desktop\n</code></pre>"},{"location":"package-management-pgadmin/#manual-deployment","title":"Manual Deployment","text":"<pre><code># Deploy to specific Kubernetes context\n./03-setup-pgadmin.sh multipass-microk8s\n./03-setup-pgadmin.sh azure-aks\n\n# Direct Ansible playbook execution\ncd /mnt/urbalurbadisk/ansible\nansible-playbook playbooks/641-adm-pgadmin.yml -e target_host=rancher-desktop\n</code></pre>"},{"location":"package-management-pgadmin/#prerequisites","title":"Prerequisites","text":"<p>Before deploying pgAdmin, ensure PostgreSQL is running and the required secrets are configured in <code>urbalurba-secrets</code>:</p> <ul> <li><code>PGADMIN_DEFAULT_EMAIL</code>: pgAdmin login email address</li> <li><code>PGADMIN_DEFAULT_PASSWORD</code>: pgAdmin login password</li> </ul>"},{"location":"package-management-pgadmin/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"package-management-pgadmin/#helm-configuration","title":"Helm Configuration","text":"<p>pgAdmin uses the runix/pgadmin4 Helm chart with comprehensive configuration:</p> <pre><code># Deployment command (from Ansible playbook)\nhelm install pgadmin runix/pgadmin4 \\\n  --namespace default \\\n  -f manifests/641-adm-pgadmin.yaml \\\n  --set env.email=\"$PGADMIN_USERNAME\" \\\n  --set env.password=\"$PGADMIN_PASSWORD\"\n</code></pre>"},{"location":"package-management-pgadmin/#auto-connection-configuration","title":"Auto-Connection Configuration","text":"<p>pgAdmin is pre-configured to automatically connect to the cluster PostgreSQL instance:</p> <pre><code># From manifests/641-adm-pgadmin.yaml\nserverDefinitions:\n  enabled: true\n  resourceType: ConfigMap\n  servers:\n    postgresql-server:\n      Name: \"PostgreSQL Database\"\n      Group: \"Servers\"\n      Username: \"postgres\"\n      Host: \"postgresql.default.svc.cluster.local\"\n      Port: 5432\n      SSLMode: \"prefer\"\n      MaintenanceDB: \"postgres\"\n      Comment: \"Pre-configured PostgreSQL server connection\"\n</code></pre>"},{"location":"package-management-pgadmin/#storage-configuration","title":"Storage Configuration","text":"<pre><code># Persistent storage for pgAdmin data\npersistentVolume:\n  enabled: true\n  size: 10Gi\n</code></pre>"},{"location":"package-management-pgadmin/#security-configuration","title":"Security Configuration","text":"<pre><code># Security context for production deployment\nsecurityContext:\n  runAsUser: 5050\n  runAsGroup: 5050\n  fsGroup: 5050\n\ncontainerSecurityContext:\n  allowPrivilegeEscalation: false\n</code></pre>"},{"location":"package-management-pgadmin/#network-configuration","title":"Network Configuration","text":"<pre><code># Traefik IngressRoute configuration\n# From manifests/741-pgadmin-ingressroute.yaml\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: HostRegexp(`pgadmin\\..+`)\n      kind: Rule\n      services:\n        - name: pgadmin-pgadmin4\n          port: 80\n</code></pre>"},{"location":"package-management-pgadmin/#access-usage","title":"\ud83c\udf10 Access &amp; Usage","text":""},{"location":"package-management-pgadmin/#web-interface-access","title":"Web Interface Access","text":"<pre><code># Primary access via DNS routing\nhttp://pgadmin.localhost\n\n# Port-forward access (alternative)\nkubectl port-forward svc/pgadmin-pgadmin4 8080:80\n# Then access: http://localhost:8080\n</code></pre>"},{"location":"package-management-pgadmin/#login-credentials","title":"Login Credentials","text":"<p>Use the credentials configured in urbalurba-secrets: - Email: Value from <code>PGADMIN_DEFAULT_EMAIL</code> - Password: Value from <code>PGADMIN_DEFAULT_PASSWORD</code></p>"},{"location":"package-management-pgadmin/#database-connection","title":"Database Connection","text":"<p>pgAdmin comes pre-configured with a PostgreSQL server connection: - Server Name: PostgreSQL Database - Host: postgresql.default.svc.cluster.local - Port: 5432 - Username: postgres - Database: postgres (maintenance database)</p>"},{"location":"package-management-pgadmin/#monitoring-verification","title":"\ud83d\udd0d Monitoring &amp; Verification","text":""},{"location":"package-management-pgadmin/#health-checks","title":"Health Checks","text":"<pre><code># Check pod status\nkubectl get pods -l app.kubernetes.io/name=pgadmin4\n\n# Check service status\nkubectl get svc pgadmin-pgadmin4\n\n# Check IngressRoute\nkubectl get ingressroute pgadmin\n\n# View pgAdmin logs\nkubectl logs -l app.kubernetes.io/name=pgadmin4\n</code></pre>"},{"location":"package-management-pgadmin/#connection-testing","title":"Connection Testing","text":"<pre><code># Test HTTP response from within cluster\nkubectl run curl-test --image=curlimages/curl --rm -it --restart=Never -- \\\n  curl -s -w \"HTTP_CODE:%{http_code}\" http://pgadmin-pgadmin4:80/\n\n# Test DNS routing\ncurl -H \"Host: pgadmin.localhost\" http://localhost/\n</code></pre>"},{"location":"package-management-pgadmin/#pgadmin-interface-testing","title":"pgAdmin Interface Testing","text":"<ol> <li>Login Test: Access <code>http://pgadmin.localhost</code> and verify login</li> <li>Server Connection: Check pre-configured PostgreSQL server connection</li> <li>Database Operations: Create test database and verify functionality</li> <li>Query Editor: Test SQL query execution and results display</li> </ol>"},{"location":"package-management-pgadmin/#management-operations","title":"\ud83d\udee0\ufe0f Management Operations","text":""},{"location":"package-management-pgadmin/#database-administration","title":"Database Administration","text":"<pre><code># Access pgAdmin web interface\nopen http://pgadmin.localhost\n\n# Create new database via pgAdmin UI:\n# 1. Right-click \"PostgreSQL Database\" server\n# 2. Create -&gt; Database...\n# 3. Enter database name and save\n</code></pre>"},{"location":"package-management-pgadmin/#user-management","title":"User Management","text":"<p>Through pgAdmin web interface: 1. Navigate to Login/Group Roles 2. Right-click to create new roles 3. Configure permissions and database access 4. Set passwords and connection limits</p>"},{"location":"package-management-pgadmin/#backup-operations","title":"Backup Operations","text":"<p>Through pgAdmin web interface: 1. Right-click database 2. Select \"Backup...\" 3. Configure backup options 4. Download backup file</p>"},{"location":"package-management-pgadmin/#advanced-operations","title":"Advanced Operations","text":"<pre><code># View pgAdmin configuration\nkubectl exec -it deployment/pgadmin-pgadmin4 -- cat /pgadmin4/pgadmin4.db\n\n# Check pgAdmin storage usage\nkubectl exec -it deployment/pgadmin-pgadmin4 -- df -h /var/lib/pgadmin\n\n# View pgAdmin process status\nkubectl exec -it deployment/pgadmin-pgadmin4 -- ps aux\n</code></pre>"},{"location":"package-management-pgadmin/#service-removal","title":"Service Removal","text":"<pre><code># Remove pgAdmin service completely\ncd provision-host/kubernetes/06-management/not-in-use/\n./03-remove-pgadmin.sh rancher-desktop\n\n# Direct Ansible playbook removal\ncd /mnt/urbalurbadisk/ansible\nansible-playbook playbooks/641-remove-pgadmin.yml -e target_host=rancher-desktop\n</code></pre> <p>Removal Process: - Uninstalls pgAdmin Helm release - Removes IngressRoute configuration - Deletes persistent volume claims and data - Waits for pods to terminate - Preserves urbalurba-secrets and namespace structure</p>"},{"location":"package-management-pgadmin/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-management-pgadmin/#common-issues","title":"Common Issues","text":"<p>Pod Won't Start: <pre><code># Check pod events and logs\nkubectl describe pod -l app.kubernetes.io/name=pgadmin4\nkubectl logs -l app.kubernetes.io/name=pgadmin4\n\n# Check storage issues\nkubectl describe pvc -l app.kubernetes.io/name=pgadmin4\n</code></pre></p> <p>Cannot Access Web Interface: <pre><code># Verify service endpoints\nkubectl describe svc pgadmin-pgadmin4\nkubectl get endpoints pgadmin-pgadmin4\n\n# Test service connectivity\nkubectl run test-pod --image=curlimages/curl --rm -it -- \\\n  curl http://pgadmin-pgadmin4.default.svc.cluster.local:80/misc/ping\n\n# Check IngressRoute configuration\nkubectl describe ingressroute pgadmin\n</code></pre></p> <p>Login Issues: <pre><code># Verify credentials in secrets\nkubectl get secret urbalurba-secrets -o jsonpath='{.data.PGADMIN_DEFAULT_EMAIL}' | base64 -d\nkubectl get secret urbalurba-secrets -o jsonpath='{.data.PGADMIN_DEFAULT_PASSWORD}' | base64 -d\n\n# Check pgAdmin configuration\nkubectl exec -it deployment/pgadmin-pgadmin4 -- \\\n  grep -r \"PGADMIN_DEFAULT_EMAIL\" /etc/pgadmin/\n</code></pre></p> <p>PostgreSQL Connection Issues: <pre><code># Test PostgreSQL connectivity from pgAdmin pod\nkubectl exec -it deployment/pgadmin-pgadmin4 -- \\\n  nc -zv postgresql.default.svc.cluster.local 5432\n\n# Verify PostgreSQL is running\nkubectl get pods -l app.kubernetes.io/name=postgresql\nkubectl logs -l app.kubernetes.io/name=postgresql --tail=20\n\n# Test PostgreSQL authentication\nkubectl exec -it postgresql-0 -- psql -U postgres -c \"SELECT version();\"\n</code></pre></p> <p>Performance Issues: <pre><code># Check resource usage\nkubectl top pod -l app.kubernetes.io/name=pgadmin4\n\n# View detailed pod specifications\nkubectl describe pod -l app.kubernetes.io/name=pgadmin4\n\n# Check storage performance\nkubectl exec -it deployment/pgadmin-pgadmin4 -- iostat -x 1 3\n</code></pre></p> <p>DNS Resolution Issues: <pre><code># Test DNS resolution\nkubectl run test-pod --image=busybox --rm -it -- \\\n  nslookup pgadmin-pgadmin4.default.svc.cluster.local\n\n# Verify IngressRoute host matching\nkubectl get ingressroute pgadmin -o yaml | grep -A 5 \"match:\"\n\n# Test with different domain patterns\ncurl -H \"Host: pgadmin.localhost\" http://127.0.0.1/\ncurl -H \"Host: pgadmin.urbalurba.no\" http://127.0.0.1/\n</code></pre></p>"},{"location":"package-management-pgadmin/#maintenance","title":"\ud83d\udccb Maintenance","text":""},{"location":"package-management-pgadmin/#regular-tasks","title":"Regular Tasks","text":"<ol> <li>Health Monitoring: Check pod and service status regularly</li> <li>Storage Monitoring: Monitor disk usage for query history and configurations</li> <li>Access Review: Regularly review user access and permissions</li> <li>Connection Testing: Verify PostgreSQL connectivity and performance</li> </ol>"},{"location":"package-management-pgadmin/#backup-procedures","title":"Backup Procedures","text":"<pre><code># Backup pgAdmin configuration and user data\nkubectl exec -it deployment/pgadmin-pgadmin4 -- \\\n  tar -czf /tmp/pgadmin-backup.tar.gz /var/lib/pgadmin\n\n# Copy backup to local system\nkubectl cp deployment/pgadmin-pgadmin4:/tmp/pgadmin-backup.tar.gz ./pgadmin-backup.tar.gz\n\n# Backup database configurations\nkubectl get configmap -l app.kubernetes.io/name=pgadmin4 -o yaml &gt; pgadmin-config-backup.yaml\n</code></pre>"},{"location":"package-management-pgadmin/#updates-and-upgrades","title":"Updates and Upgrades","text":"<pre><code># Update Helm repository\nhelm repo update runix\n\n# Check for chart updates\nhelm search repo runix/pgadmin4\n\n# Upgrade pgAdmin (if new chart version available)\nhelm upgrade pgadmin runix/pgadmin4 \\\n  -f manifests/641-adm-pgadmin.yaml \\\n  --set env.email=\"$PGADMIN_USERNAME\" \\\n  --set env.password=\"$PGADMIN_PASSWORD\"\n</code></pre>"},{"location":"package-management-pgadmin/#disaster-recovery","title":"Disaster Recovery","text":"<pre><code># Restore pgAdmin configuration from backup\nkubectl cp ./pgadmin-backup.tar.gz deployment/pgadmin-pgadmin4:/tmp/\nkubectl exec -it deployment/pgadmin-pgadmin4 -- \\\n  tar -xzf /tmp/pgadmin-backup.tar.gz -C /\n\n# Restore ConfigMaps\nkubectl apply -f pgadmin-config-backup.yaml\n\n# Restart pgAdmin to apply changes\nkubectl rollout restart deployment/pgadmin-pgadmin4\n</code></pre>"},{"location":"package-management-pgadmin/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>package-databases-postgresql.md - PostgreSQL database setup and configuration</li> <li>package-databases-postgresql-container.md - PostgreSQL custom container details</li> <li>rules-ingress-traefik.md - Traefik IngressRoute configuration standards</li> </ul> <p>\ud83d\udca1 Key Insight: pgAdmin provides a powerful web-based interface for PostgreSQL administration with automatic server configuration and DNS-based routing. The pre-configured connection to the cluster PostgreSQL instance allows novice users to immediately start database administration without needing to know internal DNS names or connection details.</p>"},{"location":"package-management-rabbitmq/","title":"RabbitMQ Management - Message Broker Administration Interface","text":"<p>Web Interface: Queue Management \u2022 Exchange Administration \u2022 User Management \u2022 Message Monitoring \u2022 Connection Tracking \u2022 Virtual Host Configuration \u2022 Performance Metrics</p> <p>File: <code>docs/package-management-rabbitmq.md</code> Purpose: Complete guide to RabbitMQ management UI usage and administration in Urbalurba infrastructure Target Audience: Message queue administrators, developers working with AMQP, system architects using pub/sub patterns Last Updated: September 23, 2024</p>"},{"location":"package-management-rabbitmq/#overview","title":"\ud83d\udccb Overview","text":"<p>The RabbitMQ Management Interface provides a comprehensive web-based administration console for the RabbitMQ message broker in the Urbalurba infrastructure. This interface is automatically deployed as part of the RabbitMQ installation and offers complete management capabilities for queues, exchanges, users, and system monitoring.</p> <p>Key Features: - Web-Based Interface: Complete RabbitMQ administration via browser - Automatic Deployment: Management UI included with RabbitMQ installation - Queue Management: Create, monitor, and manage message queues - Exchange Administration: Configure routing and message distribution - User &amp; Permission Management: Control access and virtual host permissions - Real-Time Monitoring: Live message rates, connection tracking, and performance metrics - DNS-Based Routing: Accessible via <code>rabbitmq.localhost</code> with multi-domain support - Integrated Authentication: Uses RabbitMQ's built-in user system with urbalurba-secrets</p>"},{"location":"package-management-rabbitmq/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"package-management-rabbitmq/#management-ui-components","title":"Management UI Components","text":"<pre><code>RabbitMQ Management Stack:\n\u251c\u2500\u2500 RabbitMQ Server (with management plugin enabled)\n\u251c\u2500\u2500 Management UI (embedded web interface on port 15672)\n\u251c\u2500\u2500 Service (ClusterIP exposing AMQP 5672 and Management 15672)\n\u251c\u2500\u2500 IngressRoute (Traefik routing for rabbitmq.localhost)\n\u251c\u2500\u2500 urbalurba-secrets (administrative credentials)\n\u2514\u2500\u2500 Authentication (RabbitMQ built-in user management)\n</code></pre>"},{"location":"package-management-rabbitmq/#related-infrastructure-files","title":"Related Infrastructure Files","text":"<pre><code>RabbitMQ Deployment:\n\u251c\u2500\u2500 provision-host/kubernetes/03-queues/08-setup-rabbitmq.sh  # Main deployment\n\u251c\u2500\u2500 manifests/080-rabbitmq-config.yaml                       # RabbitMQ configuration\n\u251c\u2500\u2500 manifests/081-rabbitmq-ingressroute.yaml                 # Management UI routing\n\u251c\u2500\u2500 ansible/playbooks/080-setup-rabbitmq.yml                # Deployment automation\n\u2514\u2500\u2500 docs/package-queues-rabbitmq.md                          # RabbitMQ deployment guide\n</code></pre>"},{"location":"package-management-rabbitmq/#access-authentication","title":"\ud83c\udf10 Access &amp; Authentication","text":""},{"location":"package-management-rabbitmq/#web-interface-access","title":"Web Interface Access","text":"<pre><code># Primary access via DNS routing\nhttp://rabbitmq.localhost\n\n# Port-forward access (alternative)\nkubectl port-forward svc/rabbitmq 15672:15672\n# Then access: http://localhost:15672\n</code></pre>"},{"location":"package-management-rabbitmq/#login-credentials","title":"Login Credentials","text":"<p>RabbitMQ management uses credentials configured in <code>urbalurba-secrets</code> based on the secrets-templates:</p> <pre><code># Get RabbitMQ credentials from urbalurba-secrets\nkubectl get secret urbalurba-secrets -o jsonpath='{.data.RABBITMQ_USERNAME}' | base64 -d\nkubectl get secret urbalurba-secrets -o jsonpath='{.data.RABBITMQ_PASSWORD}' | base64 -d\n</code></pre> <p>From secrets-templates configuration: - Username: <code>rabbitmq-admin</code> - Password: Uses <code>${DEFAULT_DATABASE_PASSWORD}</code> (your configured default password)</p> <p>Login Process: 1. Navigate to <code>http://rabbitmq.localhost</code> 2. Enter username: <code>rabbitmq-admin</code> 3. Enter password: (your DEFAULT_DATABASE_PASSWORD value) 4. Click \"Login\" to access the management interface</p>"},{"location":"package-management-rabbitmq/#management-operations","title":"\ud83d\udee0\ufe0f Management Operations","text":""},{"location":"package-management-rabbitmq/#queue-management","title":"Queue Management","text":"<p>Through RabbitMQ Management UI:</p>"},{"location":"package-management-rabbitmq/#create-queues","title":"Create Queues","text":"<ol> <li>Navigate to \"Queues and Streams\" tab</li> <li>Click \"Add a new queue\"</li> <li>Configure queue properties:</li> <li>Name: Queue identifier</li> <li>Durability: Survive broker restarts</li> <li>Auto Delete: Delete when unused</li> <li>Arguments: Additional queue configuration</li> <li>Click \"Add queue\"</li> </ol>"},{"location":"package-management-rabbitmq/#monitor-queues","title":"Monitor Queues","text":"<ul> <li>Message Rates: View publish/deliver/acknowledge rates</li> <li>Queue Depth: Monitor message backlog</li> <li>Consumer Count: Track active consumers</li> <li>Queue Details: Memory usage, state, and configuration</li> </ul>"},{"location":"package-management-rabbitmq/#queue-operations","title":"Queue Operations","text":"<pre><code># Queue management through UI:\n# - Purge messages: Remove all messages from queue\n# - Delete queue: Permanently remove queue\n# - Publish message: Send test messages\n# - Get messages: Retrieve and inspect messages\n</code></pre>"},{"location":"package-management-rabbitmq/#exchange-management","title":"Exchange Management","text":"<p>Through RabbitMQ Management UI:</p>"},{"location":"package-management-rabbitmq/#create-exchanges","title":"Create Exchanges","text":"<ol> <li>Navigate to \"Exchanges\" tab</li> <li>Click \"Add a new exchange\"</li> <li>Configure exchange properties:</li> <li>Name: Exchange identifier</li> <li>Type: direct, topic, fanout, headers</li> <li>Durability: Persist through restarts</li> <li>Auto Delete: Remove when unbounded</li> <li>Click \"Add exchange\"</li> </ol>"},{"location":"package-management-rabbitmq/#binding-management","title":"Binding Management","text":"<ul> <li>Create Bindings: Link exchanges to queues with routing keys</li> <li>View Bindings: Monitor routing relationships</li> <li>Test Routing: Publish messages to test routing logic</li> </ul>"},{"location":"package-management-rabbitmq/#user-management","title":"User Management","text":"<p>Through RabbitMQ Management UI:</p>"},{"location":"package-management-rabbitmq/#create-users","title":"Create Users","text":"<ol> <li>Navigate to \"Admin\" \u2192 \"Users\" tab</li> <li>Click \"Add a user\"</li> <li>Configure user properties:</li> <li>Username: User identifier</li> <li>Password: User password</li> <li>Tags: Admin, monitoring, policymaker, management</li> <li>Click \"Add user\"</li> </ol>"},{"location":"package-management-rabbitmq/#virtual-host-management","title":"Virtual Host Management","text":"<ol> <li>Navigate to \"Admin\" \u2192 \"Virtual Hosts\" tab</li> <li>Click \"Add a new virtual host\"</li> <li>Set virtual host name and description</li> <li>Configure user permissions for virtual host</li> </ol>"},{"location":"package-management-rabbitmq/#permission-management","title":"Permission Management","text":"<ul> <li>Set Permissions: Configure read/write/configure access</li> <li>Virtual Host Access: Control user access to virtual hosts</li> <li>Policy Management: Set queue and exchange policies</li> </ul>"},{"location":"package-management-rabbitmq/#monitoring-diagnostics","title":"Monitoring &amp; Diagnostics","text":""},{"location":"package-management-rabbitmq/#overview-dashboard","title":"Overview Dashboard","text":"<ul> <li>Global Statistics: Message rates, queue totals, connection counts</li> <li>Node Information: Memory usage, disk space, Erlang version</li> <li>Import/Export: Configuration backup and restore</li> </ul>"},{"location":"package-management-rabbitmq/#connection-monitoring","title":"Connection Monitoring","text":"<ol> <li>Navigate to \"Connections\" tab</li> <li>Monitor active connections:</li> <li>Client Information: IP addresses, protocols, users</li> <li>Channel Count: Active channels per connection</li> <li>Data Rates: Bytes in/out per connection</li> <li>Connection State: Running, blocking, flow control</li> </ol>"},{"location":"package-management-rabbitmq/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code># Through Management UI:\n# - Message rates: Real-time publish/deliver/acknowledge rates\n# - Memory usage: Per-queue and total broker memory\n# - Disk usage: Message persistence and logging\n# - Network I/O: Connection bandwidth utilization\n</code></pre>"},{"location":"package-management-rabbitmq/#health-checks-verification","title":"\ud83d\udd0d Health Checks &amp; Verification","text":""},{"location":"package-management-rabbitmq/#service-status-verification","title":"Service Status Verification","text":"<pre><code># Check RabbitMQ pod status\nkubectl get pods -l app.kubernetes.io/name=rabbitmq\n\n# Check RabbitMQ service status\nkubectl get svc rabbitmq\n\n# Check management UI routing\nkubectl get ingressroute rabbitmq-management\n\n# View RabbitMQ logs\nkubectl logs -l app.kubernetes.io/name=rabbitmq\n</code></pre>"},{"location":"package-management-rabbitmq/#management-ui-testing","title":"Management UI Testing","text":"<pre><code># Test HTTP response from within cluster\nkubectl run curl-test --image=curlimages/curl --rm -it --restart=Never -- \\\n  curl -s -w \"HTTP_CODE:%{http_code}\" http://rabbitmq:15672/\n\n# Test DNS routing\ncurl -H \"Host: rabbitmq.localhost\" http://localhost/\n\n# Test authentication endpoint\ncurl -u rabbitmq-admin:$RABBITMQ_PASSWORD http://rabbitmq.localhost/api/overview\n</code></pre>"},{"location":"package-management-rabbitmq/#amqp-connectivity-testing","title":"AMQP Connectivity Testing","text":"<pre><code># Test AMQP port connectivity\nkubectl run test-pod --image=busybox --rm -it -- \\\n  nc -zv rabbitmq.default.svc.cluster.local 5672\n\n# Test management API\nkubectl exec -it rabbitmq-0 -- \\\n  rabbitmqctl status\n</code></pre>"},{"location":"package-management-rabbitmq/#common-administration-tasks","title":"\ud83c\udfaf Common Administration Tasks","text":""},{"location":"package-management-rabbitmq/#message-queue-workflows","title":"Message Queue Workflows","text":""},{"location":"package-management-rabbitmq/#basic-pubsub-setup","title":"Basic Pub/Sub Setup","text":"<ol> <li>Create Exchange: Name: <code>events</code>, Type: <code>fanout</code></li> <li>Create Queues: <code>notifications</code>, <code>logging</code>, <code>analytics</code></li> <li>Bind Queues: Bind all queues to <code>events</code> exchange</li> <li>Test Publishing: Send message to <code>events</code> exchange</li> <li>Verify Distribution: Check messages appear in all queues</li> </ol>"},{"location":"package-management-rabbitmq/#topic-based-routing","title":"Topic-Based Routing","text":"<ol> <li>Create Exchange: Name: <code>logs</code>, Type: <code>topic</code></li> <li>Create Queues: <code>error.logs</code>, <code>info.logs</code>, <code>debug.logs</code></li> <li>Bind with Patterns:</li> <li><code>error.logs</code> \u2190 <code>*.error.*</code></li> <li><code>info.logs</code> \u2190 <code>*.info.*</code></li> <li><code>debug.logs</code> \u2190 <code>*.debug.*</code></li> <li>Test Routing: Publish with routing keys like <code>app.error.auth</code></li> </ol>"},{"location":"package-management-rabbitmq/#user-permission-setup","title":"User &amp; Permission Setup","text":""},{"location":"package-management-rabbitmq/#application-user-creation","title":"Application User Creation","text":"<ol> <li>Create User: Username: <code>app-service</code>, Password: <code>[secure-password]</code></li> <li>Set Tags: <code>none</code> (no administrative access)</li> <li>Virtual Host: Grant access to <code>/</code> (default vhost)</li> <li>Permissions:</li> <li>Configure: <code>app\\..*</code> (can create resources matching pattern)</li> <li>Write: <code>app\\..*</code> (can publish to matching resources)</li> <li>Read: <code>app\\..*</code> (can consume from matching resources)</li> </ol>"},{"location":"package-management-rabbitmq/#monitoring-user-setup","title":"Monitoring User Setup","text":"<ol> <li>Create User: Username: <code>monitor</code>, Password: <code>[monitor-password]</code></li> <li>Set Tags: <code>monitoring</code> (read-only monitoring access)</li> <li>Virtual Host: Grant access to <code>/</code></li> <li>Permissions: Read-only access to view statistics</li> </ol>"},{"location":"package-management-rabbitmq/#performance-optimization","title":"Performance Optimization","text":""},{"location":"package-management-rabbitmq/#queue-configuration","title":"Queue Configuration","text":"<pre><code># Through Management UI - Queue Arguments:\n# x-max-length: 10000           # Maximum queue size\n# x-message-ttl: 3600000        # Message TTL (1 hour)\n# x-max-priority: 10            # Priority queue support\n# x-dead-letter-exchange: dlx   # Dead letter handling\n</code></pre>"},{"location":"package-management-rabbitmq/#memory-management","title":"Memory Management","text":"<ul> <li>Queue Memory: Monitor per-queue memory usage</li> <li>Message Paging: Configure disk paging for large queues</li> <li>Memory Alarms: Set memory high watermark limits</li> <li>Disk Space: Monitor disk space for message persistence</li> </ul>"},{"location":"package-management-rabbitmq/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-management-rabbitmq/#common-issues","title":"Common Issues","text":"<p>Cannot Access Management UI: <pre><code># Verify RabbitMQ pod is running\nkubectl describe pod -l app.kubernetes.io/name=rabbitmq\n\n# Check management plugin status\nkubectl exec -it rabbitmq-0 -- rabbitmq-plugins list\n\n# Verify service endpoints\nkubectl describe svc rabbitmq\n</code></pre></p> <p>Authentication Failures: <pre><code># Verify credentials in secrets\nkubectl get secret urbalurba-secrets -o jsonpath='{.data.RABBITMQ_USERNAME}' | base64 -d\nkubectl get secret urbalurba-secrets -o jsonpath='{.data.RABBITMQ_PASSWORD}' | base64 -d\n\n# Check RabbitMQ user list\nkubectl exec -it rabbitmq-0 -- rabbitmqctl list_users\n</code></pre></p> <p>Queue Connection Issues: <pre><code># Test AMQP connectivity\nkubectl run test-pod --image=busybox --rm -it -- \\\n  nc -zv rabbitmq.default.svc.cluster.local 5672\n\n# Check RabbitMQ cluster status\nkubectl exec -it rabbitmq-0 -- rabbitmqctl cluster_status\n\n# View connection logs\nkubectl logs -l app.kubernetes.io/name=rabbitmq --tail=50\n</code></pre></p> <p>Performance Issues: <pre><code># Check resource usage\nkubectl top pod -l app.kubernetes.io/name=rabbitmq\n\n# Monitor queue memory usage through management UI\n# Navigate to Queues tab and check memory column\n\n# Check disk space\nkubectl exec -it rabbitmq-0 -- df -h\n</code></pre></p> <p>DNS Resolution Issues: <pre><code># Test DNS resolution\nkubectl run test-pod --image=busybox --rm -it -- \\\n  nslookup rabbitmq.default.svc.cluster.local\n\n# Verify IngressRoute configuration\nkubectl describe ingressroute rabbitmq-management\n\n# Test different domain patterns\ncurl -H \"Host: rabbitmq.localhost\" http://127.0.0.1/\n</code></pre></p>"},{"location":"package-management-rabbitmq/#maintenance-monitoring","title":"\ud83d\udccb Maintenance &amp; Monitoring","text":""},{"location":"package-management-rabbitmq/#regular-maintenance-tasks","title":"Regular Maintenance Tasks","text":"<ol> <li>Queue Monitoring: Check queue depths and consumer activity</li> <li>Connection Tracking: Monitor client connections and channels</li> <li>Memory Usage: Track broker memory and disk utilization</li> <li>User Access Review: Audit user permissions and access patterns</li> <li>Configuration Backup: Export broker configuration regularly</li> </ol>"},{"location":"package-management-rabbitmq/#backup-procedures","title":"Backup Procedures","text":"<pre><code># Export RabbitMQ configuration\nkubectl exec -it rabbitmq-0 -- rabbitmqctl export_definitions /tmp/definitions.json\n\n# Copy configuration backup\nkubectl cp rabbitmq-0:/tmp/definitions.json ./rabbitmq-definitions-backup.json\n\n# Backup persistent data\nkubectl exec -it rabbitmq-0 -- tar -czf /tmp/rabbitmq-data.tar.gz /bitnami/rabbitmq/mnesia\nkubectl cp rabbitmq-0:/tmp/rabbitmq-data.tar.gz ./rabbitmq-data-backup.tar.gz\n</code></pre>"},{"location":"package-management-rabbitmq/#performance-monitoring_1","title":"Performance Monitoring","text":"<pre><code># Through Management UI:\n# - Overview: Global message rates and resource usage\n# - Queues: Per-queue statistics and memory usage\n# - Connections: Client connection details and data rates\n# - Channels: Channel-level statistics and flow control\n# - Exchanges: Message routing statistics\n</code></pre>"},{"location":"package-management-rabbitmq/#disaster-recovery","title":"Disaster Recovery","text":"<pre><code># Restore configuration (after RabbitMQ restart)\nkubectl cp ./rabbitmq-definitions-backup.json rabbitmq-0:/tmp/definitions.json\nkubectl exec -it rabbitmq-0 -- rabbitmqctl import_definitions /tmp/definitions.json\n\n# Restore data files (requires pod restart)\nkubectl cp ./rabbitmq-data-backup.tar.gz rabbitmq-0:/tmp/rabbitmq-data.tar.gz\nkubectl exec -it rabbitmq-0 -- tar -xzf /tmp/rabbitmq-data.tar.gz -C /\n</code></pre>"},{"location":"package-management-rabbitmq/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>package-queues-rabbitmq.md - RabbitMQ deployment and configuration</li> <li>rules-ingress-traefik.md - Traefik IngressRoute configuration standards</li> <li>secrets-management-readme.md - Managing RabbitMQ credentials in urbalurba-secrets</li> </ul>"},{"location":"package-management-rabbitmq/#external-resources","title":"\ud83d\udd17 External Resources","text":"<ul> <li>RabbitMQ Management Plugin Documentation - Official management UI guide</li> <li>RabbitMQ Admin Guide - Administrative operations reference</li> <li>AMQP 0-9-1 Protocol - Protocol specification and concepts</li> </ul> <p>\ud83d\udca1 Key Insight: The RabbitMQ Management Interface provides comprehensive administrative control over the message broker through an intuitive web interface. Unlike separate management tools, it's integrated directly into RabbitMQ and offers real-time monitoring, queue management, and user administration in a single interface accessible via standard cluster DNS routing.</p>"},{"location":"package-management-readme/","title":"Management Services - Complete Administrative Interface Layer","text":"<p>File: <code>docs/package-management-readme.md</code> Purpose: Overview of all management and administration services in Urbalurba infrastructure Target Audience: Database administrators, system administrators, DevOps engineers Last Updated: September 23, 2024</p>"},{"location":"package-management-readme/#overview","title":"\ud83d\udccb Overview","text":"<p>Urbalurba infrastructure provides a comprehensive suite of management and administration interfaces for databases, message brokers, and other services. These web-based interfaces offer intuitive administrative capabilities, monitoring, and configuration management through modern browser-based consoles.</p> <p>Available Management Services: - pgAdmin: PostgreSQL database administration with auto-configuration - RedisInsight: Redis database management with manual connection setup - RabbitMQ Management: Message broker administration with integrated UI</p>"},{"location":"package-management-readme/#management-services","title":"\ud83d\udda5\ufe0f Management Services","text":""},{"location":"package-management-readme/#pgadmin-postgresql-administration-interface","title":"pgAdmin - PostgreSQL Administration Interface \ud83d\udc18","text":"<p>Status: Optional (not-in-use) | Port: 80 | Type: Database Management</p> <p>Auto-Configuration: Pre-configured PostgreSQL Connection \u2022 Server Definitions \u2022 Authentication Integration</p> <p>pgAdmin provides a comprehensive web-based interface for PostgreSQL database administration with automatic server configuration and seamless integration with the cluster PostgreSQL instance.</p> <p>Key Features: - Auto-Connected: Pre-configured connection to cluster PostgreSQL - Web-Based: Complete database administration via browser - SQL Editor: Advanced query editor with syntax highlighting - Visual Tools: Database designer and visual query builder - User Management: Role and permission administration - Backup/Restore: Database backup and restore operations</p> <p>\ud83d\udcda Complete Documentation \u2192</p>"},{"location":"package-management-readme/#redisinsight-redis-administration-interface","title":"RedisInsight - Redis Administration Interface \ud83d\udd34","text":"<p>Status: Optional (not-in-use) | Port: 5540 | Type: Database Management</p> <p>Manual Configuration: First-time Setup \u2022 Manual Connection Configuration \u2022 Flexible Multi-Instance Support</p> <p>RedisInsight offers a modern web-based interface for Redis database administration with first-time user setup and flexible connection management for multiple Redis instances.</p> <p>Key Features: - First-Time Setup: Create your own user account on initial access - Manual Connections: Configure Redis connections through web interface - Memory Analysis: Advanced memory usage analysis and optimization - CLI Integration: Built-in Redis CLI with command execution - Performance Monitoring: Real-time metrics and slow query analysis - Data Visualization: Key browser and data structure visualization</p> <p>\ud83d\udcda Complete Documentation \u2192</p>"},{"location":"package-management-readme/#rabbitmq-management-message-broker-administration","title":"RabbitMQ Management - Message Broker Administration \ud83d\udc30","text":"<p>Status: Active (with RabbitMQ) | Port: 15672 | Type: Message Broker Management</p> <p>Integrated Interface: Built-in Management Plugin \u2022 Real-time Monitoring \u2022 Queue Administration</p> <p>RabbitMQ Management provides a comprehensive administrative console that's automatically deployed with RabbitMQ installation, offering complete message broker management capabilities.</p> <p>Key Features: - Automatic Deployment: Included with RabbitMQ installation - Queue Management: Create, monitor, and manage message queues - Exchange Administration: Configure routing and message distribution - User &amp; Permission Management: Control access and virtual host permissions - Real-Time Monitoring: Live message rates and performance metrics - Connection Tracking: Monitor client connections and channels</p> <p>\ud83d\udcda Complete Documentation \u2192</p>"},{"location":"package-management-readme/#deployment-architecture","title":"\ud83c\udfd7\ufe0f Deployment Architecture","text":""},{"location":"package-management-readme/#service-activation","title":"Service Activation","text":"<pre><code>Management Interface Status:\n\u251c\u2500\u2500 pgAdmin (OPTIONAL) - Located in not-in-use/ folder\n\u251c\u2500\u2500 RedisInsight (OPTIONAL) - Located in not-in-use/ folder\n\u2514\u2500\u2500 RabbitMQ Management (AUTOMATIC) - Deployed with RabbitMQ\n</code></pre>"},{"location":"package-management-readme/#access-methods","title":"Access Methods","text":"<p>All management interfaces use Traefik IngressRoute for DNS-based routing: - pgAdmin: <code>http://pgadmin.localhost</code> - RedisInsight: <code>http://redisinsight.localhost</code> - RabbitMQ Management: <code>http://rabbitmq.localhost</code></p>"},{"location":"package-management-readme/#authentication-models","title":"Authentication Models","text":"<pre><code>Authentication Approaches:\n\u251c\u2500\u2500 pgAdmin: urbalurba-secrets (PGADMIN_DEFAULT_EMAIL/PASSWORD)\n\u251c\u2500\u2500 RedisInsight: First-time setup (user-created credentials)\n\u2514\u2500\u2500 RabbitMQ: urbalurba-secrets (RABBITMQ_USERNAME/PASSWORD)\n</code></pre>"},{"location":"package-management-readme/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"package-management-readme/#activate-database-management-pgadmin","title":"Activate Database Management (pgAdmin)","text":"<pre><code># Move from not-in-use to activate\ncd provision-host/kubernetes/06-management/\nmv not-in-use/03-setup-pgadmin.sh ./\n\n# Deploy pgAdmin\n./03-setup-pgadmin.sh rancher-desktop\n\n# Access via browser\nopen http://pgadmin.localhost\n</code></pre>"},{"location":"package-management-readme/#activate-redis-management-redisinsight","title":"Activate Redis Management (RedisInsight)","text":"<pre><code># Move from not-in-use to activate\ncd provision-host/kubernetes/06-management/\nmv not-in-use/05-setup-redisinsight.sh ./\n\n# Deploy RedisInsight\n./05-setup-redisinsight.sh rancher-desktop\n\n# Access via browser (first-time setup required)\nopen http://redisinsight.localhost\n</code></pre>"},{"location":"package-management-readme/#access-message-broker-management-rabbitmq","title":"Access Message Broker Management (RabbitMQ)","text":"<pre><code># RabbitMQ Management is automatically available when RabbitMQ is deployed\n# No separate installation required\n\n# Access via browser\nopen http://rabbitmq.localhost\n</code></pre>"},{"location":"package-management-readme/#management-interface-selection-guide","title":"\ud83d\udd0d Management Interface Selection Guide","text":""},{"location":"package-management-readme/#when-to-use-pgadmin","title":"When to Use pgAdmin \u2705","text":"<ul> <li>PostgreSQL Administration: Primary choice for PostgreSQL management</li> <li>Auto-Configuration: Prefer automatic server configuration</li> <li>Team Environments: Multiple users sharing database management</li> <li>Advanced SQL Operations: Complex query development and optimization</li> <li>Database Design: Visual database modeling and design tasks</li> </ul>"},{"location":"package-management-readme/#when-to-use-redisinsight","title":"When to Use RedisInsight \ud83d\udd34","text":"<ul> <li>Redis Administration: Essential for Redis database management</li> <li>Memory Optimization: Analyzing Redis memory usage patterns</li> <li>Performance Tuning: Monitoring Redis performance and slow queries</li> <li>Multi-Instance: Managing multiple Redis instances or clusters</li> <li>Development: Redis-specific development and debugging tasks</li> </ul>"},{"location":"package-management-readme/#when-to-use-rabbitmq-management","title":"When to Use RabbitMQ Management \ud83d\udc30","text":"<ul> <li>Message Broker Operations: Queue and exchange management</li> <li>Performance Monitoring: Real-time message broker statistics</li> <li>User Administration: Managing RabbitMQ users and permissions</li> <li>Troubleshooting: Diagnosing message flow and connection issues</li> <li>Configuration Management: Setting up routing and policies</li> </ul>"},{"location":"package-management-readme/#management-operations","title":"\ud83d\udee0\ufe0f Management Operations","text":""},{"location":"package-management-readme/#common-access-patterns","title":"Common Access Patterns","text":"<pre><code># Check management service status\nkubectl get pods -l component=management\nkubectl get pods -l app.kubernetes.io/name=pgadmin4\nkubectl get pods -l app.kubernetes.io/name=redisinsight\nkubectl get pods -l app.kubernetes.io/name=rabbitmq\n\n# Verify web interface accessibility\ncurl -H \"Host: pgadmin.localhost\" http://localhost/\ncurl -H \"Host: redisinsight.localhost\" http://localhost/\ncurl -H \"Host: rabbitmq.localhost\" http://localhost/\n</code></pre>"},{"location":"package-management-readme/#authentication-management","title":"Authentication Management","text":"<pre><code># Get pgAdmin credentials\nkubectl get secret urbalurba-secrets -o jsonpath='{.data.PGADMIN_DEFAULT_EMAIL}' | base64 -d\nkubectl get secret urbalurba-secrets -o jsonpath='{.data.PGADMIN_DEFAULT_PASSWORD}' | base64 -d\n\n# Get RabbitMQ credentials\nkubectl get secret urbalurba-secrets -o jsonpath='{.data.RABBITMQ_USERNAME}' | base64 -d\nkubectl get secret urbalurba-secrets -o jsonpath='{.data.RABBITMQ_PASSWORD}' | base64 -d\n\n# RedisInsight: No pre-configured credentials (first-time setup)\n</code></pre>"},{"location":"package-management-readme/#port-forward-alternative-access","title":"Port-Forward Alternative Access","text":"<pre><code># pgAdmin port-forward\nkubectl port-forward svc/pgadmin-pgadmin4 8080:80\n# Access: http://localhost:8080\n\n# RedisInsight port-forward\nkubectl port-forward svc/redisinsight 8081:5540\n# Access: http://localhost:8081\n\n# RabbitMQ Management port-forward\nkubectl port-forward svc/rabbitmq 8082:15672\n# Access: http://localhost:8082\n</code></pre>"},{"location":"package-management-readme/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-management-readme/#common-issues","title":"Common Issues","text":"<p>Management Interface Won't Load: <pre><code># Check pod status\nkubectl describe pod -l app.kubernetes.io/name=pgadmin4\nkubectl describe pod -l app.kubernetes.io/name=redisinsight\nkubectl describe pod -l app.kubernetes.io/name=rabbitmq\n\n# Verify service endpoints\nkubectl get endpoints pgadmin-pgadmin4 redisinsight rabbitmq\n\n# Check IngressRoute configuration\nkubectl get ingressroute pgadmin redisinsight rabbitmq-management\n</code></pre></p> <p>Authentication Issues: <pre><code># Verify secrets exist\nkubectl get secret urbalurba-secrets\n\n# Check secret values\nkubectl describe secret urbalurba-secrets\n\n# Test service connectivity\nkubectl run test-pod --image=curlimages/curl --rm -it -- \\\n  curl http://pgadmin-pgadmin4.default.svc.cluster.local:80/misc/ping\n</code></pre></p> <p>DNS Resolution Problems: <pre><code># Test internal DNS\nkubectl run test-pod --image=busybox --rm -it -- \\\n  nslookup pgadmin-pgadmin4.default.svc.cluster.local\n\n# Verify Traefik routing\nkubectl logs -n kube-system -l app.kubernetes.io/name=traefik\n\n# Test with curl\ncurl -v -H \"Host: pgadmin.localhost\" http://127.0.0.1/\n</code></pre></p>"},{"location":"package-management-readme/#service-specific-troubleshooting","title":"Service-Specific Troubleshooting","text":"<p>pgAdmin Issues: - PostgreSQL connection problems \u2192 Check PostgreSQL service status - Login failures \u2192 Verify PGADMIN_DEFAULT_EMAIL/PASSWORD in secrets - Performance issues \u2192 Check pgAdmin pod resource limits</p> <p>RedisInsight Issues: - First-time setup problems \u2192 Check persistent storage permissions - Redis connection failures \u2192 Verify Redis service and credentials - Memory issues \u2192 Monitor RedisInsight pod resource usage</p> <p>RabbitMQ Management Issues: - Management UI unavailable \u2192 Check rabbitmq_management plugin enabled - Authentication failures \u2192 Verify RABBITMQ_USERNAME/PASSWORD in secrets - Missing features \u2192 Ensure management plugin is properly loaded</p>"},{"location":"package-management-readme/#maintenance","title":"\ud83d\udccb Maintenance","text":""},{"location":"package-management-readme/#regular-tasks","title":"Regular Tasks","text":"<ol> <li>Health Monitoring: Check pod and service status regularly</li> <li>Storage Monitoring: Monitor persistent volume usage for configurations</li> <li>Access Review: Review user access and authentication credentials</li> <li>Performance Monitoring: Track resource usage and response times</li> <li>Security Updates: Update container images and configurations</li> </ol>"},{"location":"package-management-readme/#backup-procedures","title":"Backup Procedures","text":"<pre><code># pgAdmin configuration backup\nkubectl exec -it deployment/pgadmin-pgadmin4 -- \\\n  tar -czf /tmp/pgadmin-backup.tar.gz /var/lib/pgadmin\nkubectl cp deployment/pgadmin-pgadmin4:/tmp/pgadmin-backup.tar.gz ./pgadmin-backup.tar.gz\n\n# RedisInsight configuration backup\nkubectl exec -it deployment/redisinsight -- \\\n  tar -czf /tmp/redisinsight-backup.tar.gz /data\nkubectl cp deployment/redisinsight:/tmp/redisinsight-backup.tar.gz ./redisinsight-backup.tar.gz\n\n# RabbitMQ configuration backup\nkubectl exec -it rabbitmq-0 -- \\\n  rabbitmqctl export_definitions /tmp/definitions.json\nkubectl cp rabbitmq-0:/tmp/definitions.json ./rabbitmq-definitions.json\n</code></pre>"},{"location":"package-management-readme/#service-removal","title":"Service Removal","text":"<pre><code># Remove management services\ncd provision-host/kubernetes/06-management/not-in-use/\n\n# Remove pgAdmin\n./03-remove-pgadmin.sh rancher-desktop\n\n# Remove RedisInsight\n./05-remove-redisinsight.sh rancher-desktop\n\n# RabbitMQ Management is removed automatically when RabbitMQ is removed\ncd provision-host/kubernetes/03-queues/not-in-use/\n./08-remove-rabbitmq.sh rancher-desktop\n</code></pre> <p>\ud83d\udca1 Key Insight: The management layer provides intuitive web-based interfaces for all major infrastructure components, with varying authentication models to suit different security and usability requirements. pgAdmin offers auto-configuration for immediate productivity, RedisInsight provides flexible multi-instance management, and RabbitMQ Management integrates seamlessly with message broker deployment.</p>"},{"location":"package-management-redisinsight/","title":"RedisInsight - Redis Database Administration Interface","text":"<p>Web Interface: Redis Management \u2022 Multi-Database Support \u2022 Memory Analysis \u2022 Performance Monitoring \u2022 CLI Integration \u2022 Data Visualization \u2022 Query Builder</p> <p>File: <code>docs/package-management-redisinsight.md</code> Purpose: Complete guide to RedisInsight deployment and configuration in Urbalurba infrastructure Target Audience: Database administrators, developers needing Redis management tools, Redis users Last Updated: September 23, 2024</p>"},{"location":"package-management-redisinsight/#overview","title":"\ud83d\udccb Overview","text":"<p>RedisInsight provides a comprehensive web-based administration interface for Redis databases in the Urbalurba infrastructure. It's designed as an optional management service that offers full Redis administration capabilities through a modern web interface.</p> <p>Key Features: - Web-Based Interface: Full-featured Redis administration via browser - Manual Redis Connection: Configure connections to cluster Redis instances via web UI - Helm-Based Deployment: Uses redisinsight/redisinsight chart for reliable deployment - First-Time Setup: Create your own user account on initial access - DNS-Based Routing: Accessible via <code>redisinsight.localhost</code> with multi-domain support - 5GB Storage: Persistent storage for connection configurations, query history, and user data - Production Ready: Includes proper security context and resource limits</p> <p>Project Homepage: https://github.com/redis/RedisInsight</p>"},{"location":"package-management-redisinsight/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"package-management-redisinsight/#deployment-components","title":"Deployment Components","text":"<pre><code>RedisInsight Service Stack:\n\u251c\u2500\u2500 Helm Release (redisinsight/redisinsight with custom configuration)\n\u251c\u2500\u2500 Deployment (redisinsight container with security context)\n\u251c\u2500\u2500 Service (ClusterIP on port 5540)\n\u251c\u2500\u2500 IngressRoute (Traefik routing for redisinsight.localhost)\n\u251c\u2500\u2500 PersistentVolumeClaim (5GB storage)\n\u251c\u2500\u2500 urbalurba-secrets (Redis authentication credentials)\n\u2514\u2500\u2500 Pod (redisinsight container with web interface)\n</code></pre>"},{"location":"package-management-redisinsight/#file-structure","title":"File Structure","text":"<pre><code>06-management/\n\u2514\u2500\u2500 not-in-use/                         # Inactive by default\n    \u251c\u2500\u2500 05-setup-redisinsight.sh        # Main deployment script\n    \u2514\u2500\u2500 05-remove-redisinsight.sh       # Removal script\n\nmanifests/\n\u251c\u2500\u2500 651-adm-redisinsight.yaml           # RedisInsight Helm configuration\n\u2514\u2500\u2500 751-redisinsight-ingressroute.yaml  # Traefik routing configuration\n\nansible/playbooks/\n\u251c\u2500\u2500 651-adm-redisinsight.yml            # Main deployment logic\n\u2514\u2500\u2500 651-remove-redisinsight.yml         # Removal logic\n</code></pre>"},{"location":"package-management-redisinsight/#deployment","title":"\ud83d\ude80 Deployment","text":""},{"location":"package-management-redisinsight/#service-activation","title":"Service Activation","text":"<p>RedisInsight is inactive by default. To activate and deploy:</p> <pre><code># Move script from not-in-use to activate\ncd provision-host/kubernetes/06-management/\nmv not-in-use/05-setup-redisinsight.sh ./\n\n# Deploy RedisInsight\n./05-setup-redisinsight.sh rancher-desktop\n</code></pre>"},{"location":"package-management-redisinsight/#manual-deployment","title":"Manual Deployment","text":"<pre><code># Deploy to specific Kubernetes context\n./05-setup-redisinsight.sh multipass-microk8s\n./05-setup-redisinsight.sh azure-aks\n\n# Direct Ansible playbook execution\ncd /mnt/urbalurbadisk/ansible\nansible-playbook playbooks/651-adm-redisinsight.yml -e target_host=rancher-desktop\n</code></pre>"},{"location":"package-management-redisinsight/#prerequisites","title":"Prerequisites","text":"<p>RedisInsight does not require pre-configured credentials. Redis connections are added manually through the web interface after deployment.</p>"},{"location":"package-management-redisinsight/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"package-management-redisinsight/#helm-configuration","title":"Helm Configuration","text":"<p>RedisInsight uses the redisinsight/redisinsight Helm chart with comprehensive configuration:</p> <pre><code># Deployment command (from Ansible playbook)\nhelm upgrade --install redisinsight redisinsight/redisinsight \\\n  -f manifests/651-adm-redisinsight.yaml \\\n  --set persistence.storageClassName=\"$STORAGE_CLASS_NAME\" \\\n  --namespace default\n</code></pre>"},{"location":"package-management-redisinsight/#storage-configuration","title":"Storage Configuration","text":"<pre><code># From manifests/651-adm-redisinsight.yaml\npersistence:\n  enabled: true\n  accessMode: ReadWriteOnce\n  size: 5Gi\n  # storageClassName omitted for cross-platform compatibility\n</code></pre>"},{"location":"package-management-redisinsight/#security-configuration","title":"Security Configuration","text":"<pre><code># Security context for production deployment\npodSecurityContext:\n  fsGroup: 1001\n\nsecurityContext:\n  capabilities:\n    drop:\n      - ALL\n  readOnlyRootFilesystem: false\n  runAsNonRoot: true\n  runAsUser: 1001\n  runAsGroup: 1001\n  allowPrivilegeEscalation: false\n</code></pre>"},{"location":"package-management-redisinsight/#network-configuration","title":"Network Configuration","text":"<pre><code># Traefik IngressRoute configuration\n# From manifests/751-redisinsight-ingressroute.yaml\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: HostRegexp(`redisinsight\\..+`)\n      kind: Rule\n      services:\n        - name: redisinsight\n          port: 5540\n</code></pre>"},{"location":"package-management-redisinsight/#access-usage","title":"\ud83c\udf10 Access &amp; Usage","text":""},{"location":"package-management-redisinsight/#web-interface-access","title":"Web Interface Access","text":"<pre><code># Primary access via DNS routing\nhttp://redisinsight.localhost\n\n# Port-forward access (alternative)\nkubectl port-forward svc/redisinsight 8080:5540\n# Then access: http://localhost:8080\n</code></pre>"},{"location":"package-management-redisinsight/#first-time-setup","title":"First-Time Setup","text":"<ol> <li>Access RedisInsight: Navigate to <code>http://redisinsight.localhost</code></li> <li>Create Account: RedisInsight will prompt you to create a user account</li> <li>Choose Credentials: Enter your preferred username and password</li> <li>Account Storage: Credentials are stored locally in RedisInsight's persistent storage</li> </ol>"},{"location":"package-management-redisinsight/#adding-redis-database-connections","title":"Adding Redis Database Connections","text":"<p>After logging into RedisInsight, you need to manually add Redis database connections:</p>"},{"location":"package-management-redisinsight/#add-redis-connection-if-redis-is-deployed-in-cluster","title":"Add Redis Connection (If Redis is deployed in cluster)","text":"<ol> <li>Click \"Add Redis Database\"</li> <li>Fill in connection details:</li> <li>Database Alias: <code>redis-master.default.svc.cluster.local</code></li> <li>Host: <code>redis-master.default.svc.cluster.local</code></li> <li>Port: <code>6379</code></li> <li>Username: <code>default</code> (if Redis has authentication enabled)</li> <li>Password: Get from urbalurba-secrets (see below)</li> <li>Timeout: <code>30</code> seconds</li> <li>Click \"Test Connection\" to verify</li> <li>Click \"Add Redis Database\" to save</li> </ol>"},{"location":"package-management-redisinsight/#getting-redis-credentials","title":"Getting Redis Credentials","text":"<p>Redis credentials are configured in <code>urbalurba-secrets</code> based on the secrets-templates:</p> <pre><code># Get Redis password from urbalurba-secrets\nkubectl get secret urbalurba-secrets -o jsonpath='{.data.REDIS_PASSWORD}' | base64 -d\n\n# Get Redis host (should match connection host)\nkubectl get secret urbalurba-secrets -o jsonpath='{.data.REDIS_HOST}' | base64 -d\n</code></pre> <p>From secrets-templates configuration: - REDIS_PASSWORD: <code>SecretPassword1</code> (or your configured password) - REDIS_HOST: <code>redis-master.default.svc.cluster.local</code></p> <p>Note: If Redis is deployed without authentication, the password field can be left empty in RedisInsight.</p>"},{"location":"package-management-redisinsight/#connection-settings-screenshot-reference","title":"Connection Settings Screenshot Reference","text":"<p>Based on your screenshot, the connection form includes: - General Tab: Database Alias, Host, Port, Username, Password, Timeout - Security Tab: SSL/TLS configuration options - Decompression &amp; Formatters Tab: Data handling options - Select Logical Database: Choose Redis database number (default: 0) - Force Standalone Connection: For cluster bypass if needed</p>"},{"location":"package-management-redisinsight/#monitoring-verification","title":"\ud83d\udd0d Monitoring &amp; Verification","text":""},{"location":"package-management-redisinsight/#health-checks","title":"Health Checks","text":"<pre><code># Check pod status\nkubectl get pods -l app.kubernetes.io/name=redisinsight\n\n# Check service status\nkubectl get svc redisinsight\n\n# Check IngressRoute\nkubectl get ingressroute redisinsight\n\n# View RedisInsight logs\nkubectl logs -l app.kubernetes.io/name=redisinsight\n</code></pre>"},{"location":"package-management-redisinsight/#connection-testing","title":"Connection Testing","text":"<pre><code># Test HTTP response from within cluster\nkubectl run curl-test --image=curlimages/curl --rm -it --restart=Never -- \\\n  curl -s -w \"HTTP_CODE:%{http_code}\" http://redisinsight:5540/\n\n# Test DNS routing\ncurl -H \"Host: redisinsight.localhost\" http://localhost/\n</code></pre>"},{"location":"package-management-redisinsight/#redisinsight-interface-testing","title":"RedisInsight Interface Testing","text":"<ol> <li>Login Test: Access <code>http://redisinsight.localhost</code> and verify account creation/login</li> <li>Database Connection: Add Redis server connection and test connectivity</li> <li>Data Operations: Browse keys, execute commands, and verify functionality</li> <li>Memory Analysis: Check memory usage and key distribution features</li> </ol>"},{"location":"package-management-redisinsight/#management-operations","title":"\ud83d\udee0\ufe0f Management Operations","text":""},{"location":"package-management-redisinsight/#database-administration","title":"Database Administration","text":"<pre><code># Access RedisInsight web interface\nopen http://redisinsight.localhost\n\n# Through RedisInsight UI:\n# 1. Navigate to connected Redis database\n# 2. Use Browser to explore keys and data structures\n# 3. Use Workbench for command execution\n# 4. Use Memory Analysis for optimization insights\n</code></pre>"},{"location":"package-management-redisinsight/#key-management","title":"Key Management","text":"<p>Through RedisInsight web interface: 1. Browser Tab: Navigate and search Redis keys 2. Key Details: View key types, values, and TTL 3. Edit Values: Modify string, hash, list, set, and sorted set values 4. Key Operations: Delete, rename, and set expiration</p>"},{"location":"package-management-redisinsight/#performance-monitoring","title":"Performance Monitoring","text":"<p>Through RedisInsight web interface: 1. Dashboard: View real-time Redis metrics 2. Memory Analysis: Analyze memory usage patterns 3. Slow Log: Monitor slow-running commands 4. Command Timeline: Track command execution patterns</p>"},{"location":"package-management-redisinsight/#advanced-operations","title":"Advanced Operations","text":"<pre><code># View RedisInsight configuration\nkubectl exec -it deployment/redisinsight -- ls -la /data\n\n# Check RedisInsight storage usage\nkubectl exec -it deployment/redisinsight -- df -h /data\n\n# View RedisInsight process status\nkubectl exec -it deployment/redisinsight -- ps aux\n</code></pre>"},{"location":"package-management-redisinsight/#service-removal","title":"Service Removal","text":"<pre><code># Remove RedisInsight service completely\ncd provision-host/kubernetes/06-management/not-in-use/\n./05-remove-redisinsight.sh rancher-desktop\n\n# Direct Ansible playbook removal\ncd /mnt/urbalurbadisk/ansible\nansible-playbook playbooks/651-remove-redisinsight.yml -e target_host=rancher-desktop\n</code></pre> <p>Removal Process: - Uninstalls RedisInsight Helm release - Removes IngressRoute configuration - Deletes persistent volume claims and data - Waits for pods to terminate - Preserves urbalurba-secrets and namespace structure</p>"},{"location":"package-management-redisinsight/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-management-redisinsight/#common-issues","title":"Common Issues","text":"<p>Pod Won't Start: <pre><code># Check pod events and logs\nkubectl describe pod -l app.kubernetes.io/name=redisinsight\nkubectl logs -l app.kubernetes.io/name=redisinsight\n\n# Check storage issues\nkubectl describe pvc -l app.kubernetes.io/name=redisinsight\n</code></pre></p> <p>Cannot Access Web Interface: <pre><code># Verify service endpoints\nkubectl describe svc redisinsight\nkubectl get endpoints redisinsight\n\n# Test service connectivity\nkubectl run test-pod --image=curlimages/curl --rm -it -- \\\n  curl http://redisinsight.default.svc.cluster.local:5540/\n\n# Check IngressRoute configuration\nkubectl describe ingressroute redisinsight\n</code></pre></p> <p>Cannot Connect to Redis: <pre><code># Test Redis connectivity from RedisInsight pod\nkubectl exec -it deployment/redisinsight -- \\\n  nc -zv redis-master.default.svc.cluster.local 6379\n\n# Verify Redis is running\nkubectl get pods -l app.kubernetes.io/name=redis\nkubectl logs -l app.kubernetes.io/name=redis --tail=20\n\n# Test Redis authentication\nkubectl exec -it redis-master-0 -- redis-cli -a \"$REDIS_PASSWORD\" ping\n</code></pre></p> <p>First-Time Setup Issues: <pre><code># Check if RedisInsight data directory is writable\nkubectl exec -it deployment/redisinsight -- ls -la /data\n\n# Verify storage permissions\nkubectl exec -it deployment/redisinsight -- id\n\n# Check RedisInsight initialization logs\nkubectl logs -l app.kubernetes.io/name=redisinsight --tail=50\n</code></pre></p> <p>Performance Issues: <pre><code># Check resource usage\nkubectl top pod -l app.kubernetes.io/name=redisinsight\n\n# View detailed pod specifications\nkubectl describe pod -l app.kubernetes.io/name=redisinsight\n\n# Check storage performance\nkubectl exec -it deployment/redisinsight -- iostat -x 1 3\n</code></pre></p> <p>DNS Resolution Issues: <pre><code># Test DNS resolution\nkubectl run test-pod --image=busybox --rm -it -- \\\n  nslookup redisinsight.default.svc.cluster.local\n\n# Verify IngressRoute host matching\nkubectl get ingressroute redisinsight -o yaml | grep -A 5 \"match:\"\n\n# Test with different domain patterns\ncurl -H \"Host: redisinsight.localhost\" http://127.0.0.1/\ncurl -H \"Host: redisinsight.urbalurba.no\" http://127.0.0.1/\n</code></pre></p>"},{"location":"package-management-redisinsight/#maintenance","title":"\ud83d\udccb Maintenance","text":""},{"location":"package-management-redisinsight/#regular-tasks","title":"Regular Tasks","text":"<ol> <li>Health Monitoring: Check pod and service status regularly</li> <li>Storage Monitoring: Monitor disk usage for connection data and query history</li> <li>Access Review: Regularly review user accounts and Redis connections</li> <li>Connection Testing: Verify Redis connectivity and performance</li> </ol>"},{"location":"package-management-redisinsight/#backup-procedures","title":"Backup Procedures","text":"<pre><code># Backup RedisInsight configuration and user data\nkubectl exec -it deployment/redisinsight -- \\\n  tar -czf /tmp/redisinsight-backup.tar.gz /data\n\n# Copy backup to local system\nkubectl cp deployment/redisinsight:/tmp/redisinsight-backup.tar.gz ./redisinsight-backup.tar.gz\n\n# Backup Helm values\nkubectl get configmap -l app.kubernetes.io/name=redisinsight -o yaml &gt; redisinsight-config-backup.yaml\n</code></pre>"},{"location":"package-management-redisinsight/#updates-and-upgrades","title":"Updates and Upgrades","text":"<pre><code># Update Helm repository\nhelm repo update redisinsight\n\n# Check for chart updates\nhelm search repo redisinsight/redisinsight\n\n# Upgrade RedisInsight (if new chart version available)\nhelm upgrade redisinsight redisinsight/redisinsight \\\n  -f manifests/651-adm-redisinsight.yaml \\\n  --set persistence.storageClassName=\"$STORAGE_CLASS_NAME\"\n</code></pre>"},{"location":"package-management-redisinsight/#disaster-recovery","title":"Disaster Recovery","text":"<pre><code># Restore RedisInsight configuration from backup\nkubectl cp ./redisinsight-backup.tar.gz deployment/redisinsight:/tmp/\nkubectl exec -it deployment/redisinsight -- \\\n  tar -xzf /tmp/redisinsight-backup.tar.gz -C /\n\n# Restart RedisInsight to apply changes\nkubectl rollout restart deployment/redisinsight\n</code></pre> <p>\ud83d\udca1 Key Insight: RedisInsight provides a powerful web-based interface for Redis administration with first-time user setup and manual database connections. Unlike pgAdmin, RedisInsight requires users to manually configure Redis connections through the web interface, providing flexibility to connect to multiple Redis instances while maintaining security through manual credential entry.</p>"},{"location":"package-monitoring-grafana/","title":"Grafana - Visualization &amp; Dashboards","text":"<p>Key Features: Unified Visualization \u2022 Multi-Datasource Queries \u2022 Dashboard Sidecar \u2022 Explore Mode \u2022 Alert Management \u2022 User Authentication \u2022 Dashboard as Code \u2022 Plugin Ecosystem</p> <p>File: <code>docs/package-monitoring-grafana.md</code> Purpose: Complete guide to Grafana deployment and configuration for visualization and exploration in Urbalurba infrastructure Target Audience: DevOps engineers, platform administrators, SREs, developers, data analysts Last Updated: October 5, 2025</p> <p>Deployed Version: Grafana v12.1.1 (Helm Chart: grafana-10.0.0) Official Documentation: https://grafana.com/docs/grafana/v12.1/</p>"},{"location":"package-monitoring-grafana/#overview","title":"\ud83d\udccb Overview","text":"<p>Grafana is the unified visualization platform for the Urbalurba observability stack. It provides a single pane of glass for querying, visualizing, and alerting on data from Prometheus (metrics), Loki (logs), and Tempo (traces). Grafana's Explore mode enables ad-hoc investigation, while dashboards provide persistent monitoring views.</p> <p>As the front-end of the observability stack, Grafana enables: - Unified Querying: Query metrics, logs, and traces from a single interface - Dashboard Management: Auto-load dashboards from Kubernetes ConfigMaps - Correlation: Link metrics \u2192 logs \u2192 traces for complete context - Alerting: Define alert rules and notification channels - Exploration: Ad-hoc queries with Explore mode</p> <p>Key Capabilities: - Pre-Configured Datasources: Prometheus (default), Loki, Tempo ready to use - Dashboard Sidecar: Auto-loads dashboards from ConfigMaps with label <code>grafana_dashboard: \"1\"</code> - PromQL, LogQL, TraceQL: Native query language support for all backends - Correlation Links: Jump from metrics \u2192 logs \u2192 traces seamlessly - Web UI Access: <code>http://grafana.localhost</code> via Traefik IngressRoute - Persistent Storage: 10Gi PVC for dashboards and configuration</p> <p>Architecture Type: Web-based visualization and exploration platform</p>"},{"location":"package-monitoring-grafana/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"package-monitoring-grafana/#deployment-components","title":"Deployment Components","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Grafana Stack (namespace: monitoring)            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502            Grafana Deployment                  \u2502    \u2502\n\u2502  \u2502                                                \u2502    \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502\n\u2502  \u2502  \u2502  Web UI (Port 80)                        \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - Login/Authentication                  \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - Dashboard Rendering                   \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - Explore Mode                          \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - Alert Management                      \u2502 \u2502    \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502\n\u2502  \u2502                                                \u2502    \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502\n\u2502  \u2502  \u2502  Pre-Configured Datasources              \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502                                          \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  \u2022 Prometheus (default)                  \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502    url: prometheus-server:80             \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502                                          \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  \u2022 Loki                                  \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502    url: loki-gateway:80                  \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502                                          \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  \u2022 Tempo                                 \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502    url: tempo:3200                       \u2502 \u2502    \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502\n\u2502  \u2502                                                \u2502    \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502\n\u2502  \u2502  \u2502  Dashboard Sidecar (Auto-Load)           \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502                                          \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  Watches for ConfigMaps with:            \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  label: grafana_dashboard: \"1\"           \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  namespace: monitoring                   \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502                                          \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  Auto-reloads dashboards every 60s       \u2502 \u2502    \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502\n\u2502  \u2502                                                \u2502    \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502\n\u2502  \u2502  \u2502  Persistent Storage (10Gi PVC)           \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - Dashboard definitions                 \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - User preferences                      \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - Alert states                          \u2502 \u2502    \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                                                          \u2502\n\u2502  Access: http://grafana.localhost (Traefik Ingress)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                          \u25b2\n         \u2502                          \u2502\n         \u25bc                          \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Datasource Backends                                \u2502\n\u2502   - Prometheus (metrics)                             \u2502\n\u2502   - Loki (logs)                                      \u2502\n\u2502   - Tempo (traces)                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"package-monitoring-grafana/#data-flow","title":"Data Flow","text":"<pre><code>User Browser\n         \u2502\n         \u2502 HTTP (http://grafana.localhost)\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Traefik Ingress     \u2502\n\u2502  (Host: grafana.*)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u2502 Routes to Grafana Service\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Grafana Web UI              \u2502\n\u2502  (Port 80)                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  User Actions:               \u2502\n\u2502  1. Dashboard view           \u2502\n\u2502  2. Explore query            \u2502\n\u2502  3. Alert configuration      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u251c\u2500\u25ba Query Prometheus (PromQL)\n         \u251c\u2500\u25ba Query Loki (LogQL)\n         \u2514\u2500\u25ba Query Tempo (TraceQL)\n                  \u2502\n                  \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502  Render Results  \u2502\n         \u2502  - Graphs        \u2502\n         \u2502  - Tables        \u2502\n         \u2502  - Logs          \u2502\n         \u2502  - Traces        \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"package-monitoring-grafana/#dashboard-auto-loading","title":"Dashboard Auto-Loading","text":"<pre><code>ConfigMap Created\n(label: grafana_dashboard: \"1\")\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Dashboard Sidecar Container \u2502\n\u2502  (watches monitoring ns)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u2502 Detects new ConfigMap\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Load Dashboard JSON         \u2502\n\u2502  - Parse dashboard def       \u2502\n\u2502  - Register with Grafana     \u2502\n\u2502  - Assign to folder          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\nDashboard Available in UI (~30s)\n</code></pre>"},{"location":"package-monitoring-grafana/#file-structure","title":"File Structure","text":"<pre><code>manifests/\n\u251c\u2500\u2500 034-grafana-config.yaml                 # Grafana Helm values\n\u251c\u2500\u2500 035-grafana-dashboards.yaml             # (if exists) Installation test dashboards\n\u251c\u2500\u2500 036-grafana-sovdev-verification.yaml    # sovdev-logger verification dashboard\n\u2514\u2500\u2500 038-grafana-ingressroute.yaml           # Traefik IngressRoute\n\nansible/playbooks/\n\u251c\u2500\u2500 034-setup-grafana.yml                   # Deployment automation\n\u2514\u2500\u2500 034-remove-grafana.yml                  # Removal automation\n\nprovision-host/kubernetes/11-monitoring/not-in-use/\n\u251c\u2500\u2500 05-setup-grafana.sh                     # Shell script wrapper\n\u2514\u2500\u2500 05-remove-grafana.sh                    # Removal script\n\nStorage:\n\u2514\u2500\u2500 PersistentVolumeClaim\n    \u2514\u2500\u2500 grafana (10Gi)                      # Configuration and dashboards\n</code></pre>"},{"location":"package-monitoring-grafana/#deployment","title":"\ud83d\ude80 Deployment","text":""},{"location":"package-monitoring-grafana/#automated-deployment","title":"Automated Deployment","text":"<p>Via Monitoring Stack (Recommended): <pre><code># Deploy entire monitoring stack (includes Grafana)\ndocker exec -it provision-host bash\ncd /mnt/urbalurbadisk/provision-host/kubernetes/11-monitoring/not-in-use\n./00-setup-all-monitoring.sh rancher-desktop\n</code></pre></p> <p>Individual Deployment: <pre><code># Deploy Grafana only (requires Prometheus, Loki, Tempo already deployed)\ndocker exec -it provision-host bash\ncd /mnt/urbalurbadisk/provision-host/kubernetes/11-monitoring/not-in-use\n./05-setup-grafana.sh rancher-desktop\n</code></pre></p>"},{"location":"package-monitoring-grafana/#manual-deployment","title":"Manual Deployment","text":"<p>Prerequisites: - Kubernetes cluster running (Rancher Desktop) - <code>monitoring</code> namespace exists - Datasources deployed first: Prometheus, Loki, Tempo - Helm installed in provision-host container - Manifest files: <code>034-grafana-config.yaml</code>, <code>038-grafana-ingressroute.yaml</code></p> <p>Deployment Steps: <pre><code># 1. Enter provision-host container\ndocker exec -it provision-host bash\n\n# 2. Add Grafana Helm repository\nhelm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n\n# 3. Deploy Grafana\nhelm upgrade --install grafana grafana/grafana \\\n  -f /mnt/urbalurbadisk/manifests/034-grafana-config.yaml \\\n  --namespace monitoring \\\n  --create-namespace \\\n  --timeout 600s \\\n  --kube-context rancher-desktop\n\n# 4. Deploy IngressRoute for web UI access\nkubectl apply -f /mnt/urbalurbadisk/manifests/038-grafana-ingressroute.yaml\n\n# 5. Wait for pods to be ready\nkubectl wait --for=condition=ready pod \\\n  -l app.kubernetes.io/name=grafana \\\n  -n monitoring --timeout=300s\n</code></pre></p> <p>Deployment Time: ~2-3 minutes</p>"},{"location":"package-monitoring-grafana/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"package-monitoring-grafana/#grafana-configuration-manifests034-grafana-configyaml","title":"Grafana Configuration (<code>manifests/034-grafana-config.yaml</code>)","text":"<p>Admin Credentials: <pre><code>adminUser: admin\nadminPassword: SecretPassword1\n</code></pre></p> <p>Pre-Configured Datasources: <pre><code>datasources:\n  datasources.yaml:\n    apiVersion: 1\n    datasources:\n      # Prometheus (default datasource)\n      - name: Prometheus\n        type: prometheus\n        uid: prometheus\n        url: http://prometheus-server.monitoring.svc.cluster.local:80\n        access: proxy\n        isDefault: true\n        editable: true\n\n      # Loki (logs)\n      - name: Loki\n        type: loki\n        uid: loki\n        url: http://loki-gateway.monitoring.svc.cluster.local:80\n        access: proxy\n        editable: true\n\n      # Tempo (traces)\n      - name: Tempo\n        type: tempo\n        uid: tempo\n        url: http://tempo.monitoring.svc.cluster.local:3200\n        access: proxy\n        editable: true\n</code></pre></p> <p>Official Datasource Docs: - Prometheus: https://grafana.com/docs/grafana/v12.1/datasources/prometheus/ - Loki: https://grafana.com/docs/grafana/v12.1/datasources/loki/ - Tempo: https://grafana.com/docs/grafana/v12.1/datasources/tempo/</p> <p>Dashboard Sidecar (Auto-Loading): <pre><code>sidecar:\n  dashboards:\n    enabled: true\n    label: grafana_dashboard           # Watch for ConfigMaps with this label\n    labelValue: \"1\"\n    folder: /tmp/dashboards\n    searchNamespace: monitoring         # Only watch monitoring namespace\n    folderAnnotation: grafana_folder    # Optional folder organization\n    provider:\n      foldersFromFilesStructure: true\n</code></pre></p> <p>Persistent Storage: <pre><code>persistence:\n  enabled: true\n  size: 10Gi\n</code></pre></p>"},{"location":"package-monitoring-grafana/#external-access-configuration-manifests038-grafana-ingressrouteyaml","title":"External Access Configuration (<code>manifests/038-grafana-ingressroute.yaml</code>)","text":"<p>Traefik IngressRoute: <pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: grafana\n  namespace: monitoring\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: HostRegexp(`grafana\\..+`)   # Matches grafana.localhost, grafana.urbalurba.no, etc.\n      kind: Rule\n      services:\n        - name: grafana\n          port: 80\n</code></pre></p> <p>Access URLs: - Localhost: <code>http://grafana.localhost</code> - Future External: <code>http://grafana.urbalurba.no</code> (requires DNS configuration)</p>"},{"location":"package-monitoring-grafana/#resource-configuration","title":"Resource Configuration","text":"<p>Storage Requirements: - Grafana PVC: 10Gi persistent volume</p> <p>Service Endpoints: - Web UI: <code>grafana.monitoring.svc.cluster.local:80</code> - External UI: <code>http://grafana.localhost</code> (via Traefik)</p>"},{"location":"package-monitoring-grafana/#security-configuration","title":"Security Configuration","text":"<p>Authentication: - Default Credentials: <code>admin</code> / <code>SecretPassword1</code> - Configuration Source: Defined in <code>manifests/034-grafana-config.yaml</code> (lines 31-32)   <pre><code>adminUser: admin\nadminPassword: SecretPassword1\n</code></pre> - Not Hardcoded: The password is set via Helm values file, not hardcoded in the Grafana chart - Customization: Change password by editing <code>034-grafana-config.yaml</code> and running <code>helm upgrade</code> - Production Recommendation: Change default password for production deployments - Future Enhancement: Authentik SSO integration (optional)</p> <p>Network Access: - Internal: ClusterIP service for internal cluster access - External: Traefik IngressRoute at <code>grafana.localhost</code> (HTTP, port 80)</p>"},{"location":"package-monitoring-grafana/#monitoring-verification","title":"\ud83d\udd0d Monitoring &amp; Verification","text":""},{"location":"package-monitoring-grafana/#health-checks","title":"Health Checks","text":"<p>Check Pod Status: <pre><code># Grafana pods (main + sidecar containers)\nkubectl get pods -n monitoring -l app.kubernetes.io/name=grafana\n\n# Expected output:\nNAME                       READY   STATUS    RESTARTS   AGE\ngrafana-xxx                2/2     Running   0          5m\n</code></pre></p> <p>Check Service Endpoints: <pre><code># Verify service is accessible\nkubectl get svc -n monitoring -l app.kubernetes.io/name=grafana\n\n# Expected service:\ngrafana   ClusterIP   10.43.x.x   80/TCP\n</code></pre></p>"},{"location":"package-monitoring-grafana/#service-verification","title":"Service Verification","text":"<p>Test Web UI Access: <pre><code># Via Traefik IngressRoute\ncurl -H \"Host: grafana.localhost\" http://127.0.0.1/\n\n# Expected: HTML response with Grafana login page\n</code></pre></p> <p>Test Datasource Connectivity (from within pod): <pre><code># Test Prometheus datasource\nkubectl exec -n monitoring deployment/grafana -- \\\n  curl -s http://prometheus-server.monitoring.svc.cluster.local:80/api/v1/status/config\n\n# Test Loki datasource\nkubectl exec -n monitoring deployment/grafana -- \\\n  curl -s http://loki-gateway.monitoring.svc.cluster.local:80/ready\n\n# Test Tempo datasource\nkubectl exec -n monitoring deployment/grafana -- \\\n  curl -s http://tempo.monitoring.svc.cluster.local:3200/ready\n</code></pre></p>"},{"location":"package-monitoring-grafana/#verify-datasources-in-ui","title":"Verify Datasources in UI","text":"<ol> <li>Open <code>http://grafana.localhost</code></li> <li>Login: <code>admin</code> / <code>SecretPassword1</code></li> <li>Navigate to Configuration \u2192 Data sources</li> <li>Verify all three datasources are listed:</li> <li>\u2705 Prometheus (default)</li> <li>\u2705 Loki</li> <li>\u2705 Tempo</li> </ol>"},{"location":"package-monitoring-grafana/#automated-verification","title":"Automated Verification","text":"<p>The deployment playbook (<code>034-setup-grafana.yml</code>) performs automated tests: 1. \u2705 Web UI accessibility 2. \u2705 Datasource configuration verification 3. \u2705 Dashboard sidecar functionality 4. \u2705 Test data generation and visualization (Installation Test Suite)</p>"},{"location":"package-monitoring-grafana/#installation-test-suite-dashboards","title":"Installation Test Suite Dashboards","text":"<p>Grafana automatically deploys 3 validation dashboards organized in the \"Installation Test Suite\" folder. These dashboards verify end-to-end functionality of the monitoring stack by displaying test telemetry generated during setup.</p> <p>Purpose: Validate that logs, traces, and metrics flow correctly from OTLP Collector \u2192 Loki/Tempo/Prometheus \u2192 Grafana</p> <p>Dashboards Deployed (<code>manifests/035-grafana-test-dashboards.yaml</code>):</p>"},{"location":"package-monitoring-grafana/#1-test-data-logs","title":"1. Test Data - Logs","text":"<p>UID: <code>test-data-logs</code> Query: <code>{service_name=\"telemetrygen-logs\"}</code> Expected Data: 100+ log entries from <code>telemetrygen</code> tool</p> <p>What This Validates: - \u2705 OTLP Collector receives logs via HTTP - \u2705 Logs are exported from OTLP Collector to Loki - \u2705 Loki indexes logs by <code>service_name</code> label - \u2705 Grafana can query Loki datasource via LogQL - \u2705 Log panel displays structured log entries</p> <p>How Test Data is Generated (during Grafana setup): <pre><code># Ansible playbook runs this command (step 23):\nkubectl run telemetrygen-dashboard-logs \\\n  --image=ghcr.io/open-telemetry/opentelemetry-collector-contrib/telemetrygen:latest \\\n  --rm -i --restart=Never -n monitoring -- \\\n  logs --otlp-endpoint=otel-collector-opentelemetry-collector.monitoring.svc.cluster.local:4318 \\\n  --otlp-insecure --otlp-http --duration=10s --logs=100 \\\n  --service telemetrygen-logs \\\n  --body \"Test log entry for Installation Test Suite dashboard\"\n</code></pre></p> <p>How to Use: 1. Open <code>http://grafana.localhost</code> 2. Login: <code>admin</code> / <code>SecretPassword1</code> 3. Navigate to Dashboards \u2192 Installation Test Suite \u2192 Test Data - Logs 4. Verify panel shows 100+ log entries 5. Expand log entries to see structured fields (timestamp, service_name, body)</p> <p>Troubleshooting: - No logs displayed: Check OTLP Collector logs for ingestion errors - \"No data\" message: Query Loki directly: <code>kubectl exec -n monitoring loki-0 -c loki -- wget -q -O - 'http://localhost:3100/loki/api/v1/label/service_name/values'</code> - Old data only: Generate fresh test data (see command above)</p>"},{"location":"package-monitoring-grafana/#2-test-data-traces","title":"2. Test Data - Traces","text":"<p>UID: <code>test-data-traces</code> Query: <code>{resource.service.name=\"telemetrygen-traces\"}</code> Expected Data: 20+ trace entries from <code>telemetrygen</code> tool</p> <p>What This Validates: - \u2705 OTLP Collector receives traces via gRPC - \u2705 Traces are exported from OTLP Collector to Tempo - \u2705 Tempo stores trace data with resource attributes - \u2705 Grafana can query Tempo datasource via TraceQL - \u2705 Trace count stat panel shows total traces - \u2705 Trace table displays trace IDs for inspection</p> <p>How Test Data is Generated (during Grafana setup): <pre><code># Ansible playbook runs this command (step 24):\nkubectl run telemetrygen-dashboard-traces \\\n  --image=ghcr.io/open-telemetry/opentelemetry-collector-contrib/telemetrygen:latest \\\n  --rm -i --restart=Never -n monitoring -- \\\n  traces --otlp-endpoint=otel-collector-opentelemetry-collector.monitoring.svc.cluster.local:4317 \\\n  --otlp-insecure --duration=5s --traces=20 \\\n  --service telemetrygen-traces\n</code></pre></p> <p>How to Use: 1. Navigate to Dashboards \u2192 Installation Test Suite \u2192 Test Data - Traces 2. Verify Trace Count stat panel shows 20+ traces (green background = success) 3. View All Test Traces table with trace IDs 4. Click on a trace ID to open trace waterfall view (spans visualization) 5. Inspect trace spans, duration, and resource attributes</p> <p>Troubleshooting: - Trace count shows 0: Check Tempo ingestion: <code>kubectl logs -n monitoring tempo-0 | grep telemetrygen</code> - Table empty: Query Tempo API directly: <code>kubectl exec -n monitoring tempo-0 -- wget -q -O - 'http://localhost:3200/api/search?tags=service.name%3Dtelemetrygen-traces'</code> - Old traces only: Generate fresh test data (see command above)</p>"},{"location":"package-monitoring-grafana/#3-test-data-metrics","title":"3. Test Data - Metrics","text":"<p>UID: <code>test-data-metrics</code> Query: <code>up</code> (Prometheus 'up' metric for all scraped targets) Expected Data: Timeseries graph showing health of all monitored services</p> <p>What This Validates: - \u2705 Prometheus scrapes metrics from all targets - \u2705 Prometheus stores time-series data - \u2705 Grafana can query Prometheus datasource via PromQL - \u2705 Timeseries panel displays multiple metrics with legend - \u2705 Monitoring stack services are healthy (value = 1)</p> <p>How Test Data is Available: - No generation needed: Prometheus automatically scrapes <code>up</code> metric from all targets (Prometheus server, alertmanager, node-exporter, kube-state-metrics, pushgateway, OTLP Collector, Loki, Tempo, Grafana) - Metric value: <code>1</code> = service is up and responding to scrapes, <code>0</code> = service is down</p> <p>How to Use: 1. Navigate to Dashboards \u2192 Installation Test Suite \u2192 Test Data - Metrics 2. View timeseries graph showing multiple services 3. Check legend on right: All services should show <code>1</code> (up) in \"Last\" column 4. Hover over graph lines to see individual service metrics 5. Verify services like <code>prometheus-server</code>, <code>loki</code>, <code>tempo</code> are present</p> <p>Troubleshooting: - No metrics displayed: Check Prometheus targets: <code>kubectl port-forward -n monitoring svc/prometheus-server 9090:80</code> \u2192 Open <code>http://localhost:9090/targets</code> - Services showing 0: Check pod health: <code>kubectl get pods -n monitoring</code> - Missing services in legend: Verify Prometheus ServiceMonitor configuration</p> <p>Access Installation Test Suite: <pre><code># Open Grafana\nopen http://grafana.localhost\n\n# Navigate to folder\nDashboards \u2192 Browse \u2192 Installation Test Suite folder\n</code></pre></p> <p>Dashboard Files: - ConfigMap Manifest: <code>manifests/035-grafana-test-dashboards.yaml</code> - 3 ConfigMaps: <code>grafana-dashboard-test-logs</code>, <code>grafana-dashboard-test-traces</code>, <code>grafana-dashboard-test-metrics</code> - Folder Label: <code>grafana_folder: \"Installation Test Suite\"</code></p> <p>Dashboard Auto-Loading: - Dashboards are automatically loaded via Grafana sidecar (~30-60 seconds after deployment) - No manual import required - Changes to ConfigMaps automatically reload in Grafana</p> <p>Regenerate Test Data (if dashboards show no data): <pre><code># Generate logs\nkubectl run telemetrygen-logs-manual \\\n  --image=ghcr.io/open-telemetry/opentelemetry-collector-contrib/telemetrygen:latest \\\n  --rm -i --restart=Never -n monitoring -- \\\n  logs --otlp-endpoint=otel-collector-opentelemetry-collector.monitoring.svc.cluster.local:4318 \\\n  --otlp-insecure --otlp-http --duration=10s --logs=100 \\\n  --service telemetrygen-logs \\\n  --body \"Manual test log entry\"\n\n# Generate traces\nkubectl run telemetrygen-traces-manual \\\n  --image=ghcr.io/open-telemetry/opentelemetry-collector-contrib/telemetrygen:latest \\\n  --rm -i --restart=Never -n monitoring -- \\\n  traces --otlp-endpoint=otel-collector-opentelemetry-collector.monitoring.svc.cluster.local:4317 \\\n  --otlp-insecure --duration=5s --traces=20 \\\n  --service telemetrygen-traces\n</code></pre></p>"},{"location":"package-monitoring-grafana/#sovdev-logger-dashboards","title":"sovdev-logger Dashboards","text":"<p>Grafana includes two pre-built dashboards for monitoring applications using sovdev-logger, which provides zero-effort observability through automatic logs, metrics, and traces generation.</p> <p>Dashboards Deployed: - Fast Metrics Dashboard (<code>manifests/037-grafana-sovdev-metrics.yaml</code>) - Verification Dashboard (<code>manifests/036-grafana-sovdev-verification.yaml</code>)</p>"},{"location":"package-monitoring-grafana/#fast-metrics-dashboard","title":"Fast Metrics Dashboard","text":"<p>UID: <code>sovdev-metrics</code> Purpose: Real-time application monitoring using Prometheus metrics (sub-second query performance)</p> <p>Data Source: Prometheus Queries: Uses automatic metrics generated by sovdev-logger: - <code>sovdev_operations_total</code> - Total operations counter - <code>sovdev_errors_total</code> - Error counter (ERROR/FATAL levels) - <code>sovdev_operation_duration_milliseconds</code> - Duration histogram - <code>sovdev_operations_active</code> - Active operations gauge</p> <p>What This Dashboard Shows: - \u2705 Operations Rate: Requests per second by service and log type - \u2705 Error Rate: Errors per second by service and log type - \u2705 Operation Duration: P50, P95, P99 latency percentiles - \u2705 Active Operations: Currently in-progress operations - \u2705 Service Dependency Graph: Automatically generated from traces (via Tempo metrics generator)</p> <p>Dashboard Variables: - <code>service_name</code> - Filter by specific service - <code>log_type</code> - Filter by log type (API, DATABASE, BATCH, etc.) - <code>peer_service</code> - Filter by downstream service</p> <p>Benefits: - Sub-second queries: Prometheus metrics enable fast dashboard load times - Real-time monitoring: Track live application behavior - No code changes: Metrics automatically generated from sovdevLog() calls - Full dimensional filtering: service_name, peer_service, log_level, log_type</p> <p>How to Use: 1. Open <code>http://grafana.localhost</code> 2. Navigate to Dashboards \u2192 sovdev-logger \u2192 Fast Metrics Dashboard 3. Select service from <code>service_name</code> dropdown 4. View operation rates, errors, latencies, and service graphs 5. Click on panels to drill down into specific time ranges</p> <p>Requirements: - sovdev-logger in application - <code>OTEL_EXPORTER_OTLP_METRICS_ENDPOINT</code> configured - Tempo metrics generator enabled (for service graphs)</p>"},{"location":"package-monitoring-grafana/#verification-dashboard","title":"Verification Dashboard","text":"<p>UID: <code>sovdev-verification</code> Purpose: Debug and verify complete observability correlation (logs + metrics + traces)</p> <p>Data Sources: Loki (logs), Tempo (traces), Prometheus (metrics) Purpose: Verify traceId correlation and debug specific application executions</p> <p>What This Dashboard Shows: - \u2705 Log Entries: Structured logs from Loki with all attributes - \u2705 Trace Correlation: Click traceId in logs to jump to trace waterfall - \u2705 Session Filtering: Filter by session.id to isolate specific runs - \u2705 Full Context: Input/response JSON, function names, log levels - \u2705 Error Details: Exception stack traces and error messages</p> <p>Dashboard Variables: - <code>service_name</code> - Filter by specific service - <code>session_id</code> - Filter by specific execution (unique per run) - <code>log_level</code> - Filter by log level (ERROR, WARN, INFO, DEBUG)</p> <p>Benefits: - Full correlation: Link logs \u2192 traces \u2192 metrics via traceId - Session isolation: Debug specific runs without time-based filtering - Complete context: See input/response data alongside logs and traces - Error investigation: Jump from error log to full trace waterfall</p> <p>How to Use: 1. Navigate to Dashboards \u2192 sovdev-logger \u2192 Verification Dashboard 2. Option A - Debug specific session:    - Copy session ID from application startup: <code>\ud83d\udd11 Session ID: abc123-def456-ghi789</code>    - Enter in <code>session_id</code> variable    - View all logs/metrics/traces from that execution 3. Option B - Investigate errors:    - Set <code>log_level</code> to \"ERROR\"    - View error logs with stack traces    - Click traceId to see full request trace 4. Option C - Analyze specific service:    - Select <code>service_name</code>    - View chronological log stream    - Expand log entries to see full JSON context</p> <p>Requirements: - sovdev-logger in application - <code>OTEL_EXPORTER_OTLP_LOGS_ENDPOINT</code> configured - <code>OTEL_EXPORTER_OTLP_TRACES_ENDPOINT</code> configured - OTEL Collector session_id processing enabled</p> <p>Access sovdev-logger Dashboards: <pre><code># Open Grafana\nopen http://grafana.localhost\n\n# Navigate to dashboards\nDashboards \u2192 Browse \u2192 sovdev-logger folder\n</code></pre></p> <p>Dashboard Files: - Fast Metrics Dashboard: <code>manifests/037-grafana-sovdev-metrics.yaml</code> - Verification Dashboard: <code>manifests/036-grafana-sovdev-verification.yaml</code></p> <p>Auto-Loading: Both dashboards are automatically loaded via Grafana sidecar (~30-60 seconds after deployment)</p> <p>Official sovdev-logger Documentation: See <code>docs/package-monitoring-sovdev-logger.md</code> for library usage and features.</p>"},{"location":"package-monitoring-grafana/#management-operations","title":"\ud83d\udee0\ufe0f Management Operations","text":""},{"location":"package-monitoring-grafana/#access-grafana-ui","title":"Access Grafana UI","text":"<p>Open in Browser: <pre><code># Direct access (Mac host)\nopen http://grafana.localhost\n\n# Or manually navigate to:\nhttp://grafana.localhost\n</code></pre></p> <p>Login Credentials: - Username: <code>admin</code> - Password: <code>SecretPassword1</code></p>"},{"location":"package-monitoring-grafana/#dashboard-management","title":"Dashboard Management","text":"<p>Dashboards are managed as Kubernetes ConfigMaps with automatic loading via the Grafana sidecar container. This GitOps-style approach enables version-controlled dashboard definitions.</p>"},{"location":"package-monitoring-grafana/#add-new-dashboard","title":"Add New Dashboard","text":"<p>Method 1: Design in Grafana UI, Export, Convert to ConfigMap</p> <ol> <li> <p>Design dashboard in Grafana UI:    <pre><code>open http://grafana.localhost\n# Login: admin/SecretPassword1\n# Create dashboard \u2192 Add panels \u2192 Configure queries \u2192 Save\n</code></pre></p> </li> <li> <p>Export dashboard JSON:</p> </li> <li>Open dashboard \u2192 Settings (gear icon) \u2192 JSON Model</li> <li> <p>Copy entire JSON content</p> </li> <li> <p>Create ConfigMap manifest (<code>manifests/0XX-grafana-my-dashboard.yaml</code>):    <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: grafana-dashboard-my-service\n  namespace: monitoring\n  labels:\n    grafana_dashboard: \"1\"           # Required for auto-loading\ndata:\n  my-service.json: |\n    {\n      \"title\": \"My Service Metrics\",\n      \"uid\": \"my-service-metrics\",\n      \"panels\": [\n        {\n          \"type\": \"graph\",\n          \"title\": \"Request Rate\",\n          \"targets\": [\n            {\n              \"expr\": \"rate(http_requests_total{service=\\\"my-service\\\"}[5m])\",\n              \"refId\": \"A\"\n            }\n          ]\n        }\n      ]\n    }\n</code></pre></p> </li> <li> <p>Apply ConfigMap:    <pre><code>kubectl apply -f manifests/0XX-grafana-my-dashboard.yaml\n</code></pre></p> </li> <li> <p>Verify dashboard auto-loads (~30-60 seconds):</p> </li> <li>Check sidecar logs: <code>kubectl logs -n monitoring deployment/grafana -c grafana-sc-dashboard</code></li> <li>Grafana UI \u2192 Dashboards \u2192 Search for \"My Service Metrics\"</li> </ol> <p>Method 2: Write JSON Directly (for simple dashboards): <pre><code># Create ConfigMap with inline JSON\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: grafana-dashboard-simple\n  namespace: monitoring\n  labels:\n    grafana_dashboard: \"1\"\ndata:\n  simple.json: |\n    {\n      \"title\": \"Simple Dashboard\",\n      \"uid\": \"simple-dashboard\",\n      \"panels\": []\n    }\nEOF\n</code></pre></p>"},{"location":"package-monitoring-grafana/#update-existing-dashboard","title":"Update Existing Dashboard","text":"<ol> <li> <p>Edit ConfigMap manifest (<code>manifests/0XX-grafana-my-dashboard.yaml</code>):    <pre><code>vim manifests/0XX-grafana-my-dashboard.yaml\n# Modify JSON in data.my-service.json\n</code></pre></p> </li> <li> <p>Apply updated ConfigMap:    <pre><code>kubectl apply -f manifests/0XX-grafana-my-dashboard.yaml\n</code></pre></p> </li> <li> <p>Wait for automatic reload (~30-60 seconds) or force reload:    <pre><code>kubectl rollout restart deployment/grafana -n monitoring\n</code></pre></p> </li> <li> <p>Verify changes in Grafana UI (may need to refresh browser)</p> </li> </ol> <p>Alternative: Update via kubectl edit: <pre><code>kubectl edit configmap -n monitoring grafana-dashboard-my-service\n# Edit JSON directly in editor\n# Save \u2192 Auto-reloads in ~60s\n</code></pre></p>"},{"location":"package-monitoring-grafana/#delete-dashboard","title":"Delete Dashboard","text":"<p>Option 1: Remove ConfigMap (recommended for GitOps): <pre><code># Delete manifest file\nkubectl delete -f manifests/0XX-grafana-my-dashboard.yaml\n\n# Or delete directly by name\nkubectl delete configmap -n monitoring grafana-dashboard-my-service\n</code></pre> Dashboard automatically disappears from Grafana UI within ~60 seconds.</p> <p>Option 2: Delete via Grafana UI (not persistent): - Grafana UI \u2192 Dashboards \u2192 Find dashboard \u2192 Settings \u2192 Delete - \u26a0\ufe0f Dashboard will reappear if ConfigMap still exists (sidecar will reload it)</p>"},{"location":"package-monitoring-grafana/#dashboard-organization","title":"Dashboard Organization","text":"<p>Folder Assignment (via annotation): <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: grafana-dashboard-app\n  namespace: monitoring\n  labels:\n    grafana_dashboard: \"1\"\n  annotations:\n    grafana_folder: \"Application Monitoring\"  # Assigns to folder in UI\ndata:\n  app.json: |\n    { ... }\n</code></pre></p> <p>Naming Convention: - ConfigMap name: <code>grafana-dashboard-&lt;purpose&gt;</code> - Dashboard JSON key: <code>&lt;descriptive-name&gt;.json</code> - Manifest file: <code>manifests/0XX-grafana-&lt;purpose&gt;.yaml</code> (use numbering 035-039)</p>"},{"location":"package-monitoring-grafana/#examples","title":"Examples","text":"<p>Existing Dashboards: - <code>manifests/035-grafana-test-dashboards.yaml</code> - Installation Test Suite (3 dashboards):   - Test Data - Logs: Validates OTLP \u2192 Loki \u2192 Grafana flow   - Test Data - Traces: Validates OTLP \u2192 Tempo \u2192 Grafana flow   - Test Data - Metrics: Validates Prometheus \u2192 Grafana flow   - See \"Installation Test Suite Dashboards\" section above for details - <code>manifests/036-grafana-sovdev-verification.yaml</code> - sovdev-logger Verification Dashboard:   - Debug logs/traces/metrics correlation   - Session filtering for specific executions   - TraceId links to full trace waterfall   - See \"sovdev-logger Dashboards\" section above for details - <code>manifests/037-grafana-sovdev-metrics.yaml</code> - sovdev-logger Fast Metrics Dashboard:   - Real-time Prometheus metrics from sovdev-logger   - Operation rates, error rates, latencies   - Service dependency graphs   - See \"sovdev-logger Dashboards\" section above for details</p> <p>Official Dashboard Docs: https://grafana.com/docs/grafana/v12.1/dashboards/</p>"},{"location":"package-monitoring-grafana/#troubleshooting-dashboard-management","title":"Troubleshooting Dashboard Management","text":"<p>Dashboard not appearing: <pre><code># 1. Verify ConfigMap exists with correct label\nkubectl get configmap -n monitoring -l grafana_dashboard=1\n\n# 2. Check sidecar logs for errors\nkubectl logs -n monitoring deployment/grafana -c grafana-sc-dashboard --tail=50\n\n# 3. Force reload\nkubectl rollout restart deployment/grafana -n monitoring\n</code></pre></p> <p>Dashboard shows old version: <pre><code># Refresh sidecar (faster than full restart)\nkubectl delete pod -n monitoring -l app.kubernetes.io/name=grafana\n\n# Or clear browser cache and refresh\n</code></pre></p>"},{"location":"package-monitoring-grafana/#explore-mode-usage","title":"Explore Mode Usage","text":"<p>Query Logs in Loki: 1. Navigate to Explore \u2192 Select Loki datasource 2. Enter LogQL query:    <pre><code>{service_name=\"sovdev-test-company-lookup-typescript\"}\n</code></pre> 3. Run query to view log stream</p> <p>Query Metrics in Prometheus: 1. Navigate to Explore \u2192 Select Prometheus datasource 2. Enter PromQL query:    <pre><code>rate(prometheus_http_requests_total[5m])\n</code></pre> 3. Run query to view metrics graph</p> <p>Query Traces in Tempo: 1. Navigate to Explore \u2192 Select Tempo datasource 2. Enter TraceQL query:    <pre><code>{resource.service.name=\"my-app\"}\n</code></pre> 3. View trace waterfall/flamegraph</p> <p>Official Explore Docs: https://grafana.com/docs/grafana/v12.1/explore/</p>"},{"location":"package-monitoring-grafana/#correlation-workflow","title":"Correlation Workflow","text":"<p>Metrics \u2192 Logs \u2192 Traces: 1. Find metric spike in Prometheus dashboard 2. Note timestamp and service name 3. Switch to Loki, query logs for that time range 4. Find <code>trace_id</code> in log entry 5. Switch to Tempo, query by <code>trace_id</code> 6. View complete request flow with logs and trace spans</p>"},{"location":"package-monitoring-grafana/#service-removal","title":"Service Removal","text":"<p>Automated Removal: <pre><code>docker exec -it provision-host bash\ncd /mnt/urbalurbadisk/provision-host/kubernetes/11-monitoring/not-in-use\n./05-remove-grafana.sh rancher-desktop\n</code></pre></p> <p>Manual Removal: <pre><code># Remove Helm chart\nhelm uninstall grafana -n monitoring --kube-context rancher-desktop\n\n# Remove IngressRoute\nkubectl delete ingressroute -n monitoring grafana\n\n# Remove PVC (optional - preserves data if omitted)\nkubectl delete pvc -n monitoring -l app.kubernetes.io/name=grafana\n</code></pre></p>"},{"location":"package-monitoring-grafana/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-monitoring-grafana/#common-issues","title":"Common Issues","text":"<p>Cannot Access Web UI: <pre><code># 1. Check IngressRoute exists\nkubectl get ingressroute -n monitoring grafana\n\n# 2. Test with Host header\ncurl -v -H \"Host: grafana.localhost\" http://127.0.0.1/\n\n# 3. Check Traefik logs\nkubectl logs -n traefik -l app.kubernetes.io/name=traefik | grep grafana\n\n# 4. Verify Grafana pod is running\nkubectl get pods -n monitoring -l app.kubernetes.io/name=grafana\n</code></pre></p> <p>Datasource Connection Errors: <pre><code># Test datasource connectivity from Grafana pod\nkubectl exec -n monitoring deployment/grafana -- \\\n  curl -v http://prometheus-server.monitoring.svc.cluster.local:80/api/v1/status/config\n\n# Check if backend services are running\nkubectl get pods -n monitoring -l app.kubernetes.io/name=prometheus\nkubectl get pods -n monitoring -l app.kubernetes.io/name=loki\nkubectl get pods -n monitoring -l app.kubernetes.io/name=tempo\n</code></pre></p> <p>Dashboard Not Auto-Loading: <pre><code># 1. Verify ConfigMap has correct label\nkubectl get configmap -n monitoring -l grafana_dashboard=1\n\n# 2. Check sidecar logs\nkubectl logs -n monitoring deployment/grafana -c grafana-sc-dashboard\n\n# 3. Verify ConfigMap is in correct namespace\nkubectl get configmap -n monitoring my-dashboard\n\n# 4. Force reload by restarting Grafana\nkubectl rollout restart deployment/grafana -n monitoring\n</code></pre></p> <p>Login Issues: <pre><code># Reset admin password (if forgotten)\nkubectl exec -n monitoring deployment/grafana -- \\\n  grafana-cli admin reset-admin-password NewPassword123\n\n# Check Grafana logs for authentication errors\nkubectl logs -n monitoring -l app.kubernetes.io/name=grafana -c grafana\n</code></pre></p>"},{"location":"package-monitoring-grafana/#maintenance","title":"\ud83d\udccb Maintenance","text":""},{"location":"package-monitoring-grafana/#update-grafana","title":"Update Grafana:","text":"<pre><code># Update Helm chart to latest version\nhelm repo update\nhelm upgrade grafana grafana/grafana \\\n  -f /mnt/urbalurbadisk/manifests/034-grafana-config.yaml \\\n  -n monitoring \\\n  --kube-context rancher-desktop\n</code></pre>"},{"location":"package-monitoring-grafana/#backup-dashboards","title":"Backup Dashboards:","text":"<pre><code># Export all dashboards via API\nkubectl port-forward -n monitoring svc/grafana 3000:80\n\n# Use Grafana API to export (from Mac host)\ncurl -u admin:SecretPassword1 \\\n  http://localhost:3000/api/search?type=dash-db | \\\n  jq -r '.[].uid' | \\\n  xargs -I {} curl -u admin:SecretPassword1 \\\n    http://localhost:3000/api/dashboards/uid/{} \\\n    &gt; dashboard-{}.json\n</code></pre>"},{"location":"package-monitoring-grafana/#backup-pvc-data","title":"Backup PVC Data:","text":"<pre><code># Export Grafana configuration\nkubectl exec -n monitoring deployment/grafana -- \\\n  tar czf /tmp/grafana-backup.tar.gz /var/lib/grafana\n\n# Copy to local machine\nkubectl cp monitoring/grafana-xxx:/tmp/grafana-backup.tar.gz \\\n  ./grafana-backup.tar.gz -c grafana\n</code></pre>"},{"location":"package-monitoring-grafana/#use-cases","title":"\ud83d\ude80 Use Cases","text":""},{"location":"package-monitoring-grafana/#1-create-custom-dashboard","title":"1. Create Custom Dashboard","text":"<p>Using Grafana UI: 1. Navigate to Dashboards \u2192 New \u2192 New Dashboard 2. Add panel with Prometheus query:    <pre><code>rate(prometheus_http_requests_total[5m])\n</code></pre> 3. Save dashboard 4. Export JSON: Dashboard settings \u2192 JSON Model \u2192 Copy JSON 5. Create ConfigMap with exported JSON 6. Apply ConfigMap for auto-loading</p>"},{"location":"package-monitoring-grafana/#2-log-analysis-workflow","title":"2. Log Analysis Workflow","text":"<p>Find Errors in Logs: 1. Explore \u2192 Loki 2. Query:    <pre><code>{service_name=\"my-app\"} |= \"error\"\n</code></pre> 3. Filter time range to last 15 minutes 4. Expand log entries to view full context 5. Copy <code>trace_id</code> for correlation</p>"},{"location":"package-monitoring-grafana/#3-performance-monitoring","title":"3. Performance Monitoring","text":"<p>Dashboard for Service Health: - Panel 1: Request rate (PromQL)   <pre><code>rate(http_requests_total{service=\"my-app\"}[5m])\n</code></pre> - Panel 2: Error rate (PromQL)   <pre><code>rate(http_requests_total{service=\"my-app\",status=~\"5..\"}[5m])\n</code></pre> - Panel 3: Recent logs (LogQL)   <pre><code>{service_name=\"my-app\"}\n</code></pre> - Panel 4: Slow traces (TraceQL)   <pre><code>{resource.service.name=\"my-app\" &amp;&amp; duration &gt; 1s}\n</code></pre></p>"},{"location":"package-monitoring-grafana/#4-alert-configuration","title":"4. Alert Configuration","text":"<p>Create Alert Rule (in dashboard panel): 1. Edit panel \u2192 Alert tab 2. Define condition:    <pre><code>WHEN avg() OF query(A, 5m, now) IS ABOVE 100\n</code></pre> 3. Set notification channel 4. Test alert 5. Save dashboard</p> <p>Official Alerting Docs: https://grafana.com/docs/grafana/v12.1/alerting/</p> <p>\ud83d\udca1 Key Insight: Grafana serves as the unified interface for the entire observability stack, transforming raw telemetry data into actionable insights. Its dashboard sidecar pattern enables GitOps-style dashboard management via ConfigMaps, while Explore mode provides ad-hoc investigation capabilities. By correlating metrics, logs, and traces from Prometheus, Loki, and Tempo in a single interface, Grafana delivers complete observability visibility without context switching between tools.</p>"},{"location":"package-monitoring-grafana/#related-documentation","title":"\ud83d\udd17 Related Documentation","text":"<p>Monitoring Stack: - Monitoring Overview - Complete observability stack - Prometheus Metrics - Metrics datasource - Loki Logs - Logs datasource - Tempo Tracing - Traces datasource - OTLP Collector - Telemetry ingestion</p> <p>Configuration &amp; Rules: - Traefik IngressRoute - External access patterns - Naming Conventions - Manifest numbering (034, 038) - Development Workflow - Configuration management - Secrets Management - Managing admin credentials</p> <p>External Resources: - Grafana Dashboards: https://grafana.com/docs/grafana/v12.1/dashboards/ - Grafana Explore: https://grafana.com/docs/grafana/v12.1/explore/ - Prometheus Datasource: https://grafana.com/docs/grafana/v12.1/datasources/prometheus/ - Loki Datasource: https://grafana.com/docs/grafana/v12.1/datasources/loki/ - Tempo Datasource: https://grafana.com/docs/grafana/v12.1/datasources/tempo/ - Alerting: https://grafana.com/docs/grafana/v12.1/alerting/</p>"},{"location":"package-monitoring-loki/","title":"Loki - Log Aggregation System","text":"<p>Key Features: Log Aggregation \u2022 LogQL Query Language \u2022 Label-Based Indexing \u2022 Structured Metadata \u2022 Low Storage Cost \u2022 OTLP Ingestion \u2022 Grafana Integration \u2022 Multi-Tenancy</p> <p>File: <code>docs/package-monitoring-loki.md</code> Purpose: Complete guide to Loki deployment and configuration for log aggregation in Urbalurba infrastructure Target Audience: DevOps engineers, platform administrators, SREs, developers Last Updated: October 3, 2025</p> <p>Deployed Version: Loki v3.5.5 (Helm Chart: loki-6.41.1) Official Documentation: https://grafana.com/docs/loki/v3.5.x/ OTLP Integration Guide: https://grafana.com/docs/loki/latest/send-data/otel/</p>"},{"location":"package-monitoring-loki/#overview","title":"\ud83d\udccb Overview","text":"<p>Loki is a horizontally scalable, highly available log aggregation system inspired by Prometheus. Unlike traditional log aggregators that index full-text content, Loki only indexes metadata labels, dramatically reducing storage costs and operational complexity. It's designed to work seamlessly with Grafana and Prometheus.</p> <p>As part of the unified observability stack, Loki works alongside Prometheus (metrics) and Tempo (traces), with all data visualized in Grafana. Applications instrumented with OpenTelemetry send logs to the OTLP Collector, which forwards them to Loki for storage and querying.</p> <p>Key Capabilities: - Label-Based Indexing: Like Prometheus but for logs - indexes labels, not full text - LogQL: Familiar PromQL-like query syntax for powerful log filtering - OTLP Native: Ingests logs from OpenTelemetry Collector - Structured Metadata: Supports structured data extraction from logs - Low Cost: Minimal storage overhead (no full-text indexing) - Grafana Integration: Native datasource with Explore and dashboard support - Retention Management: Automatic log expiration via compactor</p> <p>Architecture Type: Distributed log aggregation with label indexing</p>"},{"location":"package-monitoring-loki/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"package-monitoring-loki/#deployment-components","title":"Deployment Components","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           Loki Stack (namespace: monitoring)         \u2502\n\u2502              SingleBinary Deployment Mode            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502            Loki SingleBinary               \u2502    \u2502\n\u2502  \u2502                                            \u2502    \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502\n\u2502  \u2502  \u2502  Ingester (Write Path)               \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - OTLP Receiver (via OTLP Collector)\u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - Label Extraction                  \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - Chunk Creation                    \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - WAL Persistence                   \u2502 \u2502    \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502\n\u2502  \u2502                                            \u2502    \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502\n\u2502  \u2502  \u2502  Querier (Read Path)                 \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - LogQL Engine                      \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - Label Matching                    \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - Stream Filtering                  \u2502 \u2502    \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502\n\u2502  \u2502                                            \u2502    \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502\n\u2502  \u2502  \u2502  Compactor                           \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - Retention Management (24h)        \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - Chunk Compaction                  \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - Index Cleanup                     \u2502 \u2502    \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502\n\u2502  \u2502                                            \u2502    \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502\n\u2502  \u2502  \u2502  Storage (10Gi PVC)                  \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - TSDB Index (/var/loki/tsdb-index) \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - Log Chunks (/var/loki/chunks)     \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - WAL (/var/loki/wal)               \u2502 \u2502    \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502\n\u2502  \u2502                                            \u2502    \u2502\n\u2502  \u2502  HTTP API: 3100   gRPC: 9095              \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u25b2                          \u2502\n         \u2502                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 OTLP Collector   \u2502    \u2502   Grafana Query      \u2502\n\u2502 (Log Export)     \u2502    \u2502   (LogQL/HTTP)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"package-monitoring-loki/#data-flow","title":"Data Flow","text":"<pre><code>Application Logs (OTLP instrumented)\n         \u2502\n         \u2502 OTLP/HTTP\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  OTLP Collector      \u2502\n\u2502  (Log Receiver)      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u2502 Loki Push API\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Loki Ingester      \u2502\n\u2502   (3100/loki/api/v1) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1. Extract labels    \u2502\n\u2502 2. Create chunks     \u2502\n\u2502 3. Index metadata    \u2502\n\u2502 4. Store to disk     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u251c\u2500\u25ba TSDB Index (labels)\n         \u251c\u2500\u25ba Log Chunks (content)\n         \u2514\u2500\u25ba WAL (write-ahead log)\n                  \u2502\n                  \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502  Query API       \u2502\n         \u2502  (LogQL Engine)  \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n                  \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502  Grafana Explore \u2502\n         \u2502  (LogQL Query)   \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"package-monitoring-loki/#file-structure","title":"File Structure","text":"<pre><code>manifests/\n\u2514\u2500\u2500 032-loki-config.yaml                    # Loki Helm values\n\nansible/playbooks/\n\u251c\u2500\u2500 032-setup-loki.yml                      # Deployment automation\n\u2514\u2500\u2500 032-remove-loki.yml                     # Removal automation\n\nprovision-host/kubernetes/11-monitoring/not-in-use/\n\u251c\u2500\u2500 03-setup-loki.sh                        # Shell script wrapper\n\u2514\u2500\u2500 03-remove-loki.sh                       # Removal script\n\nStorage:\n\u2514\u2500\u2500 PersistentVolumeClaim\n    \u2514\u2500\u2500 loki (10Gi)                         # Chunks + Index + WAL\n</code></pre>"},{"location":"package-monitoring-loki/#deployment","title":"\ud83d\ude80 Deployment","text":""},{"location":"package-monitoring-loki/#automated-deployment","title":"Automated Deployment","text":"<p>Via Monitoring Stack (Recommended): <pre><code># Deploy entire monitoring stack (includes Loki)\ndocker exec -it provision-host bash\ncd /mnt/urbalurbadisk/provision-host/kubernetes/11-monitoring/not-in-use\n./00-setup-all-monitoring.sh rancher-desktop\n</code></pre></p> <p>Individual Deployment: <pre><code># Deploy Loki only\ndocker exec -it provision-host bash\ncd /mnt/urbalurbadisk/provision-host/kubernetes/11-monitoring/not-in-use\n./03-setup-loki.sh rancher-desktop\n</code></pre></p>"},{"location":"package-monitoring-loki/#manual-deployment","title":"Manual Deployment","text":"<p>Prerequisites: - Kubernetes cluster running (Rancher Desktop) - <code>monitoring</code> namespace exists - Helm installed in provision-host container - Manifest file: <code>manifests/032-loki-config.yaml</code></p> <p>Deployment Steps: <pre><code># 1. Enter provision-host container\ndocker exec -it provision-host bash\n\n# 2. Add Grafana Helm repository\nhelm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n\n# 3. Deploy Loki\nhelm upgrade --install loki grafana/loki \\\n  -f /mnt/urbalurbadisk/manifests/032-loki-config.yaml \\\n  --namespace monitoring \\\n  --create-namespace \\\n  --timeout 600s \\\n  --kube-context rancher-desktop\n\n# 4. Wait for pods to be ready\nkubectl wait --for=condition=ready pod \\\n  -l app.kubernetes.io/name=loki \\\n  -n monitoring --timeout=300s\n</code></pre></p> <p>Deployment Time: ~2-3 minutes</p>"},{"location":"package-monitoring-loki/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"package-monitoring-loki/#loki-configuration-manifests032-loki-configyaml","title":"Loki Configuration (<code>manifests/032-loki-config.yaml</code>)","text":"<p>Deployment Mode: <pre><code>deploymentMode: SingleBinary          # Simplified deployment (all components in one pod)\n</code></pre></p> <p>Core Settings: <pre><code>loki:\n  auth_enabled: false                 # No authentication (cluster-internal only)\n\n  limits_config:\n    allow_structured_metadata: true   # Enable structured log data\n    retention_period: 24h             # Log retention period\n    ingestion_rate_mb: 10             # Max ingestion rate per stream\n    max_streams_per_user: 0           # Unlimited streams\n    max_line_size: 256000             # Max log line size (256KB)\n\n  schemaConfig:\n    configs:\n      - from: \"2024-01-01\"\n        schema: v13                   # TSDB schema version\n        store: tsdb                   # Time-series database indexing\n        object_store: filesystem      # Local filesystem storage\n</code></pre></p> <p>Key Configuration Sections:</p> <p>1. Retention &amp; Compaction: <pre><code>loki:\n  compactor:\n    retention_enabled: true\n    retention_delete_delay: 2h        # Grace period before deletion\n    compaction_interval: 10m          # Compact chunks every 10 minutes\n\n  limits_config:\n    retention_period: 24h             # Logs older than 24h are deleted\n</code></pre></p> <p>Official Retention Docs: https://grafana.com/docs/loki/v3.5.x/operations/storage/retention/</p> <p>2. Ingester (Write Path): <pre><code>loki:\n  ingester:\n    chunk_idle_period: 5m             # Flush chunks after 5m idle\n    chunk_retain_period: 30s          # Retain in memory for 30s\n    wal:\n      dir: /var/loki/wal              # Write-ahead log location\n</code></pre></p> <p>3. Schema &amp; Storage: <pre><code>loki:\n  schemaConfig:\n    configs:\n      - from: \"2024-01-01\"\n        index:\n          period: 24h                 # Daily index rotation\n          prefix: index_\n        schema: v13                   # TSDB schema (recommended)\n        store: tsdb\n\n  storage_config:\n    filesystem:\n      directory: /var/loki/chunks\n    tsdb_shipper:\n      active_index_directory: /var/loki/tsdb-index\n      cache_location: /var/loki/tsdb-cache\n</code></pre></p> <p>Official Schema Config Docs: https://grafana.com/docs/loki/v3.5.x/configure/#schema_config</p> <p>4. OTLP Integration: Loki receives logs from the OTLP Collector, which translates OTLP log format to Loki's push API format.</p> <p>OTLP Collector \u2192 Loki Flow: <pre><code># In OTLP Collector config (manifests/033-otel-collector-config.yaml)\nexporters:\n  loki:\n    endpoint: http://loki.monitoring.svc.cluster.local:3100/loki/api/v1/push\n</code></pre></p> <p>Official OTLP Integration Guide: https://grafana.com/docs/loki/latest/send-data/otel/</p>"},{"location":"package-monitoring-loki/#resource-configuration","title":"Resource Configuration","text":"<p>Storage Requirements: - Loki PVC: 10Gi persistent volume (24-hour retention) - Estimated Usage: Depends on log volume (typically 1-5GB per million log lines)</p> <p>Service Endpoints: - HTTP API: <code>loki.monitoring.svc.cluster.local:3100</code> - gRPC: <code>loki.monitoring.svc.cluster.local:9095</code> - Ready Check: <code>loki.monitoring.svc.cluster.local:3100/ready</code> - Metrics: <code>loki.monitoring.svc.cluster.local:3100/metrics</code></p>"},{"location":"package-monitoring-loki/#security-configuration","title":"Security Configuration","text":"<p>Network Access: <pre><code># Internal cluster access only (no IngressRoute)\nservice:\n  type: ClusterIP\n</code></pre></p> <p>Authentication: Disabled (<code>auth_enabled: false</code>) - Loki is accessed only from within the cluster via Grafana and OTLP Collector.</p>"},{"location":"package-monitoring-loki/#monitoring-verification","title":"\ud83d\udd0d Monitoring &amp; Verification","text":""},{"location":"package-monitoring-loki/#health-checks","title":"Health Checks","text":"<p>Check Pod Status: <pre><code># All Loki pods\nkubectl get pods -n monitoring | grep loki\n\n# Expected output (3 pods total):\nNAME                              READY   STATUS    RESTARTS   AGE\nloki-0                            2/2     Running   0          5m     # Main Loki server (SingleBinary mode) + promtail sidecar\nloki-canary-xxx                   1/1     Running   0          5m     # Loki canary for synthetic log testing\nloki-gateway-xxx                  1/1     Running   0          5m     # NGINX gateway for load balancing/routing\n</code></pre></p> <p>Pod Descriptions: - loki-0: Main Loki server running in SingleBinary mode (all components: ingester, querier, compactor) + promtail sidecar for collecting Loki's own logs - loki-canary: Synthetic log generator that continuously writes and reads test logs to verify Loki is functioning correctly - loki-gateway: NGINX reverse proxy that routes requests to Loki components (provides load balancing and unified HTTP endpoint)</p> <p>Check Service Endpoints: <pre><code># Verify service is accessible\nkubectl get svc -n monitoring -l app.kubernetes.io/name=loki\n\n# Expected services:\nloki           ClusterIP   10.43.x.x    3100/TCP,9095/TCP\nloki-headless  ClusterIP   None         3100/TCP\n</code></pre></p>"},{"location":"package-monitoring-loki/#service-verification","title":"Service Verification","text":"<p>Test Ready Endpoint: <pre><code># Check if Loki is ready to receive logs\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -s http://loki.monitoring.svc.cluster.local:3100/ready\n\n# Expected: ready (HTTP 200)\n</code></pre></p> <p>Test Labels API: <pre><code># List all label names\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -s http://loki.monitoring.svc.cluster.local:3100/loki/api/v1/labels\n\n# Expected: JSON array of label names\n</code></pre></p> <p>Test Label Values: <pre><code># Get values for a specific label (e.g., service_name)\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -s http://loki.monitoring.svc.cluster.local:3100/loki/api/v1/label/service_name/values\n\n# Expected: JSON array of service names\n</code></pre></p>"},{"location":"package-monitoring-loki/#query-api-testing","title":"Query API Testing","text":"<p>Simple LogQL Query: <pre><code># Query logs from last hour\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -s -G http://loki.monitoring.svc.cluster.local:3100/loki/api/v1/query_range \\\n  --data-urlencode 'query={service_name=\"sovdev-test-company-lookup-typescript\"}' \\\n  --data-urlencode 'start=1h'\n\n# Expected: JSON response with log streams\n</code></pre></p>"},{"location":"package-monitoring-loki/#automated-verification","title":"Automated Verification","text":"<p>The deployment playbook (<code>032-setup-loki.yml</code>) performs automated tests: 1. \u2705 Ready endpoint verification 2. \u2705 Metrics endpoint check 3. \u2705 Labels API validation 4. \u2705 Query range API test</p>"},{"location":"package-monitoring-loki/#management-operations","title":"\ud83d\udee0\ufe0f Management Operations","text":""},{"location":"package-monitoring-loki/#query-logs-in-grafana","title":"Query Logs in Grafana","text":"<p>Access Grafana: <pre><code># Open Grafana UI\nhttp://grafana.localhost\n</code></pre></p> <p>Explore Logs: 1. Navigate to Explore \u2192 Select Loki datasource 2. Choose builder or code mode 3. Run LogQL queries</p> <p>LogQL Query Examples: <pre><code># All logs from a service\n{service_name=\"sovdev-test-company-lookup-typescript\"}\n\n# Filter by log level\n{service_name=\"my-app\"} |= \"error\"\n\n# JSON field extraction\n{service_name=\"my-app\"} | json | level=\"error\"\n\n# Regex pattern matching\n{service_name=~\"sovdev-test.*\"}\n\n# Count log lines per minute\nrate({service_name=\"my-app\"}[1m])\n\n# Parse and filter structured logs\n{service_name=\"my-app\"}\n  | json\n  | functionName=\"lookup\"\n  | correlationId!=\"\"\n</code></pre></p> <p>Official LogQL Documentation: https://grafana.com/docs/loki/v3.5.x/query/</p>"},{"location":"package-monitoring-loki/#common-logql-patterns","title":"Common LogQL Patterns","text":"<p>Error Investigation: <pre><code># Find errors in last hour\n{service_name=\"my-app\"} |= \"error\"\n\n# Count errors by service\nsum by (service_name) (rate({job=\"otlp\"} |= \"error\" [5m]))\n</code></pre></p> <p>Performance Analysis: <pre><code># Slow requests (duration &gt; 1s)\n{service_name=\"my-app\"}\n  | json\n  | duration &gt; 1000\n\n# Request rate by endpoint\nsum by (endpoint) (rate({service_name=\"api\"}[1m]))\n</code></pre></p> <p>Correlation with Traces: <pre><code># Find logs for specific trace\n{service_name=\"my-app\"}\n  | json\n  | trace_id=\"abc123\"\n</code></pre></p>"},{"location":"package-monitoring-loki/#metrics-monitoring","title":"Metrics Monitoring","text":"<p>Loki Self-Monitoring: <pre><code># Get Loki internal metrics\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -s http://loki.monitoring.svc.cluster.local:3100/metrics | grep loki\n</code></pre></p> <p>Key Metrics (via Prometheus): <pre><code># Ingested log lines per second\nrate(loki_distributor_lines_received_total[5m])\n\n# Ingested bytes per second\nrate(loki_distributor_bytes_received_total[5m])\n\n# Query latency (P95)\nhistogram_quantile(0.95, rate(loki_request_duration_seconds_bucket[5m]))\n\n# Active streams\nloki_ingester_streams\n\n# Storage size (chunks)\nloki_ingester_chunk_stored_bytes_total\n</code></pre></p>"},{"location":"package-monitoring-loki/#service-removal","title":"Service Removal","text":"<p>Automated Removal: <pre><code>docker exec -it provision-host bash\ncd /mnt/urbalurbadisk/provision-host/kubernetes/11-monitoring/not-in-use\n./03-remove-loki.sh rancher-desktop\n</code></pre></p> <p>Manual Removal: <pre><code># Remove Helm chart\nhelm uninstall loki -n monitoring --kube-context rancher-desktop\n\n# Remove PVC (optional - preserves data if omitted)\nkubectl delete pvc -n monitoring -l app.kubernetes.io/name=loki\n</code></pre></p>"},{"location":"package-monitoring-loki/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-monitoring-loki/#common-issues","title":"Common Issues","text":"<p>Pods Not Starting: <pre><code># Check pod events\nkubectl describe pod -n monitoring -l app.kubernetes.io/name=loki\n\n# Common causes:\n# - PVC binding issues (check PV availability)\n# - Insufficient resources (check node capacity)\n# - Configuration errors (check loki logs)\n</code></pre></p> <p>No Logs Appearing: <pre><code># 1. Check OTLP Collector is sending logs to Loki\nkubectl logs -n monitoring -l app.kubernetes.io/name=opentelemetry-collector | grep loki\n\n# 2. Check Loki ingestion logs\nkubectl logs -n monitoring -l app.kubernetes.io/name=loki -c loki | grep -i \"ingester\\|distributor\"\n\n# 3. Verify OTLP Collector Loki exporter configuration\nkubectl get configmap -n monitoring otel-collector-opentelemetry-collector -o yaml | grep -A 10 \"loki\"\n\n# Expected: Loki endpoint at loki.monitoring.svc.cluster.local:3100\n</code></pre></p> <p>Query Failures: <pre><code># Check Loki query logs\nkubectl logs -n monitoring -l app.kubernetes.io/name=loki -c loki | grep -i \"query\\|error\"\n\n# Test query API directly\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -v http://loki.monitoring.svc.cluster.local:3100/loki/api/v1/labels\n</code></pre></p> <p>High Memory Usage: <pre><code># Check memory usage\nkubectl top pod -n monitoring -l app.kubernetes.io/name=loki\n\n# Solutions:\n# 1. Reduce ingestion_rate_mb in manifests/032-loki-config.yaml\n# 2. Lower max_streams_per_user\n# 3. Decrease chunk_idle_period (flush more frequently)\n# 4. Reduce retention_period (24h \u2192 12h)\n</code></pre></p> <p>Storage Full: <pre><code># Check PVC usage\nkubectl get pvc -n monitoring -l app.kubernetes.io/name=loki\n\n# Check storage metrics\nkubectl exec -n monitoring loki-0 -c loki -- df -h /var/loki\n\n# Solutions:\n# 1. Reduce retention period in manifests/032-loki-config.yaml\n# 2. Increase PVC size\n# 3. Enable compaction (already enabled by default)\n</code></pre></p> <p>Label Cardinality Issues: <pre><code># Check active streams (high = potential cardinality explosion)\nkubectl logs -n monitoring -l app.kubernetes.io/name=loki -c loki | grep \"streams\"\n\n# Inspect label combinations\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -s http://loki.monitoring.svc.cluster.local:3100/loki/api/v1/labels\n\n# Solutions:\n# 1. Reduce number of indexed labels in OTLP Collector config\n# 2. Use structured metadata instead of labels for high-cardinality data\n# 3. Set max_streams_per_user limit\n</code></pre></p> <p>Official Troubleshooting Guide: https://grafana.com/docs/loki/v3.5.x/operations/troubleshooting/</p>"},{"location":"package-monitoring-loki/#maintenance","title":"\ud83d\udccb Maintenance","text":""},{"location":"package-monitoring-loki/#regular-tasks","title":"Regular Tasks","text":"<p>Monitor Storage Usage: <pre><code># Check PVC status\nkubectl get pvc -n monitoring -l app.kubernetes.io/name=loki\n\n# Check storage metrics via Prometheus\nkubectl port-forward -n monitoring svc/loki 3100:3100\ncurl -s http://localhost:3100/metrics | grep loki_ingester_chunk_stored_bytes\n</code></pre></p> <p>Update Loki: <pre><code># Update Helm chart to latest version\nhelm repo update\nhelm upgrade loki grafana/loki \\\n  -f /mnt/urbalurbadisk/manifests/032-loki-config.yaml \\\n  -n monitoring \\\n  --kube-context rancher-desktop\n</code></pre></p> <p>Cleanup Old Logs (automatic): <pre><code># Retention handled automatically via compactor\nloki:\n  compactor:\n    retention_enabled: true\n  limits_config:\n    retention_period: 24h  # Logs older than 24h are deleted\n</code></pre></p>"},{"location":"package-monitoring-loki/#backup-procedures","title":"Backup Procedures","text":"<p>Snapshot Log Data: <pre><code># Export PVC data\nkubectl exec -n monitoring loki-0 -c loki -- \\\n  tar czf /tmp/loki-backup.tar.gz /var/loki\n\n# Copy to local machine\nkubectl cp monitoring/loki-0:/tmp/loki-backup.tar.gz \\\n  ./loki-backup.tar.gz -c loki\n</code></pre></p> <p>Note: Loki is designed for ephemeral log storage with short retention (24h). Long-term log archival is not a primary use case.</p>"},{"location":"package-monitoring-loki/#disaster-recovery","title":"Disaster Recovery","text":"<p>Restore from Backup: <pre><code># 1. Remove existing deployment\n./03-remove-loki.sh rancher-desktop\n\n# 2. Restore PVC data (requires direct PV access)\n# 3. Redeploy Loki\n./03-setup-loki.sh rancher-desktop\n</code></pre></p> <p>Data Loss Scenarios: - PVC deleted: Logs are lost (acceptable - 24h retention means limited impact) - Corruption: Loki auto-repairs WAL on startup - Retention expired: Expected behavior, increase retention if needed</p>"},{"location":"package-monitoring-loki/#use-cases","title":"\ud83d\ude80 Use Cases","text":""},{"location":"package-monitoring-loki/#1-application-logging-with-otlp","title":"1. Application Logging with OTLP","text":"<p>sovdev-logger Integration: <pre><code>// TypeScript application using @sovdev/logger\nimport { initializeSovdevLogger } from '@sovdev/logger';\n\n// Initialize logger (sends to OTLP Collector)\ninitializeSovdevLogger('my-service-name');\n\n// Environment configuration\nSYSTEM_ID=my-service-name\nOTEL_EXPORTER_OTLP_LOGS_ENDPOINT=http://127.0.0.1/v1/logs\nOTEL_EXPORTER_OTLP_HEADERS={\"Host\":\"otel.localhost\"}\n</code></pre></p> <p>Query in Grafana: <pre><code>{service_name=\"my-service-name\"}\n</code></pre></p>"},{"location":"package-monitoring-loki/#2-error-tracking","title":"2. Error Tracking","text":"<p>Find Errors: <pre><code># All errors\n{service_name=\"my-app\"} |= \"error\"\n\n# Parse JSON and filter by level\n{service_name=\"my-app\"} | json | level=\"error\"\n\n# Count errors per minute\nsum(rate({service_name=\"my-app\"} |= \"error\" [1m]))\n</code></pre></p> <p>Alert on Errors (via Grafana): <pre><code># Alert if error rate &gt; 10/min\nsum(rate({job=\"otlp\"} |= \"error\" [1m])) &gt; 10\n</code></pre></p>"},{"location":"package-monitoring-loki/#3-log-correlation-with-traces","title":"3. Log Correlation with Traces","text":"<p>Find logs for a trace: <pre><code># Using trace_id from Tempo\n{service_name=\"my-app\"}\n  | json\n  | trace_id=\"abc123def456\"\n</code></pre></p> <p>Grafana Workflow: 1. Find slow trace in Tempo 2. Copy <code>trace_id</code> from trace details 3. Switch to Loki datasource 4. Query logs with <code>trace_id</code> filter 5. View correlated logs and trace spans together</p>"},{"location":"package-monitoring-loki/#4-performance-analysis","title":"4. Performance Analysis","text":"<p>Find Slow Requests: <pre><code># Parse duration from JSON logs\n{service_name=\"api\"}\n  | json\n  | duration &gt; 1000  # &gt;1 second\n\n# Histogram of request durations\nhistogram_quantile(0.95,\n  sum(rate({service_name=\"api\"} | json | unwrap duration [5m]))\n  by (le)\n)\n</code></pre></p> <p>\ud83d\udca1 Key Insight: Loki's \"index labels, not content\" design makes it extremely cost-effective for cloud-native log aggregation. By indexing only metadata and using LogQL for content filtering at query time, Loki provides powerful log analysis capabilities without the operational burden and storage costs of traditional full-text indexing solutions. When integrated with OTLP Collector for ingestion and Grafana for visualization, Loki completes the observability triangle alongside Prometheus (metrics) and Tempo (traces).</p>"},{"location":"package-monitoring-loki/#related-documentation","title":"\ud83d\udd17 Related Documentation","text":"<p>Monitoring Stack: - Monitoring Overview - Complete observability stack - Prometheus Metrics - Metrics collection - Tempo Tracing - Distributed tracing - OTLP Collector - Telemetry pipeline (log ingestion) - Grafana Visualization - Dashboards and log exploration</p> <p>Configuration &amp; Rules: - Naming Conventions - Manifest numbering (032) - Development Workflow - Configuration management - Automated Deployment - Orchestration</p> <p>External Resources: - LogQL Language: https://grafana.com/docs/loki/v3.5.x/query/ - OTLP Integration: https://grafana.com/docs/loki/latest/send-data/otel/ - Retention Configuration: https://grafana.com/docs/loki/v3.5.x/operations/storage/retention/ - Schema Config: https://grafana.com/docs/loki/v3.5.x/configure/#schema_config</p>"},{"location":"package-monitoring-otel/","title":"OpenTelemetry Collector - Telemetry Pipeline","text":"<p>Key Features: OTLP Protocol \u2022 Multi-Backend Export \u2022 HTTP &amp; gRPC Receivers \u2022 Traefik Ingress \u2022 External Access \u2022 Logs/Traces/Metrics \u2022 Resource Processing \u2022 Debug Mode</p> <p>File: <code>docs/package-monitoring-otel.md</code> Purpose: Complete guide to OpenTelemetry Collector deployment and configuration for telemetry ingestion in Urbalurba infrastructure Target Audience: DevOps engineers, platform administrators, SREs, developers Last Updated: October 3, 2025</p> <p>Deployed Version: OpenTelemetry Collector v0.136.0 (Helm Chart: opentelemetry-collector-0.136.1) Official Documentation: https://opentelemetry.io/docs/collector/ Configuration Reference: https://opentelemetry.io/docs/collector/configuration/</p>"},{"location":"package-monitoring-otel/#overview","title":"\ud83d\udccb Overview","text":"<p>The OpenTelemetry Collector is a vendor-neutral telemetry gateway that receives, processes, and exports observability data. It acts as the central ingestion point for all OTLP (OpenTelemetry Protocol) telemetry from applications, routing logs to Loki, traces to Tempo, and metrics to Prometheus.</p> <p>As the hub of the observability stack, the OTLP Collector provides: - Unified Ingestion: Single endpoint for logs, traces, and metrics - Protocol Translation: Converts OTLP to backend-specific formats - Resource Enrichment: Adds cluster metadata to telemetry data - External Access: Traefik IngressRoute for applications outside the cluster - Multi-Backend Export: Routes telemetry to appropriate storage backends</p> <p>Key Capabilities: - OTLP Receivers: HTTP (4318) and gRPC (4317) endpoints - External Ingestion: Accessible via <code>http://otel.localhost/v1/logs</code> and <code>/v1/traces</code> - Smart Routing: Logs \u2192 Loki, Traces \u2192 Tempo, Metrics \u2192 Prometheus - Resource Processing: Enriches telemetry with cluster and service metadata - Debug Mode: Detailed logging for troubleshooting data flow - Batch Processing: Optimizes throughput with batching and buffering</p> <p>Architecture Type: Telemetry aggregation and routing gateway</p>"},{"location":"package-monitoring-otel/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"package-monitoring-otel/#deployment-components","title":"Deployment Components","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     OTLP Collector Stack (namespace: monitoring)         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502       OpenTelemetry Collector Deployment       \u2502    \u2502\n\u2502  \u2502                                                \u2502    \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502\n\u2502  \u2502  \u2502  Receivers (Ingestion)                   \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - OTLP/gRPC: 4317                       \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - OTLP/HTTP: 4318                       \u2502 \u2502    \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502\n\u2502  \u2502                      \u2502                        \u2502    \u2502\n\u2502  \u2502                      \u25bc                        \u2502    \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502\n\u2502  \u2502  \u2502  Processors (Enrichment)                 \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - Resource Processor                    \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502    \u2022 Adds cluster.name                   \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502    \u2022 Extracts service_name               \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - Transform Processor                   \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502    \u2022 Sets log attributes                 \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - Batch Processor                       \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502    \u2022 Optimizes throughput                \u2502 \u2502    \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502\n\u2502  \u2502                      \u2502                        \u2502    \u2502\n\u2502  \u2502                      \u25bc                        \u2502    \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502\n\u2502  \u2502  \u2502  Exporters (Multi-Backend Routing)       \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502                                          \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  Traces \u2192 otlp/tempo (4317)              \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  Logs \u2192 otlphttp/loki (/otlp)            \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  Metrics \u2192 prometheusremotewrite (/write)\u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  Debug \u2192 stdout (sampling)               \u2502 \u2502    \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                                                          \u2502\n\u2502  Ports: 4317 (gRPC), 4318 (HTTP), 8888 (metrics)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u25b2                          \u2502\n         \u2502                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Applications   \u2502    \u2502   Backend Services          \u2502\n\u2502   (OTLP SDK)     \u2502    \u2502   - Loki (logs)             \u2502\n\u2502                  \u2502    \u2502   - Tempo (traces)          \u2502\n\u2502   - TypeScript   \u2502    \u2502   - Prometheus (metrics)    \u2502\n\u2502   - Python       \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502   - C#/Go/etc    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nExternal Access (via Traefik IngressRoute):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  http://otel.localhost/v1/logs           \u2502\n\u2502  http://otel.localhost/v1/traces         \u2502\n\u2502  (Future: http://otel.urbalurba.no)      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"package-monitoring-otel/#data-flow","title":"Data Flow","text":"<pre><code>Application (OTLP Instrumented)\n         \u2502\n         \u2502 HTTP POST /v1/logs OR gRPC\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Traefik Ingress     \u2502\n\u2502  (otel.localhost)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u2502 Routes to Service\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  OTLP Collector              \u2502\n\u2502  (4318 HTTP, 4317 gRPC)      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  1. Receive OTLP data        \u2502\n\u2502  2. Resource enrichment      \u2502\n\u2502     - Add cluster.name       \u2502\n\u2502     - Extract service_name   \u2502\n\u2502  3. Transform attributes     \u2502\n\u2502  4. Batch for efficiency     \u2502\n\u2502  5. Route to backends:       \u2502\n\u2502     - Logs \u2192 Loki            \u2502\n\u2502     - Traces \u2192 Tempo         \u2502\n\u2502     - Metrics \u2192 Prometheus   \u2502\n\u2502  6. Debug sampling output    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u251c\u2500\u25ba Loki (HTTP push API)\n         \u251c\u2500\u25ba Tempo (gRPC OTLP)\n         \u251c\u2500\u25ba Prometheus (remote write)\n         \u2514\u2500\u25ba Debug logs (stdout)\n</code></pre>"},{"location":"package-monitoring-otel/#file-structure","title":"File Structure","text":"<pre><code>manifests/\n\u251c\u2500\u2500 033-otel-collector-config.yaml          # OTLP Collector Helm values\n\u2514\u2500\u2500 039-otel-collector-ingress.yaml         # Traefik IngressRoute\n\nansible/playbooks/\n\u251c\u2500\u2500 033-setup-otel-collector.yml            # Deployment automation\n\u2514\u2500\u2500 033-remove-otel-collector.yml           # Removal automation\n\nprovision-host/kubernetes/11-monitoring/not-in-use/\n\u251c\u2500\u2500 04-setup-otel-collector.sh              # Shell script wrapper\n\u2514\u2500\u2500 04-remove-otel-collector.sh             # Removal script\n\nNo persistent storage required (stateless deployment)\n</code></pre>"},{"location":"package-monitoring-otel/#deployment","title":"\ud83d\ude80 Deployment","text":""},{"location":"package-monitoring-otel/#automated-deployment","title":"Automated Deployment","text":"<p>Via Monitoring Stack (Recommended): <pre><code># Deploy entire monitoring stack (includes OTLP Collector)\ndocker exec -it provision-host bash\ncd /mnt/urbalurbadisk/provision-host/kubernetes/11-monitoring/not-in-use\n./00-setup-all-monitoring.sh rancher-desktop\n</code></pre></p> <p>Individual Deployment: <pre><code># Deploy OTLP Collector only (requires Loki, Tempo, Prometheus already deployed)\ndocker exec -it provision-host bash\ncd /mnt/urbalurbadisk/provision-host/kubernetes/11-monitoring/not-in-use\n./04-setup-otel-collector.sh rancher-desktop\n</code></pre></p>"},{"location":"package-monitoring-otel/#manual-deployment","title":"Manual Deployment","text":"<p>Prerequisites: - Kubernetes cluster running (Rancher Desktop) - <code>monitoring</code> namespace exists - Backends deployed first: Prometheus, Tempo, Loki - Helm installed in provision-host container - Manifest files: <code>033-otel-collector-config.yaml</code>, <code>039-otel-collector-ingress.yaml</code></p> <p>Deployment Steps: <pre><code># 1. Enter provision-host container\ndocker exec -it provision-host bash\n\n# 2. Add OpenTelemetry Helm repository\nhelm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts\nhelm repo update\n\n# 3. Deploy OTLP Collector\nhelm upgrade --install otel-collector open-telemetry/opentelemetry-collector \\\n  -f /mnt/urbalurbadisk/manifests/033-otel-collector-config.yaml \\\n  --namespace monitoring \\\n  --create-namespace \\\n  --timeout 600s \\\n  --kube-context rancher-desktop\n\n# 4. Deploy IngressRoute for external access\nkubectl apply -f /mnt/urbalurbadisk/manifests/039-otel-collector-ingress.yaml\n\n# 5. Wait for pods to be ready\nkubectl wait --for=condition=ready pod \\\n  -l app.kubernetes.io/name=opentelemetry-collector \\\n  -n monitoring --timeout=300s\n</code></pre></p> <p>Deployment Time: ~1-2 minutes</p>"},{"location":"package-monitoring-otel/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"package-monitoring-otel/#otlp-collector-configuration-manifests033-otel-collector-configyaml","title":"OTLP Collector Configuration (<code>manifests/033-otel-collector-config.yaml</code>)","text":"<p>Deployment Mode: <pre><code>mode: deployment                  # Kubernetes Deployment (stateless)\n</code></pre></p> <p>Receivers (Ingestion Endpoints): <pre><code>config:\n  receivers:\n    otlp:\n      protocols:\n        grpc:\n          endpoint: 0.0.0.0:4317  # gRPC receiver (recommended)\n        http:\n          endpoint: 0.0.0.0:4318  # HTTP receiver (easier for testing)\n</code></pre></p> <p>Official Receiver Docs: https://opentelemetry.io/docs/collector/configuration/#receivers</p> <p>Processors (Data Enrichment): <pre><code>config:\n  processors:\n    # Batch processing for efficiency\n    batch:\n      timeout: 10s\n      send_batch_size: 1024\n\n    # Add cluster metadata and extract resource attributes\n    resource:\n      attributes:\n        - key: cluster.name\n          value: urbalurba-local\n          action: upsert\n        - key: service_name\n          from_attribute: service.name\n          action: insert\n        - key: session_id\n          from_attribute: session.id\n          action: insert\n\n    # Transform log attributes to make them available in Loki\n    transform:\n      log_statements:\n        - context: log\n          statements:\n            - set(attributes[\"service_name\"], resource.attributes[\"service_name\"])\n            - set(attributes[\"session_id\"], resource.attributes[\"session.id\"]) where resource.attributes[\"session.id\"] != nil\n</code></pre></p> <p>Official Processor Docs: https://opentelemetry.io/docs/collector/configuration/#processors</p> <p>Exporters (Backend Routing): <pre><code>config:\n  exporters:\n    # Traces to Tempo\n    otlp/tempo:\n      endpoint: tempo.monitoring.svc.cluster.local:4317\n      tls:\n        insecure: true\n\n    # Logs to Loki (OTLP HTTP endpoint)\n    otlphttp/loki:\n      endpoint: http://loki-gateway.monitoring.svc.cluster.local:80/otlp\n      tls:\n        insecure: true\n\n    # Metrics to Prometheus\n    prometheusremotewrite:\n      endpoint: http://prometheus-server.monitoring.svc.cluster.local:80/api/v1/write\n      # Convert resource attributes to Prometheus labels\n      # IMPORTANT: This enables filtering metrics by developer_id, project_name, service_name, etc.\n      # Without this, resource attributes are dropped and metrics only show job label\n      resource_to_telemetry_conversion:\n        enabled: true\n      tls:\n        insecure: true\n\n    # Debug exporter (sampling for troubleshooting)\n    debug:\n      verbosity: detailed\n      sampling_initial: 5\n      sampling_thereafter: 200\n</code></pre></p> <p>Official Exporter Docs: https://opentelemetry.io/docs/collector/configuration/#exporters</p> <p>Pipelines (Data Routing): <pre><code>config:\n  service:\n    pipelines:\n      # Traces pipeline\n      traces:\n        receivers: [otlp]\n        processors: [batch]\n        exporters: [otlp/tempo, debug]\n\n      # Logs pipeline\n      logs:\n        receivers: [otlp]\n        processors: [resource, transform, batch]\n        exporters: [otlphttp/loki, debug]\n\n      # Metrics pipeline\n      metrics:\n        receivers: [otlp]\n        processors: [resource, batch]  # resource processor preserves attributes\n        exporters: [prometheusremotewrite, debug]\n</code></pre></p>"},{"location":"package-monitoring-otel/#external-access-configuration-manifests039-otel-collector-ingressyaml","title":"External Access Configuration (<code>manifests/039-otel-collector-ingress.yaml</code>)","text":"<p>Traefik IngressRoute: <pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: otel-collector\n  namespace: monitoring\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: HostRegexp(`otel\\..+`)        # Matches otel.localhost, otel.urbalurba.no, etc.\n      kind: Rule\n      services:\n        - name: otel-collector-opentelemetry-collector\n          port: 4318                        # HTTP endpoint only (not gRPC)\n</code></pre></p> <p>Access URLs: - Localhost: <code>http://otel.localhost/v1/logs</code>, <code>http://otel.localhost/v1/traces</code> - Future External: <code>http://otel.urbalurba.no/v1/logs</code> (requires DNS configuration)</p>"},{"location":"package-monitoring-otel/#resource-configuration","title":"Resource Configuration","text":"<p>No Persistent Storage: OTLP Collector is stateless (no PVC required)</p> <p>Service Endpoints: - OTLP gRPC: <code>otel-collector-opentelemetry-collector.monitoring.svc.cluster.local:4317</code> - OTLP HTTP: <code>otel-collector-opentelemetry-collector.monitoring.svc.cluster.local:4318</code> - Metrics: <code>otel-collector-opentelemetry-collector.monitoring.svc.cluster.local:8888</code> - Health Check: <code>otel-collector-opentelemetry-collector.monitoring.svc.cluster.local:13133</code></p> <p>Resource Limits: <pre><code>resources:\n  requests:\n    cpu: 100m\n    memory: 128Mi\n  limits:\n    cpu: 500m\n    memory: 512Mi\n</code></pre></p>"},{"location":"package-monitoring-otel/#security-configuration","title":"Security Configuration","text":"<p>Network Access: - Internal: ClusterIP service for internal cluster access - External: Traefik IngressRoute at <code>otel.localhost</code> (HTTP only, port 80)</p> <p>TLS: Disabled (<code>insecure: true</code>) for internal backends - all communication within cluster is unencrypted</p>"},{"location":"package-monitoring-otel/#monitoring-verification","title":"\ud83d\udd0d Monitoring &amp; Verification","text":""},{"location":"package-monitoring-otel/#health-checks","title":"Health Checks","text":"<p>Check Pod Status: <pre><code># OTLP Collector pods\nkubectl get pods -n monitoring -l app.kubernetes.io/name=opentelemetry-collector\n\n# Expected output:\nNAME                                                READY   STATUS\notel-collector-opentelemetry-collector-xxx          1/1     Running\n</code></pre></p> <p>Check Service Endpoints: <pre><code># Verify service is accessible\nkubectl get svc -n monitoring -l app.kubernetes.io/name=opentelemetry-collector\n\n# Expected service:\notel-collector-opentelemetry-collector   ClusterIP   10.43.x.x   4317/TCP,4318/TCP,8888/TCP\n</code></pre></p>"},{"location":"package-monitoring-otel/#service-verification","title":"Service Verification","text":"<p>Test Health Endpoint: <pre><code># Check if collector is healthy\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -s http://otel-collector-opentelemetry-collector.monitoring.svc.cluster.local:13133/\n\n# Expected: {} (empty JSON = healthy)\n</code></pre></p> <p>Test OTLP HTTP Endpoint (Internal): <pre><code># Send test log via OTLP HTTP\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -X POST http://otel-collector-opentelemetry-collector.monitoring.svc.cluster.local:4318/v1/logs \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"resourceLogs\":[{\"resource\":{\"attributes\":[{\"key\":\"service.name\",\"value\":{\"stringValue\":\"test\"}}]},\"scopeLogs\":[{\"logRecords\":[{\"body\":{\"stringValue\":\"test log\"}}]}]}]}'\n\n# Expected: No error (200 or 204 response)\n</code></pre></p> <p>Test External Access (via Traefik): <pre><code># From Mac host (outside cluster)\ncurl -X POST http://127.0.0.1/v1/logs \\\n  -H \"Host: otel.localhost\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"resourceLogs\":[{\"resource\":{\"attributes\":[{\"key\":\"service.name\",\"value\":{\"stringValue\":\"external-test\"}}]},\"scopeLogs\":[{\"logRecords\":[{\"body\":{\"stringValue\":\"external test log\"}}]}]}]}'\n\n# Expected: No error\n</code></pre></p>"},{"location":"package-monitoring-otel/#check-data-flow-to-backends","title":"Check Data Flow to Backends","text":"<p>Verify Logs Reaching Loki: <pre><code># Check collector logs for Loki exports\nkubectl logs -n monitoring -l app.kubernetes.io/name=opentelemetry-collector | grep -i loki\n\n# Query Loki for test logs\nkubectl exec -n monitoring loki-0 -c loki -- \\\n  wget -q -O - 'http://localhost:3100/loki/api/v1/label/service_name/values'\n\n# Should include \"test\" or \"external-test\"\n</code></pre></p> <p>Verify Traces Reaching Tempo: <pre><code># Check collector logs for Tempo exports\nkubectl logs -n monitoring -l app.kubernetes.io/name=opentelemetry-collector | grep -i tempo\n</code></pre></p> <p>Check Collector Metrics: <pre><code># Get collector self-monitoring metrics\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -s http://otel-collector-opentelemetry-collector.monitoring.svc.cluster.local:8888/metrics\n</code></pre></p>"},{"location":"package-monitoring-otel/#automated-verification","title":"Automated Verification","text":"<p>The deployment playbook (<code>033-setup-otel-collector.yml</code>) performs automated tests: 1. \u2705 OTLP HTTP endpoint connectivity 2. \u2705 OTLP gRPC endpoint connectivity 3. \u2705 Health check endpoint validation 4. \u2705 Test log ingestion and export</p>"},{"location":"package-monitoring-otel/#management-operations","title":"\ud83d\udee0\ufe0f Management Operations","text":""},{"location":"package-monitoring-otel/#view-collector-logs","title":"View Collector Logs","text":"<p>Real-Time Logs (Debug Mode): <pre><code># Tail collector logs (includes debug sampling output)\nkubectl logs -n monitoring -l app.kubernetes.io/name=opentelemetry-collector --follow\n\n# Filter for specific pipeline\nkubectl logs -n monitoring -l app.kubernetes.io/name=opentelemetry-collector | grep -i \"logs pipeline\"\nkubectl logs -n monitoring -l app.kubernetes.io/name=opentelemetry-collector | grep -i \"traces pipeline\"\n</code></pre></p> <p>Debug Output Examples: <pre><code># Sampled log output (every 200th log after initial 5)\n2025-10-03T10:15:32.123Z  debug  ResourceLog #0\n  service.name: sovdev-test-company-lookup-typescript\n  cluster.name: urbalurba-local\n  LogRecord #0\n    body: Company Lookup Service started\n    service_name: sovdev-test-company-lookup-typescript\n</code></pre></p>"},{"location":"package-monitoring-otel/#application-integration","title":"Application Integration","text":"<p>For application instrumentation and OTLP integration, see: - sovdev-logger Integration Guide - Multi-language logging library with OTLP support (TypeScript, Python, C#, PHP, Go, Rust)</p> <p>Quick Example (TypeScript with sovdev-logger): <pre><code># Environment configuration\nSYSTEM_ID=my-service-name\nOTEL_EXPORTER_OTLP_LOGS_ENDPOINT=http://127.0.0.1/v1/logs\nOTEL_EXPORTER_OTLP_HEADERS={\"Host\":\"otel.localhost\"}\n</code></pre></p> <p>For complete integration examples in all supported languages, see the sovdev-logger documentation above.</p>"},{"location":"package-monitoring-otel/#authentication-configuration","title":"Authentication Configuration","text":"<p>Default Configuration: The sovdev-infrastructure (urbalurba-infrastructure) monitoring stack does not use authentication for OTLP clients by default. All endpoints are accessible without credentials.</p> <p>Production Authentication Setup: For production deployments requiring client authentication, refer to: - OpenTelemetry Collector Authentication: https://opentelemetry.io/docs/collector/configuration/#extensions   - Extensions: <code>basicauth</code>, <code>bearertokenauth</code>, <code>oidc</code>   - Add to <code>extensions:</code> section in <code>033-otel-collector-config.yaml</code>   - Enable in <code>service.extensions:</code> list - Traefik Authentication Middleware: rules-ingress-traefik.md   - Forward auth for SSO integration   - BasicAuth for simple username/password protection</p>"},{"location":"package-monitoring-otel/#troubleshooting-data-flow","title":"Troubleshooting Data Flow","text":"<p>No Data Reaching Backends: <pre><code># 1. Check collector logs for export errors\nkubectl logs -n monitoring -l app.kubernetes.io/name=opentelemetry-collector | grep -i error\n\n# 2. Verify backend endpoints are reachable\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -v http://loki-gateway.monitoring.svc.cluster.local:80/ready\n\n# 3. Check collector configuration\nkubectl get configmap -n monitoring -o yaml | grep -A 20 \"exporters:\"\n</code></pre></p>"},{"location":"package-monitoring-otel/#service-removal","title":"Service Removal","text":"<p>Automated Removal: <pre><code>docker exec -it provision-host bash\ncd /mnt/urbalurbadisk/provision-host/kubernetes/11-monitoring/not-in-use\n./04-remove-otel-collector.sh rancher-desktop\n</code></pre></p> <p>Manual Removal: <pre><code># Remove Helm chart\nhelm uninstall otel-collector -n monitoring --kube-context rancher-desktop\n\n# Remove IngressRoute\nkubectl delete ingressroute -n monitoring otel-collector\n</code></pre></p>"},{"location":"package-monitoring-otel/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-monitoring-otel/#common-issues","title":"Common Issues","text":"<p>Pods Not Starting: <pre><code># Check pod events\nkubectl describe pod -n monitoring -l app.kubernetes.io/name=opentelemetry-collector\n\n# Common causes:\n# - Backend endpoints unreachable (check Loki/Tempo/Prometheus are deployed)\n# - Configuration errors (check collector logs)\n# - Image pull errors (check network)\n</code></pre></p> <p>External Access Not Working: <pre><code># 1. Check IngressRoute exists\nkubectl get ingressroute -n monitoring otel-collector\n\n# 2. Test with Host header\ncurl -v -X POST http://127.0.0.1/v1/logs \\\n  -H \"Host: otel.localhost\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{}'\n\n# 3. Check Traefik logs\nkubectl logs -n traefik -l app.kubernetes.io/name=traefik\n</code></pre></p> <p>Data Not Reaching Loki: <pre><code># Check collector \u2192 Loki export errors\nkubectl logs -n monitoring -l app.kubernetes.io/name=opentelemetry-collector | grep -i \"loki\\|error\"\n\n# Verify Loki OTLP endpoint is accessible\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -v http://loki-gateway.monitoring.svc.cluster.local:80/otlp\n</code></pre></p> <p>High Memory Usage: <pre><code># Check memory usage\nkubectl top pod -n monitoring -l app.kubernetes.io/name=opentelemetry-collector\n\n# Solutions:\n# 1. Reduce batch size in config\n# 2. Increase memory limits\n# 3. Disable debug exporter in production\n</code></pre></p>"},{"location":"package-monitoring-otel/#maintenance","title":"\ud83d\udccb Maintenance","text":""},{"location":"package-monitoring-otel/#update-otlp-collector","title":"Update OTLP Collector:","text":"<pre><code># Update Helm chart to latest version\nhelm repo update\nhelm upgrade otel-collector open-telemetry/opentelemetry-collector \\\n  -f /mnt/urbalurbadisk/manifests/033-otel-collector-config.yaml \\\n  -n monitoring \\\n  --kube-context rancher-desktop\n</code></pre>"},{"location":"package-monitoring-otel/#configuration-changes","title":"Configuration Changes:","text":"<pre><code># 1. Edit configuration\nvim /Users/terje.christensen/learn/redcross-public/urbalurba-infrastructure/manifests/033-otel-collector-config.yaml\n\n# 2. Apply changes\nhelm upgrade otel-collector open-telemetry/opentelemetry-collector \\\n  -f /mnt/urbalurbadisk/manifests/033-otel-collector-config.yaml \\\n  -n monitoring \\\n  --kube-context rancher-desktop\n\n# 3. Restart pods to pick up changes\nkubectl rollout restart deployment -n monitoring otel-collector-opentelemetry-collector\n</code></pre>"},{"location":"package-monitoring-otel/#use-cases","title":"\ud83d\ude80 Use Cases","text":""},{"location":"package-monitoring-otel/#1-sovdev-logger-integration","title":"1. sovdev-logger Integration","text":"<p>See TypeScript integration example in Management Operations section above.</p>"},{"location":"package-monitoring-otel/#2-multi-language-support","title":"2. Multi-Language Support","text":"<p>The OTLP Collector accepts telemetry from any language with OpenTelemetry SDK support: - TypeScript/JavaScript, Python, Go, Java, .NET, PHP, Ruby, Rust</p>"},{"location":"package-monitoring-otel/#3-external-application-ingestion","title":"3. External Application Ingestion","text":"<p>Applications running outside the cluster (on developer laptops, external servers) can send telemetry via the Traefik IngressRoute.</p>"},{"location":"package-monitoring-otel/#4-debugging-data-flow","title":"4. Debugging Data Flow","text":"<p>Use debug exporter with sampling to verify data is flowing through pipelines without overwhelming logs.</p> <p>\ud83d\udca1 Key Insight: The OpenTelemetry Collector acts as the universal telemetry hub, providing a vendor-neutral ingestion point that decouples applications from backend storage systems. By centralizing telemetry collection and routing, it enables easy backend migration (swap Loki for another log system) without changing application instrumentation. The Traefik IngressRoute extends this capability to external applications, making the observability stack accessible beyond cluster boundaries.</p>"},{"location":"package-monitoring-otel/#related-documentation","title":"\ud83d\udd17 Related Documentation","text":"<p>Monitoring Stack: - Monitoring Overview - Complete observability stack - Prometheus Metrics - Metrics backend - Tempo Tracing - Trace backend - Loki Logs - Log backend - Grafana Visualization - Query and visualization</p> <p>Configuration &amp; Rules: - Traefik IngressRoute - External access patterns - Naming Conventions - Manifest numbering (033, 039) - Development Workflow - Configuration management</p> <p>External Resources: - OTLP Specification: https://opentelemetry.io/docs/specs/otlp/ - Collector Configuration: https://opentelemetry.io/docs/collector/configuration/ - Receivers: https://opentelemetry.io/docs/collector/configuration/#receivers - Processors: https://opentelemetry.io/docs/collector/configuration/#processors - Exporters: https://opentelemetry.io/docs/collector/configuration/#exporters</p>"},{"location":"package-monitoring-prometheus/","title":"Prometheus - Metrics Collection &amp; Alerting","text":"<p>Key Features: Time-Series Database \u2022 PromQL Query Language \u2022 Service Discovery \u2022 Multi-Dimensional Data \u2022 Alertmanager \u2022 Pushgateway \u2022 Node Exporter \u2022 Kube-State-Metrics</p> <p>File: <code>docs/package-monitoring-prometheus.md</code> Purpose: Complete guide to Prometheus deployment and configuration for metrics monitoring in Urbalurba infrastructure Target Audience: DevOps engineers, platform administrators, SREs, developers Last Updated: October 3, 2025</p> <p>Deployed Version: Prometheus v3.6.0 (Helm Chart: prometheus-27.39.0) Official Documentation: https://prometheus.io/docs/prometheus/3.6/</p>"},{"location":"package-monitoring-prometheus/#overview","title":"\ud83d\udccb Overview","text":"<p>Prometheus is the primary metrics backend in the Urbalurba monitoring stack. It provides time-series data storage, powerful querying capabilities, and automated service discovery for Kubernetes environments. Prometheus implements a pull-based model, actively scraping metrics from instrumented applications and exporters.</p> <p>As part of the unified observability stack, Prometheus works alongside Tempo (traces) and Loki (logs), with all data visualized in Grafana.</p> <p>Key Capabilities: - Time-Series Database: Efficient storage of metrics with configurable retention (15 days default) - PromQL: Powerful query language for metrics analysis and alerting - Service Discovery: Automatic discovery of Kubernetes services via ServiceMonitor CRDs - Multi-Dimensional Data: Label-based data model for flexible querying - Remote Write: Accepts metrics from OpenTelemetry Collector - Built-in Exporters: Node metrics, Kubernetes state, push gateway for batch jobs</p> <p>Architecture Type: Pull-based metrics collector with time-series database and alerting</p>"},{"location":"package-monitoring-prometheus/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"package-monitoring-prometheus/#deployment-components","title":"Deployment Components","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Prometheus Stack (namespace: monitoring)   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502 Prometheus Server\u2502    \u2502   Alertmanager   \u2502         \u2502\n\u2502  \u2502                  \u2502    \u2502                  \u2502         \u2502\n\u2502  \u2502 - Metrics Storage\u2502    \u2502 - Alert Routing  \u2502         \u2502\n\u2502  \u2502 - PromQL Engine  \u2502    \u2502 - Deduplication  \u2502         \u2502\n\u2502  \u2502 - Scraping       \u2502\u25c4\u2500\u2500\u2500\u2524 - Notifications  \u2502         \u2502\n\u2502  \u2502 - Remote Write   \u2502    \u2502                  \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502           \u2502                                            \u2502\n\u2502           \u2502 Scrapes Metrics                            \u2502\n\u2502           \u25bc                                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502  Node Exporter   \u2502    \u2502 Kube-State-Metrics\u2502        \u2502\n\u2502  \u2502                  \u2502    \u2502                  \u2502         \u2502\n\u2502  \u2502 - Host Metrics   \u2502    \u2502 - K8s Objects    \u2502         \u2502\n\u2502  \u2502 - CPU/Memory     \u2502    \u2502 - Deployments    \u2502         \u2502\n\u2502  \u2502 - Disk I/O       \u2502    \u2502 - Pods/Services  \u2502         \u2502\n\u2502  \u2502 - Network        \u2502    \u2502 - ConfigMaps     \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502                                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502   Pushgateway    \u2502    \u2502 ServiceMonitor   \u2502         \u2502\n\u2502  \u2502                  \u2502    \u2502    Discovery     \u2502         \u2502\n\u2502  \u2502 - Batch Jobs     \u2502    \u2502                  \u2502         \u2502\n\u2502  \u2502 - Ephemeral      \u2502    \u2502 - Auto Scraping  \u2502         \u2502\n\u2502  \u2502 - Push Metrics   \u2502    \u2502 - Label Config   \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                          \u2502\n         \u25bc                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Grafana Query   \u2502    \u2502 OTLP Collector Push  \u2502\n\u2502                  \u2502    \u2502  (Remote Write API)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"package-monitoring-prometheus/#data-flow","title":"Data Flow","text":"<pre><code>Application Metrics (Prometheus format)\n         \u2502\n         \u2502 HTTP Scrape (Pull)\n         \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502  Prometheus  \u2502\n  \u2502    Server    \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u251c\u2500\u25ba Time-Series Storage (15d retention)\n         \u251c\u2500\u25ba PromQL Evaluation\n         \u251c\u2500\u25ba Alerting Rules\n         \u2514\u2500\u25ba Grafana Datasource\n\nOTLP Collector (Metrics)\n         \u2502\n         \u2502 Remote Write (Push)\n         \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502  Prometheus  \u2502\n  \u2502    Server    \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"package-monitoring-prometheus/#file-structure","title":"File Structure","text":"<pre><code>manifests/\n\u2514\u2500\u2500 030-prometheus-config.yaml              # Prometheus Helm values\n\nansible/playbooks/\n\u251c\u2500\u2500 030-setup-prometheus.yml                # Deployment automation\n\u2514\u2500\u2500 030-remove-prometheus.yml               # Removal automation\n\nprovision-host/kubernetes/11-monitoring/not-in-use/\n\u251c\u2500\u2500 01-setup-prometheus.sh                  # Shell script wrapper\n\u2514\u2500\u2500 01-remove-prometheus.sh                 # Removal script\n\nStorage:\n\u2514\u2500\u2500 PersistentVolumeClaim\n    \u251c\u2500\u2500 prometheus-server (8Gi)             # Metrics storage\n    \u2514\u2500\u2500 prometheus-alertmanager (2Gi)       # Alert state\n</code></pre>"},{"location":"package-monitoring-prometheus/#deployment","title":"\ud83d\ude80 Deployment","text":""},{"location":"package-monitoring-prometheus/#automated-deployment","title":"Automated Deployment","text":"<p>Via Monitoring Stack (Recommended): <pre><code># Deploy entire monitoring stack (includes Prometheus)\ndocker exec -it provision-host bash\ncd /mnt/urbalurbadisk/provision-host/kubernetes/11-monitoring/not-in-use\n./00-setup-all-monitoring.sh rancher-desktop\n</code></pre></p> <p>Individual Deployment: <pre><code># Deploy Prometheus only\ndocker exec -it provision-host bash\ncd /mnt/urbalurbadisk/provision-host/kubernetes/11-monitoring/not-in-use\n./01-setup-prometheus.sh rancher-desktop\n</code></pre></p>"},{"location":"package-monitoring-prometheus/#manual-deployment","title":"Manual Deployment","text":"<p>Prerequisites: - Kubernetes cluster running (Rancher Desktop) - <code>monitoring</code> namespace exists - Helm installed in provision-host container - Manifest file: <code>manifests/030-prometheus-config.yaml</code></p> <p>Deployment Steps: <pre><code># 1. Enter provision-host container\ndocker exec -it provision-host bash\n\n# 2. Add Prometheus Helm repository\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\n\n# 3. Deploy Prometheus\nhelm upgrade --install prometheus prometheus-community/prometheus \\\n  -f /mnt/urbalurbadisk/manifests/030-prometheus-config.yaml \\\n  --namespace monitoring \\\n  --create-namespace \\\n  --timeout 600s \\\n  --kube-context rancher-desktop\n\n# 4. Wait for pods to be ready\nkubectl wait --for=condition=ready pod \\\n  -l app.kubernetes.io/name=prometheus \\\n  -l app.kubernetes.io/component=server \\\n  -n monitoring --timeout=300s\n</code></pre></p> <p>Deployment Time: ~2-3 minutes</p>"},{"location":"package-monitoring-prometheus/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"package-monitoring-prometheus/#prometheus-configuration-manifests030-prometheus-configyaml","title":"Prometheus Configuration (<code>manifests/030-prometheus-config.yaml</code>)","text":"<p>Core Settings: <pre><code>server:\n  retention: 15d                    # Metrics retention period\n  persistentVolume:\n    enabled: true\n    size: 8Gi                       # Storage for time-series data\n  resources:\n    requests:\n      cpu: 200m\n      memory: 512Mi\n    limits:\n      cpu: 500m\n      memory: 1Gi\n  extraArgs:\n    web.enable-remote-write-receiver: \"\"  # REQUIRED: Enables /api/v1/write endpoint for OTLP Collector\n</code></pre></p> <p>Key Configuration Sections:</p> <p>1. Alertmanager (Alert Processing): <pre><code>alertmanager:\n  enabled: true\n  persistentVolume:\n    enabled: true\n    size: 2Gi\n  resources:\n    requests:\n      cpu: 100m\n      memory: 128Mi\n</code></pre></p> <p>2. Node Exporter (Host Metrics): <pre><code>nodeExporter:\n  enabled: true\n  hostNetwork: false                # Use pod network\n</code></pre></p> <p>3. Pushgateway (Batch Job Metrics): <pre><code>pushgateway:\n  enabled: true\n  persistentVolume:\n    enabled: false                  # Ephemeral storage\n  resources:\n    requests:\n      cpu: 50m\n      memory: 64Mi\n</code></pre></p> <p>4. Kube-State-Metrics (Kubernetes Objects): <pre><code>kubeStateMetrics:\n  enabled: true                     # Pod/Deployment/Service metrics\n</code></pre></p> <p>5. ServiceMonitor (Auto-Discovery): <pre><code>serviceMonitors:\n  enabled: true                     # Automatic service discovery\n</code></pre></p>"},{"location":"package-monitoring-prometheus/#resource-configuration","title":"Resource Configuration","text":"<p>Storage Requirements: - Prometheus Server: 8Gi persistent volume (15-day retention) - Alertmanager: 2Gi persistent volume (alert state) - Pushgateway: No persistence (ephemeral metrics)</p> <p>Memory &amp; CPU: - Server: 512Mi request, 1Gi limit / 200m CPU request, 500m limit - Alertmanager: 128Mi request, 256Mi limit / 100m CPU request, 200m limit - Pushgateway: 64Mi request, 128Mi limit / 50m CPU request, 100m limit</p>"},{"location":"package-monitoring-prometheus/#security-configuration","title":"Security Configuration","text":"<p>Network Access: <pre><code># Internal cluster access only (no IngressRoute by default)\nService: prometheus-server.monitoring.svc.cluster.local:80\n</code></pre></p> <p>Optional External Access: <pre><code># Port forwarding for local development\nkubectl port-forward -n monitoring svc/prometheus-server 9090:80\n\n# Access at: http://localhost:9090\n</code></pre></p>"},{"location":"package-monitoring-prometheus/#monitoring-verification","title":"\ud83d\udd0d Monitoring &amp; Verification","text":""},{"location":"package-monitoring-prometheus/#health-checks","title":"Health Checks","text":"<p>Check Pod Status: <pre><code># All Prometheus components\nkubectl get pods -n monitoring -l app.kubernetes.io/name=prometheus\n\n# Expected output (5 pods total):\nNAME                                               READY   STATUS\nprometheus-server-xxx                              2/2     Running    # Main Prometheus server + config reloader\nprometheus-alertmanager-xxx                        1/1     Running    # Alert processing and routing\nprometheus-kube-state-metrics-xxx                  1/1     Running    # Kubernetes object metrics\nprometheus-prometheus-node-exporter-xxx            1/1     Running    # Host/node metrics (CPU, memory, disk)\nprometheus-prometheus-pushgateway-xxx              1/1     Running    # Batch job metrics receiver\n</code></pre></p> <p>Pod Descriptions: - prometheus-server: Main Prometheus server (scraping, storage, querying) + config-reload sidecar - prometheus-alertmanager: Processes and routes alerts to notification channels - prometheus-kube-state-metrics: Exposes Kubernetes object state as Prometheus metrics (pods, deployments, services) - prometheus-prometheus-node-exporter: DaemonSet that collects host-level metrics from the node (CPU, memory, disk I/O, network) - prometheus-prometheus-pushgateway: Allows ephemeral/batch jobs to push metrics to Prometheus</p> <p>Check Service Endpoints: <pre><code># Verify services are accessible\nkubectl get svc -n monitoring -l app.kubernetes.io/name=prometheus\n\n# Expected services:\nprometheus-server                    ClusterIP   10.43.x.x    80/TCP\nprometheus-alertmanager              ClusterIP   10.43.x.x    9093/TCP\nprometheus-prometheus-pushgateway    ClusterIP   10.43.x.x    9091/TCP\nprometheus-prometheus-node-exporter  ClusterIP   10.43.x.x    9100/TCP\n</code></pre></p>"},{"location":"package-monitoring-prometheus/#service-verification","title":"Service Verification","text":"<p>Test Prometheus API: <pre><code># Runtime info endpoint\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -s http://prometheus-server.monitoring.svc.cluster.local:80/api/v1/status/runtimeinfo\n\n# Expected: JSON response with runtime information\n</code></pre></p> <p>Test Metrics Endpoint: <pre><code># Prometheus self-monitoring metrics\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -s http://prometheus-server.monitoring.svc.cluster.local:80/metrics | head -20\n</code></pre></p>"},{"location":"package-monitoring-prometheus/#data-ingestion-testing","title":"Data Ingestion Testing","text":"<p>Push Test Metric: <pre><code># Push metric to Pushgateway\nkubectl run prometheus-data-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  /bin/sh -c 'echo \"test_metric 42\" | curl -X POST --data-binary @- \\\n  http://prometheus-prometheus-pushgateway.monitoring.svc.cluster.local:9091/metrics/job/test/instance/test'\n</code></pre></p> <p>Query Test Metric: <pre><code># Wait 15 seconds for scrape interval\nsleep 15\n\n# Query Prometheus for the test metric\nkubectl run prometheus-query --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -s --data-urlencode 'query=test_metric' \\\n  \"http://prometheus-server.monitoring.svc.cluster.local:80/api/v1/query\"\n\n# Expected: JSON response with \"status\":\"success\" and test_metric value\n</code></pre></p>"},{"location":"package-monitoring-prometheus/#automated-verification","title":"Automated Verification","text":"<p>The deployment playbook (<code>030-setup-prometheus.yml</code>) performs automated tests: 1. \u2705 Server API connectivity test 2. \u2705 Metrics endpoint test 3. \u2705 Pushgateway ingestion test 4. \u2705 Query test metric verification</p>"},{"location":"package-monitoring-prometheus/#management-operations","title":"\ud83d\udee0\ufe0f Management Operations","text":""},{"location":"package-monitoring-prometheus/#prometheus-ui-access-development","title":"Prometheus UI Access (Development)","text":"<p>Port Forwarding: <pre><code># Forward Prometheus UI to localhost\nkubectl port-forward -n monitoring svc/prometheus-server 9090:80\n\n# Open browser\nhttp://localhost:9090\n</code></pre></p> <p>UI Features: - Graph: PromQL query and visualization - Alerts: Active alerts and rules - Status: Targets, service discovery, configuration - Metrics Explorer: Browse available metrics</p>"},{"location":"package-monitoring-prometheus/#common-promql-queries","title":"Common PromQL Queries","text":"<p>Node Metrics: <pre><code># CPU usage by node\n100 - (avg by (instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)\n\n# Memory usage\nnode_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100\n\n# Disk usage\n100 - (node_filesystem_avail_bytes / node_filesystem_size_bytes * 100)\n</code></pre></p> <p>Kubernetes Metrics: <pre><code># Pod count by namespace\ncount by (namespace) (kube_pod_info)\n\n# Deployment replicas\nkube_deployment_status_replicas{deployment=\"prometheus-server\"}\n\n# Container restarts\nkube_pod_container_status_restarts_total\n</code></pre></p> <p>Prometheus Self-Monitoring: <pre><code># Scrape duration\nprometheus_target_interval_length_seconds\n\n# Active time series\nprometheus_tsdb_head_series\n\n# Storage size\nprometheus_tsdb_storage_blocks_bytes\n</code></pre></p>"},{"location":"package-monitoring-prometheus/#service-removal","title":"Service Removal","text":"<p>Automated Removal: <pre><code>docker exec -it provision-host bash\ncd /mnt/urbalurbadisk/provision-host/kubernetes/11-monitoring/not-in-use\n./01-remove-prometheus.sh rancher-desktop\n</code></pre></p> <p>Manual Removal: <pre><code># Remove Helm chart\nhelm uninstall prometheus -n monitoring --kube-context rancher-desktop\n\n# Remove PVCs (optional - preserves data if omitted)\nkubectl delete pvc -n monitoring -l app.kubernetes.io/name=prometheus\n</code></pre></p>"},{"location":"package-monitoring-prometheus/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-monitoring-prometheus/#common-issues","title":"Common Issues","text":"<p>Pods Not Starting: <pre><code># Check pod events\nkubectl describe pod -n monitoring -l app.kubernetes.io/name=prometheus\n\n# Common causes:\n# - Insufficient resources (check node capacity)\n# - PVC binding issues (check PV availability)\n# - Image pull errors (check network connectivity)\n</code></pre></p> <p>High Memory Usage: <pre><code># Check Prometheus memory usage\nkubectl top pod -n monitoring -l app.kubernetes.io/name=prometheus\n\n# Solutions:\n# 1. Reduce retention period in manifests/030-prometheus-config.yaml\n# 2. Increase memory limits\n# 3. Check for cardinality explosion (too many unique label combinations)\n</code></pre></p> <p>Metrics Not Appearing: <pre><code># Check scrape targets\nkubectl port-forward -n monitoring svc/prometheus-server 9090:80\n# Visit http://localhost:9090/targets\n\n# Check ServiceMonitor configuration\nkubectl get servicemonitor -n monitoring\n\n# Verify application metrics endpoint\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n &lt;app-namespace&gt; -- \\\n  curl -s http://&lt;service&gt;:&lt;port&gt;/metrics\n</code></pre></p> <p>Remote Write Failures (from OTLP Collector): <pre><code># Check Prometheus server logs\nkubectl logs -n monitoring -l app.kubernetes.io/name=prometheus -l app.kubernetes.io/component=server\n\n# Check OTLP Collector logs for remote write errors\nkubectl logs -n monitoring -l app.kubernetes.io/name=opentelemetry-collector | grep -i prometheus\n\n# Verify remote write endpoint is accessible\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -v http://prometheus-server.monitoring.svc.cluster.local:80/api/v1/write\n</code></pre></p> <p>IMPORTANT: If Prometheus returns 404 or error \"remote write receiver needs to be enabled\", check that the remote-write-receiver flag is enabled: <pre><code># Check Prometheus startup flags\nkubectl get pod -n monitoring -l app.kubernetes.io/name=prometheus,app.kubernetes.io/component=server \\\n  -o jsonpath='{.items[0].spec.containers[?(@.name==\"prometheus-server\")].args}' | jq -r '.[]' | grep \"remote-write\"\n\n# Should see: --web.enable-remote-write-receiver\n# If missing, add to manifests/030-prometheus-config.yaml:\n#   server:\n#     extraArgs:\n#       web.enable-remote-write-receiver: \"\"\n</code></pre></p> <p>Alertmanager Not Firing Alerts: <pre><code># Check Alertmanager logs\nkubectl logs -n monitoring -l app.kubernetes.io/name=prometheus -l app.kubernetes.io/component=alertmanager\n\n# Check alert rules in Prometheus UI\nkubectl port-forward -n monitoring svc/prometheus-server 9090:80\n# Visit http://localhost:9090/alerts\n\n# Verify Alertmanager configuration\nkubectl get configmap -n monitoring prometheus-alertmanager -o yaml\n</code></pre></p>"},{"location":"package-monitoring-prometheus/#maintenance","title":"\ud83d\udccb Maintenance","text":""},{"location":"package-monitoring-prometheus/#regular-tasks","title":"Regular Tasks","text":"<p>Monitor Storage Usage: <pre><code># Check PVC usage\nkubectl get pvc -n monitoring\n\n# Check Prometheus TSDB size via PromQL\nkubectl port-forward -n monitoring svc/prometheus-server 9090:80\n# Query: prometheus_tsdb_storage_blocks_bytes\n</code></pre></p> <p>Update Prometheus: <pre><code># Update Helm chart to latest version\nhelm repo update\nhelm upgrade prometheus prometheus-community/prometheus \\\n  -f /mnt/urbalurbadisk/manifests/030-prometheus-config.yaml \\\n  -n monitoring \\\n  --kube-context rancher-desktop\n</code></pre></p> <p>Cleanup Old Metrics (automatic): <pre><code># Retention handled automatically via server.retention setting\nserver:\n  retention: 15d  # Metrics older than 15 days are purged\n</code></pre></p>"},{"location":"package-monitoring-prometheus/#backup-procedures","title":"Backup Procedures","text":"<p>Snapshot Time-Series Data: <pre><code># Create snapshot via API\nkubectl run curl-snap --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -X POST http://prometheus-server.monitoring.svc.cluster.local:80/api/v1/admin/tsdb/snapshot\n\n# Snapshot stored in: /prometheus/snapshots/\n</code></pre></p> <p>Backup PVC: <pre><code># Export PVC data (requires read/write access)\nkubectl exec -n monitoring deployment/prometheus-server -- \\\n  tar czf /tmp/prometheus-backup.tar.gz /prometheus\n\n# Copy to local machine\nkubectl cp monitoring/prometheus-server-xxx:/tmp/prometheus-backup.tar.gz \\\n  ./prometheus-backup.tar.gz\n</code></pre></p>"},{"location":"package-monitoring-prometheus/#disaster-recovery","title":"Disaster Recovery","text":"<p>Restore from Backup: <pre><code># 1. Remove existing deployment\n./01-remove-prometheus.sh rancher-desktop\n\n# 2. Restore PVC data (manual process, requires direct PV access)\n# 3. Redeploy Prometheus\n./01-setup-prometheus.sh rancher-desktop\n</code></pre></p> <p>Data Loss Scenarios: - PVC deleted: Metrics are lost, redeploy and start fresh collection - Corruption: Prometheus auto-repairs TSDB on startup (check logs) - Retention expired: Expected behavior, increase retention if needed</p>"},{"location":"package-monitoring-prometheus/#use-cases","title":"\ud83d\ude80 Use Cases","text":""},{"location":"package-monitoring-prometheus/#1-application-metrics-monitoring","title":"1. Application Metrics Monitoring","text":"<p>Instrument Application: <pre><code>// Go example with Prometheus client\nimport \"github.com/prometheus/client_golang/prometheus/promhttp\"\n\nhttp.Handle(\"/metrics\", promhttp.Handler())\n</code></pre></p> <p>Expose via ServiceMonitor: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-app\n  labels:\n    app: my-app\n  annotations:\n    prometheus.io/scrape: \"true\"\n    prometheus.io/port: \"8080\"\n    prometheus.io/path: \"/metrics\"\n</code></pre></p> <p>Query in Grafana: <pre><code>rate(http_requests_total{job=\"my-app\"}[5m])\n</code></pre></p>"},{"location":"package-monitoring-prometheus/#2-alert-on-high-resource-usage","title":"2. Alert on High Resource Usage","text":"<p>Create Alert Rule: <pre><code># Add to Prometheus configuration\ngroups:\n  - name: resource_alerts\n    rules:\n      - alert: HighMemoryUsage\n        expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 &lt; 10\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"High memory usage detected\"\n</code></pre></p>"},{"location":"package-monitoring-prometheus/#3-integrate-with-otlp-collector","title":"3. Integrate with OTLP Collector","text":"<p>Receive Metrics from OTLP: <pre><code># OTLP Collector configuration (manifests/033-otel-collector-config.yaml)\nexporters:\n  prometheusremotewrite:\n    endpoint: http://prometheus-server.monitoring.svc.cluster.local:80/api/v1/write\n</code></pre></p> <p>Verify Metrics Ingestion: <pre><code># Query OTLP-sourced metrics\nkubectl port-forward -n monitoring svc/prometheus-server 9090:80\n# Query: {job=\"otel-collector\"}\n</code></pre></p>"},{"location":"package-monitoring-prometheus/#4-dashboard-creation-in-grafana","title":"4. Dashboard Creation in Grafana","text":"<p>Add Prometheus Datasource (pre-configured): <pre><code>datasources:\n  - name: Prometheus\n    type: prometheus\n    url: http://prometheus-server.monitoring.svc.cluster.local:80\n    access: proxy\n</code></pre></p> <p>Create Dashboard Panel: <pre><code>{\n  \"targets\": [{\n    \"expr\": \"rate(prometheus_http_requests_total[5m])\",\n    \"legendFormat\": \"{{handler}}\"\n  }]\n}\n</code></pre></p> <p>\ud83d\udca1 Key Insight: Prometheus serves as the metrics foundation for the entire Urbalurba observability stack. Its pull-based architecture, combined with ServiceMonitor auto-discovery and PromQL's powerful query capabilities, provides comprehensive visibility into infrastructure and application health. When integrated with OTLP Collector, Loki, and Tempo, it forms a complete observability solution visualized in Grafana.</p>"},{"location":"package-monitoring-prometheus/#related-documentation","title":"\ud83d\udd17 Related Documentation","text":"<p>Monitoring Stack: - Monitoring Overview - Complete observability stack - Tempo Tracing - Distributed tracing backend - Loki Logs - Log aggregation - OTLP Collector - Telemetry pipeline - Grafana Visualization - Dashboards</p> <p>Configuration &amp; Rules: - Naming Conventions - Manifest numbering (030) - Development Workflow - Configuration management - Automated Deployment - Orchestration</p>"},{"location":"package-monitoring-readme/","title":"Monitoring &amp; Observability - Complete Observability Stack","text":"<p>File: <code>docs/package-monitoring-readme.md</code> Purpose: Overview of all monitoring and observability services in Urbalurba infrastructure Target Audience: DevOps engineers, developers, system administrators, platform engineers Last Updated: October 3, 2025</p> <p>Deployed Versions: - Prometheus v3.6.0 (chart 27.39.0) - Tempo v2.8.2 (chart 1.23.3) - Loki v3.5.5 (chart 6.41.1) - OpenTelemetry Collector v0.136.0 (chart 0.136.1) - Grafana v12.1.1 (chart 10.0.0)</p>"},{"location":"package-monitoring-readme/#overview","title":"\ud83d\udccb Overview","text":"<p>Urbalurba infrastructure provides a comprehensive observability stack built on industry-standard open-source tools. The monitoring system implements the three pillars of observability: metrics (Prometheus), traces (Tempo), and logs (Loki), unified through OpenTelemetry and visualized in Grafana.</p> <p>This architecture enables: - Full-stack observability: Monitor infrastructure, applications, and business metrics - Distributed tracing: Track requests across microservices - Log aggregation: Centralized logging with powerful query capabilities - Unified visualization: Single pane of glass for all observability data</p> <p>Available Monitoring Services: - Prometheus: Metrics collection, storage, and alerting - Tempo: Distributed tracing backend - Loki: Log aggregation and querying - OpenTelemetry Collector: Vendor-neutral telemetry pipeline - Grafana: Visualization, dashboards, and data exploration</p>"},{"location":"package-monitoring-readme/#monitoring-services","title":"\ud83d\udcca Monitoring Services","text":""},{"location":"package-monitoring-readme/#prometheus-metrics-alerting","title":"Prometheus - Metrics &amp; Alerting \ud83e\udd47","text":"<p>Status: \u2705 Active | Port: 9090 | Type: Metrics Database</p> <p>Key Features: Time-Series Database \u2022 PromQL Query Language \u2022 Service Discovery \u2022 Multi-Dimensional Data Model \u2022 Alerting Rules \u2022 Prometheus Operator</p> <p>Prometheus serves as the primary metrics backend with powerful querying capabilities and native Kubernetes integration. Uses Prometheus Operator for automated service monitoring and alert management.</p> <p>Key Capabilities: - Metrics Collection: Pull-based scraping from Kubernetes services - Time-Series Storage: Efficient storage with configurable retention - PromQL: Powerful query language for metrics analysis - Service Discovery: Automatic discovery of Kubernetes services</p> <p>Configuration: <code>manifests/030-prometheus-config.yaml</code> Deployment: <code>ansible/playbooks/030-setup-prometheus.yml</code></p> <p>\ud83d\udcda Complete Documentation \u2192</p>"},{"location":"package-monitoring-readme/#tempo-distributed-tracing","title":"Tempo - Distributed Tracing \ud83d\udd0d","text":"<p>Status: \u2705 Active | Port: 3100 (query), 4317 (gRPC), 4318 (HTTP) | Type: Trace Backend</p> <p>Key Features: Distributed Tracing \u2022 Jaeger/Zipkin/OTLP Support \u2022 Cost-Effective Storage \u2022 High-Volume Ingestion \u2022 TraceQL Query Language</p> <p>High-performance distributed tracing backend designed for cloud-native applications. Accepts traces via OpenTelemetry, Jaeger, and Zipkin protocols with minimal storage overhead.</p> <p>Key Capabilities: - OTLP Native: Primary ingestion via OpenTelemetry Collector - TraceQL: Query traces with powerful filtering - Low Storage Cost: Efficient object storage backend - Multi-Tenancy: Isolated trace data per tenant</p> <p>Configuration: <code>manifests/031-tempo-config.yaml</code> Deployment: <code>ansible/playbooks/031-setup-tempo.yml</code></p> <p>\ud83d\udcda Complete Documentation \u2192</p>"},{"location":"package-monitoring-readme/#loki-log-aggregation","title":"Loki - Log Aggregation \ud83d\udcdd","text":"<p>Status: \u2705 Active | Port: 3100 | Type: Log Database</p> <p>Key Features: Log Aggregation \u2022 LogQL Query Language \u2022 Label-Based Indexing \u2022 Cost-Effective Storage \u2022 Grafana Integration \u2022 Multi-Tenancy</p> <p>Like Prometheus but for logs - Loki indexes labels not full-text, making it extremely efficient for cloud-native logging. Designed to work seamlessly with Grafana and Prometheus.</p> <p>Key Capabilities: - Label-Based Indexing: Fast queries without full-text indexing - LogQL: Familiar PromQL-like query syntax - OTLP Ingestion: Receives logs via OpenTelemetry Collector - Low Cost: Minimal storage and operational overhead</p> <p>Configuration: <code>manifests/032-loki-config.yaml</code> Deployment: <code>ansible/playbooks/032-setup-loki.yml</code></p> <p>\ud83d\udcda Complete Documentation \u2192</p>"},{"location":"package-monitoring-readme/#opentelemetry-collector-telemetry-pipeline","title":"OpenTelemetry Collector - Telemetry Pipeline \ud83d\udd04","text":"<p>Status: \u2705 Active | Port: 4317 (gRPC), 4318 (HTTP) | Type: Telemetry Gateway</p> <p>Key Features: Vendor-Neutral Protocol \u2022 Logs/Traces/Metrics \u2022 HTTP &amp; gRPC Endpoints \u2022 Traefik IngressRoute \u2022 External Ingestion \u2022 Multi-Backend Export</p> <p>Central telemetry collection hub that receives OpenTelemetry Protocol (OTLP) data from applications and routes it to Prometheus, Tempo, and Loki backends.</p> <p>Key Capabilities: - OTLP Receivers: HTTP (4318) and gRPC (4317) endpoints - External Access: Traefik IngressRoute at <code>http://otel.localhost/v1/logs</code> - Multi-Export: Routes logs to Loki, traces to Tempo, metrics to Prometheus - Protocol Translation: Converts OTLP to backend-specific formats</p> <p>Configuration: <code>manifests/033-otel-collector-config.yaml</code> IngressRoute: <code>manifests/039-otel-collector-ingressroute.yaml</code> Deployment: <code>ansible/playbooks/033-setup-otel-collector.yml</code></p> <p>\ud83d\udcda Complete Documentation \u2192</p>"},{"location":"package-monitoring-readme/#grafana-visualization-platform","title":"Grafana - Visualization Platform \ud83d\udcc8","text":"<p>Status: \u2705 Active | Port: 80 (UI) | Type: Visualization &amp; Dashboards</p> <p>Key Features: Unified Dashboards \u2022 Multi-Datasource Queries \u2022 Dashboard Sidecar \u2022 Alert Management \u2022 User Authentication \u2022 Dashboard as Code</p> <p>Grafana provides unified visualization for all observability data with pre-configured datasources for Prometheus, Tempo, and Loki. Dashboards are managed as ConfigMaps and auto-loaded via sidecar.</p> <p>Key Capabilities: - Pre-Configured Datasources: Prometheus, Loki, Tempo ready to use - Dashboard Sidecar: Auto-loads dashboards from ConfigMaps - Unified Queries: Correlate metrics, logs, and traces - Authentik SSO: Optional authentication via forward auth</p> <p>Configuration: <code>manifests/034-grafana-config.yaml</code> Dashboards: <code>manifests/035-grafana-dashboards.yaml</code>, <code>036-grafana-sovdev-verification.yaml</code> IngressRoute: <code>manifests/038-grafana-ingressroute.yaml</code> Deployment: <code>ansible/playbooks/034-setup-grafana.yml</code></p> <p>\ud83d\udcda Complete Documentation \u2192</p>"},{"location":"package-monitoring-readme/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"package-monitoring-readme/#observability-data-flow","title":"Observability Data Flow","text":"<pre><code>Applications (with OTLP SDK)\n         \u2502\n         \u251c\u2500\u25ba Logs \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u251c\u2500\u25ba Traces \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n         \u2514\u2500\u25ba Metrics \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n                                      \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  OpenTelemetry Collector     \u2502\n                    \u2502  (OTLP Receiver)             \u2502\n                    \u2502  - HTTP: 4318                \u2502\n                    \u2502  - gRPC: 4317                \u2502\n                    \u2502  - Ingress: otel.localhost   \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502                \u2502                \u2502\n                \u25bc                \u25bc                \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502   Loki   \u2502     \u2502  Tempo   \u2502    \u2502Prometheus\u2502\n         \u2502  (Logs)  \u2502     \u2502 (Traces) \u2502    \u2502 (Metrics)\u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502                \u2502                \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502    Grafana      \u2502\n                        \u2502  (Visualization)\u2502\n                        \u2502  grafana.localhost\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"package-monitoring-readme/#namespace-deployment","title":"Namespace &amp; Deployment","text":"<p>All monitoring services are deployed in the <code>monitoring</code> namespace:</p> <pre><code>kubectl get pods -n monitoring\n\nCOMPONENT                           STATUS\notel-collector-xxx                  Running   # OTLP ingestion\nprometheus-xxx                      Running   # Metrics backend\ntempo-xxx                           Running   # Trace backend\nloki-0                             Running   # Log backend\ngrafana-xxx                         Running   # Visualization\n</code></pre>"},{"location":"package-monitoring-readme/#file-structure","title":"File Structure","text":"<pre><code>manifests/\n\u251c\u2500\u2500 030-prometheus-config.yaml              # Prometheus Helm values\n\u251c\u2500\u2500 031-tempo-config.yaml                   # Tempo Helm values\n\u251c\u2500\u2500 032-loki-config.yaml                    # Loki Helm values\n\u251c\u2500\u2500 033-otel-collector-config.yaml          # OTLP Collector Helm values\n\u251c\u2500\u2500 034-grafana-config.yaml                 # Grafana Helm values\n\u251c\u2500\u2500 035-grafana-dashboards.yaml             # Installation test dashboards\n\u251c\u2500\u2500 036-grafana-sovdev-verification.yaml    # sovdev-logger verification\n\u251c\u2500\u2500 038-grafana-ingressroute.yaml           # Grafana UI ingress\n\u2514\u2500\u2500 039-otel-collector-ingressroute.yaml    # OTLP Collector ingress\n\nansible/playbooks/\n\u251c\u2500\u2500 030-setup-prometheus.yml                # Prometheus deployment\n\u251c\u2500\u2500 030-remove-prometheus.yml               # Prometheus removal\n\u251c\u2500\u2500 031-setup-tempo.yml                     # Tempo deployment\n\u251c\u2500\u2500 031-remove-tempo.yml                    # Tempo removal\n\u251c\u2500\u2500 032-setup-loki.yml                      # Loki deployment\n\u251c\u2500\u2500 032-remove-loki.yml                     # Loki removal\n\u251c\u2500\u2500 033-setup-otel-collector.yml            # OTLP Collector deployment\n\u251c\u2500\u2500 033-remove-otel-collector.yml           # OTLP Collector removal\n\u251c\u2500\u2500 034-setup-grafana.yml                   # Grafana deployment\n\u2514\u2500\u2500 034-remove-grafana.yml                  # Grafana removal\n\nprovision-host/kubernetes/11-monitoring/not-in-use/\n\u251c\u2500\u2500 00-setup-all-monitoring.sh              # Deploy all monitoring services\n\u251c\u2500\u2500 00-remove-all-monitoring.sh             # Remove all monitoring services\n\u251c\u2500\u2500 01-setup-prometheus.sh                  # Prometheus deployment script\n\u251c\u2500\u2500 01-remove-prometheus.sh                 # Prometheus removal script\n\u251c\u2500\u2500 02-setup-tempo.sh                       # Tempo deployment script\n\u251c\u2500\u2500 02-remove-tempo.sh                      # Tempo removal script\n\u251c\u2500\u2500 03-setup-loki.sh                        # Loki deployment script\n\u251c\u2500\u2500 03-remove-loki.sh                       # Loki removal script\n\u251c\u2500\u2500 04-setup-otel-collector.sh              # OTLP Collector deployment script\n\u251c\u2500\u2500 04-remove-otel-collector.sh             # OTLP Collector removal script\n\u251c\u2500\u2500 05-setup-grafana.sh                     # Grafana deployment script\n\u2514\u2500\u2500 05-remove-grafana.sh                    # Grafana removal script\n</code></pre>"},{"location":"package-monitoring-readme/#storage-persistence","title":"Storage &amp; Persistence","text":"<p>All monitoring services use Kubernetes PersistentVolumeClaims: - Prometheus: Configurable retention (default 15d) - Tempo: Object storage for traces - Loki: Chunk storage for logs - Grafana: Dashboard and configuration persistence</p>"},{"location":"package-monitoring-readme/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"package-monitoring-readme/#deploy-complete-monitoring-stack","title":"Deploy Complete Monitoring Stack","text":"<p>Option 1: Automated Deployment (Recommended) <pre><code># All monitoring services deploy automatically during cluster build\n./install-rancher.sh\n</code></pre></p> <p>Option 2: Manual Deployment (for testing/development) <pre><code># Enter provision-host container\ndocker exec -it provision-host bash\n\n# Navigate to monitoring scripts\ncd /mnt/urbalurbadisk/provision-host/kubernetes/11-monitoring/not-in-use\n\n# Deploy all monitoring services\n./00-setup-all-monitoring.sh rancher-desktop\n</code></pre></p> <p>Option 3: Deploy Individual Services <pre><code># Deploy in order (dependencies matter!)\n./01-setup-prometheus.sh rancher-desktop\n./02-setup-tempo.sh rancher-desktop\n./03-setup-loki.sh rancher-desktop\n./04-setup-otel-collector.sh rancher-desktop\n./05-setup-grafana.sh rancher-desktop\n</code></pre></p>"},{"location":"package-monitoring-readme/#access-monitoring-services","title":"Access Monitoring Services","text":"<p>Grafana UI: <pre><code># Open in browser\nhttp://grafana.localhost\n\n# Default credentials (if auth not configured)\n# Username: admin\n# Password: (from urbalurba-secrets ConfigMap)\n</code></pre></p> <p>OTLP Collector Ingestion: <pre><code># Logs endpoint\nhttp://otel.localhost/v1/logs\n\n# Traces endpoint\nhttp://otel.localhost/v1/traces\n\n# Required header for localhost routing\nHost: otel.localhost\n</code></pre></p> <p>Prometheus UI (internal only): <pre><code># Port forward to access\nkubectl port-forward -n monitoring svc/prometheus-server 9090:80\n\n# Open in browser\nhttp://localhost:9090\n</code></pre></p>"},{"location":"package-monitoring-readme/#verify-stack-health","title":"Verify Stack Health","text":"<pre><code># Check all monitoring pods\nkubectl get pods -n monitoring\n\n# Verify Grafana datasources\nkubectl exec -n monitoring deployment/grafana -- \\\n  curl -s http://localhost:3000/api/datasources\n\n# Test OTLP endpoint\ncurl -X POST http://127.0.0.1/v1/logs \\\n  -H \"Host: otel.localhost\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"resourceLogs\":[{\"resource\":{\"attributes\":[{\"key\":\"service.name\",\"value\":{\"stringValue\":\"test\"}}]},\"scopeLogs\":[{\"logRecords\":[{\"body\":{\"stringValue\":\"test log\"}}]}]}]}'\n</code></pre>"},{"location":"package-monitoring-readme/#integration-patterns","title":"\ud83d\udd0d Integration Patterns","text":""},{"location":"package-monitoring-readme/#application-instrumentation","title":"Application Instrumentation","text":"<p>OpenTelemetry SDK (Recommended): <pre><code>// TypeScript example using @sovdev/logger\nimport { initializeSovdevLogger } from '@sovdev/logger';\n\n// Initialize with OTLP endpoint\ninitializeSovdevLogger('my-service-name');\n\n// Environment variables\nSYSTEM_ID=my-service-name\nOTEL_EXPORTER_OTLP_LOGS_ENDPOINT=http://127.0.0.1/v1/logs\nOTEL_EXPORTER_OTLP_HEADERS={\"Host\":\"otel.localhost\"}\n</code></pre></p> <p>Query Logs in Grafana: <pre><code># LogQL query\n{service_name=\"my-service-name\"}\n\n# Filter by level\n{service_name=\"my-service-name\"} |= \"error\"\n\n# Regex pattern\n{service_name=~\"sovdev-test.*\"}\n</code></pre></p>"},{"location":"package-monitoring-readme/#dashboard-management","title":"Dashboard Management","text":"<p>Auto-Loading Pattern: <pre><code># Create ConfigMap with label\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-dashboard\n  namespace: monitoring\n  labels:\n    grafana_dashboard: \"1\"  # Triggers sidecar auto-load\ndata:\n  dashboard.json: |\n    { ... dashboard JSON ... }\n</code></pre></p> <p>Apply and Verify: <pre><code>kubectl apply -f manifests/036-my-dashboard.yaml\n\n# Wait ~30 seconds for sidecar to reload\nkubectl rollout restart deployment/grafana -n monitoring\n</code></pre></p>"},{"location":"package-monitoring-readme/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-monitoring-readme/#common-issues","title":"Common Issues","text":"<p>No data in Grafana: <pre><code># 1. Verify datasource configuration\nkubectl exec -n monitoring deployment/grafana -- \\\n  curl -s http://localhost:3000/api/datasources\n\n# 2. Check Loki for labels\nkubectl exec -n monitoring loki-0 -c loki -- \\\n  wget -q -O - 'http://localhost:3100/loki/api/v1/labels'\n\n# 3. Test OTLP collector connectivity\nkubectl logs -n monitoring -l app.kubernetes.io/name=opentelemetry-collector\n</code></pre></p> <p>Dashboard not loading: <pre><code># 1. Verify ConfigMap exists with correct label\nkubectl get configmap -n monitoring -l grafana_dashboard=1\n\n# 2. Check Grafana sidecar logs\nkubectl logs -n monitoring deployment/grafana -c grafana-sc-dashboard\n\n# 3. Restart Grafana to force reload\nkubectl rollout restart deployment/grafana -n monitoring\n</code></pre></p> <p>OTLP ingestion failing: <pre><code># 1. Verify IngressRoute exists\nkubectl get ingressroute -n monitoring otel-collector\n\n# 2. Check Host header routing\ncurl -v -X POST http://127.0.0.1/v1/logs \\\n  -H \"Host: otel.localhost\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{...}'\n\n# 3. Check OTLP collector logs\nkubectl logs -n monitoring -l app.kubernetes.io/name=opentelemetry-collector --tail=50\n</code></pre></p>"},{"location":"package-monitoring-readme/#related-documentation","title":"\ud83d\udd17 Related Documentation","text":"<p>Core Documentation: - Prometheus Metrics Backend - Metrics collection and alerting - Tempo Tracing Backend - Distributed tracing - Loki Log Aggregation - Log storage and querying - OpenTelemetry Collector - OTLP ingestion pipeline - Grafana Visualization - Dashboards and exploration</p> <p>Integration Guides: - Traefik IngressRoute Patterns - Routing configuration for Grafana and OTLP - Secrets Management - Managing Grafana admin credentials - Naming Conventions - Manifest and playbook numbering (030-039) - Development Workflow - Working with monitoring configuration</p> <p>Provisioning: - Automated Deployment Rules - Orchestration patterns - Provisioning Scripts - Shell script standards</p> <p>\ud83d\udca1 Key Insight: The monitoring stack is designed as a unified observability platform where all three pillars (metrics, logs, traces) are collected via OpenTelemetry, stored in purpose-built backends (Prometheus, Loki, Tempo), and visualized together in Grafana. This architecture provides complete visibility into application behavior while maintaining operational simplicity through standardized protocols and automation.</p>"},{"location":"package-monitoring-sovdev-logger/","title":"sovdev-logger - Multi-Language OTLP Integration","text":"<p>Key Features: Structured Logging \u2022 OpenTelemetry Integration \u2022 Multi-Language Support \u2022 Context Preservation \u2022 Exception Tracking \u2022 Batched Export \u2022 Console &amp; OTLP Output</p> <p>File: <code>docs/package-monitoring-sovdev-logger.md</code> Purpose: Complete integration guide for sovdev-logger library across TypeScript, Python, C#, PHP, Go, and Rust Target Audience: Application developers, backend engineers, integration teams Last Updated: October 5, 2025</p>"},{"location":"package-monitoring-sovdev-logger/#zero-effort-observability","title":"\ud83c\udfaf Zero-Effort Observability","text":"<p>sovdev-logger transforms every log entry into complete observability by automatically generating logs, metrics, and traces without code changes.</p>"},{"location":"package-monitoring-sovdev-logger/#key-features","title":"Key Features","text":""},{"location":"package-monitoring-sovdev-logger/#1-automatic-prometheus-metrics","title":"1. Automatic Prometheus Metrics","text":"<p>Every <code>sovdevLog()</code> call automatically creates Prometheus metrics: - <code>sovdev_operations_total</code> - Total operations counter - <code>sovdev_errors_total</code> - Error counter (ERROR/FATAL levels) - <code>sovdev_operation_duration_milliseconds</code> - Duration histogram - <code>sovdev_operations_active</code> - Active operations gauge</p> <p>All metrics include dimensional labels: <code>service_name</code>, <code>peer_service</code>, <code>log_level</code>, <code>log_type</code></p>"},{"location":"package-monitoring-sovdev-logger/#2-automatic-distributed-tracing","title":"2. Automatic Distributed Tracing","text":"<p>Every <code>sovdevLog()</code> call automatically creates a trace span: - Span name format: <code>{functionName} [{logType}]</code> - Full attributes: service.name, peer.service, log.level, log.type, function.name - Automatic span events for input/response data - Automatic error status based on exceptions and log levels - Enables service dependency graphs in Tempo/Grafana</p>"},{"location":"package-monitoring-sovdev-logger/#3-session-grouping","title":"3. Session Grouping","text":"<p>Each application execution gets a unique <code>session.id</code>: - Groups all logs, metrics, and traces from single run - Filter specific execution: <code>{service_name=\"my-service\"} | session_id=\"abc123\"</code> - Simplifies debugging and testing by isolating runs - Session ID printed at startup: <code>\ud83d\udd11 Session ID: abc123-def456-ghi789</code></p>"},{"location":"package-monitoring-sovdev-logger/#benefits","title":"Benefits","text":"<ul> <li>Zero Developer Effort: Write logs once, get full observability (logs + metrics + traces)</li> <li>Fast Queries: Prometheus metrics enable sub-second dashboard responses</li> <li>Service Graphs: Automatic dependency visualization from trace relationships</li> <li>Session Filtering: Debug specific runs without time-based filtering</li> <li>Full Correlation: Link logs, metrics, and traces via traceId and session.id</li> </ul>"},{"location":"package-monitoring-sovdev-logger/#grafana-dashboards","title":"Grafana Dashboards","text":"<p>Pre-built dashboards included: - Fast Metrics Dashboard (<code>037-grafana-sovdev-metrics.yaml</code>) - Prometheus-based with sub-second queries - Verification Dashboard (<code>036-grafana-sovdev-verification.yaml</code>) - TraceId correlation and debugging</p>"},{"location":"package-monitoring-sovdev-logger/#overview","title":"\ud83d\udccb Overview","text":"<p>sovdev-logger is a multi-language structured logging library that provides OpenTelemetry Protocol (OTLP) integration for the Urbalurba monitoring stack. It standardizes log output across different programming languages, ensuring consistent structured logging with automatic OTLP export to Loki via the OpenTelemetry Collector.</p> <p>As the application-side logging interface, sovdev-logger enables: - Structured JSON Logging: Consistent log format across all languages - Automatic OTLP Export: Direct integration with OpenTelemetry Collector - Context Preservation: Trace ID, span ID, correlation ID tracking - Dual Output: Console logs (development) + OTLP export (production) - Exception Tracking: Automatic error serialization with stack traces - Batched Performance: Buffered log export for high-throughput applications</p> <p>Key Capabilities: - System Identification: <code>systemId</code> parameter for multi-tenant log routing - Function-Level Tracing: <code>functionName</code> for pinpoint error location - Input/Response Capture: <code>inputJSON</code> and <code>responseJSON</code> for complete context - Log Levels: DEBUG, INFO, WARN, ERROR, FATAL with severity mapping - Auto-Flush: Graceful shutdown ensures no log loss - Environment Configuration: OTLP endpoint via environment variables</p> <p>Architecture Type: Application-side logging library with OTLP exporter</p>"},{"location":"package-monitoring-sovdev-logger/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"package-monitoring-sovdev-logger/#data-flow","title":"Data Flow","text":"<pre><code>Application Code\n         \u2502\n         \u2502 sovdevLog(level, function, message, exception, input, response)\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  sovdev-logger Library           \u2502\n\u2502                                  \u2502\n\u2502  1. Structure log entry (JSON)   \u2502\n\u2502  2. Add context (traceId, etc.)  \u2502\n\u2502  3. Set severity level           \u2502\n\u2502  4. Batch for export             \u2502\n\u2502                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502  Console   \u2502 \u2502 OTLP Exporter\u2502\u2502\n\u2502  \u2502  Output    \u2502 \u2502  (batched)   \u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                    \u2502\n         \u25bc                    \u25bc\n  Developer Console    OTLP Collector\n                            \u2502\n                            \u25bc\n                          Loki\n                            \u2502\n                            \u25bc\n                       Grafana UI\n</code></pre>"},{"location":"package-monitoring-sovdev-logger/#log-entry-structure","title":"Log Entry Structure","text":"<pre><code>{\n  \"timestamp\": \"2025-10-03T10:30:15.234Z\",\n  \"level\": \"ERROR\",\n  \"systemId\": \"my-service-name\",\n  \"functionName\": \"processOrder\",\n  \"message\": \"Order processing failed\",\n  \"traceId\": \"d04584f7-bf95-41e0-9e20-f8063b7658b6\",\n  \"spanId\": \"abc123def456\",\n  \"correlationId\": \"req-789012\",\n  \"exception\": {\n    \"type\": \"ValidationError\",\n    \"message\": \"Invalid order quantity\",\n    \"stack\": \"ValidationError: Invalid order quantity\\n    at processOrder (orders.ts:45:12)...\"\n  },\n  \"inputJSON\": {\n    \"orderId\": \"ORD-12345\",\n    \"quantity\": -5,\n    \"customerId\": \"CUST-67890\"\n  },\n  \"responseJSON\": {\n    \"status\": \"failed\",\n    \"errorCode\": \"INVALID_QUANTITY\",\n    \"validationErrors\": [\"Quantity must be positive\"]\n  }\n}\n</code></pre>"},{"location":"package-monitoring-sovdev-logger/#integration-with-monitoring-stack","title":"Integration with Monitoring Stack","text":"<pre><code>Application (sovdev-logger)\n         \u2502\n         \u2502 HTTP POST http://otel.localhost/v1/logs\n         \u2502 Header: Host: otel.localhost\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Traefik IngressRoute            \u2502\n\u2502  (routes to OTLP Collector)      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  OTLP Collector                  \u2502\n\u2502  - Resource enrichment           \u2502\n\u2502  - Attribute transformation      \u2502\n\u2502  - Batch processing              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Loki (Log Storage)              \u2502\n\u2502  - Indexed by service_name       \u2502\n\u2502  - Queryable via LogQL           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Grafana (Visualization)         \u2502\n\u2502  - Dashboard queries             \u2502\n\u2502  - Explore log streams           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"package-monitoring-sovdev-logger/#language-specific-integration","title":"\ud83d\ude80 Language-Specific Integration","text":""},{"location":"package-monitoring-sovdev-logger/#typescript-javascript","title":"TypeScript / JavaScript","text":"<p>Library Location: <code>terchris/grafana1/sovdev-logger/typescript/</code></p> <p>Installation (from source): <pre><code>cd terchris/grafana1/sovdev-logger/typescript\nnpm install\nnpm run build\n\n# Use in your project\nnpm link  # From sovdev-logger directory\nnpm link sovdev-logger  # From your project directory\n</code></pre></p> <p>Basic Usage: <pre><code>import {\n  initializeSovdevLogger,\n  sovdevLog,\n  flushSovdevLogs,\n  SOVDEV_LOGLEVELS\n} from 'sovdev-logger';\n\n// 1. Initialize once at application startup\ninitializeSovdevLogger('my-service-name');\n\n// 2. Basic logging\nsovdevLog(\n  SOVDEV_LOGLEVELS.INFO,\n  'main',\n  'Application started',\n  null\n);\n\n// 3. Log with input and response context\nconst input = { userId: '12345', action: 'getData' };\nconst response = { status: 'success', data: ['item1', 'item2'] };\n\nsovdevLog(\n  SOVDEV_LOGLEVELS.INFO,\n  'processRequest',\n  'Request processed successfully',\n  null,\n  input,\n  response\n);\n\n// 4. Log errors with exception\ntry {\n  throw new Error('Something went wrong');\n} catch (error) {\n  sovdevLog(\n    SOVDEV_LOGLEVELS.ERROR,\n    'processOrder',\n    'Order processing failed',\n    error,\n    { orderId: '12345' }\n  );\n}\n\n// 5. CRITICAL: Flush before application exit\n// Without flushing, batched logs still in buffer will be lost\nawait flushSovdevLogs();\n</code></pre></p> <p>Environment Configuration: <pre><code># Required\nSYSTEM_ID=my-service-name                                   # Your application identifier\nOTEL_EXPORTER_OTLP_LOGS_ENDPOINT=http://127.0.0.1/v1/logs   # OTLP logs endpoint\nOTEL_EXPORTER_OTLP_METRICS_ENDPOINT=http://127.0.0.1/v1/metrics  # OTLP metrics endpoint\nOTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://127.0.0.1/v1/traces   # OTLP traces endpoint\nOTEL_EXPORTER_OTLP_HEADERS='{\"Host\":\"otel.localhost\"}'     # Host header for Traefik routing\n\n# Optional\nLOG_TO_CONSOLE=true                             # Enable console output (default: true)\nOTEL_LOG_LEVEL=debug                            # OpenTelemetry SDK debug logging\n</code></pre></p> <p>Complete Example (<code>examples/basic/simple-logging.ts</code>): <pre><code>import {\n  initializeSovdevLogger,\n  sovdevLog,\n  flushSovdevLogs,\n  SOVDEV_LOGLEVELS\n} from 'sovdev-logger';\n\nasync function main() {\n  // Initialize with system ID\n  initializeSovdevLogger('basic-example');\n\n  // Log at different levels\n  sovdevLog(SOVDEV_LOGLEVELS.INFO, 'main', 'Application started', null);\n  sovdevLog(SOVDEV_LOGLEVELS.DEBUG, 'main', 'Debug info', null, { debugData: 'context' });\n  sovdevLog(SOVDEV_LOGLEVELS.WARN, 'main', 'Warning', null, { reason: 'demo' });\n\n  // Log with input/response\n  const input = { userId: '12345', action: 'getData' };\n  const response = { status: 'success', data: ['item1', 'item2'] };\n  sovdevLog(SOVDEV_LOGLEVELS.INFO, 'processRequest', 'Request processed', null, input, response);\n\n  // Log error with exception\n  try {\n    throw new Error('Demo error');\n  } catch (error) {\n    sovdevLog(SOVDEV_LOGLEVELS.ERROR, 'main', 'Error occurred', error, { context: 'demo' });\n  }\n\n  sovdevLog(SOVDEV_LOGLEVELS.INFO, 'main', 'Application finished', null);\n\n  // Flush before exit\n  await flushSovdevLogs();\n}\n\nmain().catch(console.error);\n</code></pre></p> <p>Running the Example: <pre><code>cd terchris/grafana1/sovdev-logger/typescript/examples/basic\n\n# Set environment variables\nexport SYSTEM_ID=sovdev-test-typescript\nexport OTEL_EXPORTER_OTLP_LOGS_ENDPOINT=http://127.0.0.1/v1/logs\nexport OTEL_EXPORTER_OTLP_METRICS_ENDPOINT=http://127.0.0.1/v1/metrics\nexport OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://127.0.0.1/v1/traces\nexport OTEL_EXPORTER_OTLP_HEADERS='{\"Host\":\"otel.localhost\"}'\nexport LOG_TO_CONSOLE=true\n\n# Run\nnpx tsx simple-logging.ts\n\n# You'll see console output with session ID:\n# \ud83d\udd11 Session ID: abc123-def456-ghi789\n</code></pre></p> <p>Official TypeScript Documentation: https://opentelemetry.io/docs/languages/js/</p>"},{"location":"package-monitoring-sovdev-logger/#python","title":"Python","text":"<p>Status: Planned (not yet implemented)</p> <p>Planned API: <pre><code>from sovdev_logger import initialize_sovdev_logger, sovdev_log, flush_sovdev_logs, SOVDEV_LOGLEVELS\n\n# Initialize\ninitialize_sovdev_logger('my-service-name')\n\n# Log\nsovdev_log(\n    SOVDEV_LOGLEVELS.INFO,\n    'main',\n    'Application started',\n    None\n)\n\n# Log with context\nsovdev_log(\n    SOVDEV_LOGLEVELS.ERROR,\n    'process_order',\n    'Order failed',\n    exception,\n    input_json={'order_id': '12345'},\n    response_json={'status': 'failed'}\n)\n\n# Flush before exit\nflush_sovdev_logs()\n</code></pre></p> <p>Environment Configuration (same as TypeScript): <pre><code>SYSTEM_ID=my-service-name\nOTEL_EXPORTER_OTLP_LOGS_ENDPOINT=http://127.0.0.1/v1/logs\nOTEL_EXPORTER_OTLP_METRICS_ENDPOINT=http://127.0.0.1/v1/metrics\nOTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://127.0.0.1/v1/traces\nOTEL_EXPORTER_OTLP_HEADERS='{\"Host\":\"otel.localhost\"}'\n</code></pre></p> <p>Official Python Documentation: https://opentelemetry.io/docs/languages/python/</p>"},{"location":"package-monitoring-sovdev-logger/#c-net","title":"C# (.NET)","text":"<p>Status: Planned (not yet implemented)</p> <p>Planned API: <pre><code>using SovdevLogger;\n\n// Initialize\nSovdevLogger.Initialize(\"my-service-name\");\n\n// Log\nSovdevLogger.Log(\n    SovdevLogLevels.INFO,\n    \"Main\",\n    \"Application started\",\n    null\n);\n\n// Log with context\nSovdevLogger.Log(\n    SovdevLogLevels.ERROR,\n    \"ProcessOrder\",\n    \"Order failed\",\n    exception,\n    inputJson: new { orderId = \"12345\" },\n    responseJson: new { status = \"failed\" }\n);\n\n// Flush before exit\nawait SovdevLogger.FlushAsync();\n</code></pre></p> <p>Environment Configuration (same as TypeScript): <pre><code>SYSTEM_ID=my-service-name\nOTEL_EXPORTER_OTLP_LOGS_ENDPOINT=http://127.0.0.1/v1/logs\nOTEL_EXPORTER_OTLP_METRICS_ENDPOINT=http://127.0.0.1/v1/metrics\nOTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://127.0.0.1/v1/traces\nOTEL_EXPORTER_OTLP_HEADERS={\"Host\":\"otel.localhost\"}\n</code></pre></p> <p>Official C# Documentation: https://opentelemetry.io/docs/languages/net/</p>"},{"location":"package-monitoring-sovdev-logger/#php","title":"PHP","text":"<p>Status: Planned (not yet implemented)</p> <p>Planned API: <pre><code>&lt;?php\nuse SovdevLogger\\Logger;\nuse SovdevLogger\\LogLevels;\n\n// Initialize\nLogger::initialize('my-service-name');\n\n// Log\nLogger::log(\n    LogLevels::INFO,\n    'main',\n    'Application started',\n    null\n);\n\n// Log with context\nLogger::log(\n    LogLevels::ERROR,\n    'processOrder',\n    'Order failed',\n    $exception,\n    ['orderId' =&gt; '12345'],\n    ['status' =&gt; 'failed']\n);\n\n// Flush before exit\nLogger::flush();\n?&gt;\n</code></pre></p> <p>Environment Configuration (same as TypeScript): <pre><code>SYSTEM_ID=my-service-name\nOTEL_EXPORTER_OTLP_LOGS_ENDPOINT=http://127.0.0.1/v1/logs\nOTEL_EXPORTER_OTLP_METRICS_ENDPOINT=http://127.0.0.1/v1/metrics\nOTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://127.0.0.1/v1/traces\nOTEL_EXPORTER_OTLP_HEADERS='{\"Host\":\"otel.localhost\"}'\n</code></pre></p> <p>Official PHP Documentation: https://opentelemetry.io/docs/languages/php/</p>"},{"location":"package-monitoring-sovdev-logger/#go","title":"Go","text":"<p>Status: Planned (not yet implemented)</p> <p>Planned API: <pre><code>package main\n\nimport \"github.com/sovdev/sovdev-logger-go\"\n\nfunc main() {\n    // Initialize\n    sovdevlogger.Initialize(\"my-service-name\")\n\n    // Log\n    sovdevlogger.Log(\n        sovdevlogger.INFO,\n        \"main\",\n        \"Application started\",\n        nil,\n        nil,\n        nil,\n    )\n\n    // Log with context\n    sovdevlogger.Log(\n        sovdevlogger.ERROR,\n        \"processOrder\",\n        \"Order failed\",\n        err,\n        map[string]interface{}{\"orderId\": \"12345\"},\n        map[string]interface{}{\"status\": \"failed\"},\n    )\n\n    // Flush before exit\n    sovdevlogger.Flush()\n}\n</code></pre></p> <p>Environment Configuration (same as TypeScript): <pre><code>SYSTEM_ID=my-service-name\nOTEL_EXPORTER_OTLP_LOGS_ENDPOINT=http://127.0.0.1/v1/logs\nOTEL_EXPORTER_OTLP_METRICS_ENDPOINT=http://127.0.0.1/v1/metrics\nOTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://127.0.0.1/v1/traces\nOTEL_EXPORTER_OTLP_HEADERS='{\"Host\":\"otel.localhost\"}'\n</code></pre></p> <p>Official Go Documentation: https://opentelemetry.io/docs/languages/go/</p>"},{"location":"package-monitoring-sovdev-logger/#rust","title":"Rust","text":"<p>Status: Planned (not yet implemented)</p> <p>Planned API: <pre><code>use sovdev_logger::{initialize, log, flush, LogLevel};\nuse std::collections::HashMap;\n\nfn main() {\n    // Initialize\n    initialize(\"my-service-name\");\n\n    // Log\n    log(\n        LogLevel::Info,\n        \"main\",\n        \"Application started\",\n        None,\n        None,\n        None,\n    );\n\n    // Log with context\n    let mut input = HashMap::new();\n    input.insert(\"orderId\", \"12345\");\n\n    let mut response = HashMap::new();\n    response.insert(\"status\", \"failed\");\n\n    log(\n        LogLevel::Error,\n        \"process_order\",\n        \"Order failed\",\n        Some(&amp;error),\n        Some(&amp;input),\n        Some(&amp;response),\n    );\n\n    // Flush before exit\n    flush();\n}\n</code></pre></p> <p>Environment Configuration (same as TypeScript): <pre><code>SYSTEM_ID=my-service-name\nOTEL_EXPORTER_OTLP_LOGS_ENDPOINT=http://127.0.0.1/v1/logs\nOTEL_EXPORTER_OTLP_METRICS_ENDPOINT=http://127.0.0.1/v1/metrics\nOTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://127.0.0.1/v1/traces\nOTEL_EXPORTER_OTLP_HEADERS='{\"Host\":\"otel.localhost\"}'\n</code></pre></p> <p>Official Rust Documentation: https://opentelemetry.io/docs/languages/rust/</p>"},{"location":"package-monitoring-sovdev-logger/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"package-monitoring-sovdev-logger/#environment-variables","title":"Environment Variables","text":"<p>Required: <pre><code>SYSTEM_ID=&lt;your-service-name&gt;                   # Identifies your application in logs\nOTEL_EXPORTER_OTLP_LOGS_ENDPOINT=&lt;otlp-url&gt;     # OTLP logs endpoint\nOTEL_EXPORTER_OTLP_METRICS_ENDPOINT=&lt;otlp-url&gt;  # OTLP metrics endpoint\nOTEL_EXPORTER_OTLP_TRACES_ENDPOINT=&lt;otlp-url&gt;   # OTLP traces endpoint\nOTEL_EXPORTER_OTLP_HEADERS='{\"Host\":\"&lt;host&gt;\"}'  # Traefik routing header\n</code></pre></p> <p>Optional: <pre><code>LOG_TO_CONSOLE=true                             # Enable console output (default: true)\nOTEL_LOG_LEVEL=debug                            # OpenTelemetry SDK debug logging\n</code></pre></p>"},{"location":"package-monitoring-sovdev-logger/#local-development-configuration","title":"Local Development Configuration","text":"<p>For Mac host (outside cluster): <pre><code>export SYSTEM_ID=my-service-dev\nexport OTEL_EXPORTER_OTLP_LOGS_ENDPOINT=http://127.0.0.1/v1/logs\nexport OTEL_EXPORTER_OTLP_METRICS_ENDPOINT=http://127.0.0.1/v1/metrics\nexport OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://127.0.0.1/v1/traces\nexport OTEL_EXPORTER_OTLP_HEADERS='{\"Host\":\"otel.localhost\"}'\nexport LOG_TO_CONSOLE=true\n</code></pre></p> <p>For Kubernetes pod (inside cluster): <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-app\nspec:\n  containers:\n    - name: app\n      image: my-app:latest\n      env:\n        - name: SYSTEM_ID\n          value: \"my-service-prod\"\n        - name: OTEL_EXPORTER_OTLP_LOGS_ENDPOINT\n          value: \"http://otel-collector-opentelemetry-collector.monitoring.svc.cluster.local:4318/v1/logs\"\n        - name: OTEL_EXPORTER_OTLP_METRICS_ENDPOINT\n          value: \"http://otel-collector-opentelemetry-collector.monitoring.svc.cluster.local:4318/v1/metrics\"\n        - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\n          value: \"http://otel-collector-opentelemetry-collector.monitoring.svc.cluster.local:4318/v1/traces\"\n        - name: LOG_TO_CONSOLE\n          value: \"false\"\n</code></pre></p>"},{"location":"package-monitoring-sovdev-logger/#production-configuration","title":"Production Configuration","text":"<p>Docker Compose: <pre><code>services:\n  my-app:\n    image: my-app:latest\n    environment:\n      SYSTEM_ID: my-service-prod\n      OTEL_EXPORTER_OTLP_LOGS_ENDPOINT: http://otel-collector:4318/v1/logs\n      OTEL_EXPORTER_OTLP_METRICS_ENDPOINT: http://otel-collector:4318/v1/metrics\n      OTEL_EXPORTER_OTLP_TRACES_ENDPOINT: http://otel-collector:4318/v1/traces\n      LOG_TO_CONSOLE: \"false\"\n</code></pre></p>"},{"location":"package-monitoring-sovdev-logger/#log-levels","title":"Log Levels","text":"Level Severity Use Case DEBUG Detailed Development debugging, verbose tracing INFO Informational Normal operations, business events WARN Warning Recoverable issues, potential problems ERROR Error Application errors, failed operations FATAL Fatal Critical failures requiring immediate attention"},{"location":"package-monitoring-sovdev-logger/#verification-debugging","title":"\ud83d\udd0d Verification &amp; Debugging","text":""},{"location":"package-monitoring-sovdev-logger/#test-otlp-endpoint-connectivity","title":"Test OTLP Endpoint Connectivity","text":"<pre><code># From Mac host\ncurl -X POST http://127.0.0.1/v1/logs \\\n  -H \"Host: otel.localhost\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"resourceLogs\":[{\"resource\":{\"attributes\":[{\"key\":\"service.name\",\"value\":{\"stringValue\":\"test\"}}]},\"scopeLogs\":[{\"logRecords\":[{\"body\":{\"stringValue\":\"test log\"}}]}]}]}'\n\n# Expected: No error (200 or 204 response)\n</code></pre>"},{"location":"package-monitoring-sovdev-logger/#query-logs-in-loki","title":"Query Logs in Loki","text":"<pre><code># Check if logs from your service are indexed\nkubectl exec -n monitoring loki-0 -c loki -- \\\n  wget -q -O - 'http://localhost:3100/loki/api/v1/label/service_name/values'\n\n# Should include your systemId (e.g., \"my-service-name\")\n</code></pre>"},{"location":"package-monitoring-sovdev-logger/#view-logs-in-grafana","title":"View Logs in Grafana","text":"<ol> <li>Open <code>http://grafana.localhost</code></li> <li>Login: <code>admin</code> / <code>SecretPassword1</code></li> <li>Navigate to Explore \u2192 Select Loki datasource</li> <li>Query:    <pre><code>{service_name=\"my-service-name\"}\n</code></pre></li> <li>Verify logs appear with structured fields</li> </ol>"},{"location":"package-monitoring-sovdev-logger/#troubleshooting-no-logs-appearing","title":"Troubleshooting No Logs Appearing","text":"<p>1. Check application console output: <pre><code># If LOG_TO_CONSOLE=true, you should see JSON logs in stdout\n# Example output:\n{\n  \"timestamp\": \"2025-10-03T10:30:15.234Z\",\n  \"level\": \"INFO\",\n  \"systemId\": \"my-service-name\",\n  \"functionName\": \"main\",\n  \"message\": \"Application started\"\n}\n</code></pre></p> <p>2. Verify OTLP Collector is receiving logs: <pre><code># Check OTLP Collector logs for your service\nkubectl logs -n monitoring -l app.kubernetes.io/name=opentelemetry-collector | grep \"my-service-name\"\n</code></pre></p> <p>3. Check Loki ingestion: <pre><code># Query Loki for recent logs\nkubectl exec -n monitoring loki-0 -c loki -- \\\n  wget -q -O - 'http://localhost:3100/loki/api/v1/query_range?query={service_name=\"my-service-name\"}&amp;limit=10'\n</code></pre></p> <p>4. Common issues: - No logs in OTLP Collector: Check <code>OTEL_EXPORTER_OTLP_LOGS_ENDPOINT</code> and <code>OTEL_EXPORTER_OTLP_HEADERS</code> - Logs not flushed: Ensure <code>flushSovdevLogs()</code> is called before application exit - Wrong label in Loki: Query by <code>service_name</code> (not <code>systemId</code> - OTLP maps <code>systemId</code> \u2192 <code>service.name</code> \u2192 <code>service_name</code>) - Traefik routing failure: Verify <code>Host: otel.localhost</code> header is set correctly</p>"},{"location":"package-monitoring-sovdev-logger/#best-practices","title":"\ud83d\udee0\ufe0f Best Practices","text":""},{"location":"package-monitoring-sovdev-logger/#1-always-initialize-once","title":"1. Always Initialize Once","text":"<pre><code>// \u2705 CORRECT: Initialize at application startup\ninitializeSovdevLogger('my-service');\n\n// \u274c WRONG: Do not initialize inside functions\nfunction someFunction() {\n  initializeSovdevLogger('my-service');  // Bad: reinitializes logger\n}\n</code></pre>"},{"location":"package-monitoring-sovdev-logger/#2-always-flush-before-exit","title":"2. Always Flush Before Exit","text":"<pre><code>// \u2705 CORRECT: Flush ensures batched logs are sent\nasync function main() {\n  initializeSovdevLogger('my-service');\n  sovdevLog(SOVDEV_LOGLEVELS.INFO, 'main', 'Done', null);\n  await flushSovdevLogs();  // Critical: ensures logs reach OTLP Collector\n}\n\n// \u274c WRONG: Without flush, last logs may be lost\nasync function main() {\n  initializeSovdevLogger('my-service');\n  sovdevLog(SOVDEV_LOGLEVELS.INFO, 'main', 'Done', null);\n  // Missing flush: logs still in buffer will be lost\n}\n</code></pre>"},{"location":"package-monitoring-sovdev-logger/#3-use-meaningful-function-names","title":"3. Use Meaningful Function Names","text":"<pre><code>// \u2705 CORRECT: Function names help locate errors\nsovdevLog(SOVDEV_LOGLEVELS.ERROR, 'processOrder', 'Validation failed', error);\n// Grafana query: {service_name=\"my-service\"} | json | functionName=\"processOrder\"\n\n// \u274c WRONG: Generic names make debugging harder\nsovdevLog(SOVDEV_LOGLEVELS.ERROR, 'main', 'Error', error);\n</code></pre>"},{"location":"package-monitoring-sovdev-logger/#4-include-context-with-inputresponse","title":"4. Include Context with Input/Response","text":"<pre><code>// \u2705 CORRECT: Full context for debugging\nsovdevLog(\n  SOVDEV_LOGLEVELS.ERROR,\n  'processPayment',\n  'Payment failed',\n  error,\n  { orderId: '12345', amount: 99.99, currency: 'USD' },  // input\n  { status: 'failed', errorCode: 'INSUFFICIENT_FUNDS' }  // response\n);\n\n// \u274c WRONG: No context makes troubleshooting difficult\nsovdevLog(SOVDEV_LOGLEVELS.ERROR, 'processPayment', 'Payment failed', error);\n</code></pre>"},{"location":"package-monitoring-sovdev-logger/#5-use-appropriate-log-levels","title":"5. Use Appropriate Log Levels","text":"<pre><code>// \u2705 CORRECT: Level matches severity\nsovdevLog(SOVDEV_LOGLEVELS.INFO, 'processOrder', 'Order created', null);\nsovdevLog(SOVDEV_LOGLEVELS.WARN, 'processOrder', 'Slow response detected', null);\nsovdevLog(SOVDEV_LOGLEVELS.ERROR, 'processOrder', 'Order failed', error);\n\n// \u274c WRONG: Using INFO for errors hides critical issues\nsovdevLog(SOVDEV_LOGLEVELS.INFO, 'processOrder', 'Order failed', error);\n</code></pre>"},{"location":"package-monitoring-sovdev-logger/#6-avoid-logging-sensitive-data","title":"6. Avoid Logging Sensitive Data","text":"<pre><code>// \u2705 CORRECT: Redact sensitive fields\nsovdevLog(\n  SOVDEV_LOGLEVELS.INFO,\n  'login',\n  'User login',\n  null,\n  { username: user.email, password: '[REDACTED]' }\n);\n\n// \u274c WRONG: Logging passwords, tokens, API keys\nsovdevLog(\n  SOVDEV_LOGLEVELS.INFO,\n  'login',\n  'User login',\n  null,\n  { username: user.email, password: user.password }  // Security risk\n);\n</code></pre>"},{"location":"package-monitoring-sovdev-logger/#use-cases","title":"\ud83d\ude80 Use Cases","text":""},{"location":"package-monitoring-sovdev-logger/#1-api-request-logging","title":"1. API Request Logging","text":"<pre><code>async function handleRequest(req, res) {\n  const input = { method: req.method, url: req.url, userId: req.user?.id };\n\n  try {\n    const result = await processRequest(req);\n    sovdevLog(\n      SOVDEV_LOGLEVELS.INFO,\n      'handleRequest',\n      'Request processed',\n      null,\n      input,\n      { status: 200, data: result }\n    );\n    res.json(result);\n  } catch (error) {\n    sovdevLog(\n      SOVDEV_LOGLEVELS.ERROR,\n      'handleRequest',\n      'Request failed',\n      error,\n      input,\n      { status: 500, error: error.message }\n    );\n    res.status(500).json({ error: 'Internal server error' });\n  }\n}\n</code></pre>"},{"location":"package-monitoring-sovdev-logger/#2-background-job-logging","title":"2. Background Job Logging","text":"<pre><code>async function processJob(jobId) {\n  sovdevLog(SOVDEV_LOGLEVELS.INFO, 'processJob', `Job ${jobId} started`, null);\n\n  try {\n    const input = await fetchJobData(jobId);\n    const result = await executeJob(input);\n\n    sovdevLog(\n      SOVDEV_LOGLEVELS.INFO,\n      'processJob',\n      `Job ${jobId} completed`,\n      null,\n      { jobId, input },\n      { status: 'success', result }\n    );\n  } catch (error) {\n    sovdevLog(\n      SOVDEV_LOGLEVELS.ERROR,\n      'processJob',\n      `Job ${jobId} failed`,\n      error,\n      { jobId }\n    );\n  } finally {\n    await flushSovdevLogs();  // Flush at job completion\n  }\n}\n</code></pre>"},{"location":"package-monitoring-sovdev-logger/#3-database-operation-logging","title":"3. Database Operation Logging","text":"<pre><code>async function queryDatabase(query, params) {\n  const startTime = Date.now();\n\n  try {\n    const result = await db.query(query, params);\n    const duration = Date.now() - startTime;\n\n    sovdevLog(\n      SOVDEV_LOGLEVELS.INFO,\n      'queryDatabase',\n      'Query executed',\n      null,\n      { query, params, duration },\n      { rowCount: result.rows.length }\n    );\n\n    return result;\n  } catch (error) {\n    sovdevLog(\n      SOVDEV_LOGLEVELS.ERROR,\n      'queryDatabase',\n      'Query failed',\n      error,\n      { query, params }\n    );\n    throw error;\n  }\n}\n</code></pre> <p>\ud83d\udca1 Key Insight: sovdev-logger bridges the gap between application code and the observability stack by providing a simple, consistent logging interface that automatically exports structured logs to OpenTelemetry. By standardizing log format across multiple languages, it enables cross-service correlation and unified querying in Grafana, regardless of the programming language used to build each service.</p>"},{"location":"package-monitoring-sovdev-logger/#related-documentation","title":"\ud83d\udd17 Related Documentation","text":"<p>Monitoring Stack: - Monitoring Overview - Complete observability stack - OTLP Collector - Telemetry ingestion gateway - Loki Logs - Log storage backend - Grafana Visualization - Query and visualization</p> <p>Configuration &amp; Rules: - Traefik IngressRoute - External access patterns - Development Workflow - Application integration</p> <p>External Resources: - OpenTelemetry Protocol Specification: https://opentelemetry.io/docs/specs/otlp/ - OpenTelemetry Logs API: https://opentelemetry.io/docs/specs/otel/logs/ - Grafana LogQL: https://grafana.com/docs/loki/latest/query/</p>"},{"location":"package-monitoring-tempo/","title":"Tempo - Distributed Tracing Backend","text":"<p>Key Features: Distributed Tracing \u2022 OTLP Protocol \u2022 gRPC &amp; HTTP Endpoints \u2022 TraceQL Queries \u2022 Cost-Effective Storage \u2022 Jaeger/Zipkin Compatible \u2022 Multi-Tenancy</p> <p>File: <code>docs/package-monitoring-tempo.md</code> Purpose: Complete guide to Tempo deployment and configuration for distributed tracing in Urbalurba infrastructure Target Audience: DevOps engineers, platform administrators, SREs, developers Last Updated: October 3, 2025</p> <p>Deployed Version: Tempo v2.8.2 (Helm Chart: tempo-1.23.3) Official Documentation: https://grafana.com/docs/tempo/v2.8.x/</p>"},{"location":"package-monitoring-tempo/#overview","title":"\ud83d\udccb Overview","text":"<p>Tempo is a high-performance distributed tracing backend designed for cloud-native applications. It provides cost-effective trace storage and powerful querying capabilities through TraceQL. Unlike traditional tracing backends, Tempo only indexes a small set of metadata, dramatically reducing storage and operational costs.</p> <p>As part of the unified observability stack, Tempo works alongside Prometheus (metrics) and Loki (logs), with all data visualized in Grafana. Applications instrumented with OpenTelemetry send traces to the OTLP Collector, which forwards them to Tempo for storage and querying.</p> <p>Key Capabilities: - OTLP Native: Primary ingestion via OpenTelemetry Collector (gRPC 4317, HTTP 4318) - TraceQL: SQL-like query language for powerful trace filtering and analysis - Low Storage Cost: Indexes only metadata, stores traces in efficient object storage format - Multi-Protocol: Supports OTLP, Jaeger, and Zipkin protocols - Scalable Architecture: Designed for high-volume trace ingestion - Grafana Integration: Native datasource for trace visualization and exploration</p> <p>Architecture Type: Append-only trace storage with metadata indexing</p>"},{"location":"package-monitoring-tempo/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"package-monitoring-tempo/#deployment-components","title":"Deployment Components","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           Tempo Stack (namespace: monitoring)        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502             Tempo Server                   \u2502    \u2502\n\u2502  \u2502                                            \u2502    \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502\n\u2502  \u2502  \u2502  OTLP Receivers                      \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - gRPC: 4317                        \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - HTTP: 4318                        \u2502 \u2502    \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502\n\u2502  \u2502                                            \u2502    \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502\n\u2502  \u2502  \u2502  Trace Storage                       \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - Metadata Index                    \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - Trace Blocks (10Gi PVC)           \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - 24h Retention                     \u2502 \u2502    \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502\n\u2502  \u2502                                            \u2502    \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502\n\u2502  \u2502  \u2502  Query APIs                          \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - HTTP API: 3200                    \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - TraceQL Engine                    \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502  - Search API                        \u2502 \u2502    \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u25b2                          \u2502\n         \u2502                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 OTLP Collector   \u2502    \u2502   Grafana Query      \u2502\n\u2502 (Trace Export)   \u2502    \u2502   (TraceQL/HTTP)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"package-monitoring-tempo/#data-flow","title":"Data Flow","text":"<pre><code>Application (OTLP instrumented)\n         \u2502\n         \u2502 OTLP/HTTP or OTLP/gRPC\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  OTLP Collector      \u2502\n\u2502  (Trace Receiver)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u2502 OTLP Export\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Tempo Backend      \u2502\n\u2502   (4317/4318)        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 - Receive traces     \u2502\n\u2502 - Extract metadata   \u2502\n\u2502 - Store trace blocks \u2502\n\u2502 - Index for search   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u251c\u2500\u25ba Persistent Storage (10Gi)\n         \u2514\u2500\u25ba Query API (3200)\n                  \u2502\n                  \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502  Grafana Explore \u2502\n         \u2502  (TraceQL Query) \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"package-monitoring-tempo/#file-structure","title":"File Structure","text":"<pre><code>manifests/\n\u2514\u2500\u2500 031-tempo-config.yaml                   # Tempo Helm values\n\nansible/playbooks/\n\u251c\u2500\u2500 031-setup-tempo.yml                     # Deployment automation\n\u2514\u2500\u2500 031-remove-tempo.yml                    # Removal automation\n\nprovision-host/kubernetes/11-monitoring/not-in-use/\n\u251c\u2500\u2500 02-setup-tempo.sh                       # Shell script wrapper\n\u2514\u2500\u2500 02-remove-tempo.sh                      # Removal script\n\nStorage:\n\u2514\u2500\u2500 PersistentVolumeClaim\n    \u2514\u2500\u2500 tempo (10Gi)                        # Trace blocks storage\n</code></pre>"},{"location":"package-monitoring-tempo/#deployment","title":"\ud83d\ude80 Deployment","text":""},{"location":"package-monitoring-tempo/#automated-deployment","title":"Automated Deployment","text":"<p>Via Monitoring Stack (Recommended): <pre><code># Deploy entire monitoring stack (includes Tempo)\ndocker exec -it provision-host bash\ncd /mnt/urbalurbadisk/provision-host/kubernetes/11-monitoring/not-in-use\n./00-setup-all-monitoring.sh rancher-desktop\n</code></pre></p> <p>Individual Deployment: <pre><code># Deploy Tempo only\ndocker exec -it provision-host bash\ncd /mnt/urbalurbadisk/provision-host/kubernetes/11-monitoring/not-in-use\n./02-setup-tempo.sh rancher-desktop\n</code></pre></p>"},{"location":"package-monitoring-tempo/#manual-deployment","title":"Manual Deployment","text":"<p>Prerequisites: - Kubernetes cluster running (Rancher Desktop) - <code>monitoring</code> namespace exists - Helm installed in provision-host container - Manifest file: <code>manifests/031-tempo-config.yaml</code></p> <p>Deployment Steps: <pre><code># 1. Enter provision-host container\ndocker exec -it provision-host bash\n\n# 2. Add Grafana Helm repository\nhelm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n\n# 3. Deploy Tempo\nhelm upgrade --install tempo grafana/tempo \\\n  -f /mnt/urbalurbadisk/manifests/031-tempo-config.yaml \\\n  --namespace monitoring \\\n  --create-namespace \\\n  --timeout 600s \\\n  --kube-context rancher-desktop\n\n# 4. Wait for pods to be ready\nkubectl wait --for=condition=ready pod \\\n  -l app.kubernetes.io/name=tempo \\\n  -n monitoring --timeout=300s\n</code></pre></p> <p>Deployment Time: ~2-3 minutes</p>"},{"location":"package-monitoring-tempo/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"package-monitoring-tempo/#tempo-configuration-manifests031-tempo-configyaml","title":"Tempo Configuration (<code>manifests/031-tempo-config.yaml</code>)","text":"<p>Core Settings: <pre><code>tempo:\n  receivers:\n    otlp:\n      protocols:\n        grpc:\n          endpoint: 0.0.0.0:4317    # OTLP gRPC receiver\n        http:\n          endpoint: 0.0.0.0:4318    # OTLP HTTP receiver\n\n  retention: 24h                     # Trace retention period\n\npersistence:\n  enabled: true\n  size: 10Gi                         # Trace block storage\n\nservice:\n  type: ClusterIP                    # Internal cluster access only\n</code></pre></p> <p>Key Configuration Sections:</p> <p>1. OTLP Receivers (Primary Ingestion): <pre><code>tempo:\n  receivers:\n    otlp:\n      protocols:\n        # Recommended for production (lower overhead)\n        grpc:\n          endpoint: 0.0.0.0:4317\n        # Alternative for HTTP-only environments\n        http:\n          endpoint: 0.0.0.0:4318\n</code></pre></p> <p>2. Retention Policy: <pre><code>tempo:\n  retention: 24h                     # Traces older than 24h are deleted\n</code></pre></p> <p>3. Storage Backend: <pre><code>persistence:\n  enabled: true\n  size: 10Gi                         # Adjust based on trace volume\n</code></pre></p> <p>4. Metrics Generator (Automatic Service Graphs): <pre><code>tempo:\n  metricsGenerator:\n    enabled: true\n    remoteWriteUrl: \"http://prometheus-server.monitoring.svc.cluster.local:80/api/v1/write\"\n\n  structuredConfig:\n    metrics_generator:\n      registry:\n        external_labels:\n          source: tempo\n      storage:\n        path: /var/tempo/generator/wal\n        remote_write:\n          - url: http://prometheus-server.monitoring.svc.cluster.local:80/api/v1/write\n            send_exemplars: true\n      traces_storage:\n        path: /var/tempo/generator/traces\n      processor:\n        service_graphs:\n          dimensions:\n            - service.name\n            - peer.service\n          histogram_buckets: [0.1, 0.2, 0.5, 1, 2, 5, 10]\n        span_metrics:\n          dimensions:\n            - service.name\n            - peer.service\n            - log.type\n    overrides:\n      defaults:\n        metrics_generator:\n          processors: [service-graphs, span-metrics]\n</code></pre></p> <p>Key Features: - Service Graphs: Automatically generate service dependency metrics from traces - Span Metrics: Create Prometheus metrics for trace calls, latency, and errors - Remote Write: Send generated metrics to Prometheus for visualization - Dimensions: Track service.name, peer.service, and log.type for detailed filtering</p> <p>Generated Prometheus Metrics: - <code>traces_spanmetrics_calls_total</code> - Total calls between services - <code>traces_spanmetrics_latency_bucket</code> - Latency histogram distribution - <code>traces_spanmetrics_size_total</code> - Span size tracking</p> <p>Example Prometheus Queries: <pre><code># Service dependency graph\ntraces_spanmetrics_calls_total{service_name=\"my-service\"}\n\n# Average latency between services\nrate(traces_spanmetrics_latency_sum[1m]) / rate(traces_spanmetrics_latency_count[1m])\n\n# Error rate by service\nrate(traces_spanmetrics_calls_total{status_code=\"STATUS_CODE_ERROR\"}[1m])\n</code></pre></p>"},{"location":"package-monitoring-tempo/#resource-configuration","title":"Resource Configuration","text":"<p>Storage Requirements: - Tempo PVC: 10Gi persistent volume (24-hour retention) - Estimated Usage: ~400-500MB per million spans (varies by trace size)</p> <p>Service Endpoints: - OTLP gRPC: <code>tempo.monitoring.svc.cluster.local:4317</code> - OTLP HTTP: <code>tempo.monitoring.svc.cluster.local:4318</code> - HTTP API: <code>tempo.monitoring.svc.cluster.local:3200</code> - Ready Check: <code>tempo.monitoring.svc.cluster.local:3200/ready</code></p>"},{"location":"package-monitoring-tempo/#security-configuration","title":"Security Configuration","text":"<p>Network Access: <pre><code>service:\n  type: ClusterIP                    # Internal cluster access only\n</code></pre></p> <p>No External Access: Tempo is internal-only. Traces are sent via OTLP Collector, and queries are performed through Grafana.</p>"},{"location":"package-monitoring-tempo/#monitoring-verification","title":"\ud83d\udd0d Monitoring &amp; Verification","text":""},{"location":"package-monitoring-tempo/#health-checks","title":"Health Checks","text":"<p>Check Pod Status: <pre><code># Tempo pods\nkubectl get pods -n monitoring -l app.kubernetes.io/name=tempo\n\n# Expected output:\nNAME        READY   STATUS    RESTARTS   AGE\ntempo-0     1/1     Running   0          5m\n</code></pre></p> <p>Check Service Endpoints: <pre><code># Verify services are accessible\nkubectl get svc -n monitoring -l app.kubernetes.io/name=tempo\n\n# Expected services:\ntempo        ClusterIP   10.43.x.x    3200/TCP,4317/TCP,4318/TCP\n</code></pre></p>"},{"location":"package-monitoring-tempo/#service-verification","title":"Service Verification","text":"<p>Test HTTP API: <pre><code># Test API echo endpoint\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -s http://tempo.monitoring.svc.cluster.local:3200/api/echo\n\n# Expected: HTTP 200 response\n</code></pre></p> <p>Test Ready Endpoint: <pre><code># Check if Tempo is ready to receive traces\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -s http://tempo.monitoring.svc.cluster.local:3200/ready\n\n# Expected: HTTP 200 response\n</code></pre></p> <p>Test OTLP Endpoints: <pre><code># Test gRPC port accessibility\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -v telnet://tempo.monitoring.svc.cluster.local:4317\n\n# Test HTTP port accessibility\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -s -o /dev/null -w \"%{http_code}\" \\\n  http://tempo.monitoring.svc.cluster.local:4318/\n</code></pre></p>"},{"location":"package-monitoring-tempo/#search-api-testing","title":"Search API Testing","text":"<p>Query Traces: <pre><code># Test search API (returns trace metadata)\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -s \"http://tempo.monitoring.svc.cluster.local:3200/api/search\"\n\n# Expected: JSON response with traces/metrics\n</code></pre></p>"},{"location":"package-monitoring-tempo/#automated-verification","title":"Automated Verification","text":"<p>The deployment playbook (<code>031-setup-tempo.yml</code>) performs automated tests: 1. \u2705 HTTP API endpoint connectivity 2. \u2705 Ready endpoint verification 3. \u2705 Metrics endpoint check 4. \u2705 Search API validation 5. \u2705 OTLP gRPC port accessibility 6. \u2705 OTLP HTTP port accessibility</p>"},{"location":"package-monitoring-tempo/#management-operations","title":"\ud83d\udee0\ufe0f Management Operations","text":""},{"location":"package-monitoring-tempo/#query-traces-in-grafana","title":"Query Traces in Grafana","text":"<p>Access Grafana: <pre><code># Open Grafana UI\nhttp://grafana.localhost\n</code></pre></p> <p>Explore Traces: 1. Navigate to Explore \u2192 Select Tempo datasource 2. Choose query type:    - Search: Find traces by service/operation    - TraceQL: Use SQL-like queries    - Trace ID: Lookup specific trace</p> <p>TraceQL Examples: <pre><code># Find traces with errors\n{ status = error }\n\n# Find slow traces (&gt;1s duration)\n{ duration &gt; 1s }\n\n# Find traces by service name\n{ resource.service.name = \"sovdev-test-company-lookup-typescript\" }\n\n# Complex query: slow traces with errors from specific service\n{ resource.service.name =~ \"sovdev.*\" &amp;&amp; status = error &amp;&amp; duration &gt; 1s }\n</code></pre></p> <p>Official TraceQL Documentation: https://grafana.com/docs/tempo/v2.8.x/traceql/</p>"},{"location":"package-monitoring-tempo/#http-api-queries","title":"HTTP API Queries","text":"<p>Search for Traces: <pre><code># Search by service name\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -s \"http://tempo.monitoring.svc.cluster.local:3200/api/search?q=service.name%3D%22my-service%22\"\n</code></pre></p> <p>Retrieve Trace by ID: <pre><code># Get specific trace (replace TRACE_ID)\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -s \"http://tempo.monitoring.svc.cluster.local:3200/api/traces/TRACE_ID\"\n</code></pre></p>"},{"location":"package-monitoring-tempo/#metrics-monitoring","title":"Metrics Monitoring","text":"<p>Tempo Self-Monitoring: <pre><code># Get Tempo internal metrics\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -s http://tempo.monitoring.svc.cluster.local:3200/metrics | grep tempo\n</code></pre></p> <p>Key Metrics (via Prometheus): <pre><code># Ingested spans per second\nrate(tempo_ingester_spans_ingested_total[5m])\n\n# Trace queries per second\nrate(tempo_query_frontend_queries_total[5m])\n\n# Storage bytes used\ntempo_ingester_bytes_total\n</code></pre></p>"},{"location":"package-monitoring-tempo/#service-removal","title":"Service Removal","text":"<p>Automated Removal: <pre><code>docker exec -it provision-host bash\ncd /mnt/urbalurbadisk/provision-host/kubernetes/11-monitoring/not-in-use\n./02-remove-tempo.sh rancher-desktop\n</code></pre></p> <p>Manual Removal: <pre><code># Remove Helm chart\nhelm uninstall tempo -n monitoring --kube-context rancher-desktop\n\n# Remove PVC (optional - preserves data if omitted)\nkubectl delete pvc -n monitoring -l app.kubernetes.io/name=tempo\n</code></pre></p>"},{"location":"package-monitoring-tempo/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-monitoring-tempo/#common-issues","title":"Common Issues","text":"<p>Pods Not Starting: <pre><code># Check pod events\nkubectl describe pod -n monitoring -l app.kubernetes.io/name=tempo\n\n# Common causes:\n# - PVC binding issues (check PV availability)\n# - Insufficient resources (check node capacity)\n# - Image pull errors (check network)\n</code></pre></p> <p>No Traces Appearing: <pre><code># 1. Check OTLP Collector is sending traces to Tempo\nkubectl logs -n monitoring -l app.kubernetes.io/name=opentelemetry-collector | grep tempo\n\n# 2. Check Tempo ingestion logs\nkubectl logs -n monitoring -l app.kubernetes.io/name=tempo | grep -i \"trace\\|span\"\n\n# 3. Verify OTLP Collector configuration\nkubectl get configmap -n monitoring otel-collector-opentelemetry-collector -o yaml | grep -A 10 \"tempo\"\n\n# Expected: Tempo endpoint at tempo.monitoring.svc.cluster.local:4317\n</code></pre></p> <p>Trace Query Failures: <pre><code># Check Tempo query logs\nkubectl logs -n monitoring -l app.kubernetes.io/name=tempo | grep -i error\n\n# Test search API directly\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -v \"http://tempo.monitoring.svc.cluster.local:3200/api/search\"\n</code></pre></p> <p>Storage Full: <pre><code># Check PVC usage\nkubectl get pvc -n monitoring\n\n# Check trace block size via metrics\nkubectl port-forward -n monitoring svc/tempo 3200:3200\ncurl -s http://localhost:3200/metrics | grep tempo_ingester_bytes\n\n# Solutions:\n# 1. Reduce retention period in manifests/031-tempo-config.yaml\n# 2. Increase PVC size\n# 3. Reduce trace sampling rate at application level\n</code></pre></p> <p>OTLP Ingestion Errors: <pre><code># Check if Tempo is accepting OTLP traces\nkubectl logs -n monitoring -l app.kubernetes.io/name=tempo | grep -i \"otlp\\|grpc\\|http\"\n\n# Test OTLP HTTP endpoint\nkubectl run curl-test --image=curlimages/curl --rm -i --restart=Never \\\n  -n monitoring -- \\\n  curl -X POST -v http://tempo.monitoring.svc.cluster.local:4318/v1/traces \\\n  -H \"Content-Type: application/json\" \\\n  -d '{}'\n\n# Expected: 400 or 405 (endpoint is reachable, empty payload rejected)\n</code></pre></p>"},{"location":"package-monitoring-tempo/#maintenance","title":"\ud83d\udccb Maintenance","text":""},{"location":"package-monitoring-tempo/#regular-tasks","title":"Regular Tasks","text":"<p>Monitor Storage Usage: <pre><code># Check PVC status\nkubectl get pvc -n monitoring -l app.kubernetes.io/name=tempo\n\n# Check storage metrics\nkubectl port-forward -n monitoring svc/tempo 3200:3200\ncurl -s http://localhost:3200/metrics | grep tempo_ingester_blocks_total\n</code></pre></p> <p>Update Tempo: <pre><code># Update Helm chart to latest version\nhelm repo update\nhelm upgrade tempo grafana/tempo \\\n  -f /mnt/urbalurbadisk/manifests/031-tempo-config.yaml \\\n  -n monitoring \\\n  --kube-context rancher-desktop\n</code></pre></p> <p>Cleanup Old Traces (automatic): <pre><code># Retention handled automatically via tempo.retention setting\ntempo:\n  retention: 24h  # Traces older than 24 hours are purged\n</code></pre></p>"},{"location":"package-monitoring-tempo/#backup-procedures","title":"Backup Procedures","text":"<p>Snapshot Trace Blocks: <pre><code># Export PVC data\nkubectl exec -n monitoring tempo-0 -- \\\n  tar czf /tmp/tempo-backup.tar.gz /var/tempo\n\n# Copy to local machine\nkubectl cp monitoring/tempo-0:/tmp/tempo-backup.tar.gz \\\n  ./tempo-backup.tar.gz\n</code></pre></p> <p>Note: Tempo is designed as ephemeral storage with short retention. Long-term trace archival is not a primary use case.</p>"},{"location":"package-monitoring-tempo/#disaster-recovery","title":"Disaster Recovery","text":"<p>Restore from Backup: <pre><code># 1. Remove existing deployment\n./02-remove-tempo.sh rancher-desktop\n\n# 2. Restore PVC data (requires direct PV access)\n# 3. Redeploy Tempo\n./02-setup-tempo.sh rancher-desktop\n</code></pre></p> <p>Data Loss Scenarios: - PVC deleted: Traces are lost (not critical - 24h retention means limited impact) - Corruption: Tempo auto-repairs blocks on startup - Retention expired: Expected behavior, adjust retention if needed</p>"},{"location":"package-monitoring-tempo/#use-cases","title":"\ud83d\ude80 Use Cases","text":""},{"location":"package-monitoring-tempo/#1-application-tracing-with-opentelemetry","title":"1. Application Tracing with OpenTelemetry","text":"<p>Instrument Application (example with Go): <pre><code>import (\n    \"go.opentelemetry.io/otel\"\n    \"go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracehttp\"\n)\n\n// Configure OTLP exporter to send to OTLP Collector\nexporter, _ := otlptracehttp.New(ctx,\n    otlptracehttp.WithEndpoint(\"otel.localhost\"),\n    otlptracehttp.WithURLPath(\"/v1/traces\"),\n    otlptracehttp.WithHeaders(map[string]string{\n        \"Host\": \"otel.localhost\",\n    }),\n)\n</code></pre></p> <p>Environment Configuration: <pre><code>OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://127.0.0.1/v1/traces\nOTEL_EXPORTER_OTLP_HEADERS={\"Host\":\"otel.localhost\"}\n</code></pre></p> <p>Query in Grafana: <pre><code>{ resource.service.name = \"my-app\" }\n</code></pre></p>"},{"location":"package-monitoring-tempo/#2-correlate-logs-and-traces","title":"2. Correlate Logs and Traces","text":"<p>sovdev-logger Integration: <pre><code>// TypeScript application with sovdev-logger\nimport { initializeSovdevLogger } from '@sovdev/logger';\n\n// Logs include trace_id and span_id for correlation\nlogger.info(\"Processing request\", {\n    trace_id: context.traceId,\n    span_id: context.spanId\n});\n</code></pre></p> <p>Grafana Workflow: 1. Find trace in Tempo with TraceQL 2. Note <code>trace_id</code> from trace details 3. Switch to Loki datasource 4. Query logs: <code>{service_name=\"my-app\"} | json | trace_id=\"TRACE_ID\"</code> 5. View correlated logs and traces together</p>"},{"location":"package-monitoring-tempo/#3-performance-analysis","title":"3. Performance Analysis","text":"<p>Find Slow Requests: <pre><code># Traces slower than 2 seconds\n{ duration &gt; 2s }\n\n# Group by service\n{ duration &gt; 2s } | group by resource.service.name\n</code></pre></p> <p>Analyze Bottlenecks: 1. Query slow traces in Grafana Explore 2. View trace waterfall/flamegraph 3. Identify slow spans (database queries, API calls, etc.) 4. Optimize identified bottlenecks</p>"},{"location":"package-monitoring-tempo/#4-error-investigation","title":"4. Error Investigation","text":"<p>Find Failed Requests: <pre><code># Traces with errors\n{ status = error }\n\n# Errors from specific service in last hour\n{ resource.service.name = \"api-service\" &amp;&amp; status = error }\n</code></pre></p> <p>Debug Workflow: 1. Find error traces with TraceQL 2. Examine trace details and span attributes 3. Correlate with logs (trace_id) 4. Identify root cause from span data</p> <p>\ud83d\udca1 Key Insight: Tempo's design philosophy is \"store everything, index nothing (except metadata)\". This approach dramatically reduces costs while enabling powerful trace analysis through TraceQL. When integrated with OTLP Collector for ingestion and Grafana for visualization, Tempo provides complete distributed tracing capabilities for microservices architectures without the operational complexity of traditional tracing backends.</p>"},{"location":"package-monitoring-tempo/#related-documentation","title":"\ud83d\udd17 Related Documentation","text":"<p>Monitoring Stack: - Monitoring Overview - Complete observability stack - Prometheus Metrics - Metrics collection - Loki Logs - Log aggregation - OTLP Collector - Telemetry pipeline (trace ingestion) - Grafana Visualization - Dashboards and trace exploration</p> <p>Configuration &amp; Rules: - Naming Conventions - Manifest numbering (031) - Development Workflow - Configuration management - Automated Deployment - Orchestration</p> <p>External Resources: - TraceQL Language: https://grafana.com/docs/tempo/v2.8.x/traceql/ - OTLP Specification: https://opentelemetry.io/docs/specs/otlp/ - Tempo Configuration: https://grafana.com/docs/tempo/v2.8.x/configuration/</p>"},{"location":"package-queues-rabbitmq/","title":"RabbitMQ - Message Broker and Queue System","text":"<p>Key Features: Message Queuing \u2022 Pub/Sub Messaging \u2022 Routing &amp; Exchange \u2022 Reliability &amp; Durability \u2022 Management UI \u2022 Clustering Support \u2022 Authentication</p> <p>File: <code>docs/package-queues-rabbitmq.md</code> Purpose: Complete guide to RabbitMQ deployment and configuration in Urbalurba infrastructure Target Audience: Developers, DevOps engineers, backend developers working with message queues and asynchronous systems Last Updated: September 23, 2025</p>"},{"location":"package-queues-rabbitmq/#overview","title":"\ud83d\udccb Overview","text":"<p>RabbitMQ serves as the primary message broker and queue system in the Urbalurba infrastructure. It provides reliable message queuing, publish/subscribe messaging, and advanced routing capabilities for distributed and microservices architectures.</p> <p>Key Features: - Message Broker: Advanced message routing with exchanges, queues, and bindings - Reliability: Message persistence, acknowledgments, and delivery guarantees - Management UI: Web-based administration and monitoring interface - Helm-Based Deployment: Uses Bitnami RabbitMQ chart with secure configuration - Secret Management: Integrates with urbalurba-secrets for secure authentication - Automated Testing: Includes comprehensive connectivity and API verification - Standalone Architecture: Single-instance deployment for simplicity and resource efficiency</p>"},{"location":"package-queues-rabbitmq/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"package-queues-rabbitmq/#deployment-components","title":"Deployment Components","text":"<pre><code>RabbitMQ Service Stack:\n\u251c\u2500\u2500 Helm Release (bitnami/rabbitmq)\n\u251c\u2500\u2500 StatefulSet (rabbitmq:4.1.3 container)\n\u251c\u2500\u2500 ConfigMap (RabbitMQ configuration)\n\u251c\u2500\u2500 Service (ClusterIP on port 5672 AMQP, 15672 Management)\n\u251c\u2500\u2500 PersistentVolumeClaim (8GB storage)\n\u251c\u2500\u2500 urbalurba-secrets (authentication credentials)\n\u2514\u2500\u2500 Pod (rabbitmq container with management plugin enabled)\n</code></pre>"},{"location":"package-queues-rabbitmq/#file-structure","title":"File Structure","text":"<pre><code>03-queues/\n\u251c\u2500\u2500 not-in-use/\n    \u251c\u2500\u2500 08-setup-rabbitmq.sh       # Main deployment script\n    \u2514\u2500\u2500 08-remove-rabbitmq.sh      # Removal script\n\nmanifests/\n\u2514\u2500\u2500 080-rabbitmq-config.yaml      # RabbitMQ Helm configuration\n\nansible/playbooks/\n\u251c\u2500\u2500 080-setup-rabbitmq.yml        # Main deployment logic\n\u2514\u2500\u2500 080-remove-rabbitmq.yml       # Removal logic\n</code></pre>"},{"location":"package-queues-rabbitmq/#deployment","title":"\ud83d\ude80 Deployment","text":""},{"location":"package-queues-rabbitmq/#manual-deployment","title":"Manual Deployment","text":"<p>RabbitMQ is currently in the <code>03-queues/not-in-use</code> category and can be deployed manually:</p> <pre><code># Deploy RabbitMQ with default settings\ncd provision-host/kubernetes/03-queues/not-in-use/\n./08-setup-rabbitmq.sh rancher-desktop\n\n# Deploy to specific Kubernetes context\n./08-setup-rabbitmq.sh multipass-microk8s\n./08-setup-rabbitmq.sh azure-aks\n</code></pre>"},{"location":"package-queues-rabbitmq/#prerequisites","title":"Prerequisites","text":"<p>Before deploying RabbitMQ, ensure the required secrets are configured in <code>urbalurba-secrets</code>:</p> <ul> <li><code>RABBITMQ_USERNAME</code>: RabbitMQ admin username</li> <li><code>RABBITMQ_PASSWORD</code>: RabbitMQ admin password</li> </ul>"},{"location":"package-queues-rabbitmq/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"package-queues-rabbitmq/#rabbitmq-configuration","title":"RabbitMQ Configuration","text":"<p>RabbitMQ uses the Bitnami RabbitMQ 4.1.3 image with authentication and management plugin enabled:</p> <pre><code># From manifests/080-rabbitmq-config.yaml\nservice:\n  type: ClusterIP\n\nreplicaCount: 1  # Single instance for simplicity\n\nauth:\n  username: # Set by playbook using --set auth.username=&lt;username&gt;\n  password: # Set by playbook using --set auth.password=&lt;password&gt;\n  generateErlangCookie: true\n\nplugins: \"rabbitmq_management\"  # Management UI enabled\n\narchitecture: standalone\n</code></pre>"},{"location":"package-queues-rabbitmq/#helm-configuration","title":"Helm Configuration","text":"<pre><code># Deployment command (from Ansible playbook)\nhelm upgrade --install rabbitmq bitnami/rabbitmq \\\n  -f manifests/080-rabbitmq-config.yaml \\\n  --set auth.username=\"$RABBITMQ_USERNAME\" \\\n  --set auth.password=\"$RABBITMQ_PASSWORD\"\n</code></pre>"},{"location":"package-queues-rabbitmq/#resource-configuration","title":"Resource Configuration","text":"<pre><code># Resource limits and requests\nresources:\n  requests:\n    memory: \"128Mi\"\n    cpu: \"100m\"\n  limits:\n    memory: \"256Mi\"\n    cpu: \"200m\"\n\n# Storage configuration\npersistence:\n  enabled: true\n  size: 8Gi\n  accessMode: ReadWriteOnce\n</code></pre>"},{"location":"package-queues-rabbitmq/#security-configuration","title":"Security Configuration","text":"<pre><code># Authentication configuration\nauth:\n  generateErlangCookie: true  # Prevents cookie mismatch issues\n\n# Memory management\nextraConfiguration: |\n  vm_memory_high_watermark.relative = 0.4\n</code></pre>"},{"location":"package-queues-rabbitmq/#monitoring-verification","title":"\ud83d\udd0d Monitoring &amp; Verification","text":""},{"location":"package-queues-rabbitmq/#health-checks","title":"Health Checks","text":"<pre><code># Check pod status\nkubectl get pods -l app.kubernetes.io/name=rabbitmq\n\n# Check StatefulSet status\nkubectl get statefulset rabbitmq\n\n# Check service status\nkubectl get svc rabbitmq\n\n# View RabbitMQ logs\nkubectl logs -l app.kubernetes.io/name=rabbitmq\n</code></pre>"},{"location":"package-queues-rabbitmq/#rabbitmq-connection-testing","title":"RabbitMQ Connection Testing","text":"<pre><code># Access management UI (port forward required)\nkubectl port-forward svc/rabbitmq 15672:15672\n# Then open http://localhost:15672 in browser\n\n# Test AMQP connectivity from within cluster\nkubectl run rabbitmq-client --image=rabbitmq:4.1.3 --rm -it --restart=Never -- \\\n  rabbitmqctl -n rabbit@rabbitmq.default.svc.cluster.local status\n\n# Check service endpoints\nkubectl get endpoints rabbitmq\n</code></pre>"},{"location":"package-queues-rabbitmq/#management-ui-access","title":"Management UI Access","text":"<p>Primary Method - Cluster Ingress (Recommended): <pre><code># Access via cluster ingress (no port-forward needed)\n# URL: http://rabbitmq.localhost\n# Username: user (from secrets)\n# Password: [from secrets]\n</code></pre></p> <p>Alternative Method - Port Forward: <pre><code># Port forward for local access\nkubectl port-forward svc/rabbitmq 15672:15672\n\n# Access via browser\n# URL: http://localhost:15672\n# Username: user (from secrets)\n# Password: [from secrets]\n</code></pre></p> <p>External Access (when configured): - URL: <code>https://rabbitmq.urbalurba.no</code> (via Cloudflare tunnel) - Same credentials as internal access</p>"},{"location":"package-queues-rabbitmq/#automated-verification","title":"Automated Verification","text":"<p>The deployment includes comprehensive testing of RabbitMQ functionality:</p> <p>Verification Process: 1. Two-stage pod readiness: Waits for Running and Ready status 2. Management API connectivity: Tests HTTP response on port 15672 3. Service connectivity: Verifies internal cluster communication 4. Authentication validation: Confirms credentials work with management UI 5. Port verification: Checks AMQP (5672) and Management (15672) ports</p>"},{"location":"package-queues-rabbitmq/#management-operations","title":"\ud83d\udee0\ufe0f Management Operations","text":""},{"location":"package-queues-rabbitmq/#rabbitmq-administration","title":"RabbitMQ Administration","text":"<pre><code># Access RabbitMQ management CLI\nkubectl exec -it rabbitmq-0 -- rabbitmqctl status\n\n# List users\nkubectl exec -it rabbitmq-0 -- rabbitmqctl list_users\n\n# List queues\nkubectl exec -it rabbitmq-0 -- rabbitmqctl list_queues\n\n# List exchanges\nkubectl exec -it rabbitmq-0 -- rabbitmqctl list_exchanges\n\n# Check cluster status\nkubectl exec -it rabbitmq-0 -- rabbitmqctl cluster_status\n</code></pre>"},{"location":"package-queues-rabbitmq/#queue-management","title":"Queue Management","text":"<pre><code># Create a queue\nkubectl exec -it rabbitmq-0 -- rabbitmqadmin declare queue name=test-queue\n\n# Publish a message\nkubectl exec -it rabbitmq-0 -- rabbitmqadmin publish exchange=amq.default routing_key=test-queue payload=\"Hello World\"\n\n# Get messages\nkubectl exec -it rabbitmq-0 -- rabbitmqadmin get queue=test-queue\n\n# Delete a queue\nkubectl exec -it rabbitmq-0 -- rabbitmqadmin delete queue name=test-queue\n</code></pre>"},{"location":"package-queues-rabbitmq/#user-management","title":"User Management","text":"<pre><code># Add a new user\nkubectl exec -it rabbitmq-0 -- rabbitmqctl add_user newuser password123\n\n# Set user permissions\nkubectl exec -it rabbitmq-0 -- rabbitmqctl set_permissions -p / newuser \".*\" \".*\" \".*\"\n\n# Set user tags\nkubectl exec -it rabbitmq-0 -- rabbitmqctl set_user_tags newuser administrator\n\n# Delete user\nkubectl exec -it rabbitmq-0 -- rabbitmqctl delete_user newuser\n</code></pre>"},{"location":"package-queues-rabbitmq/#service-removal","title":"Service Removal","text":"<pre><code># Remove RabbitMQ service (preserves data by default)\ncd provision-host/kubernetes/03-queues/not-in-use/\n./08-remove-rabbitmq.sh rancher-desktop\n\n# Completely remove including data\nansible-playbook ansible/playbooks/080-remove-rabbitmq.yml \\\n  -e target_host=rancher-desktop\n</code></pre> <p>Removal Process: - Uninstalls RabbitMQ Helm release - Waits for pods to terminate - Removes persistent volume claims and services - Preserves urbalurba-secrets and namespace structure - Provides data retention options and recovery instructions</p>"},{"location":"package-queues-rabbitmq/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-queues-rabbitmq/#common-issues","title":"Common Issues","text":"<p>Pod Won't Start: <pre><code># Check pod events and logs\nkubectl describe pod -l app.kubernetes.io/name=rabbitmq\nkubectl logs -l app.kubernetes.io/name=rabbitmq\n\n# Check Erlang cookie issues\nkubectl exec -it rabbitmq-0 -- cat /opt/bitnami/rabbitmq/secrets/rabbitmq-erlang-cookie\n</code></pre></p> <p>Authentication Issues: <pre><code># Check RabbitMQ credentials in secrets\nkubectl get secret urbalurba-secrets -o jsonpath=\"{.data.RABBITMQ_USERNAME}\" | base64 -d\nkubectl get secret urbalurba-secrets -o jsonpath=\"{.data.RABBITMQ_PASSWORD}\" | base64 -d\n\n# Test authentication via management API\nkubectl exec -it rabbitmq-0 -- curl -u \"rabbitmq-admin:password\" http://localhost:15672/api/overview\n\n# Check user permissions\nkubectl exec -it rabbitmq-0 -- rabbitmqctl list_user_permissions rabbitmq-admin\n</code></pre></p> <p>Connection Issues: <pre><code># Verify service endpoints\nkubectl describe svc rabbitmq\nkubectl get endpoints rabbitmq\n\n# Test DNS resolution\nkubectl run test-pod --image=busybox --rm -it -- \\\n  nslookup rabbitmq.default.svc.cluster.local\n\n# Check RabbitMQ node status\nkubectl exec -it rabbitmq-0 -- rabbitmqctl node_health_check\n</code></pre></p> <p>Performance Issues: <pre><code># Check resource usage\nkubectl top pod rabbitmq-0\n\n# Monitor RabbitMQ statistics\nkubectl exec -it rabbitmq-0 -- rabbitmqctl list_queues name messages memory\n\n# Check memory usage\nkubectl exec -it rabbitmq-0 -- rabbitmqctl status | grep memory\n\n# Monitor connections\nkubectl exec -it rabbitmq-0 -- rabbitmqctl list_connections\n</code></pre></p> <p>Management UI Issues: <pre><code># Check management plugin status\nkubectl exec -it rabbitmq-0 -- rabbitmq-plugins list\n\n# Restart management plugin\nkubectl exec -it rabbitmq-0 -- rabbitmq-plugins disable rabbitmq_management\nkubectl exec -it rabbitmq-0 -- rabbitmq-plugins enable rabbitmq_management\n\n# Check management UI logs\nkubectl logs rabbitmq-0 | grep management\n</code></pre></p>"},{"location":"package-queues-rabbitmq/#maintenance","title":"\ud83d\udccb Maintenance","text":""},{"location":"package-queues-rabbitmq/#regular-tasks","title":"Regular Tasks","text":"<ol> <li>Health Monitoring: Check pod and service status daily</li> <li>Queue Monitoring: Monitor queue lengths and message rates</li> <li>Backup Schedule: Implement regular backup of queue definitions and messages</li> <li>Performance Monitoring: Monitor memory usage, connection counts, and message throughput</li> </ol>"},{"location":"package-queues-rabbitmq/#backup-procedures","title":"Backup Procedures","text":"<pre><code># Export queue definitions\nkubectl exec rabbitmq-0 -- rabbitmqadmin export definitions.json\n\n# Backup definitions file\nkubectl cp rabbitmq-0:definitions.json ./rabbitmq-definitions-backup-$(date +%Y%m%d).json\n\n# Export specific queue\nkubectl exec rabbitmq-0 -- rabbitmqadmin export queue=myqueue queue-backup.json\n\n# Copy persistent data (if needed)\nkubectl cp rabbitmq-0:/opt/bitnami/rabbitmq/.rabbitmq/mnesia ./rabbitmq-data-backup-$(date +%Y%m%d)/\n</code></pre>"},{"location":"package-queues-rabbitmq/#disaster-recovery","title":"Disaster Recovery","text":"<pre><code># Restore definitions\nkubectl cp ./rabbitmq-definitions-backup.json rabbitmq-0:definitions.json\nkubectl exec rabbitmq-0 -- rabbitmqadmin import definitions.json\n\n# Restore from persistent volume backup\n# (Requires pod restart after restoring PV data)\nkubectl delete pod rabbitmq-0  # StatefulSet will recreate\n</code></pre>"},{"location":"package-queues-rabbitmq/#use-cases","title":"\ud83d\ude80 Use Cases","text":""},{"location":"package-queues-rabbitmq/#message-queuing","title":"Message Queuing","text":"<pre><code># Create work queue\nkubectl exec rabbitmq-0 -- rabbitmqadmin declare queue name=work_queue durable=true\n\n# Send task to queue\nkubectl exec rabbitmq-0 -- rabbitmqadmin publish exchange=amq.default routing_key=work_queue payload='{\"task\":\"process_image\",\"id\":123}' properties='{\"delivery_mode\":2}'\n\n# Consume from queue\nkubectl exec rabbitmq-0 -- rabbitmqadmin get queue=work_queue count=1\n</code></pre>"},{"location":"package-queues-rabbitmq/#publishsubscribe","title":"Publish/Subscribe","text":"<pre><code># Create fanout exchange\nkubectl exec rabbitmq-0 -- rabbitmqadmin declare exchange name=notifications type=fanout\n\n# Create subscriber queues\nkubectl exec rabbitmq-0 -- rabbitmqadmin declare queue name=email_notifications\nkubectl exec rabbitmq-0 -- rabbitmqadmin declare queue name=sms_notifications\n\n# Bind queues to exchange\nkubectl exec rabbitmq-0 -- rabbitmqadmin declare binding source=notifications destination=email_notifications\nkubectl exec rabbitmq-0 -- rabbitmqadmin declare binding source=notifications destination=sms_notifications\n\n# Publish to all subscribers\nkubectl exec rabbitmq-0 -- rabbitmqadmin publish exchange=notifications routing_key=\"\" payload='{\"event\":\"user_registered\",\"user_id\":456}'\n</code></pre>"},{"location":"package-queues-rabbitmq/#requestresponse","title":"Request/Response","text":"<pre><code># Create RPC queue\nkubectl exec rabbitmq-0 -- rabbitmqadmin declare queue name=rpc_queue\n\n# Setup reply-to queue\nkubectl exec rabbitmq-0 -- rabbitmqadmin declare queue name=rpc_reply exclusive=true\n\n# Send RPC request\nkubectl exec rabbitmq-0 -- rabbitmqadmin publish exchange=amq.default routing_key=rpc_queue payload='{\"method\":\"calculate\",\"params\":[1,2,3]}' properties='{\"reply_to\":\"rpc_reply\",\"correlation_id\":\"123\"}'\n</code></pre>"},{"location":"package-queues-rabbitmq/#dead-letter-queues","title":"Dead Letter Queues","text":"<pre><code># Create main queue with DLX\nkubectl exec rabbitmq-0 -- rabbitmqadmin declare queue name=main_queue arguments='{\"x-dead-letter-exchange\":\"dlx\",\"x-message-ttl\":60000}'\n\n# Create dead letter exchange and queue\nkubectl exec rabbitmq-0 -- rabbitmqadmin declare exchange name=dlx type=direct\nkubectl exec rabbitmq-0 -- rabbitmqadmin declare queue name=dead_letters\nkubectl exec rabbitmq-0 -- rabbitmqadmin declare binding source=dlx destination=dead_letters routing_key=dead\n</code></pre> <p>\ud83d\udca1 Key Insight: RabbitMQ provides essential message broker capabilities that enable reliable asynchronous communication between services. Use RabbitMQ for decoupling services, handling background tasks, implementing event-driven architectures, and ensuring message delivery guarantees in distributed systems.</p>"},{"location":"package-queues-readme/","title":"Queue Services - Message Brokers and Caching Layer","text":"<p>File: <code>docs/package-queues-readme.md</code> Purpose: Overview of all queue and caching services in Urbalurba infrastructure Target Audience: Backend developers, DevOps engineers, system architects Last Updated: September 23, 2025</p>"},{"location":"package-queues-readme/#overview","title":"\ud83d\udccb Overview","text":"<p>Urbalurba infrastructure provides a comprehensive suite of queue and caching services supporting various messaging patterns and performance optimization needs. From high-performance in-memory caching to reliable message brokers, the platform offers production-ready solutions for asynchronous communication, data caching, and event-driven architectures.</p> <p>Available Queue Services: - Redis: In-memory data store and cache for high-performance operations - RabbitMQ: Message broker for reliable asynchronous communication</p>"},{"location":"package-queues-readme/#queue-cache-services","title":"\ud83d\ude80 Queue &amp; Cache Services","text":""},{"location":"package-queues-readme/#redis-in-memory-cache-data-store","title":"Redis - In-Memory Cache &amp; Data Store \u26a1","text":"<p>Status: Active | Port: 6379 | Type: In-Memory Database/Cache</p> <p>Key Capabilities: Sub-millisecond Latency \u2022 Data Structures \u2022 Pub/Sub \u2022 Session Storage \u2022 Rate Limiting \u2022 Persistence Options \u2022 Lua Scripting</p> <p>Redis serves as the primary caching layer and in-memory data store with enterprise-grade features for high-performance applications. Provides essential caching capabilities for Authentik authentication and other services requiring fast data access.</p> <p>Key Features: - High Performance: Sub-millisecond latency for read/write operations - Data Structures: Strings, hashes, lists, sets, sorted sets, streams - Authentik Integration: Required for authentication service performance - Persistence Options: RDB snapshots for data durability - Helm Deployment: Bitnami Redis chart with secure configuration</p> <p>\ud83d\udcda Complete Documentation \u2192</p>"},{"location":"package-queues-readme/#rabbitmq-message-broker","title":"RabbitMQ - Message Broker \ud83d\udce8","text":"<p>Status: Optional (not-in-use) | Port: 5672 (AMQP), 15672 (Management) | Type: Message Queue</p> <p>Key Capabilities: Message Queuing \u2022 Pub/Sub \u2022 Routing \u2022 Dead Letter Queues \u2022 Message Persistence \u2022 Management UI \u2022 Clustering Support</p> <p>RabbitMQ provides reliable message broker capabilities for asynchronous communication between services. Enables decoupled architectures with guaranteed message delivery, advanced routing, and comprehensive management tools.</p> <p>Key Features: - Message Patterns: Queuing, pub/sub, RPC, routing - Reliability: Message persistence and delivery guarantees - Management UI: Web-based administration interface - Dead Letter Queues: Handle failed message processing - Security: Centralized authentication via urbalurba-secrets</p> <p>\ud83d\udcda Complete Documentation \u2192</p>"},{"location":"package-queues-readme/#deployment-architecture","title":"\ud83c\udfd7\ufe0f Deployment Architecture","text":""},{"location":"package-queues-readme/#service-activation","title":"Service Activation","text":"<pre><code>Queue Services Deployment Status:\n\u251c\u2500\u2500 Redis (ACTIVE) - Required for Authentik and caching\n\u2514\u2500\u2500 RabbitMQ (INACTIVE) - Located in not-in-use/ folder\n</code></pre>"},{"location":"package-queues-readme/#storage-persistence","title":"Storage &amp; Persistence","text":"<p>Queue services use different persistence strategies: - Redis: 6GB persistent storage with RDB snapshots - RabbitMQ: 8GB persistent storage for message durability</p>"},{"location":"package-queues-readme/#secret-management","title":"Secret Management","text":"<p>Authentication managed through <code>urbalurba-secrets</code>: <pre><code>Queue Service Credentials:\n\u251c\u2500\u2500 REDIS_PASSWORD / REDIS_HOST (Redis)\n\u2514\u2500\u2500 RABBITMQ_USERNAME / RABBITMQ_PASSWORD / RABBITMQ_HOST (RabbitMQ)\n</code></pre></p>"},{"location":"package-queues-readme/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"package-queues-readme/#deploy-redis-required-for-authentik","title":"Deploy Redis (Required for Authentik)","text":"<pre><code># Automatic deployment during cluster provisioning\n./provision-kubernetes.sh rancher-desktop\n\n# Manual deployment\ncd provision-host/kubernetes/03-queues/\n./06-setup-redis.sh rancher-desktop\n</code></pre>"},{"location":"package-queues-readme/#activate-rabbitmq-optional","title":"Activate RabbitMQ (Optional)","text":"<pre><code># Deploy RabbitMQ when message broker is needed\ncd provision-host/kubernetes/03-queues/not-in-use/\n./08-setup-rabbitmq.sh rancher-desktop\n</code></pre>"},{"location":"package-queues-readme/#service-selection-guide","title":"\ud83d\udd0d Service Selection Guide","text":""},{"location":"package-queues-readme/#when-to-use-redis","title":"When to Use Redis \u26a1","text":"<ul> <li>Caching Layer: Session storage, API response caching</li> <li>Rate Limiting: API throttling and request limiting</li> <li>Real-time Features: Leaderboards, counters, analytics</li> <li>Pub/Sub: Simple publish/subscribe messaging</li> <li>Required for: Authentik authentication service</li> </ul>"},{"location":"package-queues-readme/#when-to-use-rabbitmq","title":"When to Use RabbitMQ \ud83d\udce8","text":"<ul> <li>Message Queuing: Background job processing</li> <li>Service Decoupling: Asynchronous communication between microservices</li> <li>Event-Driven Architecture: Event publishing and consumption</li> <li>Reliability Required: Guaranteed message delivery</li> <li>Complex Routing: Topic-based or content-based routing</li> </ul>"},{"location":"package-queues-readme/#redis-vs-rabbitmq-decision-matrix","title":"Redis vs RabbitMQ Decision Matrix","text":"Use Case Redis RabbitMQ Session Cache \u2705 Best \u274c Not suitable API Response Cache \u2705 Best \u274c Not suitable Simple Pub/Sub \u2705 Good \u2705 Better for complex patterns Job Queue \u26a0\ufe0f Basic \u2705 Best with guarantees Message Routing \u274c Limited \u2705 Advanced routing Dead Letter Queue \u274c Manual \u2705 Built-in support Persistence \u26a0\ufe0f Optional \u2705 Built-in"},{"location":"package-queues-readme/#management-operations","title":"\ud83d\udee0\ufe0f Management Operations","text":""},{"location":"package-queues-readme/#common-operations","title":"Common Operations","text":"<pre><code># Check service status\nkubectl get pods -l app.kubernetes.io/name=redis\nkubectl get pods -l app.kubernetes.io/name=rabbitmq\n\n# View service logs\nkubectl logs -l app.kubernetes.io/name=redis\nkubectl logs -l app.kubernetes.io/name=rabbitmq\n\n# Connect to services\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword\nkubectl exec -it rabbitmq-0 -- rabbitmqctl status\n</code></pre>"},{"location":"package-queues-readme/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code># Redis monitoring\nkubectl exec redis-master-0 -- redis-cli -a yourpassword info stats\nkubectl exec redis-master-0 -- redis-cli -a yourpassword info memory\n\n# RabbitMQ monitoring\nkubectl exec rabbitmq-0 -- rabbitmqctl list_queues name messages memory\nkubectl exec rabbitmq-0 -- rabbitmqctl list_connections\n</code></pre>"},{"location":"package-queues-readme/#backup-procedures","title":"Backup Procedures","text":"<pre><code># Redis backup (RDB snapshot)\nkubectl exec redis-master-0 -- redis-cli -a yourpassword bgsave\nkubectl cp redis-master-0:/data/dump.rdb ./redis-backup.rdb\n\n# RabbitMQ backup (definitions export)\nkubectl exec rabbitmq-0 -- rabbitmqadmin export definitions.json\nkubectl cp rabbitmq-0:definitions.json ./rabbitmq-definitions-backup.json\n</code></pre>"},{"location":"package-queues-readme/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-queues-readme/#common-issues","title":"Common Issues","text":"<ul> <li>Connection Refused: Check service endpoints and authentication</li> <li>Memory Issues: Monitor memory usage and eviction policies</li> <li>Authentication Failed: Verify urbalurba-secrets configuration</li> <li>Performance Degradation: Check resource limits and persistence settings</li> </ul>"},{"location":"package-queues-readme/#diagnostic-commands","title":"Diagnostic Commands","text":"<pre><code># Check service endpoints\nkubectl get endpoints redis-master rabbitmq\n\n# Verify storage\nkubectl get pvc -l app.kubernetes.io/name=redis\nkubectl get pvc -l app.kubernetes.io/name=rabbitmq\n\n# Test Redis connectivity\nkubectl run redis-test --image=redis:8.2.1 --rm -it -- \\\n  redis-cli -h redis-master.default.svc.cluster.local -a yourpassword ping\n\n# Test RabbitMQ connectivity\nkubectl port-forward svc/rabbitmq 15672:15672\n# Then access http://localhost:15672\n</code></pre>"},{"location":"package-queues-readme/#maintenance","title":"\ud83d\udccb Maintenance","text":""},{"location":"package-queues-readme/#regular-tasks","title":"Regular Tasks","text":"<ol> <li>Monitor Memory: Check memory usage and eviction rates</li> <li>Backup Schedule: Implement automated backup procedures</li> <li>Performance Monitoring: Track latency and throughput metrics</li> <li>Security Updates: Update container images regularly</li> <li>Connection Monitoring: Track client connections and usage patterns</li> </ol>"},{"location":"package-queues-readme/#service-removal","title":"Service Removal","text":"<pre><code># Remove services (preserves data by default)\ncd provision-host/kubernetes/03-queues/not-in-use/\n./06-remove-redis.sh rancher-desktop\n./08-remove-rabbitmq.sh rancher-desktop\n</code></pre>"},{"location":"package-queues-readme/#use-case-examples","title":"\ud83c\udfaf Use Case Examples","text":""},{"location":"package-queues-readme/#redis-session-management","title":"Redis: Session Management","text":"<pre><code># Store user session\nkubectl exec redis-master-0 -- redis-cli -a yourpassword \\\n  HSET session:user123 username \"john\" last_activity \"$(date +%s)\"\n\n# Set session expiry\nkubectl exec redis-master-0 -- redis-cli -a yourpassword \\\n  EXPIRE session:user123 1800\n</code></pre>"},{"location":"package-queues-readme/#redis-rate-limiting","title":"Redis: Rate Limiting","text":"<pre><code># Implement rate limiting (10 requests per minute)\nkubectl exec redis-master-0 -- redis-cli -a yourpassword \\\n  SET rate:api:user123 1 EX 60 NX\n\nkubectl exec redis-master-0 -- redis-cli -a yourpassword \\\n  INCR rate:api:user123\n</code></pre>"},{"location":"package-queues-readme/#rabbitmq-work-queue","title":"RabbitMQ: Work Queue","text":"<pre><code># Create work queue\nkubectl exec rabbitmq-0 -- rabbitmqadmin declare queue name=task_queue durable=true\n\n# Send task to queue\nkubectl exec rabbitmq-0 -- rabbitmqadmin publish \\\n  exchange=amq.default routing_key=task_queue \\\n  payload='{\"task\":\"process_image\",\"id\":123}'\n</code></pre>"},{"location":"package-queues-readme/#rabbitmq-event-publishing","title":"RabbitMQ: Event Publishing","text":"<pre><code># Create event exchange\nkubectl exec rabbitmq-0 -- rabbitmqadmin declare exchange \\\n  name=events type=topic\n\n# Publish event\nkubectl exec rabbitmq-0 -- rabbitmqadmin publish \\\n  exchange=events routing_key=user.registered \\\n  payload='{\"user_id\":456,\"timestamp\":\"2025-09-22T10:00:00Z\"}'\n</code></pre>"},{"location":"package-queues-readme/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>Redis Documentation - In-memory cache and data store</li> <li>RabbitMQ Documentation - Message broker service</li> <li>Authentik Documentation - Redis dependency for authentication</li> <li>Secrets Management - Queue service credential configuration</li> <li>Troubleshooting Guide - Queue service troubleshooting</li> </ul> <p>\ud83d\udca1 Key Insight: The queue services layer provides essential infrastructure for high-performance caching and reliable message delivery. Redis serves as the foundation for session management and caching (required for Authentik), while RabbitMQ enables sophisticated messaging patterns for event-driven architectures and service decoupling. Choose Redis for speed and simplicity, RabbitMQ for reliability and advanced routing.</p>"},{"location":"package-queues-redis/","title":"Redis - In-Memory Data Store and Cache","text":"<p>Key Features: High-Performance Caching \u2022 Session Storage \u2022 Message Queuing \u2022 Real-Time Analytics \u2022 Pub/Sub Messaging \u2022 Data Structures \u2022 Persistence</p> <p>File: <code>docs/package-databases-redis.md</code> Purpose: Complete guide to Redis deployment and configuration in Urbalurba infrastructure Target Audience: Developers, DevOps engineers, backend developers working with caching and real-time data Last Updated: September 22, 2024</p>"},{"location":"package-queues-redis/#overview","title":"\ud83d\udccb Overview","text":"<p>Redis serves as the primary in-memory data store and cache in the Urbalurba infrastructure. It provides high-performance caching, session storage, message queuing, and real-time data processing capabilities for modern applications.</p> <p>Key Features: - High-Performance Cache: Sub-millisecond latency for read/write operations - Data Structures: Strings, hashes, lists, sets, sorted sets, bitmaps, and streams - Persistence: Configurable data persistence with RDB snapshots - Helm-Based Deployment: Uses Bitnami Redis chart with custom configuration - Secret Management: Integrates with urbalurba-secrets for secure authentication - Automated Testing: Includes comprehensive connectivity and data operation verification - Standalone Architecture: Single-instance deployment for simplicity and reliability</p>"},{"location":"package-queues-redis/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"package-queues-redis/#deployment-components","title":"Deployment Components","text":"<pre><code>Redis Service Stack:\n\u251c\u2500\u2500 Helm Release (bitnami/redis)\n\u251c\u2500\u2500 StatefulSet (redis:8.2.1 container)\n\u251c\u2500\u2500 ConfigMap (Redis configuration)\n\u251c\u2500\u2500 Service (ClusterIP on port 6379)\n\u251c\u2500\u2500 PersistentVolumeClaim (6GB storage)\n\u251c\u2500\u2500 urbalurba-secrets (authentication credentials)\n\u2514\u2500\u2500 Pod (redis container with auth enabled)\n</code></pre>"},{"location":"package-queues-redis/#file-structure","title":"File Structure","text":"<pre><code>03-queues/\n\u251c\u2500\u2500 06-setup-redis.sh           # Main deployment script (active)\n\u2514\u2500\u2500 not-in-use/\n    \u2514\u2500\u2500 06-remove-redis.sh      # Removal script\n\nmanifests/\n\u2514\u2500\u2500 050-redis-config.yaml      # Redis Helm configuration\n\nansible/playbooks/\n\u251c\u2500\u2500 050-setup-redis.yml        # Main deployment logic\n\u2514\u2500\u2500 050-remove-redis.yml       # Removal logic\n</code></pre>"},{"location":"package-queues-redis/#deployment","title":"\ud83d\ude80 Deployment","text":""},{"location":"package-queues-redis/#manual-deployment","title":"Manual Deployment","text":"<p>Redis is currently in the <code>03-queues</code> category and can be deployed manually:</p> <pre><code># Deploy Redis with default settings\ncd provision-host/kubernetes/03-queues/\n./06-setup-redis.sh rancher-desktop\n\n# Deploy to specific Kubernetes context\n./06-setup-redis.sh multipass-microk8s\n./06-setup-redis.sh azure-aks\n</code></pre>"},{"location":"package-queues-redis/#prerequisites","title":"Prerequisites","text":"<p>Before deploying Redis, ensure the required secrets are configured in <code>urbalurba-secrets</code>:</p> <ul> <li><code>REDIS_PASSWORD</code>: Redis authentication password</li> </ul>"},{"location":"package-queues-redis/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"package-queues-redis/#redis-configuration","title":"Redis Configuration","text":"<p>Redis uses the official Redis 8.2.1 image with authentication enabled:</p> <pre><code># From manifests/050-redis-config.yaml\nimage:\n  registry: docker.io\n  repository: redis\n  tag: 8.2.1\n  pullPolicy: IfNotPresent\n\nauth:\n  enabled: true\n  # Password set by playbook using --set global.redis.password=&lt;password&gt;\n\narchitecture: standalone  # Single instance for simplicity\n</code></pre>"},{"location":"package-queues-redis/#helm-configuration","title":"Helm Configuration","text":"<pre><code># Deployment command (from Ansible playbook)\nhelm install redis bitnami/redis \\\n  -f manifests/050-redis-config.yaml \\\n  --set global.redis.password=\"$REDIS_PASSWORD\"\n</code></pre>"},{"location":"package-queues-redis/#resource-configuration","title":"Resource Configuration","text":"<pre><code># Resource limits and requests\nmaster:\n  resources:\n    limits:\n      cpu: 200m\n      memory: 256Mi\n    requests:\n      cpu: 100m\n      memory: 128Mi\n\n  # Storage configuration\n  persistence:\n    enabled: true\n    size: 6Gi\n</code></pre>"},{"location":"package-queues-redis/#service-configuration","title":"Service Configuration","text":"<pre><code># Service settings\nservice:\n  type: ClusterIP\n\n# Replica configuration (disabled for standalone)\nreplica:\n  replicaCount: 0\n\n# Metrics (disabled by default)\nmetrics:\n  enabled: false\n</code></pre>"},{"location":"package-queues-redis/#monitoring-verification","title":"\ud83d\udd0d Monitoring &amp; Verification","text":""},{"location":"package-queues-redis/#health-checks","title":"Health Checks","text":"<pre><code># Check pod status\nkubectl get pods -l app.kubernetes.io/name=redis\n\n# Check StatefulSet status\nkubectl get statefulset redis-master\n\n# Check service status\nkubectl get svc redis-master\n\n# View Redis logs\nkubectl logs -l app.kubernetes.io/name=redis\n</code></pre>"},{"location":"package-queues-redis/#redis-connection-testing","title":"Redis Connection Testing","text":"<pre><code># Test connection from within cluster (requires password)\nkubectl run redis-client --image=redis:8.2.1 --rm -it --restart=Never -- \\\n  redis-cli -h redis-master.default.svc.cluster.local -a yourpassword\n\n# Check if Redis is ready\nkubectl exec -it redis-master-0 -- redis-cli ping\n\n# Test with authentication (replace yourpassword)\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword ping\n</code></pre>"},{"location":"package-queues-redis/#data-operations-testing","title":"Data Operations Testing","text":"<pre><code># Connect to Redis CLI\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword\n\n# Basic operations\nSET hello world\nGET hello\nEXISTS hello\nDEL hello\n\n# List operations\nLPUSH mylist \"item1\" \"item2\" \"item3\"\nLRANGE mylist 0 -1\nLPOP mylist\n\n# Hash operations\nHSET user:1000 name \"John Doe\" email \"john@example.com\"\nHGET user:1000 name\nHGETALL user:1000\n\n# Set operations\nSADD myset \"member1\" \"member2\" \"member3\"\nSMEMBERS myset\nSISMEMBER myset \"member1\"\n</code></pre>"},{"location":"package-queues-redis/#automated-verification","title":"Automated Verification","text":"<p>The deployment includes comprehensive testing of Redis functionality:</p> <p>Verification Process: 1. Connects to Redis server using authentication 2. Tests basic SET/GET operations 3. Performs data structure operations (lists, hashes, sets) 4. Validates authentication and connectivity 5. Verifies persistence and data integrity</p>"},{"location":"package-queues-redis/#management-operations","title":"\ud83d\udee0\ufe0f Management Operations","text":""},{"location":"package-queues-redis/#redis-administration","title":"Redis Administration","text":"<pre><code># Access Redis CLI with authentication\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword\n\n# Get Redis info\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword info\n\n# Check memory usage\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword info memory\n\n# Check connected clients\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword info clients\n\n# Monitor Redis commands in real-time\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword monitor\n</code></pre>"},{"location":"package-queues-redis/#advanced-operations","title":"Advanced Operations","text":"<pre><code># Configure Redis settings\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword config set maxmemory 256mb\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword config set maxmemory-policy allkeys-lru\n\n# Check current configuration\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword config get \"*\"\n\n# Flush databases (careful!)\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword flushdb   # Current DB\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword flushall  # All DBs\n\n# Check Redis statistics\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword info stats\n</code></pre>"},{"location":"package-queues-redis/#backup-operations","title":"Backup Operations","text":"<pre><code># Create Redis backup (RDB snapshot)\nkubectl exec redis-master-0 -- redis-cli -a yourpassword bgsave\n\n# Check last save time\nkubectl exec redis-master-0 -- redis-cli -a yourpassword lastsave\n\n# Copy RDB file from pod\nkubectl cp redis-master-0:/data/dump.rdb ./redis-backup.rdb\n\n# Restore from backup (requires Redis restart)\nkubectl cp ./redis-backup.rdb redis-master-0:/data/dump.rdb\nkubectl delete pod redis-master-0  # Pod will restart and load backup\n</code></pre>"},{"location":"package-queues-redis/#service-removal","title":"Service Removal","text":"<pre><code># Remove Redis service (preserves data by default)\ncd provision-host/kubernetes/03-queues/not-in-use/\n./06-remove-redis.sh rancher-desktop\n\n# Completely remove including data\nansible-playbook ansible/playbooks/050-remove-redis.yml \\\n  -e target_host=rancher-desktop\n</code></pre> <p>Removal Process: - Uninstalls Redis Helm release - Waits for pods to terminate - Removes persistent volume claims - Preserves urbalurba-secrets and namespace structure - Provides data retention options and recovery instructions</p>"},{"location":"package-queues-redis/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-queues-redis/#common-issues","title":"Common Issues","text":"<p>Pod Won't Start: <pre><code># Check pod events and logs\nkubectl describe pod -l app.kubernetes.io/name=redis\nkubectl logs -l app.kubernetes.io/name=redis\n\n# Check Redis configuration\nkubectl describe configmap redis-configuration\n</code></pre></p> <p>Authentication Issues: <pre><code># Check Redis password in secrets\nkubectl get secret urbalurba-secrets -o jsonpath=\"{.data.REDIS_PASSWORD}\" | base64 -d\n\n# Test authentication\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword auth yourpassword\n\n# Check Redis auth configuration\nkubectl exec -it redis-master-0 -- redis-cli config get requirepass\n</code></pre></p> <p>Connection Issues: <pre><code># Verify service endpoints\nkubectl describe svc redis-master\nkubectl get endpoints redis-master\n\n# Test DNS resolution\nkubectl run test-pod --image=busybox --rm -it -- \\\n  nslookup redis-master.default.svc.cluster.local\n\n# Check Redis server status\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword info server\n</code></pre></p> <p>Performance Issues: <pre><code># Check resource usage\nkubectl top pod redis-master-0\n\n# Monitor Redis performance\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword info stats\n\n# Check slow log\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword slowlog get 10\n\n# Monitor Redis operations\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword monitor\n</code></pre></p> <p>Memory Issues: <pre><code># Check memory usage\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword info memory\n\n# Check memory configuration\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword config get maxmemory*\n\n# Check eviction statistics\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword info stats | grep evicted\n</code></pre></p>"},{"location":"package-queues-redis/#maintenance","title":"\ud83d\udccb Maintenance","text":""},{"location":"package-queues-redis/#regular-tasks","title":"Regular Tasks","text":"<ol> <li>Health Monitoring: Check pod and service status daily</li> <li>Memory Monitoring: Monitor memory usage and eviction patterns</li> <li>Backup Schedule: Implement regular RDB snapshots for data persistence</li> <li>Performance Monitoring: Monitor command execution times and client connections</li> </ol>"},{"location":"package-queues-redis/#backup-procedures","title":"Backup Procedures","text":"<pre><code># Manual backup creation\nkubectl exec redis-master-0 -- redis-cli -a yourpassword bgsave\n\n# Automated backup script\nkubectl exec redis-master-0 -- redis-cli -a yourpassword config set save \"900 1 300 10 60 10000\"\n\n# Export specific keys\nkubectl exec redis-master-0 -- redis-cli -a yourpassword --scan --pattern \"user:*\" | \\\n  xargs kubectl exec redis-master-0 -- redis-cli -a yourpassword dump\n\n# Copy backup file\nkubectl cp redis-master-0:/data/dump.rdb ./backup-$(date +%Y%m%d).rdb\n</code></pre>"},{"location":"package-queues-redis/#disaster-recovery","title":"Disaster Recovery","text":"<pre><code># Restore from RDB backup\nkubectl cp ./backup.rdb redis-master-0:/data/dump.rdb\nkubectl delete pod redis-master-0  # Restart to load backup\n\n# Verify restore\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword dbsize\nkubectl exec -it redis-master-0 -- redis-cli -a yourpassword info keyspace\n</code></pre>"},{"location":"package-queues-redis/#use-cases","title":"\ud83d\ude80 Use Cases","text":""},{"location":"package-queues-redis/#caching","title":"Caching","text":"<pre><code># Application cache pattern\nSET cache:user:1000 '{\"name\":\"John\",\"email\":\"john@example.com\"}' EX 3600\nGET cache:user:1000\n\n# Cache invalidation\nDEL cache:user:1000\nFLUSHDB  # Clear all cache\n</code></pre>"},{"location":"package-queues-redis/#session-storage","title":"Session Storage","text":"<pre><code># Store user session\nHSET session:abc123 user_id 1000 login_time 1672531200 last_activity 1672534800\nEXPIRE session:abc123 1800  # 30 minutes TTL\n\n# Retrieve session data\nHGETALL session:abc123\nTTL session:abc123\n</code></pre>"},{"location":"package-queues-redis/#message-queuing","title":"Message Queuing","text":"<pre><code># Producer: Add tasks to queue\nLPUSH task_queue '{\"type\":\"email\",\"recipient\":\"user@example.com\",\"subject\":\"Welcome\"}'\nLPUSH task_queue '{\"type\":\"notification\",\"user_id\":1000,\"message\":\"New message\"}'\n\n# Consumer: Process tasks from queue\nBRPOP task_queue 10  # Blocking pop with 10-second timeout\n</code></pre>"},{"location":"package-queues-redis/#real-time-analytics","title":"Real-time Analytics","text":"<pre><code># Increment counters\nINCR page_views:home\nINCR user_actions:1000:login\n\n# Time-series data\nZADD user_scores 100 \"user1\" 150 \"user2\" 200 \"user3\"\nZRANGE user_scores 0 -1 WITHSCORES\n\n# Rate limiting\nINCR rate_limit:api:user:1000\nEXPIRE rate_limit:api:user:1000 3600\n</code></pre> <p>\ud83d\udca1 Key Insight: Redis provides essential in-memory data storage and caching capabilities that complement the primary PostgreSQL database. Use Redis for high-frequency read/write operations, session management, real-time features, and as a performance multiplier for database-backed applications.</p>"},{"location":"package-search-elasticsearch/","title":"Elasticsearch - Search and Analytics Engine","text":"<p>Key Features: Full-Text Search \u2022 Real-Time Analytics \u2022 Document Storage \u2022 RESTful API \u2022 Distributed Architecture \u2022 Aggregations \u2022 Query DSL</p> <p>File: <code>docs/package-search-elasticsearch.md</code> Purpose: Complete guide to Elasticsearch deployment and configuration in Urbalurba infrastructure Target Audience: Developers, DevOps engineers, data engineers working with search, analytics, and document storage Last Updated: September 23, 2025</p>"},{"location":"package-search-elasticsearch/#overview","title":"\ud83d\udccb Overview","text":"<p>Elasticsearch serves as the primary search and analytics engine in the Urbalurba infrastructure. It provides powerful full-text search capabilities, real-time analytics, and scalable document storage for applications requiring advanced search functionality.</p> <p>Key Features: - Search Engine: Advanced full-text search with relevance scoring and highlighting - Analytics: Real-time aggregations and data analysis capabilities - Document Store: JSON document storage with automatic schema detection - RESTful API: HTTP-based API for all operations - Helm-Based Deployment: Uses Bitnami Elasticsearch chart with secure configuration - Secret Management: Integrates with urbalurba-secrets for secure authentication - Automated Testing: Includes comprehensive connectivity and functionality verification - Single-Node Architecture: Optimized deployment for development and testing environments</p>"},{"location":"package-search-elasticsearch/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"package-search-elasticsearch/#deployment-components","title":"Deployment Components","text":"<pre><code>Elasticsearch Service Stack:\n\u251c\u2500\u2500 Helm Release (bitnami/elasticsearch)\n\u251c\u2500\u2500 StatefulSet (elasticsearch:8.16.1 container)\n\u251c\u2500\u2500 ConfigMap (Elasticsearch configuration)\n\u251c\u2500\u2500 Service (ClusterIP on port 9200 HTTP, 9300 Transport)\n\u251c\u2500\u2500 PersistentVolumeClaim (8GB storage)\n\u251c\u2500\u2500 urbalurba-secrets (authentication credentials)\n\u2514\u2500\u2500 Pod (elasticsearch container with security enabled)\n</code></pre>"},{"location":"package-search-elasticsearch/#file-structure","title":"File Structure","text":"<pre><code>04-search/\n\u251c\u2500\u2500 not-in-use/\n    \u251c\u2500\u2500 07-setup-elasticsearch.sh       # Main deployment script\n    \u2514\u2500\u2500 07-remove-elasticsearch.sh      # Removal script\n\nmanifests/\n\u2514\u2500\u2500 060-elasticsearch-config.yaml      # Elasticsearch Helm configuration\n\nansible/playbooks/\n\u251c\u2500\u2500 060-setup-elasticsearch.yml        # Main deployment logic\n\u2514\u2500\u2500 060-remove-elasticsearch.yml       # Removal logic\n</code></pre>"},{"location":"package-search-elasticsearch/#deployment","title":"\ud83d\ude80 Deployment","text":""},{"location":"package-search-elasticsearch/#manual-deployment","title":"Manual Deployment","text":"<p>Elasticsearch is currently in the <code>04-search/not-in-use</code> category and can be deployed manually:</p> <pre><code># Deploy Elasticsearch with default settings (8.16.1)\ncd provision-host/kubernetes/04-search/not-in-use/\n./07-setup-elasticsearch.sh rancher-desktop\n\n# Deploy to specific Kubernetes context\n./07-setup-elasticsearch.sh multipass-microk8s\n./07-setup-elasticsearch.sh azure-aks\n\n# Deploy with specific version\nansible-playbook /mnt/urbalurbadisk/ansible/playbooks/060-setup-elasticsearch.yml \\\n  -e target_host=rancher-desktop \\\n  -e elasticsearch_version=8.16.1\n</code></pre>"},{"location":"package-search-elasticsearch/#prerequisites","title":"Prerequisites","text":"<p>Before deploying Elasticsearch, ensure the required secrets are configured in <code>urbalurba-secrets</code>:</p> <ul> <li><code>ELASTICSEARCH_USERNAME</code>: Elasticsearch admin username (default: elastic)</li> <li><code>ELASTICSEARCH_PASSWORD</code>: Elasticsearch admin password</li> </ul>"},{"location":"package-search-elasticsearch/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"package-search-elasticsearch/#elasticsearch-configuration","title":"Elasticsearch Configuration","text":"<p>Elasticsearch uses the Bitnami Elasticsearch 8.16.1 image with security and authentication enabled:</p> <pre><code># From manifests/060-elasticsearch-config.yaml\nservice:\n  type: ClusterIP\n\n# Single-node configuration for development\nmaster:\n  replicaCount: 1\ndata:\n  replicaCount: 0\ncoordinating:\n  replicaCount: 0\ningest:\n  enabled: false\n\n# Security configuration\nsecurity:\n  enabled: true\n  elasticPassword: # Set by playbook using --set security.elasticPassword=&lt;password&gt;\n\narchitecture: standalone\n</code></pre>"},{"location":"package-search-elasticsearch/#helm-configuration","title":"Helm Configuration","text":"<pre><code># Deployment command (from Ansible playbook)\nhelm upgrade --install elasticsearch bitnami/elasticsearch \\\n  -f manifests/060-elasticsearch-config.yaml \\\n  --set security.elasticPassword=\"$ELASTICSEARCH_PASSWORD\" \\\n  --set image.tag=8.16.1 \\\n  --set master.replicaCount=1 \\\n  --set data.replicaCount=0 \\\n  --set coordinating.replicaCount=0 \\\n  --set ingest.enabled=false\n</code></pre>"},{"location":"package-search-elasticsearch/#resource-configuration","title":"Resource Configuration","text":"<pre><code># Resource limits and requests\nresources:\n  requests:\n    memory: \"512Mi\"\n    cpu: \"250m\"\n  limits:\n    memory: \"1Gi\"\n    cpu: \"500m\"\n\n# Storage configuration\npersistence:\n  enabled: true\n  size: 8Gi\n  accessMode: ReadWriteOnce\n</code></pre>"},{"location":"package-search-elasticsearch/#security-configuration","title":"Security Configuration","text":"<pre><code># Authentication configuration\nsecurity:\n  enabled: true\n\n# JVM heap configuration\nextraConfiguration: |\n  -Xms512m\n  -Xmx512m\n</code></pre>"},{"location":"package-search-elasticsearch/#monitoring-verification","title":"\ud83d\udd0d Monitoring &amp; Verification","text":""},{"location":"package-search-elasticsearch/#health-checks","title":"Health Checks","text":"<pre><code># Check pod status\nkubectl get pods -l app.kubernetes.io/name=elasticsearch\n\n# Check StatefulSet status\nkubectl get statefulset elasticsearch\n\n# Check service status\nkubectl get svc elasticsearch\n\n# View Elasticsearch logs\nkubectl logs -l app.kubernetes.io/name=elasticsearch\n</code></pre>"},{"location":"package-search-elasticsearch/#elasticsearch-connection-testing","title":"Elasticsearch Connection Testing","text":"<pre><code># Test cluster health\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X GET \\\n  \"http://localhost:9200/_cluster/health?pretty\"\n\n# Check cluster nodes\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X GET \\\n  \"http://localhost:9200/_nodes?pretty\"\n\n# Test basic functionality\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X GET \\\n  \"http://localhost:9200/\"\n\n# Check service endpoints\nkubectl get endpoints elasticsearch\n</code></pre>"},{"location":"package-search-elasticsearch/#port-forward-for-external-access","title":"Port Forward for External Access","text":"<pre><code># Port forward for local access\nkubectl port-forward svc/elasticsearch 9200:9200\n\n# Access via browser or curl\n# URL: http://localhost:9200\n# Username: elastic (from secrets)\n# Password: [from secrets]\n\n# Test with curl\ncurl -u \"elastic:password\" \"http://localhost:9200/_cluster/health?pretty\"\n</code></pre>"},{"location":"package-search-elasticsearch/#automated-verification","title":"Automated Verification","text":"<p>The deployment includes comprehensive testing of Elasticsearch functionality:</p> <p>Verification Process: 1. Two-stage pod readiness: Waits for Running and Ready status 2. Cluster health check: Verifies cluster status (green/yellow) 3. Index creation test: Creates a test index to verify write operations 4. Document indexing test: Indexes a test document 5. Search functionality test: Performs search query to verify read operations 6. Cleanup: Removes test index after verification</p>"},{"location":"package-search-elasticsearch/#management-operations","title":"\ud83d\udee0\ufe0f Management Operations","text":""},{"location":"package-search-elasticsearch/#elasticsearch-administration","title":"Elasticsearch Administration","text":"<pre><code># Access Elasticsearch container\nkubectl exec -it elasticsearch-0 -- bash\n\n# Check cluster status\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X GET \\\n  \"http://localhost:9200/_cluster/health?pretty\"\n\n# List all indices\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X GET \\\n  \"http://localhost:9200/_cat/indices?v\"\n\n# Check node information\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X GET \\\n  \"http://localhost:9200/_nodes/stats?pretty\"\n\n# Monitor cluster stats\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X GET \\\n  \"http://localhost:9200/_cluster/stats?pretty\"\n</code></pre>"},{"location":"package-search-elasticsearch/#index-management","title":"Index Management","text":"<pre><code># Create an index\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X PUT \\\n  \"http://localhost:9200/my-index\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"settings\":{\"number_of_shards\":1,\"number_of_replicas\":0}}'\n\n# Check index settings\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X GET \\\n  \"http://localhost:9200/my-index/_settings?pretty\"\n\n# Delete an index\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X DELETE \\\n  \"http://localhost:9200/my-index\"\n\n# List index mapping\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X GET \\\n  \"http://localhost:9200/my-index/_mapping?pretty\"\n</code></pre>"},{"location":"package-search-elasticsearch/#document-operations","title":"Document Operations","text":"<pre><code># Index a document\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X POST \\\n  \"http://localhost:9200/my-index/_doc\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"title\":\"My Document\",\"content\":\"This is test content\",\"timestamp\":\"2025-09-23T00:00:00Z\"}'\n\n# Get a document by ID\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X GET \\\n  \"http://localhost:9200/my-index/_doc/DOCUMENT_ID\"\n\n# Search documents\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X GET \\\n  \"http://localhost:9200/my-index/_search?q=title:Document&amp;pretty\"\n\n# Update a document\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X POST \\\n  \"http://localhost:9200/my-index/_doc/DOCUMENT_ID/_update\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"doc\":{\"content\":\"Updated content\"}}'\n\n# Delete a document\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X DELETE \\\n  \"http://localhost:9200/my-index/_doc/DOCUMENT_ID\"\n</code></pre>"},{"location":"package-search-elasticsearch/#service-removal","title":"Service Removal","text":"<pre><code># Remove Elasticsearch service (preserves data by default)\ncd provision-host/kubernetes/04-search/not-in-use/\n./07-remove-elasticsearch.sh rancher-desktop\n\n# Completely remove including data\nansible-playbook ansible/playbooks/060-remove-elasticsearch.yml \\\n  -e target_host=rancher-desktop\n</code></pre> <p>Removal Process: - Uninstalls Elasticsearch Helm release - Waits for pods to terminate - Removes persistent volume claims and services - Preserves urbalurba-secrets and namespace structure - Provides data retention options and recovery instructions</p>"},{"location":"package-search-elasticsearch/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-search-elasticsearch/#common-issues","title":"Common Issues","text":"<p>Pod Won't Start: <pre><code># Check pod events and logs\nkubectl describe pod -l app.kubernetes.io/name=elasticsearch\nkubectl logs -l app.kubernetes.io/name=elasticsearch\n\n# Check Java heap issues\nkubectl exec -it elasticsearch-0 -- cat /opt/bitnami/elasticsearch/config/jvm.options\n\n# Check disk space\nkubectl exec -it elasticsearch-0 -- df -h\n</code></pre></p> <p>Authentication Issues: <pre><code># Check Elasticsearch credentials in secrets\nkubectl get secret urbalurba-secrets -o jsonpath=\"{.data.ELASTICSEARCH_USERNAME}\" | base64 -d\nkubectl get secret urbalurba-secrets -o jsonpath=\"{.data.ELASTICSEARCH_PASSWORD}\" | base64 -d\n\n# Test authentication\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" \\\n  \"http://localhost:9200/_security/_authenticate?pretty\"\n\n# Reset elastic password (if needed)\nkubectl exec -it elasticsearch-0 -- /opt/bitnami/elasticsearch/bin/elasticsearch-reset-password -u elastic\n</code></pre></p> <p>Connection Issues: <pre><code># Verify service endpoints\nkubectl describe svc elasticsearch\nkubectl get endpoints elasticsearch\n\n# Test DNS resolution\nkubectl run test-pod --image=busybox --rm -it -- \\\n  nslookup elasticsearch.default.svc.cluster.local\n\n# Check Elasticsearch node status\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s \\\n  \"http://localhost:9200/_nodes?pretty\"\n</code></pre></p> <p>Performance Issues: <pre><code># Check resource usage\nkubectl top pod elasticsearch-0\n\n# Monitor Elasticsearch performance\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s \\\n  \"http://localhost:9200/_nodes/stats/indices,jvm,process?pretty\"\n\n# Check heap usage\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s \\\n  \"http://localhost:9200/_nodes/stats/jvm?pretty\"\n\n# Monitor slow queries\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s \\\n  \"http://localhost:9200/_nodes/stats/indices/search?pretty\"\n</code></pre></p> <p>Index Issues: <pre><code># Check index health\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s \\\n  \"http://localhost:9200/_cat/indices?v&amp;health=yellow,red\"\n\n# Check shard allocation\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s \\\n  \"http://localhost:9200/_cat/shards?v\"\n\n# Force index recovery\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X POST \\\n  \"http://localhost:9200/_recovery?pretty\"\n\n# Check cluster allocation explanation\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s \\\n  \"http://localhost:9200/_cluster/allocation/explain?pretty\"\n</code></pre></p>"},{"location":"package-search-elasticsearch/#maintenance","title":"\ud83d\udccb Maintenance","text":""},{"location":"package-search-elasticsearch/#regular-tasks","title":"Regular Tasks","text":"<ol> <li>Health Monitoring: Check cluster health and node status daily</li> <li>Index Monitoring: Monitor index sizes, shard allocation, and performance</li> <li>Backup Schedule: Implement regular snapshots of indices</li> <li>Performance Monitoring: Monitor query performance, heap usage, and disk space</li> </ol>"},{"location":"package-search-elasticsearch/#backup-procedures","title":"Backup Procedures","text":"<pre><code># Create snapshot repository (filesystem)\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X PUT \\\n  \"http://localhost:9200/_snapshot/backup_repo\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"type\":\"fs\",\"settings\":{\"location\":\"/opt/bitnami/elasticsearch/snapshots\"}}'\n\n# Create a snapshot\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X PUT \\\n  \"http://localhost:9200/_snapshot/backup_repo/snapshot_$(date +%Y%m%d_%H%M%S)\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"indices\":\"*\",\"ignore_unavailable\":true,\"include_global_state\":false}'\n\n# List snapshots\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X GET \\\n  \"http://localhost:9200/_snapshot/backup_repo/_all?pretty\"\n\n# Copy snapshot data\nkubectl cp elasticsearch-0:/opt/bitnami/elasticsearch/snapshots \\\n  ./elasticsearch-backup-$(date +%Y%m%d)/\n</code></pre>"},{"location":"package-search-elasticsearch/#disaster-recovery","title":"Disaster Recovery","text":"<pre><code># Restore from snapshot\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X POST \\\n  \"http://localhost:9200/_snapshot/backup_repo/SNAPSHOT_NAME/_restore\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"indices\":\"*\",\"ignore_unavailable\":true,\"include_global_state\":false}'\n\n# Monitor restore progress\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X GET \\\n  \"http://localhost:9200/_recovery?pretty\"\n\n# Restore specific indices\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X POST \\\n  \"http://localhost:9200/_snapshot/backup_repo/SNAPSHOT_NAME/_restore\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"indices\":\"index1,index2\",\"rename_pattern\":\"(.+)\",\"rename_replacement\":\"restored_$1\"}'\n</code></pre>"},{"location":"package-search-elasticsearch/#use-cases","title":"\ud83d\ude80 Use Cases","text":""},{"location":"package-search-elasticsearch/#full-text-search","title":"Full-Text Search","text":"<pre><code># Create search index with text analysis\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X PUT \\\n  \"http://localhost:9200/search_index\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"settings\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"my_analyzer\": {\n            \"tokenizer\": \"standard\",\n            \"filter\": [\"lowercase\", \"stop\"]\n          }\n        }\n      }\n    },\n    \"mappings\": {\n      \"properties\": {\n        \"title\": {\"type\": \"text\", \"analyzer\": \"my_analyzer\"},\n        \"content\": {\"type\": \"text\", \"analyzer\": \"my_analyzer\"},\n        \"tags\": {\"type\": \"keyword\"}\n      }\n    }\n  }'\n\n# Index searchable documents\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X POST \\\n  \"http://localhost:9200/search_index/_doc\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"title\":\"Getting Started with Elasticsearch\",\"content\":\"Elasticsearch is a powerful search engine\",\"tags\":[\"search\",\"tutorial\"]}'\n\n# Perform full-text search\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X GET \\\n  \"http://localhost:9200/search_index/_search\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"query\":{\"match\":{\"content\":\"search engine\"}},\"highlight\":{\"fields\":{\"content\":{}}}}'\n</code></pre>"},{"location":"package-search-elasticsearch/#analytics-and-aggregations","title":"Analytics and Aggregations","text":"<pre><code># Create analytics index\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X PUT \\\n  \"http://localhost:9200/analytics\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"mappings\": {\n      \"properties\": {\n        \"timestamp\": {\"type\": \"date\"},\n        \"user_id\": {\"type\": \"keyword\"},\n        \"action\": {\"type\": \"keyword\"},\n        \"value\": {\"type\": \"double\"}\n      }\n    }\n  }'\n\n# Index analytics data\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X POST \\\n  \"http://localhost:9200/analytics/_doc\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"timestamp\":\"2025-09-23T10:00:00Z\",\"user_id\":\"user123\",\"action\":\"login\",\"value\":1}'\n\n# Perform aggregations\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X GET \\\n  \"http://localhost:9200/analytics/_search\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"size\": 0,\n    \"aggs\": {\n      \"actions_per_hour\": {\n        \"date_histogram\": {\n          \"field\": \"timestamp\",\n          \"calendar_interval\": \"hour\"\n        }\n      },\n      \"top_actions\": {\n        \"terms\": {\n          \"field\": \"action\",\n          \"size\": 10\n        }\n      }\n    }\n  }'\n</code></pre>"},{"location":"package-search-elasticsearch/#log-analysis","title":"Log Analysis","text":"<pre><code># Create log index with timestamp field\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X PUT \\\n  \"http://localhost:9200/logs\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"mappings\": {\n      \"properties\": {\n        \"@timestamp\": {\"type\": \"date\"},\n        \"level\": {\"type\": \"keyword\"},\n        \"message\": {\"type\": \"text\"},\n        \"service\": {\"type\": \"keyword\"},\n        \"host\": {\"type\": \"keyword\"}\n      }\n    }\n  }'\n\n# Index log entries\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X POST \\\n  \"http://localhost:9200/logs/_doc\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"@timestamp\":\"2025-09-23T10:00:00Z\",\"level\":\"ERROR\",\"message\":\"Database connection failed\",\"service\":\"api\",\"host\":\"server1\"}'\n\n# Search logs by level and time range\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X GET \\\n  \"http://localhost:9200/logs/_search\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"query\": {\n      \"bool\": {\n        \"filter\": [\n          {\"term\": {\"level\": \"ERROR\"}},\n          {\"range\": {\"@timestamp\": {\"gte\": \"2025-09-23T09:00:00Z\", \"lte\": \"2025-09-23T11:00:00Z\"}}}\n        ]\n      }\n    },\n    \"sort\": [{\"@timestamp\": {\"order\": \"desc\"}}]\n  }'\n</code></pre>"},{"location":"package-search-elasticsearch/#geospatial-search","title":"Geospatial Search","text":"<pre><code># Create geospatial index\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X PUT \\\n  \"http://localhost:9200/locations\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"mappings\": {\n      \"properties\": {\n        \"name\": {\"type\": \"text\"},\n        \"location\": {\"type\": \"geo_point\"},\n        \"category\": {\"type\": \"keyword\"}\n      }\n    }\n  }'\n\n# Index location data\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X POST \\\n  \"http://localhost:9200/locations/_doc\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"name\":\"Coffee Shop\",\"location\":{\"lat\":59.9139,\"lon\":10.7522},\"category\":\"restaurant\"}'\n\n# Search by distance\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s -X GET \\\n  \"http://localhost:9200/locations/_search\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"query\": {\n      \"geo_distance\": {\n        \"distance\": \"5km\",\n        \"location\": {\"lat\": 59.9139, \"lon\": 10.7522}\n      }\n    }\n  }'\n</code></pre> <p>\ud83d\udca1 Key Insight: Elasticsearch provides powerful search and analytics capabilities that enable real-time data exploration, full-text search, and complex aggregations. Use Elasticsearch for implementing search functionality, log analysis, monitoring dashboards, and any application requiring fast, scalable document storage and retrieval.</p>"},{"location":"package-search-readme/","title":"Search Services - Overview and Management","text":"<p>Key Components: Elasticsearch \u2022 Full-Text Search \u2022 Analytics Engine \u2022 Document Storage \u2022 Real-Time Search</p> <p>File: <code>docs/package-search-readme.md</code> Purpose: Overview of search services available in Urbalurba infrastructure Target Audience: Developers, architects, and operations teams implementing search functionality Last Updated: September 23, 2025</p>"},{"location":"package-search-readme/#overview","title":"\ud83d\udccb Overview","text":"<p>The Search Services package provides powerful search and analytics capabilities for applications requiring advanced text search, real-time analytics, and document storage. These services enable full-text search, data exploration, log analysis, and complex aggregations across your applications.</p> <p>Available Services: - Elasticsearch: Primary search and analytics engine with RESTful API - Future Services: Potential integration with OpenSearch, Solr, or specialized search solutions</p>"},{"location":"package-search-readme/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"package-search-readme/#service-categories","title":"Service Categories","text":"<pre><code>Search Services Stack:\n\u251c\u2500\u2500 04-search/\n\u2502   \u251c\u2500\u2500 not-in-use/\n\u2502   \u2502   \u251c\u2500\u2500 07-setup-elasticsearch.sh    # Elasticsearch deployment\n\u2502   \u2502   \u2514\u2500\u2500 07-remove-elasticsearch.sh   # Elasticsearch removal\n\u2502   \u2514\u2500\u2500 (Future search services)\n\u251c\u2500\u2500 manifests/\n\u2502   \u2514\u2500\u2500 060-elasticsearch-config.yaml    # Elasticsearch configuration\n\u2514\u2500\u2500 ansible/playbooks/\n    \u251c\u2500\u2500 060-setup-elasticsearch.yml      # Elasticsearch deployment logic\n    \u2514\u2500\u2500 060-remove-elasticsearch.yml     # Elasticsearch removal logic\n</code></pre>"},{"location":"package-search-readme/#service-integration","title":"Service Integration","text":"<pre><code>Application Layer\n\u251c\u2500\u2500 Search API (Full-text search)\n\u251c\u2500\u2500 Analytics Dashboard (Real-time metrics)\n\u251c\u2500\u2500 Log Aggregation (Centralized logging)\n\u2514\u2500\u2500 Data Exploration (Interactive queries)\n    \u2193\nSearch Layer\n\u251c\u2500\u2500 Elasticsearch (8.16.1)\n\u2502   \u251c\u2500\u2500 Full-text search engine\n\u2502   \u251c\u2500\u2500 Real-time analytics\n\u2502   \u251c\u2500\u2500 Document storage\n\u2502   \u2514\u2500\u2500 RESTful API\n\u2514\u2500\u2500 (Future: OpenSearch, Solr)\n    \u2193\nStorage Layer\n\u251c\u2500\u2500 Persistent Volumes (Document storage)\n\u251c\u2500\u2500 Index Storage (Optimized for search)\n\u2514\u2500\u2500 Backup Storage (Snapshots)\n</code></pre>"},{"location":"package-search-readme/#available-services","title":"\ud83d\ude80 Available Services","text":""},{"location":"package-search-readme/#elasticsearch-search-analytics-engine","title":"Elasticsearch - Search &amp; Analytics Engine \ud83d\udd0d","text":"<p>Status: Available (not-in-use) | Port: 9200 | Type: Search Engine</p> <p>Key Capabilities: Full-Text Search \u2022 Real-Time Analytics \u2022 Document Storage \u2022 RESTful API \u2022 Query DSL \u2022 Aggregations \u2022 Log Analysis</p> <p>Elasticsearch provides comprehensive search and analytics capabilities for applications requiring advanced text search, real-time data exploration, and document storage. Essential for implementing search functionality, log aggregation, and analytics dashboards.</p> <p>Key Features: - Search Engine: Advanced full-text search with relevance scoring - Analytics Platform: Real-time aggregations and data analysis - Document Store: JSON document storage with automatic indexing - RESTful API: HTTP-based API for all operations - Security: Centralized authentication via urbalurba-secrets</p> <p>\ud83d\udcda Complete Documentation \u2192</p>"},{"location":"package-search-readme/#service-selection-guide","title":"\ud83d\udd04 Service Selection Guide","text":""},{"location":"package-search-readme/#when-to-use-elasticsearch","title":"When to Use Elasticsearch","text":"<p>\u2705 Choose Elasticsearch for: - Full-text search requirements: Product catalogs, content management, documentation search - Real-time analytics: Dashboards, metrics, business intelligence - Log aggregation: Centralized logging from multiple services - Complex data exploration: Interactive queries, faceted search - High-volume data: Large datasets requiring fast search and aggregation - RESTful integration: Applications preferring HTTP API access</p> <p>\u274c Consider alternatives when: - Simple key-value lookup: Use Redis or database indices instead - Very small datasets: Database LIKE queries may be sufficient - Real-time requirements: Sub-millisecond latency needs (use Redis) - Memory constraints: Elasticsearch requires significant RAM for optimal performance</p>"},{"location":"package-search-readme/#search-service-decision-matrix","title":"Search Service Decision Matrix","text":"Use Case Volume Complexity Real-Time Recommended Service Product Search High High Moderate Elasticsearch Content Search Medium High Low Elasticsearch Log Analysis High Medium Low Elasticsearch Analytics Dashboard High High High Elasticsearch Simple Search Low Low High Database + Redis Autocomplete Medium Low High Elasticsearch + Redis Document Storage High Medium Low Elasticsearch Geospatial Search Medium High Moderate Elasticsearch"},{"location":"package-search-readme/#common-operations","title":"\ud83d\udee0\ufe0f Common Operations","text":""},{"location":"package-search-readme/#service-management","title":"Service Management","text":"<pre><code># Check search services\nkubectl get pods -l app.kubernetes.io/name=elasticsearch\n\n# Monitor service logs\nkubectl logs -l app.kubernetes.io/name=elasticsearch -f\n\n# Check service status\nkubectl get svc elasticsearch\n</code></pre>"},{"location":"package-search-readme/#quick-health-check","title":"Quick Health Check","text":"<pre><code># Elasticsearch health\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" -s \\\n  \"http://localhost:9200/_cluster/health\"\n\n# Resource usage\nkubectl top pod elasticsearch-0\n</code></pre>"},{"location":"package-search-readme/#deployremove-services","title":"Deploy/Remove Services","text":"<pre><code># Deploy Elasticsearch\ncd provision-host/kubernetes/04-search/not-in-use/\n./07-setup-elasticsearch.sh rancher-desktop\n\n# Remove Elasticsearch\n./07-remove-elasticsearch.sh rancher-desktop\n</code></pre>"},{"location":"package-search-readme/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"package-search-readme/#common-issues","title":"Common Issues","text":"<ul> <li>Pod Won't Start: Check pod events, logs, and Java heap settings</li> <li>Search Performance: Monitor cluster stats, heap usage, and query performance</li> <li>Authentication Failed: Verify urbalurba-secrets configuration</li> <li>Index Issues: Check index health, shard allocation, and cluster status</li> </ul>"},{"location":"package-search-readme/#quick-diagnostics","title":"Quick Diagnostics","text":"<pre><code># Check pod status\nkubectl describe pod elasticsearch-0\nkubectl logs elasticsearch-0\n\n# Verify credentials\nkubectl get secret urbalurba-secrets -o jsonpath=\"{.data.ELASTICSEARCH_PASSWORD}\" | base64 -d\n\n# Test connectivity\nkubectl exec -it elasticsearch-0 -- curl -u \"elastic:password\" \\\n  \"http://localhost:9200/_cluster/health\"\n</code></pre>"},{"location":"package-search-readme/#service-documentation","title":"\ud83d\udccb Service Documentation","text":""},{"location":"package-search-readme/#detailed-service-guides","title":"Detailed Service Guides","text":"<ul> <li>Elasticsearch: Complete deployment, configuration, and management guide</li> <li>Future Services: Documentation will be added as new search services are integrated</li> </ul>"},{"location":"package-search-readme/#related-documentation","title":"Related Documentation","text":"<ul> <li>Infrastructure Rules: Deployment standards and patterns</li> <li>Secrets Management: Authentication and credential management</li> <li>Troubleshooting: General troubleshooting procedures</li> </ul>"},{"location":"package-search-readme/#integration-examples","title":"\ud83d\ude80 Integration Examples","text":""},{"location":"package-search-readme/#common-integration-patterns","title":"Common Integration Patterns","text":"<ul> <li>Full-Text Search: Product catalogs, documentation search, content discovery</li> <li>Log Aggregation: Centralized logging from multiple services</li> <li>Analytics Dashboards: Real-time metrics and business intelligence</li> <li>Monitoring: Application performance and system health tracking</li> </ul> <p>See detailed implementation examples in Elasticsearch Documentation</p> <p>\ud83d\udca1 Key Insight: Search services are essential for modern applications requiring advanced data discovery, real-time analytics, and full-text search capabilities. Choose Elasticsearch for comprehensive search and analytics needs, and consider complementary services like Redis for high-speed caching and simple lookups to create a complete search architecture.</p>"},{"location":"provision-host-kubernetes/","title":"Provision Host for Kubernetes","text":"<p>Purpose: User guide for managing and deploying applications on Kubernetes clusters using the Urbalurba automated provisioning system.</p> <p>Target Audience: Users who want to deploy, activate, or manage services on their Kubernetes cluster.</p>"},{"location":"provision-host-kubernetes/#overview","title":"Overview","text":"<p>The provision-host container provides a complete automated deployment system for Kubernetes applications. Simply run <code>./install-rancher.sh</code> and all active services deploy automatically in the correct order.</p> <p>See also overview-system-architecture.md </p> <p>Key Benefits: - \u2705 Fully Automated: One command deploys entire cluster - \u2705 Dependency Management: Services deploy in correct order automatically - \u2705 Easy Service Management: Move scripts in/out of <code>not-in-use/</code> folders to control what gets deployed when the cluster is built - \u2705 Safe Removal: Removal scripts are protected from accidental execution</p> <p>For Technical Details: See Rules for Automated Kubernetes Deployment</p>"},{"location":"provision-host-kubernetes/#service-categories","title":"Service Categories","text":"<p>Services are organized by category to ensure proper deployment order. Each category contains setup and removal scripts for different applications.</p> <p>Current Service Categories:</p> <pre><code>/mnt/urbalurbadisk/provision-host/kubernetes/\n\u251c\u2500\u2500 01-core/\n\u2502   \u251c\u2500\u2500 020-setup-nginx.sh\n\u2502   \u2514\u2500\u2500 not-in-use\n\u251c\u2500\u2500 02-databases/\n\u2502   \u251c\u2500\u2500 05-setup-postgres.sh\n\u2502   \u2514\u2500\u2500 not-in-use/\n\u2502       \u251c\u2500\u2500 04-setup-mongodb.sh\n\u2502       \u2514\u2500\u2500 08-setup-mssql.sh\n\u251c\u2500\u2500 03-queues/\n\u2502   \u251c\u2500\u2500 06-setup-redis.sh\n\u2502   \u2514\u2500\u2500 not-in-use/\n\u2502       \u2514\u2500\u2500 08-setup-rabbitmq.sh\n\u251c\u2500\u2500 04-search/\n\u2502   \u2514\u2500\u2500 not-in-use/\n\u2502       \u2514\u2500\u2500 07-setup-elasticsearch.sh\n\u251c\u2500\u2500 05-apim/\n\u2502   \u2514\u2500\u2500 not-in-use/\n\u2502       \u2514\u2500\u2500 09-setup-gravitee.sh\n\u251c\u2500\u2500 06-management/\n\u2502   \u2514\u2500\u2500 not-in-use/\n\u2502       \u2514\u2500\u2500 03-setup-pgadmin.sh\n\u251c\u2500\u2500 07-ai/\n\u2502   \u251c\u2500\u2500 01-setup-litellm-openwebui.sh\n\u2502   \u2514\u2500\u2500 not-in-use/\n\u2502       \u2514\u2500\u2500 02-setup-open-webui.sh\n\u251c\u2500\u2500 08-development/\n\u2502   \u251c\u2500\u2500 02-setup-argocd.sh\n\u2502   \u2514\u2500\u2500 not-in-use\n\u251c\u2500\u2500 09-network/\n\u2502   \u251c\u2500\u2500 01-tailscale-net-start.sh\n\u2502   \u2514\u2500\u2500 not-in-use\n\u251c\u2500\u2500 10-datascience/\n\u2502   \u2514\u2500\u2500 not-in-use/\n\u2502       \u251c\u2500\u2500 unity-catalog setup scripts\n\u2502       \u2514\u2500\u2500 jupyter setup scripts\n\u251c\u2500\u2500 11-monitoring/\n\u2502   \u2514\u2500\u2500 not-in-use/\n\u2502       \u2514\u2500\u2500 monitoring setup scripts\n\u251c\u2500\u2500 12-auth/\n\u2502   \u251c\u2500\u2500 01-setup-authentik.sh\n\u2502   \u2514\u2500\u2500 not-in-use/\n\u251c\u2500\u2500 not-used-apps/\n\u2502   \u2514\u2500\u2500 04-cloud-setup-log-monitor.sh\n\u2514\u2500\u2500 provision-kubernetes.sh\n</code></pre>"},{"location":"provision-host-kubernetes/#how-automated-deployment-works","title":"How Automated Deployment Works","text":"<p>When you run <code>./install-rancher.sh</code>, it automatically calls the deployment system which:</p> <ol> <li>Deploys Core Systems First: Networking, storage, DNS infrastructure</li> <li>Then Databases: PostgreSQL, Redis, and other data services</li> <li>Then Applications: AI services, authentication, monitoring, etc.</li> <li>Provides Progress Updates: Shows what's being deployed and any issues</li> <li>Generates Summary Report: Complete status of all deployments</li> </ol>"},{"location":"provision-host-kubernetes/#managing-active-services","title":"Managing Active Services","text":"<p>\ud83c\udf9b\ufe0f Control What Gets Deployed:</p> <p>Each category has a <code>not-in-use/</code> folder containing optional services. You control your cluster configuration by moving scripts:</p> <ul> <li>\ud83d\udcc1 Active Services: Scripts in the category folder (e.g., <code>07-ai/01-setup-litellm-openwebui.sh</code>)</li> <li>\ud83d\udcc1 Inactive Services: Scripts in <code>not-in-use/</code> folder (e.g., <code>07-ai/not-in-use/02-setup-open-webui.sh</code>)</li> </ul> <p>Technical Details: See Active vs Inactive Management in the rules documentation.</p>"},{"location":"provision-host-kubernetes/#quick-start-guide","title":"Quick Start Guide","text":""},{"location":"provision-host-kubernetes/#deploy-everything-recommended","title":"\ud83d\ude80 Deploy Everything (Recommended)","text":"<p>The easiest way to get a complete cluster:</p> <pre><code># From your host machine in the repository root:\n./install-rancher.sh\n</code></pre> <p>This automatically: 1. Sets up the provision-host container 2. Deploys all active services in dependency order 3. Provides a complete working cluster</p>"},{"location":"provision-host-kubernetes/#deploy-individual-services","title":"\ud83c\udfaf Deploy Individual Services","text":"<p>If you only want to deploy specific services manually:</p> <pre><code># Access the provision-host container:\ndocker exec -it provision-host bash\n\n# Deploy a specific service:\ncd /mnt/urbalurbadisk/provision-host/kubernetes/07-ai\n./01-setup-litellm-openwebui.sh rancher-desktop\n</code></pre>"},{"location":"provision-host-kubernetes/#declarative-cluster-configuration","title":"Declarative Cluster Configuration","text":"<p>The system is designed to build a complete, reproducible cluster every time. Your repository configuration determines exactly what services get deployed automatically.</p>"},{"location":"provision-host-kubernetes/#how-it-works","title":"\ud83c\udfaf How It Works","text":"<p>The repository is your cluster blueprint: - Services in category folders \u2192 Deploy automatically during cluster build - Services in <code>not-in-use/</code> folders \u2192 Available but not deployed - Every <code>./install-rancher.sh</code> creates the exact same cluster based on your current configuration</p>"},{"location":"provision-host-kubernetes/#configure-your-cluster","title":"\u2699\ufe0f Configure Your Cluster","text":"<p>To include a service in automatic deployment: <pre><code># Move setup script to category folder (from your host machine):\ncd provision-host/kubernetes/02-databases\nmv not-in-use/04-setup-mongodb.sh ./\n\n# Now MongoDB deploys automatically on every cluster rebuild\n./install-rancher.sh\n</code></pre></p> <p>To exclude a service from automatic deployment: <pre><code># Move setup script to not-in-use folder:\ncd provision-host/kubernetes/02-databases\nmv 04-setup-mongodb.sh not-in-use/\n\n# Now MongoDB won't deploy automatically\n./install-rancher.sh\n</code></pre></p>"},{"location":"provision-host-kubernetes/#manual-service-deployment","title":"\ud83d\ude80 Manual Service Deployment","text":"<p>Deploy a service without changing automatic configuration: <pre><code># Run script directly from not-in-use folder:\ndocker exec -it provision-host bash\ncd /mnt/urbalurbadisk/provision-host/kubernetes/02-databases/not-in-use\n./04-setup-mongodb.sh rancher-desktop\n</code></pre></p> <p>Remove a deployed service: <pre><code># Run removal script (always kept in not-in-use for safety):\ndocker exec -it provision-host bash\ncd /mnt/urbalurbadisk/provision-host/kubernetes/02-databases/not-in-use\n./04-remove-mongodb.sh rancher-desktop\n</code></pre></p>"},{"location":"provision-host-kubernetes/#benefits-of-this-approach","title":"\ud83d\udd04 Benefits of This Approach","text":"<ul> <li>\u2705 Reproducible: Same cluster configuration every rebuild</li> <li>\u2705 Version Controlled: Your cluster config is in git</li> <li>\u2705 Flexible: Test services manually before adding to automatic deployment</li> <li>\u2705 Safe: Removal scripts never run automatically</li> </ul>"},{"location":"provision-host-kubernetes/#available-services","title":"Available Services","text":"<p>The platform includes a comprehensive set of services organized by category:</p> Category Active Services Available (Inactive) Services \ud83d\udd27 Core Systems Nginx \ud83d\uddc4\ufe0f Databases PostgreSQL, Redis MongoDB, MySQL, MSSQL \ud83d\udd0d Search &amp; Queues Elasticsearch, RabbitMQ \ud83d\udeaa API Management Gravitee \u26a1 Management Tools pgAdmin, phpMyAdmin \ud83e\udd16 AI Services LiteLLM + OpenWebUI OpenWebUI (standalone) \ud83d\udd04 Development ArgoCD \ud83c\udf10 Network Tailscale \ud83d\udd10 Authentication Authentik Keycloak <p>Legend: - Active Services: Deploy automatically with <code>./install-rancher.sh</code> - Available Services: In <code>not-in-use/</code> folders, can be activated by moving to parent directory</p>"},{"location":"provision-host-kubernetes/#access-your-services","title":"Access Your Services","text":"<p>After deployment, access services via:</p> <ul> <li>Local Development: <code>http://service-name.localhost</code> (e.g., <code>http://authentik.localhost</code>)</li> <li>External Access: Configure via Cloudflare tunnels or Tailscale</li> <li>Port Forward: <code>kubectl port-forward svc/service-name local-port:service-port -n namespace</code></li> </ul>"},{"location":"provision-host-kubernetes/#technical-reference","title":"Technical Reference","text":"<p>For developers and advanced users: - \ud83d\udccb Automated Deployment Rules - How the orchestration system works - \ud83d\udd27 Provisioning Rules - How to write deployment scripts - \ud83d\udcd6 Provision Host Overview - Complete platform documentation</p>"},{"location":"provision-host-rancher/","title":"Provision Host Rancher Desktop Guide","text":"<p>File: <code>docs/provision-host-rancher.md</code> Purpose: Rancher Desktop specific setup and MicroK8s compatibility Target Audience: Users migrating from MicroK8s or troubleshooting Rancher Desktop issues</p>"},{"location":"provision-host-rancher/#overview","title":"Overview","text":"<p>While Rancher Desktop is now the default Kubernetes provider, the provision host includes a compatibility layer for existing MicroK8s-based scripts and configurations. This guide covers Rancher Desktop specifics and migration considerations.</p>"},{"location":"provision-host-rancher/#microk8s-compatibility-layer","title":"MicroK8s Compatibility Layer","text":"<p>The provision host automatically creates compatibility aliases so existing MicroK8s scripts work without modification:</p>"},{"location":"provision-host-rancher/#context-aliasing","title":"Context Aliasing","text":"<ul> <li>MicroK8s: Uses <code>default</code> as the primary context name</li> <li>Rancher Desktop: Uses <code>rancher-desktop</code> as the primary context</li> <li>Compatibility: Creates a <code>default</code> context alias pointing to the <code>rancher-desktop</code> cluster</li> </ul>"},{"location":"provision-host-rancher/#storage-class-mapping","title":"Storage Class Mapping","text":"<ul> <li>MicroK8s: Uses <code>microk8s-hostpath</code> storage class</li> <li>Rancher Desktop: Uses <code>local-path</code> storage class</li> <li>Compatibility: Creates <code>microk8s-hostpath</code> alias pointing to <code>local-path</code></li> </ul> <p>This allows scripts written for MicroK8s to run unchanged on Rancher Desktop.</p>"},{"location":"provision-host-rancher/#installation-process","title":"Installation Process","text":"<p>The <code>./install-rancher.sh</code> script automatically handles Rancher Desktop setup. Key log messages to watch for:</p>"},{"location":"provision-host-rancher/#successful-compatibility-setup","title":"Successful Compatibility Setup","text":"<pre><code>Creating 'default' context alias for rancher-desktop...\n'default' context is correctly set up\n</code></pre> <pre><code>storageclass.storage.k8s.io/microk8s-hostpath created\n</code></pre> <p>These confirm the compatibility layer is working.</p>"},{"location":"provision-host-rancher/#troubleshooting","title":"Troubleshooting","text":""},{"location":"provision-host-rancher/#context-issues","title":"Context Issues","text":"<p>Check available contexts: <code>kubectl config get-contexts</code></p> <p>Manually create default context if missing: <pre><code>kubectl config set-context default --cluster=rancher-desktop --user=rancher-desktop\n</code></pre></p>"},{"location":"provision-host-rancher/#storage-class-issues","title":"Storage Class Issues","text":"<p>Verify the alias exists: <code>kubectl get storageclass microk8s-hostpath</code></p> <p>Create manually if missing: <pre><code>kubectl apply -f /mnt/urbalurbadisk/manifests/000-storage-class-alias.yaml\n</code></pre></p> <p>Related Documentation: - Provision Host Tools Guide - Complete tool reference - Provision Host Kubernetes Guide - Service deployment</p>"},{"location":"provision-host-readme/","title":"Provision Host Documentation Guide","text":"<p>File: <code>docs/provision-host-readme.md</code> Purpose: Central entry point for all provision host documentation and guides Target Audience: Developers, DevOps engineers, and infrastructure administrators Last Updated: September 21, 2024</p>"},{"location":"provision-host-readme/#overview","title":"\ud83d\udccb Overview","text":"<p>This is the central starting point for understanding the provision host system - a comprehensive Docker container that serves as the management hub for Urbalurba infrastructure. The provision host contains all necessary tools for managing multi-cloud environments, Kubernetes clusters, and infrastructure automation.</p>"},{"location":"provision-host-readme/#what-is-the-provision-host","title":"\ud83d\udd27 What is the Provision Host?","text":"<p>The provision host is a self-contained Docker container that serves as your complete infrastructure management environment. All cluster and cloud operations are performed from within this container - no need to install any tools on your local machine.</p>"},{"location":"provision-host-readme/#container-first-approach","title":"Container-First Approach","text":"<ul> <li>No Local Tool Installation: AWS CLI, kubectl, Terraform, etc. all run in the container</li> <li>Consistent Environment: Same container works identically on Windows, Linux, and macOS</li> <li>Version Controlled: All tool versions are pinned and tested together</li> <li>Isolation: No conflicts with locally installed tools or different versions</li> </ul>"},{"location":"provision-host-readme/#fully-automated-setup","title":"Fully Automated Setup","text":"<ul> <li>One-Command Deployment: Run <code>./install-rancher.sh</code> to set up everything</li> <li>Two-Stage Process: First creates and provisions the container, then deploys all cluster services</li> <li>Zero Manual Steps: Complete infrastructure from container to running services automatically</li> </ul>"},{"location":"provision-host-readme/#documentation-guides","title":"\ud83d\udcda Documentation Guides","text":""},{"location":"provision-host-readme/#container-tools-reference","title":"Container Tools Reference","text":"<p>\ud83d\udcd6 Provision Host Tools Guide</p> <p>Complete reference for all tools and software available in the provision host container - pre-configured with all major cloud providers, Kubernetes tools, automation frameworks, and networking capabilities. Includes detailed capabilities, usage examples, and authentication setup.</p> <p>When to use: Understanding available tools, troubleshooting tool issues, cloud authentication setup</p>"},{"location":"provision-host-readme/#kubernetes-service-deployment","title":"Kubernetes Service Deployment","text":"<p>\u2638\ufe0f Provision Host Kubernetes Guide</p> <p>User guide for deploying and managing applications on Kubernetes clusters using the automated provisioning system:</p> <ul> <li>Declarative Configuration: Repository file organization determines what gets deployed automatically</li> <li>One-Command Deployment: <code>./install-rancher.sh</code> builds complete, reproducible clusters</li> <li>Service Management: Activate/deactivate services by moving scripts in/out of <code>not-in-use/</code> folders</li> <li>Available Services: AI services, databases, authentication, monitoring, and more</li> <li>Manual Operations: Deploy/test individual services without changing automatic configuration</li> </ul> <p>When to use: Setting up your cluster configuration, understanding available services, managing what gets deployed automatically</p>"},{"location":"provision-host-readme/#rancher-desktop-integration","title":"Rancher Desktop Integration","text":"<p>\ud83d\udda5\ufe0f Provision Host Rancher Guide</p> <p>Specific setup and compatibility for Rancher Desktop environments:</p> <ul> <li>Rancher Desktop Setup: Container creation and Kubernetes integration</li> <li>MicroK8s Compatibility: Context aliasing, storage class mapping</li> <li>Installation Workflow: Complete setup process and verification</li> <li>Troubleshooting: Common issues and solutions</li> </ul> <p>When to use: Using Rancher Desktop as Kubernetes provider, migrating from MicroK8s, troubleshooting Rancher-specific issues</p>"},{"location":"provision-host-readme/#quick-start-paths","title":"\ud83d\ude80 Quick Start Paths","text":""},{"location":"provision-host-readme/#new-developer-getting-started","title":"New Developer Getting Started:","text":"<ol> <li>Run <code>./install-rancher.sh</code> - One command sets up everything automatically</li> <li>Tools Guide - Understand what's available</li> <li>Kubernetes Guide - Deploy your first services</li> </ol>"},{"location":"provision-host-readme/#devops-engineer-doing-multi-cloud","title":"DevOps Engineer Doing Multi-Cloud:","text":"<ol> <li>Tools Guide - Cloud provider capabilities</li> <li>Jump to specific cloud authentication sections</li> </ol>"},{"location":"provision-host-readme/#using-rancher-desktop","title":"Using Rancher Desktop:","text":"<ol> <li>Rancher Guide - Platform-specific setup</li> <li>Kubernetes Guide - Service deployment</li> </ol>"},{"location":"provision-host-readme/#troubleshooting","title":"Troubleshooting:","text":"<ul> <li>Container issues? \u2192 Tools Guide</li> <li>Installation problems? \u2192 Setup Guide</li> <li>Service deployment failures? \u2192 Kubernetes Guide</li> <li>Rancher Desktop issues? \u2192 Rancher Guide</li> </ul>"},{"location":"provision-host-readme/#architecture-overview","title":"\ud83c\udfd7\ufe0f Architecture Overview","text":"<pre><code>Host Machine (Windows/Linux/macOS)\n\u2514\u2500\u2500 Docker + Rancher Desktop\n    \u2502\n    \u2502 ./install-rancher.sh (One Command Setup)\n    \u2502\n    \u251c\u2500\u25ba 1. Creates &amp; Provisions Container\n    \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   \u2502                  Provision Host Container                   \u2502\n    \u2502   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n    \u2502   \u2502  Cloud Tools: AWS CLI, Azure CLI, GCP SDK, OCI CLI, Terraform \u2502\n    \u2502   \u2502  K8s Tools: kubectl, Helm, k9s, Ansible                   \u2502\n    \u2502   \u2502  Network: Cloudflared, Tailscale                          \u2502\n    \u2502   \u2502  Dev Tools: GitHub CLI, Python, yq/jq                     \u2502\n    \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502                             \u2502\n    \u2514\u2500\u25ba 2. Deploys All Services   \u25bc\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                    Kubernetes Cluster                        \u2502\n        \u2502              (Rancher Desktop / MicroK8s)                    \u2502\n        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n        \u2502  Services: Authentik, PostgreSQL, Redis, OpenWebUI, etc.    \u2502\n        \u2502  Storage: PVCs, ConfigMaps, Secrets                         \u2502\n        \u2502  Networking: Traefik, Ingress, Services                     \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"provision-host-readme/#key-concepts","title":"\ud83c\udfaf Key Concepts","text":"<ul> <li>Zero Local Installation: Only Docker required on your machine - all tools run in the container</li> <li>OS Agnostic: Identical experience on Windows, Linux, and macOS</li> <li>Container-First: All management tools run in a consistent Docker environment</li> <li>Multi-Cloud Ready: Support for all major cloud providers out of the box</li> <li>Kubernetes Native: Designed for Kubernetes-first infrastructure patterns</li> <li>Automation Focused: Ansible playbooks and Infrastructure as Code</li> <li>Developer Friendly: Pre-configured tools and streamlined workflows</li> </ul>"},{"location":"provision-host-readme/#getting-help","title":"\ud83d\udcde Getting Help","text":"<ul> <li>Tool not working? Check the Tools Guide</li> <li>Setup failing? Follow the Setup Guide step by step</li> <li>Service won't deploy? Review the Kubernetes Guide</li> <li>Rancher issues? See the Rancher Guide</li> </ul> <p>Related Documentation: - Rules Documentation - Infrastructure rules and standards - Secrets Management - Security and secrets handling - Ingress Configuration - Networking and routing</p>"},{"location":"provision-host-tools/","title":"Provision Host Container Documentation","text":""},{"location":"provision-host-tools/#overview","title":"Overview","text":"<p>The provision host container is a comprehensive Docker container that serves as the central management environment for the Urbalurba infrastructure. It contains all necessary tools for managing Kubernetes clusters, cloud providers, and infrastructure automation.</p>"},{"location":"provision-host-tools/#key-features","title":"Key Features","text":"<ul> <li>Multi-Cloud Management: Full support for AWS, Azure, Google Cloud, Oracle Cloud, and Terraform</li> <li>Kubernetes Tools: kubectl, Helm, k9s, and Ansible with Kubernetes modules</li> <li>Networking: Cloudflare tunnels and Tailscale VPN capabilities</li> <li>Automation: Ansible playbooks and infrastructure as code with Terraform</li> <li>Cross-Platform: Supports both x86_64 and ARM64 architectures</li> <li>Developer Tools: GitHub CLI, Python, yq/jq for YAML/JSON processing</li> <li>Pre-configured: Ansible inventory, Helm repositories, and kubeconfig management</li> </ul>"},{"location":"provision-host-tools/#container-architecture","title":"Container Architecture","text":""},{"location":"provision-host-tools/#base-image","title":"Base Image","text":"<ul> <li>Ubuntu 22.04 LTS: Provides a stable, long-term support base</li> <li>User: Runs as <code>ansible</code> user with sudo privileges</li> <li>Working Directory: <code>/mnt/urbalurbadisk</code></li> </ul>"},{"location":"provision-host-tools/#pre-installed-base-tools","title":"Pre-installed Base Tools","text":"<p>The container comes with essential tools pre-installed: - Git, Python3, pip - curl, wget, jq, yq - Network utilities (ping, netcat, dnsutils, traceroute) - vim, bash-completion</p>"},{"location":"provision-host-tools/#provisioned-tools-and-software","title":"Provisioned Tools and Software","text":"<p>After the container is created, it's provisioned with additional tools through a series of scripts:</p>"},{"location":"provision-host-tools/#1-core-software-provision-host-00-coreswsh","title":"1. Core Software (<code>provision-host-00-coresw.sh</code>)","text":"<ul> <li>GitHub CLI (gh): GitHub repository management and automation</li> <li>Supports both x86_64 and aarch64 architectures</li> </ul>"},{"location":"provision-host-tools/#2-cloud-provider-tools-provision-host-01-cloudproviderssh","title":"2. Cloud Provider Tools (<code>provision-host-01-cloudproviders.sh</code>)","text":"<p>The container supports ALL major cloud providers with their official CLIs. You can select which cloud provider tools to install during setup:</p>"},{"location":"provision-host-tools/#cloud-provider-selection","title":"Cloud Provider Selection","text":"<p>When running <code>./install-rancher.sh</code> or the container creation script, you can specify which cloud provider tools to install:</p> <pre><code># Install Azure CLI only (default)\n./install-rancher.sh\n./install-rancher.sh az\n\n# Install AWS CLI only\n./install-rancher.sh aws\n\n# Install Google Cloud SDK only\n./install-rancher.sh gcp\n\n# Install Oracle Cloud CLI only\n./install-rancher.sh oci\n\n# Install Terraform only\n./install-rancher.sh tf\n\n# Install ALL cloud provider tools\n./install-rancher.sh all\n</code></pre>"},{"location":"provision-host-tools/#tools-installed-per-selection","title":"Tools Installed Per Selection","text":"Selection Tools Installed <code>az</code> / <code>azure</code> (default) Azure CLI <code>aws</code> AWS CLI v2 <code>gcp</code> / <code>google</code> Google Cloud SDK (gcloud, bq, gsutil) <code>oci</code> / <code>oracle</code> Oracle Cloud Infrastructure CLI <code>tf</code> / <code>terraform</code> Terraform <code>all</code> All of the above"},{"location":"provision-host-tools/#available-cloud-provider-tools","title":"Available Cloud Provider Tools","text":""},{"location":"provision-host-tools/#azure-cli","title":"Azure CLI","text":"<ul> <li>Resource Management: VMs, Storage, Networking, AKS</li> <li>Authentication: Service principals, managed identities, interactive login</li> <li>DevOps Integration: Azure DevOps, pipelines, artifacts</li> <li>Database Services: CosmosDB, SQL Database, PostgreSQL</li> <li>AI/ML Services: Azure ML, Cognitive Services</li> <li>Monitoring: Application Insights, Log Analytics</li> <li>Bicep: Can be installed with <code>az bicep install</code> for Infrastructure as Code (alternative to ARM templates)</li> </ul>"},{"location":"provision-host-tools/#aws-cli-v2","title":"AWS CLI (v2)","text":"<ul> <li>Compute: EC2, Lambda, ECS, EKS</li> <li>Storage: S3, EBS, EFS</li> <li>Database: RDS, DynamoDB, Aurora</li> <li>Networking: VPC, Route53, CloudFront</li> <li>IAM: Users, roles, policies, MFA</li> <li>Infrastructure: CloudFormation, CDK support</li> <li>Monitoring: CloudWatch, X-Ray</li> </ul>"},{"location":"provision-host-tools/#google-cloud-sdk-gcloud","title":"Google Cloud SDK (gcloud)","text":"<ul> <li>Compute: Compute Engine, GKE, Cloud Run, Functions</li> <li>Storage: Cloud Storage, Filestore</li> <li>Database: Cloud SQL, Spanner, Firestore</li> <li>BigData: BigQuery (bq command), Dataflow, Pub/Sub</li> <li>AI/ML: Vertex AI, AutoML</li> <li>Operations: Cloud Monitoring, Logging</li> <li>IAM: Service accounts, roles, permissions</li> </ul>"},{"location":"provision-host-tools/#oracle-cloud-infrastructure-oci-cli","title":"Oracle Cloud Infrastructure (OCI) CLI","text":"<ul> <li>Compute: Instances, container instances</li> <li>Database: Autonomous DB, MySQL, NoSQL</li> <li>Networking: VCN, load balancers, FastConnect</li> <li>Storage: Block volumes, object storage, file storage</li> <li>Identity: Compartments, policies, groups</li> <li>Kubernetes: OKE (Oracle Kubernetes Engine)</li> <li>Installed in isolated Python venv for compatibility</li> </ul>"},{"location":"provision-host-tools/#terraform","title":"Terraform","text":"<ul> <li>Multi-Cloud Support: Works with 300+ providers</li> <li>Infrastructure as Code: Declarative configuration</li> <li>State Management: Remote state, locking, versioning</li> <li>Module Support: Reusable infrastructure components</li> <li>Plan &amp; Apply: Preview changes before applying</li> <li>Import Existing: Bring existing infrastructure under management</li> <li>Supports: AWS, Azure, GCP, OCI, Kubernetes, and more</li> </ul>"},{"location":"provision-host-tools/#3-kubernetes-tools-provision-host-02-kubetoolssh","title":"3. Kubernetes Tools (<code>provision-host-02-kubetools.sh</code>)","text":""},{"location":"provision-host-tools/#ansible","title":"Ansible","text":"<ul> <li>Configuration management and automation</li> <li>Kubernetes module support</li> <li>Custom playbooks for service deployment</li> <li>Pre-configured with:</li> <li>Inventory at <code>/mnt/urbalurbadisk/ansible/inventory/hosts</code></li> <li>Roles path at <code>/mnt/urbalurbadisk/ansible/roles</code></li> <li>Host key checking disabled for automation</li> </ul>"},{"location":"provision-host-tools/#kubectl","title":"kubectl","text":"<ul> <li>Kubernetes cluster management</li> <li>Multi-context support</li> <li>Rancher Desktop and MicroK8s compatibility</li> </ul>"},{"location":"provision-host-tools/#helm","title":"Helm","text":"<ul> <li>Kubernetes package management</li> <li>Chart repository management</li> <li>Values file templating</li> </ul>"},{"location":"provision-host-tools/#k9s","title":"k9s","text":"<ul> <li>Terminal-based Kubernetes UI</li> <li>Real-time cluster monitoring</li> <li>Resource management interface</li> </ul>"},{"location":"provision-host-tools/#4-networking-tools-provision-host-03-netsh","title":"4. Networking Tools (<code>provision-host-03-net.sh</code>)","text":""},{"location":"provision-host-tools/#cloudflared","title":"Cloudflared","text":"<ul> <li>Cloudflare Tunnel client</li> <li>Zero-trust networking</li> <li>Secure tunnel creation and management</li> </ul>"},{"location":"provision-host-tools/#tailscale","title":"Tailscale","text":"<ul> <li>Mesh VPN solution</li> <li>Peer-to-peer secure networking</li> <li>MagicDNS support</li> <li>Note: Installed but requires separate configuration</li> </ul>"},{"location":"provision-host-tools/#5-helm-repositories-provision-host-04-helmreposh","title":"5. Helm Repositories (<code>provision-host-04-helmrepo.sh</code>)","text":"<p>Pre-configured repositories: - Bitnami: Production-ready charts for databases, web apps, and more - Runix: pgAdmin and other administrative tools - Gravitee: API management platform charts</p>"},{"location":"provision-host-tools/#directory-structure","title":"Directory Structure","text":"<pre><code>/mnt/urbalurbadisk/\n\u251c\u2500\u2500 ansible/                 # Ansible playbooks and roles\n\u2502   \u251c\u2500\u2500 inventory/\n\u2502   \u251c\u2500\u2500 playbooks/\n\u2502   \u2514\u2500\u2500 roles/\n\u251c\u2500\u2500 manifests/               # Kubernetes manifests\n\u251c\u2500\u2500 provision-host/          # Provisioning scripts\n\u2502   \u2514\u2500\u2500 kubernetes/         # Kubernetes setup scripts\n\u251c\u2500\u2500 secrets/                # Secure storage (gitignored)\n\u251c\u2500\u2500 kubeconfig/             # Kubernetes configurations\n\u2514\u2500\u2500 topsecret/              # Sensitive configurations\n</code></pre>"},{"location":"provision-host-tools/#usage","title":"Usage","text":""},{"location":"provision-host-tools/#accessing-the-container","title":"Accessing the Container","text":"<pre><code># Access interactive shell\ndocker exec -it provision-host bash\n\n# Run commands directly\ndocker exec provision-host kubectl get pods -A\n</code></pre>"},{"location":"provision-host-tools/#cloud-provider-authentication","title":"Cloud Provider Authentication","text":""},{"location":"provision-host-tools/#azure","title":"Azure","text":"<pre><code>az login\naz account set --subscription \"subscription-name\"\n</code></pre>"},{"location":"provision-host-tools/#aws","title":"AWS","text":"<pre><code>aws configure\n# Or use environment variables\nexport AWS_ACCESS_KEY_ID=xxx\nexport AWS_SECRET_ACCESS_KEY=xxx\n</code></pre>"},{"location":"provision-host-tools/#google-cloud","title":"Google Cloud","text":"<pre><code>gcloud auth login\ngcloud config set project PROJECT_ID\n</code></pre>"},{"location":"provision-host-tools/#oracle-cloud","title":"Oracle Cloud","text":"<pre><code>oci setup config\n</code></pre>"},{"location":"provision-host-tools/#kubernetes-management","title":"Kubernetes Management","text":"<pre><code># List contexts\nkubectl config get-contexts\n\n# Switch context\nkubectl config use-context rancher-desktop\n\n# Deploy with Helm\nhelm install myapp bitnami/postgresql\n\n# Run Ansible playbook\nansible-playbook /mnt/urbalurbadisk/ansible/playbooks/setup.yml\n</code></pre>"},{"location":"provision-host-tools/#container-lifecycle","title":"Container Lifecycle","text":""},{"location":"provision-host-tools/#building-the-container","title":"Building the Container","text":"<pre><code>cd provision-host-rancher/\ndocker build -t provision-host:latest .\n</code></pre>"},{"location":"provision-host-tools/#running-the-container","title":"Running the Container","text":"<pre><code>docker run -d \\\n  --name provision-host \\\n  -v ${HOME}/.kube:/home/ansible/.kube \\\n  -v ${PWD}:/mnt/urbalurbadisk \\\n  provision-host:latest\n</code></pre>"},{"location":"provision-host-tools/#provisioning-after-creation","title":"Provisioning After Creation","text":"<pre><code># Inside the container or via docker exec\ncd /mnt/urbalurbadisk/provision-host/\n\n# Run all provisioning scripts with Azure CLI (default)\n./provision-host-provision.sh\n\n# Or specify a different cloud provider\n./provision-host-provision.sh aws    # AWS only\n./provision-host-provision.sh gcp    # Google Cloud only\n./provision-host-provision.sh all    # All cloud providers\n\n# Or run individual scripts manually\n./provision-host-00-coresw.sh\n./provision-host-01-cloudproviders.sh aws  # Specify cloud provider\n./provision-host-02-kubetools.sh\n./provision-host-03-net.sh\n./provision-host-04-helmrepo.sh\n</code></pre>"},{"location":"provision-host-tools/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>RUNNING_IN_CONTAINER=true</code>: Set automatically, used by scripts to detect container environment</li> <li><code>KUBECONFIG</code>: Points to merged kubeconfig file</li> <li><code>PATH</code>: Includes user's local bin directory</li> </ul>"},{"location":"provision-host-tools/#security-considerations","title":"Security Considerations","text":""},{"location":"provision-host-tools/#credentials-management","title":"Credentials Management","text":"<ul> <li>Cloud credentials should be mounted as volumes or set via environment variables</li> <li>Use Kubernetes secrets for sensitive data</li> <li>Never commit credentials to git</li> </ul>"},{"location":"provision-host-tools/#network-security","title":"Network Security","text":"<ul> <li>Container runs with limited privileges</li> <li>Use Tailscale or Cloudflare tunnels for secure access</li> <li>SSH keys should be mounted from host when needed</li> </ul>"},{"location":"provision-host-tools/#troubleshooting","title":"Troubleshooting","text":""},{"location":"provision-host-tools/#common-issues","title":"Common Issues","text":""},{"location":"provision-host-tools/#1-permission-denied","title":"1. Permission Denied","text":"<pre><code># Switch to root temporarily if needed\ndocker exec -u root provision-host bash\n</code></pre>"},{"location":"provision-host-tools/#2-kubernetes-context-issues","title":"2. Kubernetes Context Issues","text":"<pre><code># Verify kubeconfig\nkubectl config view\n# Check current context\nkubectl config current-context\n</code></pre>"},{"location":"provision-host-tools/#3-cloud-cli-authentication","title":"3. Cloud CLI Authentication","text":"<ul> <li>Ensure credentials are properly mounted or configured</li> <li>Check token expiration and refresh as needed</li> </ul>"},{"location":"provision-host-tools/#4-tool-not-found","title":"4. Tool Not Found","text":"<ul> <li>Verify provisioning scripts have been run</li> <li>Check PATH includes <code>/home/ansible/.local/bin</code></li> </ul>"},{"location":"provision-host-tools/#maintenance","title":"Maintenance","text":""},{"location":"provision-host-tools/#updating-tools","title":"Updating Tools","text":"<pre><code># Update cloud CLIs\naz upgrade\naws --version  # Check for updates\ngcloud components update\n\n# Update Kubernetes tools\nhelm repo update\n# kubectl is typically updated via snap or direct download\n</code></pre>"},{"location":"provision-host-tools/#container-updates","title":"Container Updates","text":"<ol> <li>Update Dockerfile if base image changes needed</li> <li>Rebuild container with new tag</li> <li>Re-run provisioning scripts for tool updates</li> </ol>"},{"location":"provision-host-tools/#best-practices","title":"Best Practices","text":"<ol> <li>Version Control: Keep provisioning scripts in git</li> <li>Idempotency: Ensure scripts can be run multiple times safely</li> <li>Documentation: Document any custom configurations</li> <li>Backup: Backup important configurations and secrets</li> <li>Security: Regularly update tools and base image for security patches</li> </ol>"},{"location":"provision-host-tools/#architecture-support","title":"Architecture Support","text":"<p>The container and all provisioned tools support: - x86_64 (AMD64) - aarch64 (ARM64)</p> <p>This ensures compatibility with both Intel/AMD and Apple Silicon machines.</p>"},{"location":"provision-host-tools/#integration-with-cicd","title":"Integration with CI/CD","text":"<p>The provision host container can be used in CI/CD pipelines: - GitLab CI: Use as a base image for jobs - Jenkins: Run as a Jenkins agent - GitHub Actions: Use in self-hosted runners</p>"},{"location":"provision-host-tools/#related-documentation","title":"Related Documentation","text":"<ul> <li>Provision Host Setup</li> <li>Kubernetes Provisioning</li> <li>Rancher Desktop Integration</li> </ul>"},{"location":"rules-automated-kubernetes-deployment/","title":"Rules for Automated Kubernetes Deployment","text":"<p>File: <code>docs/rules-automated-kubernetes-deployment.md</code> Purpose: Define the ORCHESTRATION LAYER - how deployment scripts are organized, discovered, and executed automatically when cluster is built Target Audience: Developers, DevOps engineers, and LLMs working with the automated deployment system Scope: Directory structure, naming conventions, execution order, and automation flow</p>"},{"location":"rules-automated-kubernetes-deployment/#relationship-to-other-rules","title":"Relationship to Other Rules","text":"<p>This document covers the orchestration and automation framework: - How scripts are organized in numbered directories - How <code>provision-kubernetes.sh</code> discovers and executes scripts - Alphabetic ordering and dependency management - Active/inactive script management</p> <p>For how to write individual deployment scripts, see: \u2192 Rules for Provisioning - Implementation patterns for scripts and playbooks</p>"},{"location":"rules-automated-kubernetes-deployment/#core-principles","title":"Core Principles","text":"<ol> <li>Automated Orchestration: The <code>provision-kubernetes.sh</code> script is the master orchestrator that executes all deployment scripts</li> <li>Alphabetic Execution Order: Directories and scripts are executed in strict alphabetic order based on their names</li> <li>Sequential Dependency Management: Applications MUST be deployed in order based on their dependencies via alphabetic naming</li> <li>Idempotency: All deployment scripts MUST be safe to run multiple times</li> <li>Error Resilience: Deployment process MUST continue even if individual scripts fail, with errors tracked</li> <li>Parameterization: All scripts MUST accept target host as a parameter</li> </ol>"},{"location":"rules-automated-kubernetes-deployment/#automated-orchestration-system","title":"Automated Orchestration System","text":""},{"location":"rules-automated-kubernetes-deployment/#master-script-provision-kubernetessh","title":"Master Script: provision-kubernetes.sh","text":"<p>The <code>provision-kubernetes.sh</code> is the central automation controller that orchestrates all deployments:</p> <p>Repository Path: <code>provision-host/kubernetes/provision-kubernetes.sh</code> Container Path: <code>/mnt/urbalurbadisk/provision-host/kubernetes/provision-kubernetes.sh</code> (when running inside provision-host)</p> <p>Key Functions: - Automatically discovers all numbered directories (e.g., <code>01-core</code>, <code>02-databases</code>) - Executes directories in strict alphabetic order (not numeric - this is critical!) - Within each directory, executes all <code>*.sh</code> scripts in alphabetic order - Ignores scripts in <code>not-in-use/</code> folders - Passes target host parameter to every script - Continues execution even if individual scripts fail - Generates comprehensive summary report</p> <p>CRITICAL: The system uses alphabetic sorting, not numeric sorting: - <code>01</code> comes before <code>02</code> (correct) - <code>10</code> comes before <code>2</code> (would be wrong - always use leading zeros!) - This is why <code>05-setup-postgres.sh</code> comes before <code>10-setup-mysql.sh</code></p>"},{"location":"rules-automated-kubernetes-deployment/#automated-execution","title":"Automated Execution","text":"<p>IMPORTANT: This script is called automatically by <code>install-rancher.sh</code> during cluster build:</p> <pre><code># Automatically executed by install-rancher.sh:\ndocker exec provision-host bash -c \"cd /mnt/urbalurbadisk/provision-host/kubernetes &amp;&amp; ./provision-kubernetes.sh rancher-desktop default\"\n</code></pre> <p>Path Context: Commands shown above use container paths (<code>/mnt/urbalurbadisk/</code>) because they run inside the provision-host container.</p> <p>The complete cluster setup flow: 1. User runs <code>./install-rancher.sh</code> in the repo on his host machine (Windows, Mac, Linux) 2. install-rancher.sh creates the provision-host container and sets it up with all tools to manage the cluster 3. install-rancher.sh automatically calls provision-kubernetes.sh inside the provision-host container 4. provision-kubernetes.sh deploys all services in alphabetic order</p>"},{"location":"rules-automated-kubernetes-deployment/#manual-usage-for-testingdebugging","title":"Manual Usage (for testing/debugging)","text":"<pre><code># From within provision-host container:\ncd /mnt/urbalurbadisk\n./provision-host/kubernetes/provision-kubernetes.sh [target-host]\n</code></pre> <p>Default target-host is <code>rancher-desktop</code> if not specified.</p>"},{"location":"rules-automated-kubernetes-deployment/#directory-structure-rules","title":"Directory Structure Rules","text":""},{"location":"rules-automated-kubernetes-deployment/#category-numbering-standards","title":"Category Numbering Standards","text":"<p>Deployment scripts MUST be organized in numbered categories:</p> <p>Repository Structure: <pre><code>provision-host/kubernetes/\n\u251c\u2500\u2500 01-core/             # Storage, ingress, DNS, basic infrastructure\n\u251c\u2500\u2500 02-databases/        # PostgreSQL, MySQL, MongoDB, etc.\n\u251c\u2500\u2500 03-queues/          # Redis, RabbitMQ, message brokers\n\u251c\u2500\u2500 04-search/          # Elasticsearch, Solr, search engines\n\u251c\u2500\u2500 05-apim/            # API management platforms\n\u251c\u2500\u2500 06-management/      # Admin tools (pgAdmin, phpMyAdmin, etc.)\n\u251c\u2500\u2500 07-ai/              # AI/ML services (OpenWebUI, LiteLLM, etc.)\n\u251c\u2500\u2500 08-development/     # CI/CD tools (ArgoCD, Jenkins, etc.)\n\u251c\u2500\u2500 09-network/         # VPN, tunnels, network tools\n\u251c\u2500\u2500 10-datascience/    # Jupyter, Unity Catalog, analytics\n\u251c\u2500\u2500 11-monitoring/      # Prometheus, Grafana, observability\n\u2514\u2500\u2500 12-auth/            # Authentication services (Authentik, Keycloak)\n</code></pre></p> <p>Container Path: <code>/mnt/urbalurbadisk/provision-host/kubernetes/</code> (when mounted in provision-host)</p>"},{"location":"rules-automated-kubernetes-deployment/#script-naming-convention","title":"Script Naming Convention","text":"<p>\u26a0\ufe0f See doc/rules-naming-conventions.md for complete naming patterns.</p> <p>Scripts must follow standard naming patterns. For implementation details (script structure, error handling), see: \u2192 Rules for Provisioning - Script Template Pattern section</p>"},{"location":"rules-automated-kubernetes-deployment/#active-vs-inactive-management","title":"Active vs Inactive Management","text":"<p>Purpose: Control what gets deployed during automated cluster build by <code>install-rancher.sh</code></p> <ul> <li>Active scripts: Placed directly in the category folder - will be deployed automatically</li> <li>Inactive scripts: Placed in <code>not-in-use/</code> subfolder - skipped during automated build</li> <li>Activation: Move script from <code>not-in-use/</code> to parent directory for next cluster build</li> <li>Deactivation: Move script to <code>not-in-use/</code> to exclude from automated deployment</li> </ul> <p>IMPORTANT: Scripts in <code>not-in-use/</code> can still be run manually anytime: <pre><code># Manual execution of inactive script (from provision-host container):\ncd /mnt/urbalurbadisk/provision-host/kubernetes/02-databases/not-in-use/\n./04-setup-mongodb.sh rancher-desktop\n</code></pre></p> <p>Note: The path above uses the container mount point (<code>/mnt/urbalurbadisk/</code>).</p> <p>This allows you to: - Keep optional services ready but not auto-deployed - Test services before adding to automated build - Maintain different cluster configurations</p>"},{"location":"rules-automated-kubernetes-deployment/#script-requirements-for-orchestration","title":"Script Requirements for Orchestration","text":""},{"location":"rules-automated-kubernetes-deployment/#compatibility-with-automation","title":"Compatibility with Automation","text":"<p>For scripts to work with the automated orchestration system, they MUST:</p> <ol> <li>Be executable: File permissions must be 755 (<code>chmod +x script.sh</code>)</li> <li>Accept target host as first parameter: The orchestrator passes this automatically</li> <li>Follow naming convention: <code>[NN]-setup-[service].sh</code> for proper alphabetic ordering</li> <li>Be placed in correct directory: Active scripts in category folder, inactive in <code>not-in-use/</code></li> </ol> <p>For implementation details (how to write the scripts), see: \u2192 Rules for Provisioning</p>"},{"location":"rules-automated-kubernetes-deployment/#dependency-management-rules","title":"Dependency Management Rules","text":""},{"location":"rules-automated-kubernetes-deployment/#execution-order-alphabetic","title":"Execution Order (Alphabetic!)","text":"<ol> <li>Categories are processed in alphabetic order (01 before 02, 10 before 11, etc.)</li> <li>Scripts within categories execute in alphabetic order</li> <li>Dependencies MUST be satisfied by proper alphabetic ordering:</li> <li>Databases (02) before applications that use them</li> <li>Authentication (12) after databases it depends on</li> <li>Monitoring (11) after services to monitor</li> </ol>"},{"location":"rules-automated-kubernetes-deployment/#automation-integration","title":"Automation Integration","text":"<p>The <code>provision-kubernetes.sh</code> master script implements these orchestration requirements:</p> <ol> <li>Discover all category directories in alphabetic order</li> <li>Execute scripts within each directory in alphabetic order</li> <li>Skip scripts in <code>not-in-use/</code> folders</li> <li>Pass target host parameter to every script</li> <li>Continue execution even if individual scripts fail</li> <li>Track successes and failures for summary report</li> <li>Generate comprehensive summary of all deployment results</li> <li>Return appropriate exit code based on overall success/failure</li> </ol> <p>Key Point: This discovery pattern ensures strict alphabetic execution order for both directories and scripts.</p> <p>Related Documentation: - Provision Host Kubernetes Guide - Rules Overview - Secrets Management</p>"},{"location":"rules-development-workflow/","title":"Development Workflow Rules","text":"<p>File: <code>docs/rules-development-workflow.md</code> Purpose: Define how to work with the urbalurba-infrastructure codebase, including file operations, command execution, and project standards Target Audience: All contributors and AI assistants working with the repository Last Updated: October 3, 2025</p> <p>\u26a0\ufe0f CRITICAL: All paths in this document and throughout the project are relative to the repository root unless explicitly stated otherwise.</p> <p>Repository Root: <code>/Users/terje.christensen/learn/redcross-public/urbalurba-infrastructure/</code></p>"},{"location":"rules-development-workflow/#rules","title":"Rules","text":"<p>There are many rules to follow read docs/rules-readme.md for an overview.  Or just all docs/rules-*.md </p>"},{"location":"rules-development-workflow/#path-convention","title":"Path Convention","text":"<p>When paths are referenced anywhere in this project:</p> <p>\u2705 Correct: <code>manifests/030-prometheus-config.yaml</code> \u2705 Correct: <code>ansible/playbooks/030-setup-prometheus.yml</code> \u2705 Correct: <code>docs/rules-development-workflow.md</code></p> <p>\u274c Wrong: <code>/Users/terje.christensen/learn/redcross-public/urbalurba-infrastructure/manifests/030-prometheus-config.yaml</code></p> <p>Exception: Absolute paths are only used when referring to external locations or when explicitly needed for clarity.</p>"},{"location":"rules-development-workflow/#two-development-workflows","title":"Two Development Workflows","text":"<p>Depending on who is working (AI assistant vs. human developer), there are different workflows:</p>"},{"location":"rules-development-workflow/#workflow-a-claude-code-ai-assistant","title":"Workflow A: Claude Code (AI Assistant)","text":"<p>Used when: Claude Code AI assistant is performing tasks</p> <p>Characteristics: - Claude operates directly on the Mac host filesystem - No manual file synchronization required - Faster iteration and immediate feedback</p> <p>Operations:</p> <ol> <li> <p>File Operations (Read/Write/Edit)    <pre><code>Claude writes directly to:\n/Users/terje.christensen/learn/redcross-public/urbalurba-infrastructure/\n\nExamples:\n- Create: manifests/036-grafana-sovdev-verification.yaml\n- Edit: ansible/playbooks/030-setup-prometheus.yml\n- Read: docs/rules-development-workflow.md\n</code></pre></p> </li> <li> <p>kubectl Commands (Direct on Mac)    <pre><code>kubectl get pods -n monitoring\nkubectl apply -f manifests/036-grafana-sovdev-verification.yaml\nkubectl logs -n monitoring -l app=grafana\n</code></pre></p> </li> <li> <p>Ansible Playbooks (Via provision-host container)    <pre><code>docker exec provision-host bash -c \"cd /mnt/urbalurbadisk/provision-host/kubernetes/11-monitoring/not-in-use &amp;&amp; ./01-setup-prometheus.sh rancher-desktop\"\n</code></pre></p> </li> <li> <p>Verification (Multiple methods)</p> </li> <li>kubectl on Mac</li> <li>File reads on Mac</li> <li>Container commands when needed</li> <li>Direct curl/API calls from Mac</li> </ol> <p>Advantages: - \u2705 No manual sync step - \u2705 Immediate feedback - \u2705 All changes in git repository - \u2705 Can iterate quickly</p> <p>Limitations: - Ansible playbooks must still run in provision-host container - Some Ansible tasks require container context</p>"},{"location":"rules-development-workflow/#workflow-b-manual-human-developer","title":"Workflow B: Manual (Human Developer)","text":"<p>Used when: Human developer is working directly with files and commands</p> <p>Characteristics: - Manual file synchronization required - Work done both on Mac and in provision-host container - More explicit control over each step</p> <p>Step-by-Step Process:</p> <p>1. Edit Files on Mac <pre><code># Work in repository root\ncd /Users/terje.christensen/learn/redcross-public/urbalurba-infrastructure\n\n# Edit files with your editor\nvim manifests/030-prometheus-config.yaml\ncode ansible/playbooks/030-setup-prometheus.yml\n</code></pre></p> <p>2. Sync Files to provision-host Container <pre><code># CRITICAL: Run after ANY file change\n./copy2provisionhost.sh\n</code></pre></p> <p>This copies changed files from Mac to <code>/mnt/urbalurbadisk/</code> in the provision-host container.</p> <p>3. Execute Scripts in provision-host Container <pre><code># Enter container\ndocker exec -it provision-host bash\n\n# You'll be at: ansible@lima-rancher-desktop:/mnt/urbalurbadisk\n\n# Navigate to scripts directory\ncd provision-host/kubernetes/11-monitoring/not-in-use\n\n# Run setup scripts\n./01-setup-prometheus.sh rancher-desktop\n</code></pre></p> <p>4. Verify with kubectl (Mac or Container) <pre><code># On Mac\nkubectl get pods -n monitoring\n\n# Or in container\nkubectl get pods -n monitoring\n</code></pre></p> <p>5. Update Documentation and Sync Again <pre><code># On Mac\nvim docs/rules-development-workflow.md\n\n# Sync changes\n./copy2provisionhost.sh\n</code></pre></p> <p>Common Mistake: \u274c Editing files and forgetting to run <code>./copy2provisionhost.sh</code> \u2705 Always sync after every file change</p>"},{"location":"rules-development-workflow/#directory-structure","title":"Directory Structure","text":"<p>Mac Host: <pre><code>/Users/terje.christensen/learn/redcross-public/urbalurba-infrastructure/\n\u251c\u2500\u2500 manifests/               # Kubernetes manifests (Helm values, ConfigMaps, IngressRoutes)\n\u251c\u2500\u2500 ansible/\n\u2502   \u2514\u2500\u2500 playbooks/           # Ansible playbooks for automation\n\u251c\u2500\u2500 provision-host/\n\u2502   \u2514\u2500\u2500 kubernetes/\n\u2502       \u2514\u2500\u2500 11-monitoring/   # Monitoring setup scripts\n\u2502           \u2514\u2500\u2500 not-in-use/  # Testing area for new scripts\n\u251c\u2500\u2500 topsecret/               # Secrets management (NOT in git)\n\u2502   \u251c\u2500\u2500 create-kubernetes-secrets.sh\n\u2502   \u2514\u2500\u2500 kubernetes/\n\u2502       \u2514\u2500\u2500 kubernetes-secrets.yml\n\u251c\u2500\u2500 docs/                     # Documentation and rules\n\u2514\u2500\u2500 terchris/                # Personal working area (experiments, backups)\n</code></pre></p> <p>Provision-Host Container (after sync): <pre><code>/mnt/urbalurbadisk/\n\u251c\u2500\u2500 manifests/\n\u251c\u2500\u2500 ansible/playbooks/\n\u251c\u2500\u2500 provision-host/kubernetes/\n\u251c\u2500\u2500 topsecret/\n\u2514\u2500\u2500 docs/\n</code></pre></p> <p>Mirror Relationship: The provision-host container at <code>/mnt/urbalurbadisk/</code> mirrors the Mac repository at <code>/Users/terje.christensen/learn/redcross-public/urbalurba-infrastructure/</code></p>"},{"location":"rules-development-workflow/#file-naming-conventions","title":"File Naming Conventions","text":"<p>\u26a0\ufe0f See doc/rules-naming-conventions.md for complete details.</p> <p>Quick Summary:</p> <p>Manifests: <code>NNN-component-type.yaml</code> - 000-029: Core infrastructure - 030-039: Monitoring (Prometheus, Grafana, Loki, Tempo, OTEL) - 040-069: Databases - 070-079: Authentication - 200-229: AI services</p> <p>Ansible Playbooks: <code>NNN-action-component.yml</code> - Number matches manifest (030 playbook \u2192 030 manifest) - Actions: <code>setup-</code>, <code>remove-</code>, <code>update-</code>, <code>test-</code></p> <p>Shell Scripts: <code>NN-action-component.sh</code> - Sequential numbering (01, 02, 03...) - Wrappers around Ansible playbooks</p> <p>Example Flow: <pre><code>manifests/030-prometheus-config.yaml\n    \u2193 (used by)\nansible/playbooks/030-setup-prometheus.yml\n    \u2193 (called by)\nprovision-host/kubernetes/11-monitoring/not-in-use/01-setup-prometheus.sh\n</code></pre></p>"},{"location":"rules-development-workflow/#command-execution-rules","title":"Command Execution Rules","text":""},{"location":"rules-development-workflow/#kubectl-commands","title":"kubectl Commands","text":"<p>Location: Can run on Mac host OR provision-host container</p> <pre><code># Both work identically\nkubectl get pods -n monitoring\nkubectl apply -f manifests/030-prometheus-config.yaml\nkubectl logs -n monitoring pod-name\n</code></pre>"},{"location":"rules-development-workflow/#ansible-playbooks","title":"Ansible Playbooks","text":"<p>Location: Must run in provision-host container</p> <pre><code># CORRECT: Via shell script wrapper\ndocker exec provision-host bash -c \"cd /mnt/urbalurbadisk/provision-host/kubernetes/11-monitoring/not-in-use &amp;&amp; ./01-setup-prometheus.sh rancher-desktop\"\n\n# ALSO CORRECT: Inside container\ndocker exec -it provision-host bash\ncd /mnt/urbalurbadisk/provision-host/kubernetes/11-monitoring/not-in-use\n./01-setup-prometheus.sh rancher-desktop\n\n# WRONG: Calling playbook directly (skip script wrapper)\nansible-playbook ansible/playbooks/030-setup-prometheus.yml -e \"target_host=rancher-desktop\"\n</code></pre> <p>Why scripts? Shell scripts provide proper context, error handling, and wrapper logic around Ansible playbooks.</p>"},{"location":"rules-development-workflow/#file-operations","title":"File Operations","text":"<p>Location: Mac host (both workflows)</p> <pre><code># Edit files on Mac\nvim manifests/036-grafana-sovdev-verification.yaml\n\n# If manual workflow: sync to container\n./copy2provisionhost.sh\n</code></pre>"},{"location":"rules-development-workflow/#project-rules-and-standards","title":"Project Rules and Standards","text":"<p>\u26a0\ufe0f IMPORTANT: This project has established rules that MUST be followed:</p>"},{"location":"rules-development-workflow/#core-rules-documents","title":"Core Rules Documents","text":"<ol> <li>doc/rules-development-workflow.md (this file)</li> <li>Development workflows (Claude Code vs Manual)</li> <li>File operations and command execution</li> <li> <p>Directory structure and naming conventions</p> </li> <li> <p>doc/rules-readme.md (to be created)</p> </li> <li>Overview of all project rules</li> <li> <p>Quick reference for developers</p> </li> <li> <p>doc/rules-automated-kubernetes-deployment.md (to be created)</p> </li> <li>Ansible playbook patterns</li> <li>Helm chart deployment standards</li> <li> <p>External manifest file usage (not inline values)</p> </li> <li> <p>doc/rules-ingress-traefik.md (to be created)</p> </li> <li>IngressRoute patterns</li> <li>HostRegexp for multi-domain support</li> <li> <p>Middleware configuration (auth, CSP headers)</p> </li> <li> <p>doc/rules-secrets-management.md (to be created)</p> </li> <li>topsecret system usage</li> <li>Never commit secrets to git</li> <li> <p>urbalurba-secrets ConfigMap pattern</p> </li> <li> <p>doc/rules-provisioning.md (to be created)</p> </li> <li>Shell script organization</li> <li>Naming conventions</li> <li> <p>Testing patterns</p> </li> <li> <p>doc/rules-git-workflow.md (to be created)</p> </li> <li>Commit message standards</li> <li>Branch naming</li> <li> <p>PR requirements</p> </li> <li> <p>doc/rules-howtodoc.md (to be created)</p> </li> <li>Documentation structure</li> <li>Markdown formatting</li> <li>Usage instructions pattern</li> </ol> <p>Before making changes to the codebase, review relevant rules files to ensure compliance.</p>"},{"location":"rules-development-workflow/#quick-reference","title":"Quick Reference","text":""},{"location":"rules-development-workflow/#common-tasks","title":"Common Tasks","text":"<p>Create new manifest: <pre><code># 1. Determine number range (030-039 for monitoring)\n# 2. Create file with proper header\nvim manifests/036-grafana-sovdev-verification.yaml\n</code></pre></p> <p>Create new Ansible playbook: <pre><code># 1. Match manifest number (036 \u2192 036-setup-*.yml)\n# 2. Reference external manifest with -f flag\nvim ansible/playbooks/036-setup-grafana-sovdev.yml\n</code></pre></p> <p>Deploy to cluster: <pre><code># If manual workflow: sync first\n./copy2provisionhost.sh\n\n# Run via shell script wrapper\ndocker exec provision-host bash -c \"cd /mnt/urbalurbadisk/provision-host/kubernetes/11-monitoring/not-in-use &amp;&amp; ./06-setup-grafana-sovdev.sh rancher-desktop\"\n</code></pre></p> <p>Verify deployment: <pre><code>kubectl get pods -n monitoring\nkubectl get configmap -n monitoring\nkubectl logs -n monitoring -l app=grafana\n</code></pre></p>"},{"location":"rules-development-workflow/#troubleshooting","title":"Troubleshooting","text":""},{"location":"rules-development-workflow/#files-not-syncing-to-container","title":"Files not syncing to container","text":"<p>Problem: Changes on Mac not visible in provision-host container Solution: Run <code>./copy2provisionhost.sh</code> after every file change</p>"},{"location":"rules-development-workflow/#playbook-fails-with-file-not-found","title":"Playbook fails with file not found","text":"<p>Problem: Ansible can't find manifest file Solution: Check that file exists at correct path relative to repository root</p>"},{"location":"rules-development-workflow/#kubectl-command-fails","title":"kubectl command fails","text":"<p>Problem: Cannot connect to cluster Solution: Verify Rancher Desktop is running, check <code>kubectl config current-context</code></p>"},{"location":"rules-development-workflow/#script-permission-denied","title":"Script permission denied","text":"<p>Problem: Shell script not executable Solution: <code>chmod +x provision-host/kubernetes/11-monitoring/not-in-use/XX-setup-component.sh</code></p>"},{"location":"rules-development-workflow/#summary","title":"Summary","text":"<p>Key Points: 1. \u2705 All paths are relative to repository root 2. \u2705 Claude Code works directly on Mac (no manual sync) 3. \u2705 Manual workflow requires <code>./copy2provisionhost.sh</code> after file changes 4. \u2705 Ansible playbooks run in provision-host container via shell scripts 5. \u2705 kubectl works on Mac or container 6. \u2705 Follow numbering conventions (manifests, playbooks, scripts) 7. \u2705 Always review relevant docs/rules-*.md files before changes</p> <p>When in doubt: - Check this document - Review other docs/rules-*.md files - Look at existing examples in the codebase - Follow established patterns</p>"},{"location":"rules-documentation/","title":"Documentation Guide","text":"<p>This guide explains how documentation works in Urbalurba Infrastructure and how to contribute to it.</p>"},{"location":"rules-documentation/#documentation-architecture","title":"Documentation Architecture","text":"<p>Documentation is deployed to three environments automatically:</p> Environment URL Use Case Local Development <code>http://127.0.0.1:8000</code> Live editing with instant preview GitHub Pages <code>https://terchris.github.io/urbalurba-infrastructure/</code> Public documentation In-Cluster <code>http://localhost/docs/</code> Documentation within K8s cluster"},{"location":"rules-documentation/#how-it-works","title":"How It Works","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         docs/ folder                                \u2502\n\u2502                    (Markdown source files)                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         mkdocs.yml                                  \u2502\n\u2502                  (Navigation &amp; theme config)                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u25bc                       \u25bc                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  mkdocs serve \u2502      \u2502 GitHub Action \u2502      \u2502 provision-host\u2502\n\u2502  (local dev)  \u2502      \u2502 (on push)     \u2502      \u2502 (cluster)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                       \u2502                       \u2502\n        \u25bc                       \u25bc                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 localhost:8000\u2502      \u2502 GitHub Pages  \u2502      \u2502 nginx /docs/  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"rules-documentation/#writing-documentation","title":"Writing Documentation","text":""},{"location":"rules-documentation/#local-development-workflow","title":"Local Development Workflow","text":"<ol> <li> <p>Start the local server:    <pre><code>cd urbalurba-infrastructure\nmkdocs serve\n</code></pre></p> </li> <li> <p>Open browser at <code>http://127.0.0.1:8000</code></p> </li> <li> <p>Edit markdown files in <code>docs/</code> - changes appear instantly</p> </li> <li> <p>Commit and push when ready - GitHub Pages updates automatically</p> </li> </ol>"},{"location":"rules-documentation/#prerequisites","title":"Prerequisites","text":"<p>Install MkDocs with Material theme locally:</p> <pre><code>pip install mkdocs-material\n</code></pre>"},{"location":"rules-documentation/#file-naming-conventions","title":"File Naming Conventions","text":"<p>Documentation files follow a naming pattern:</p> Pattern Example Description <code>package-{category}-{name}.md</code> <code>package-ai-litellm.md</code> Service documentation <code>hosts-{platform}.md</code> <code>hosts-rancher-kubernetes.md</code> Platform guides <code>rules-{topic}.md</code> <code>rules-git-workflow.md</code> Standards and conventions <code>networking-{topic}.md</code> <code>networking-tailscale-setup.md</code> Network configuration <code>overview-{topic}.md</code> <code>overview-getting-started.md</code> Getting started guides <code>troubleshooting-{topic}.md</code> <code>troubleshooting-readme.md</code> Problem solving"},{"location":"rules-documentation/#adding-a-new-document","title":"Adding a New Document","text":"<ol> <li> <p>Create the markdown file in <code>docs/</code>:    <pre><code>touch docs/package-databases-newdb.md\n</code></pre></p> </li> <li> <p>Add to navigation in <code>mkdocs.yml</code>:    <pre><code>nav:\n  - Packages:\n    - Databases:\n      - NewDB: package-databases-newdb.md\n</code></pre></p> </li> <li> <p>Write content using the template below</p> </li> <li> <p>Preview locally with <code>mkdocs serve</code></p> </li> <li> <p>Commit and push to deploy</p> </li> </ol>"},{"location":"rules-documentation/#document-template","title":"Document Template","text":"<pre><code># Service Name\n\nBrief description of what this service does and why it's included.\n\n## Overview\n\n- **Purpose**: What problem does it solve?\n- **Port**: Internal port number\n- **Namespace**: Kubernetes namespace (usually `default`)\n\n## Quick Start\n\n```bash\n# How to access or test the service\nkubectl port-forward svc/service-name 8080:80\n</code></pre>"},{"location":"rules-documentation/#configuration","title":"Configuration","text":"<p>Explain key configuration options.</p>"},{"location":"rules-documentation/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and solutions.</p>"},{"location":"rules-documentation/#related-documentation","title":"Related Documentation","text":"<ul> <li>Related Service <pre><code>## Markdown Features\n\nMkDocs Material supports rich formatting:\n\n### Admonitions (Callout Boxes)\n\n```markdown\n!!! note \"Optional Title\"\n    This is a note callout.\n\n!!! warning\n    This is a warning without a custom title.\n\n!!! tip \"Pro Tip\"\n    Helpful tips go here.\n\n!!! danger \"Critical\"\n    Important warnings about destructive operations.\n</code></pre></li> </ul>"},{"location":"rules-documentation/#code-blocks-with-syntax-highlighting","title":"Code Blocks with Syntax Highlighting","text":"<pre><code>```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: example\n```\n\n```bash\nkubectl apply -f manifest.yaml\n```\n\n```python\ndef hello():\n    print(\"Hello, World!\")\n```\n</code></pre>"},{"location":"rules-documentation/#tabs","title":"Tabs","text":"<pre><code>=== \"Rancher Desktop\"\n    Instructions for Rancher Desktop users.\n\n=== \"Azure AKS\"\n    Instructions for Azure AKS users.\n\n=== \"MicroK8s\"\n    Instructions for MicroK8s users.\n</code></pre>"},{"location":"rules-documentation/#tables","title":"Tables","text":"<pre><code>| Column 1 | Column 2 | Column 3 |\n|----------|----------|----------|\n| Value 1  | Value 2  | Value 3  |\n</code></pre>"},{"location":"rules-documentation/#links","title":"Links","text":"<pre><code>[Link to another doc](package-ai-litellm.md)\n[External link](https://example.com)\n</code></pre>"},{"location":"rules-documentation/#technical-details","title":"Technical Details","text":""},{"location":"rules-documentation/#github-pages-deployment","title":"GitHub Pages Deployment","text":"<p>The <code>.github/workflows/docs.yml</code> workflow:</p> <ul> <li>Triggers on push to <code>main</code> when <code>docs/</code> or <code>mkdocs.yml</code> changes</li> <li>Installs <code>mkdocs-material</code> (includes built-in tags plugin)</li> <li>Runs <code>mkdocs gh-deploy --force</code></li> <li>Publishes to <code>gh-pages</code> branch</li> </ul>"},{"location":"rules-documentation/#in-cluster-deployment","title":"In-Cluster Deployment","text":"<p>During cluster provisioning:</p> <ol> <li><code>provision-host-05-builddocs.sh</code> runs <code>mkdocs build</code></li> <li>Output goes to <code>testdata/docs/</code></li> <li><code>020-setup-web-files.yml</code> copies docs to nginx PVC</li> <li>Nginx serves documentation at <code>/docs/</code> path</li> </ol> <p>To manually rebuild docs in the cluster:</p> <pre><code># Inside provision-host container\ncd /mnt/urbalurbadisk\n./provision-host/provision-host-05-builddocs.sh\n\n# Then re-run the nginx setup\ncd ansible\nansible-playbook playbooks/020-setup-nginx.yml\n</code></pre>"},{"location":"rules-documentation/#configuration-files","title":"Configuration Files","text":"File Purpose <code>mkdocs.yml</code> Site name, theme, navigation, plugins <code>docs/index.md</code> Homepage content <code>.github/workflows/docs.yml</code> GitHub Pages deployment <code>provision-host/provision-host-05-builddocs.sh</code> In-cluster build script"},{"location":"rules-documentation/#best-practices","title":"Best Practices","text":""},{"location":"rules-documentation/#content-guidelines","title":"Content Guidelines","text":"<ol> <li>Be concise - Get to the point quickly</li> <li>Use examples - Show, don't just tell</li> <li>Include troubleshooting - Anticipate common problems</li> <li>Link related docs - Help users navigate</li> <li>Keep current - Update when code changes</li> </ol>"},{"location":"rules-documentation/#structure-guidelines","title":"Structure Guidelines","text":"<ol> <li>Start with overview - What is this? Why use it?</li> <li>Quick start first - Let users try it immediately</li> <li>Details after - Deep dive for those who need it</li> <li>Troubleshooting last - Problem solving at the end</li> </ol>"},{"location":"rules-documentation/#technical-writing-tips","title":"Technical Writing Tips","text":"<ul> <li>Use active voice: \"Run the command\" not \"The command should be run\"</li> <li>Use present tense: \"This creates\" not \"This will create\"</li> <li>Be specific: \"Port 5432\" not \"the default port\"</li> <li>Use consistent terminology throughout</li> </ul>"},{"location":"rules-documentation/#troubleshooting_1","title":"Troubleshooting","text":""},{"location":"rules-documentation/#local-preview-not-working","title":"Local Preview Not Working","text":"<pre><code># Check if mkdocs is installed\nmkdocs --version\n\n# Install if missing\npip install mkdocs-material\n</code></pre>"},{"location":"rules-documentation/#github-pages-not-updating","title":"GitHub Pages Not Updating","text":"<ol> <li>Check GitHub Actions tab for failed workflows</li> <li>Verify changes are in <code>docs/</code> or <code>mkdocs.yml</code></li> <li>Ensure push is to <code>main</code> branch</li> </ol>"},{"location":"rules-documentation/#in-cluster-docs-missing","title":"In-Cluster Docs Missing","text":"<pre><code># Check if docs were built\nls /mnt/urbalurbadisk/testdata/docs/\n\n# Rebuild if needed\n./provision-host/provision-host-05-builddocs.sh\n\n# Re-deploy nginx content\nansible-playbook playbooks/020-setup-nginx.yml\n</code></pre>"},{"location":"rules-documentation/#navigation-not-showing-new-page","title":"Navigation Not Showing New Page","text":"<p>Ensure the file is added to <code>nav:</code> section in <code>mkdocs.yml</code>:</p> <pre><code>nav:\n  - Section:\n    - Page Title: filename.md  # Add this line\n</code></pre>"},{"location":"rules-git-workflow/","title":"Git Workflow Rules","text":"<p>File: <code>docs/rules-git-workflow.md</code> Purpose: Standardized Git workflow and branching strategy for urbalurba-infrastructure Target Audience: All contributors to the repository Last Updated: September 21, 2024</p>"},{"location":"rules-git-workflow/#overview","title":"\ud83d\udccb Overview","text":"<p>This document establishes Git workflow rules to ensure consistent, professional development practices and maintain code quality through proper branching, review, and merge strategies.</p>"},{"location":"rules-git-workflow/#core-principles","title":"\ud83c\udfaf Core Principles","text":""},{"location":"rules-git-workflow/#principle-1-feature-branch-workflow","title":"Principle 1: Feature Branch Workflow","text":"<ul> <li>All development work happens on feature branches</li> <li>Never commit directly to <code>main</code> branch</li> <li>Feature branches are short-lived and focused on single features/fixes</li> </ul>"},{"location":"rules-git-workflow/#principle-2-pull-request-required","title":"Principle 2: Pull Request Required","text":"<ul> <li>All changes to <code>main</code> must go through Pull Requests (PRs)</li> <li>PRs enable code review, discussion, and quality control</li> <li>PRs provide permanent documentation of changes and reasoning</li> </ul>"},{"location":"rules-git-workflow/#principle-3-clean-history","title":"Principle 3: Clean History","text":"<ul> <li>Commit messages should be clear and descriptive</li> <li>Feature branches should be deleted after merge</li> <li>Main branch should have a clean, linear history</li> </ul>"},{"location":"rules-git-workflow/#mandatory-workflow-steps","title":"\ud83d\ude80 Mandatory Workflow Steps","text":""},{"location":"rules-git-workflow/#step-1-create-feature-branch","title":"Step 1: Create Feature Branch","text":"<pre><code># Always start from latest main\ngit checkout main\ngit pull origin main\n\n# Create descriptive feature branch\ngit checkout -b feature/descriptive-name\n</code></pre> <p>Branch Naming Convention: - <code>feature/</code> + descriptive name using kebab-case - Examples: <code>feature/litellm-shared-postgres</code>, <code>feature/git-workflow-rules</code> - Be specific: <code>feature/fix-tika-readiness</code> not <code>feature/fix-bug</code></p>"},{"location":"rules-git-workflow/#step-2-development-and-commits","title":"Step 2: Development and Commits","text":"<pre><code># Make your changes\n# Commit frequently with clear messages\ngit add .\ngit commit -m \"Clear description of what changed and why\"\n</code></pre> <p>Commit Message Rules: - Start with action verb (add, fix, update, remove, refactor) - Be specific about what changed - Include context if needed - Examples:   - \u2705 <code>Fix LiteLLM pod readiness check to wait for Ready condition</code>   - \u2705 <code>Add 30-second initialization pause for OpenWebUI service test</code>   - \u274c <code>bug fix</code>   - \u274c <code>updates</code></p>"},{"location":"rules-git-workflow/#step-3-push-and-create-pull-request","title":"Step 3: Push and Create Pull Request","text":"<pre><code># Push feature branch to remote\ngit push origin feature/your-branch-name\n\n# Create PR using GitHub CLI (preferred)\ngh pr create --title \"Descriptive PR Title\" --body \"$(cat &lt;&lt;'EOF'\n## Summary\n- Bullet point of key changes\n- What problem this solves\n- Impact on existing functionality\n\n## Technical Changes\n- Specific files/components modified\n- New patterns or approaches introduced\n- Breaking changes (if any)\n\n## Test Results\n- How you verified the changes work\n- Specific test scenarios covered\n- Performance impact (if applicable)\n\n\ud83e\udd16 Generated with [Claude Code](https://claude.ai/code)\n\nCo-Authored-By: Claude &lt;noreply@anthropic.com&gt;\nEOF\n)\"\n</code></pre> <p>PR Title Format: - Start with action verb - Be specific and descriptive - Examples:   - \u2705 <code>Fix LiteLLM deployment with shared PostgreSQL and enhanced reliability</code>   - \u2705 <code>Add Git workflow rules documentation</code>   - \u274c <code>Updates</code>   - \u274c <code>Bug fixes</code></p>"},{"location":"rules-git-workflow/#step-4-code-review-and-merge","title":"Step 4: Code Review and Merge","text":"<pre><code># Open PR in browser for review\ngh pr view --web\n\n# After review/approval, merge via GitHub web interface\n# Choose \"Squash and merge\" for clean history\n</code></pre>"},{"location":"rules-git-workflow/#step-5-clean-up","title":"Step 5: Clean Up","text":"<pre><code># Switch back to main and pull merged changes\ngit checkout main\ngit pull origin main\n\n# Delete local feature branch\ngit branch -d feature/your-branch-name\n</code></pre>"},{"location":"rules-git-workflow/#required-pr-content","title":"\u2705 Required PR Content","text":""},{"location":"rules-git-workflow/#pr-description-template","title":"PR Description Template","text":"<p>Every PR must include:</p> <pre><code>## Summary\n- [Bullet point describing main change]\n- [Problem this solves]\n- [Impact on users/system]\n\n## Technical Changes\n- [Specific files modified]\n- [New patterns introduced]\n- [Dependencies added/removed]\n\n## Test Results\n- [How changes were verified]\n- [Test scenarios covered]\n- [Performance impact]\n\n## Breaking Changes\n- [List any breaking changes]\n- [Migration steps required]\n\n\ud83e\udd16 Generated with [Claude Code](https://claude.ai/code)\n\nCo-Authored-By: Claude &lt;noreply@anthropic.com&gt;\n</code></pre>"},{"location":"rules-git-workflow/#required-pr-checks","title":"Required PR Checks","text":"<p>Before creating PR, verify: - [ ] All changes committed and pushed - [ ] PR title follows naming convention - [ ] PR description is complete and detailed - [ ] Changes have been tested - [ ] No secrets or sensitive data included - [ ] Code follows existing patterns and conventions</p>"},{"location":"rules-git-workflow/#prohibited-practices","title":"\ud83d\udeab Prohibited Practices","text":""},{"location":"rules-git-workflow/#never-do-this","title":"\u274c Never Do This:","text":"<ul> <li>Direct commits to <code>main</code> branch</li> <li>Force push to shared branches</li> <li>Commit secrets, API keys, or sensitive data</li> <li>Create PR without description</li> <li>Leave stale feature branches</li> <li>Merge without review (except for solo documentation updates)</li> </ul>"},{"location":"rules-git-workflow/#avoid-these-patterns","title":"\u274c Avoid These Patterns:","text":"<ul> <li>Generic commit messages (\"fix\", \"update\", \"changes\")</li> <li>Large PRs that change multiple unrelated things</li> <li>Keeping feature branches alive after merge</li> <li>Working on main branch directly</li> </ul>"},{"location":"rules-git-workflow/#tools-and-setup","title":"\ud83d\udd27 Tools and Setup","text":""},{"location":"rules-git-workflow/#required-tools","title":"Required Tools","text":"<pre><code># Install GitHub CLI for PR management\nbrew install gh\n\n# Authenticate with GitHub\ngh auth login\n</code></pre>"},{"location":"rules-git-workflow/#recommended-git-configuration","title":"Recommended Git Configuration","text":"<pre><code># Set up helpful aliases\ngit config --global alias.co checkout\ngit config --global alias.br branch\ngit config --global alias.ci commit\ngit config --global alias.st status\n\n# Set up default branch behavior\ngit config --global pull.rebase false\ngit config --global init.defaultBranch main\n</code></pre>"},{"location":"rules-git-workflow/#workflow-examples","title":"\ud83d\udcca Workflow Examples","text":""},{"location":"rules-git-workflow/#example-1-adding-new-feature","title":"Example 1: Adding New Feature","text":"<pre><code># 1. Start from main\ngit checkout main &amp;&amp; git pull origin main\n\n# 2. Create feature branch\ngit checkout -b feature/authentik-oauth-integration\n\n# 3. Make changes and commit\ngit add . &amp;&amp; git commit -m \"Add Authentik OAuth integration for OpenWebUI\"\n\n# 4. Push and create PR\ngit push origin feature/authentik-oauth-integration\ngh pr create --title \"Add Authentik OAuth integration for OpenWebUI\" --body \"...\"\n\n# 5. Merge via web interface, then cleanup\ngit checkout main &amp;&amp; git pull origin main\ngit branch -d feature/authentik-oauth-integration\n</code></pre>"},{"location":"rules-git-workflow/#example-2-fixing-bug","title":"Example 2: Fixing Bug","text":"<pre><code># 1. Start from main\ngit checkout main &amp;&amp; git pull origin main\n\n# 2. Create fix branch\ngit checkout -b feature/fix-tika-service-connectivity\n\n# 3. Make fix and commit\ngit add . &amp;&amp; git commit -m \"Fix Tika service connectivity timeout issues\"\n\n# 4. Create PR with detailed description\ngh pr create --title \"Fix Tika service connectivity timeout issues\" --body \"...\"\n\n# 5. Merge and cleanup\ngit checkout main &amp;&amp; git pull origin main\ngit branch -d feature/fix-tika-service-connectivity\n</code></pre>"},{"location":"rules-git-workflow/#pr-review-standards","title":"\ud83c\udfaf PR Review Standards","text":""},{"location":"rules-git-workflow/#review-criteria","title":"Review Criteria","text":"<p>When reviewing PRs, check for: - [ ] Clear problem statement and solution in PR description - [ ] Descriptive PR title following naming convention - [ ] Complete PR description using required template - [ ] Appropriate branch naming (feature/descriptive-name) - [ ] Clean commit messages following standards - [ ] No sensitive data (secrets, API keys) included - [ ] Documentation updated for user-facing changes</p>"},{"location":"rules-git-workflow/#emergency-procedures","title":"\ud83d\udea8 Emergency Procedures","text":""},{"location":"rules-git-workflow/#hotfix-process","title":"Hotfix Process","text":"<p>For critical production issues: 1. Create <code>hotfix/issue-description</code> branch from main 2. Make minimal fix with detailed commit message 3. Create PR with <code>[HOTFIX]</code> prefix in title 4. Fast-track review and merge 5. Follow up with proper investigation in separate feature branch</p>"},{"location":"rules-git-workflow/#rollback-process","title":"Rollback Process","text":"<p>If merged change causes issues: 1. Create <code>feature/revert-problematic-change</code> branch 2. Use <code>git revert</code> to create rollback commit 3. Create PR explaining rollback reasoning 4. Merge immediately if critical 5. Create follow-up feature branch to address root cause</p>"},{"location":"rules-git-workflow/#learning-resources","title":"\ud83d\udcda Learning Resources","text":""},{"location":"rules-git-workflow/#git-best-practices","title":"Git Best Practices","text":"<ul> <li>GitHub Flow</li> <li>Writing Good Commit Messages</li> <li>Pull Request Best Practices</li> </ul>"},{"location":"rules-git-workflow/#internal-documentation","title":"Internal Documentation","text":"<ul> <li><code>docs/rules-provisioning.md</code> - Deployment and infrastructure rules</li> <li><code>docs/rules-ingress-traefik.md</code> - Networking and ingress rules</li> </ul>"},{"location":"rules-git-workflow/#summary","title":"\ud83d\udcdd Summary","text":""},{"location":"rules-git-workflow/#golden-rules","title":"Golden Rules","text":"<ol> <li>Always use feature branches - never work directly on main</li> <li>Always create Pull Requests - enable review and documentation</li> <li>Write descriptive commit messages - explain what and why</li> <li>Test your changes - verify functionality before PR</li> <li>Clean up after merge - delete feature branches</li> <li>Document significant changes - update relevant documentation</li> </ol>"},{"location":"rules-git-workflow/#benefits-of-this-workflow","title":"Benefits of This Workflow","text":"<ul> <li>\u2705 Quality Control - Code review prevents bugs and maintains standards</li> <li>\u2705 Documentation - PR descriptions provide change history and context</li> <li>\u2705 Collaboration - Team members can discuss and improve changes</li> <li>\u2705 Safety - Feature branches protect main from experimental code</li> <li>\u2705 Traceability - Clear audit trail of who changed what and why</li> </ul> <p>This workflow ensures professional development practices while maintaining the agility needed for infrastructure experimentation and rapid iteration.</p>"},{"location":"rules-howtodoc/","title":"Documentation Creation Rules and Standards","text":"<p>File: <code>docs/rules-howtodoc.md</code> Purpose: Define mandatory rules and patterns for creating documentation in the Urbalurba Infrastructure Target Audience: Developers, technical writers, and LLMs creating documentation Last Updated: September 23, 2025</p>"},{"location":"rules-howtodoc/#overview","title":"\ud83d\udccb Overview","text":"<p>This document establishes mandatory rules for creating and maintaining documentation in the <code>docs/</code> folder. These rules ensure consistency, discoverability, and maintainability across all documentation types in the Urbalurba infrastructure.</p>"},{"location":"rules-howtodoc/#core-documentation-architecture","title":"\ud83c\udfd7\ufe0f Core Documentation Architecture","text":""},{"location":"rules-howtodoc/#rule-1-three-tier-naming-convention","title":"Rule 1: Three-Tier Naming Convention","text":"<p>ALL documentation files MUST follow the hierarchical naming pattern:</p> <pre><code>&lt;larger-grouping&gt;-&lt;focus-grouping&gt;-&lt;service&gt;.md\n</code></pre>"},{"location":"rules-howtodoc/#mandatory-naming-patterns","title":"Mandatory Naming Patterns:","text":"<ul> <li>Service Documentation: <code>package-databases-postgresql.md</code></li> <li>Index Files: <code>package-databases-readme.md</code></li> <li>Technical Guides: <code>rules-ingress-traefik.md</code></li> <li>Overview Documents: <code>overview-system-architecture.md</code></li> </ul>"},{"location":"rules-howtodoc/#approved-larger-groupings","title":"\u2705 Approved Larger Groupings:","text":"<ul> <li><code>package</code> - Service and application deployments</li> <li><code>overview</code> - High-level guides and architectural overviews</li> <li><code>rules</code> - Standards, patterns, and mandatory procedures</li> <li><code>hosts</code> - Host system configuration and setup</li> <li><code>networking</code> - Network configuration and connectivity</li> <li><code>provision</code> - Infrastructure provisioning and management</li> <li><code>troubleshooting</code> - Problem diagnosis and resolution</li> <li><code>secrets</code> - Security and secrets management</li> </ul>"},{"location":"rules-howtodoc/#approved-package-focus-groupings","title":"\u2705 Approved Package Focus Groupings:","text":"<ul> <li><code>ai</code> - Artificial Intelligence and ML services</li> <li><code>auth</code> - Authentication and authorization systems</li> <li><code>core</code> - Essential infrastructure services</li> <li><code>databases</code> - Data storage and management</li> <li><code>datascience</code> - Analytics and data processing</li> <li><code>development</code> - Developer tools and workflows</li> <li><code>management</code> - Administrative and monitoring tools</li> <li><code>queues</code> - Message brokers and queuing systems</li> <li><code>search</code> - Search and indexing services</li> </ul>"},{"location":"rules-howtodoc/#rule-2-index-file-pattern","title":"Rule 2: Index File Pattern","text":"<p>Every focus grouping MUST have an index file:</p> <pre><code>&lt;larger-grouping&gt;-&lt;focus-grouping&gt;-readme.md\n</code></pre> <p>Purpose: Serve as the entry point and navigation hub for all related service documentation.</p>"},{"location":"rules-howtodoc/#required-index-file-content","title":"Required Index File Content:","text":"<ol> <li>Overview of the focus grouping purpose</li> <li>Service listing with status indicators</li> <li>Quick start instructions</li> <li>Architecture summary</li> <li>Cross-references to related documentation</li> </ol>"},{"location":"rules-howtodoc/#document-structure-rules","title":"\ud83d\udcdd Document Structure Rules","text":""},{"location":"rules-howtodoc/#rule-3-standard-header-format","title":"Rule 3: Standard Header Format","text":"<p>ALL documentation files MUST start with this header:</p> <pre><code># [Service Name] - [Brief Description]\n\n**Key Features**: Feature1 \u2022 Feature2 \u2022 Feature3 \u2022 Feature4 \u2022 Feature5 \u2022 Feature6\n\n**File**: `docs/[filename].md`\n**Purpose**: [Single sentence describing document purpose]\n**Target Audience**: [Specific user groups]\n**Last Updated**: [Date in format: September 23, 2025]\n</code></pre>"},{"location":"rules-howtodoc/#header-requirements","title":"Header Requirements:","text":"<ul> <li>Title: Clear service name with descriptive subtitle</li> <li>Key Features: 3-7 bullet points using \u2022 separator</li> <li>File Path: Exact relative path from repository root</li> <li>Purpose: One sentence maximum, specific and actionable</li> <li>Target Audience: Comma-separated list of user personas</li> <li>Last Updated: Date in full month format</li> </ul>"},{"location":"rules-howtodoc/#rule-4-service-documentation-template","title":"Rule 4: Service Documentation Template","text":"<p>Service documentation (<code>package-*-[service].md</code>) MUST follow this structure:</p> <p><pre><code># [Service Name] - [Brief Description]\n\n**Key Features**: [List using \u2022 separator]\n**File**: `docs/package-[category]-[service].md`\n**Purpose**: Complete guide to [service] deployment and configuration in Urbalurba infrastructure\n**Target Audience**: [Specific personas]\n**Last Updated**: [Date]\n\n## \ud83d\udccb Overview\n[2-3 paragraphs describing service role and capabilities]\n\n**Key Features**:\n- **Feature 1**: Description\n- **Feature 2**: Description\n- **Architecture Type**: Description\n\n## \ud83c\udfd7\ufe0f Architecture\n### **Deployment Components**\n</code></pre> [ASCII diagram of service stack] <pre><code>### **File Structure**\n</code></pre> [Directory structure showing related files] <pre><code>## \ud83d\ude80 Deployment\n### **Manual Deployment**\n### **Prerequisites**\n\n## \u2699\ufe0f Configuration\n### **[Service] Configuration**\n### **Resource Configuration**\n### **Security Configuration**\n\n## \ud83d\udd0d Monitoring &amp; Verification\n### **Health Checks**\n### **Service Verification**\n### **[Service] Access Testing**\n### **Automated Verification**\n\n## \ud83d\udee0\ufe0f Management Operations\n### **[Service] Administration**\n### **Service Removal**\n\n## \ud83d\udd27 Troubleshooting\n### **Common Issues**\n\n## \ud83d\udccb Maintenance\n### **Regular Tasks**\n### **Backup Procedures**\n### **Disaster Recovery**\n\n## \ud83d\ude80 Use Cases\n[3-4 practical examples with code]\n</code></pre></p>"},{"location":"rules-howtodoc/#rule-5-index-documentation-template","title":"Rule 5: Index Documentation Template","text":"<p>Index files (<code>*-readme.md</code>) MUST follow this structure:</p> <pre><code># [Category Name] - [Description]\n\n**File**: `docs/[category]-readme.md`\n**Purpose**: Overview of all [category] services in Urbalurba infrastructure\n**Target Audience**: [User groups]\n**Last Updated**: [Date]\n\n## \ud83d\udccb Overview\n[Category description and purpose]\n\n**Available [Category] Services**:\n- **Service 1**: Brief description\n- **Service 2**: Brief description\n\n## [Icon] [Category] Services\n### **Service Name - Primary Service** \ud83e\udd47\n**Status**: Active | **Port**: [port] | **Type**: [type]\n\n[Service description paragraph]\n\n**Key Features**:\n- **Feature 1**: Description\n- **Feature 2**: Description\n\n**Documentation**: [package-category-service.md](./package-category-service.md)\n\n## \ud83d\ude80 Quick Start\n[Category-wide deployment instructions]\n\n## \ud83d\udd17 Related Documentation\n[Cross-references to related docs]\n</code></pre>"},{"location":"rules-howtodoc/#content-quality-rules","title":"\ud83c\udfaf Content Quality Rules","text":""},{"location":"rules-howtodoc/#rule-6-mandatory-metadata-fields","title":"Rule 6: Mandatory Metadata Fields","text":"<p>Every document MUST include:</p>"},{"location":"rules-howtodoc/#required-fields","title":"Required Fields:","text":"<ul> <li><code>**File**</code>: Exact path from repository root</li> <li><code>**Purpose**</code>: Single sentence description</li> <li><code>**Target Audience**</code>: Specific user personas</li> <li><code>**Last Updated**</code>: Date in full format</li> </ul>"},{"location":"rules-howtodoc/#service-documents-must-add","title":"Service Documents Must Add:","text":"<ul> <li><code>**Key Features**</code>: 3-7 items with \u2022 separator</li> <li>Architecture diagrams (ASCII or Mermaid)</li> <li>File structure listings</li> <li>Practical use case examples</li> </ul>"},{"location":"rules-howtodoc/#index-documents-must-add","title":"Index Documents Must Add:","text":"<ul> <li>Service status indicators (Active/Inactive/Development)</li> <li>Quick start section</li> <li>Cross-reference links</li> </ul>"},{"location":"rules-howtodoc/#rule-7-status-indicators","title":"Rule 7: Status Indicators","text":"<p>Use these standardized status indicators:</p> <pre><code>**Status**: \u2705 Active | \ud83d\udd04 Development | \u23f8\ufe0f Inactive | \ud83d\udeab Deprecated\n</code></pre>"},{"location":"rules-howtodoc/#rule-8-cross-reference-standards","title":"Rule 8: Cross-Reference Standards","text":""},{"location":"rules-howtodoc/#link-format","title":"Link Format:","text":"<pre><code>[Document Title](./filename.md)\n**Documentation**: [package-category-service.md](./package-category-service.md)\n</code></pre>"},{"location":"rules-howtodoc/#required-cross-references","title":"Required Cross-References:","text":"<ul> <li>Index files MUST link to all service documentation</li> <li>Service files MUST link back to index file</li> <li>Related services MUST be cross-referenced</li> <li>Prerequisites MUST link to setup documentation</li> </ul>"},{"location":"rules-howtodoc/#technical-writing-standards","title":"\ud83d\udd27 Technical Writing Standards","text":""},{"location":"rules-howtodoc/#rule-9-code-and-configuration-examples","title":"Rule 9: Code and Configuration Examples","text":""},{"location":"rules-howtodoc/#required-elements","title":"Required Elements:","text":"<ul> <li>Command examples: Include description comments</li> <li>Configuration snippets: Show context and purpose</li> <li>File paths: Always use absolute paths from repository root</li> <li>Variable placeholders: Use <code>[variable-name]</code> format</li> </ul>"},{"location":"rules-howtodoc/#code-block-standards","title":"Code Block Standards:","text":"<pre><code># Description of what this does\n```bash\ncommand --option value\n</code></pre> <pre><code># Configuration section purpose\nkey: value\n</code></pre>"},{"location":"rules-howtodoc/#rule-10-emoji-and-visual-indicators","title":"Rule 10: Emoji and Visual Indicators","text":"<p>Use standardized emoji patterns:</p> <pre><code>## \ud83d\udccb Overview          # Overview sections\n## \ud83d\ude80 Deployment        # Getting started/deployment\n## \ud83c\udfd7\ufe0f Architecture      # System design/structure\n## \u2699\ufe0f Configuration     # Setup and config\n## \ud83d\udd0d Monitoring        # Verification/monitoring\n## \ud83d\udee0\ufe0f Management        # Operations/admin\n## \ud83d\udd27 Troubleshooting   # Problem solving\n## \ud83d\udccb Maintenance       # Ongoing tasks\n## \ud83d\udd17 Related           # Cross-references\n</code></pre>"},{"location":"rules-howtodoc/#status-indicators","title":"Status Indicators:","text":"<pre><code>\u2705 Success/Active/Complete\n\ud83d\udd04 In Progress/Development\n\u26a0\ufe0f Warning/Attention Required\n\u274c Error/Failed/Deprecated\n\ud83c\udfaf Target/Goal/Objective\n\ud83d\udca1 Tip/Insight/Key Point\n</code></pre>"},{"location":"rules-howtodoc/#file-organization-rules","title":"\ud83d\udcc2 File Organization Rules","text":""},{"location":"rules-howtodoc/#rule-11-directory-structure","title":"Rule 11: Directory Structure","text":"<p>Files MUST be placed directly in <code>/doc/</code> folder:</p> <pre><code>doc/\n\u251c\u2500\u2500 README.md                           # Master index\n\u251c\u2500\u2500 overview-*.md                       # High-level guides\n\u251c\u2500\u2500 package-category-readme.md          # Category indexes\n\u251c\u2500\u2500 package-category-service.md         # Service documentation\n\u251c\u2500\u2500 rules-*.md                          # Standards and patterns\n\u251c\u2500\u2500 hosts-*.md                          # Host configuration\n\u251c\u2500\u2500 networking-*.md                     # Network setup\n\u251c\u2500\u2500 provision-*.md                      # Infrastructure management\n\u2514\u2500\u2500 troubleshooting-*.md                # Problem resolution\n</code></pre>"},{"location":"rules-howtodoc/#forbidden","title":"Forbidden:","text":"<ul> <li>Subdirectories: All <code>.md</code> files must be in root <code>/doc/</code></li> <li>Spaces in names: Use hyphens instead</li> <li>Version numbers: Use \"Last Updated\" metadata instead</li> <li>Duplicate names: Each filename must be unique</li> </ul>"},{"location":"rules-howtodoc/#rule-12-new-category-creation","title":"Rule 12: New Category Creation","text":"<p>Before creating a new larger grouping:</p>"},{"location":"rules-howtodoc/#required-justification","title":"Required Justification:","text":"<ol> <li>Scope: Does it warrant 3+ service documents?</li> <li>Distinction: Is it clearly different from existing categories?</li> <li>Longevity: Will it remain relevant long-term?</li> <li>User Value: Does it improve navigation and discoverability?</li> </ol>"},{"location":"rules-howtodoc/#creation-process","title":"Creation Process:","text":"<ol> <li>Document proposal with justification</li> <li>Create index file first: <code>[grouping]-readme.md</code></li> <li>Add to main README.md navigation</li> <li>Create minimum 2 service documents</li> </ol>"},{"location":"rules-howtodoc/#quality-assurance-rules","title":"\ud83d\udd0d Quality Assurance Rules","text":""},{"location":"rules-howtodoc/#rule-13-review-requirements","title":"Rule 13: Review Requirements","text":"<p>Before publishing documentation:</p>"},{"location":"rules-howtodoc/#self-review-checklist","title":"Self-Review Checklist:","text":"<ul> <li>[ ] Header format matches Rule 3 exactly</li> <li>[ ] Structure follows appropriate template (Rule 4 or 5)</li> <li>[ ] All required metadata fields present</li> <li>[ ] Code examples tested and working</li> <li>[ ] Cross-references valid and functional</li> <li>[ ] Grammar and spelling checked</li> <li>[ ] Last Updated date is current</li> </ul>"},{"location":"rules-howtodoc/#content-validation","title":"Content Validation:","text":"<ul> <li>[ ] Commands execute successfully</li> <li>[ ] File paths exist and are correct</li> <li>[ ] Configuration examples are valid</li> <li>[ ] Screenshots/diagrams are current</li> <li>[ ] Use cases demonstrate real value</li> </ul>"},{"location":"rules-howtodoc/#rule-14-maintenance-standards","title":"Rule 14: Maintenance Standards","text":""},{"location":"rules-howtodoc/#update-requirements","title":"Update Requirements:","text":"<ul> <li>Content changes: Update \"Last Updated\" date</li> <li>Significant revisions: Update purpose or target audience if needed</li> <li>Broken links: Fix within 24 hours of discovery</li> <li>Deprecated features: Mark clearly with status indicators</li> </ul>"},{"location":"rules-howtodoc/#regular-reviews","title":"Regular Reviews:","text":"<ul> <li>Monthly: Check for broken internal links</li> <li>Quarterly: Validate code examples and commands</li> <li>Semi-annually: Review structure and cross-references</li> <li>Annually: Comprehensive content audit</li> </ul>"},{"location":"rules-howtodoc/#anti-patterns-and-violations","title":"\ud83d\udea8 Anti-Patterns and Violations","text":""},{"location":"rules-howtodoc/#rule-15-forbidden-practices","title":"Rule 15: Forbidden Practices","text":""},{"location":"rules-howtodoc/#never-do-this","title":"Never Do This:","text":"<pre><code>\u274c # How to setup postgres              # Unclear, no context\n\u274c **File**: postgres.md               # Wrong path format\n\u274c **Purpose**: This document...       # Verbose, not specific\n\u274c **Target Audience**: Everyone       # Too broad\n\u274c **Last Updated**: 2024              # Incomplete date format\n\u274c See documentation for setup         # Vague cross-reference\n\u274c Run this command: sudo rm -rf /     # Dangerous without context\n</code></pre>"},{"location":"rules-howtodoc/#always-do-this","title":"Always Do This:","text":"<pre><code>\u2705 # PostgreSQL - Primary Database Service\n\u2705 **File**: `docs/package-databases-postgresql.md`\n\u2705 **Purpose**: Complete guide to PostgreSQL deployment and configuration in Urbalurba infrastructure\n\u2705 **Target Audience**: Database administrators, developers, architects\n\u2705 **Last Updated**: September 23, 2025\n\u2705 **Documentation**: [PostgreSQL Setup Guide](./package-databases-postgresql.md)\n\u2705 # Deploy PostgreSQL with authentication\n   kubectl apply -f manifests/050-postgresql-config.yaml\n</code></pre>"},{"location":"rules-howtodoc/#success-metrics","title":"\ud83d\udcc8 Success Metrics","text":""},{"location":"rules-howtodoc/#rule-16-quality-indicators","title":"Rule 16: Quality Indicators","text":"<p>Documentation quality is measured by:</p>"},{"location":"rules-howtodoc/#structure-compliance","title":"Structure Compliance:","text":"<ul> <li>\u2705 100% header format compliance</li> <li>\u2705 Template structure adherence</li> <li>\u2705 Required section completeness</li> <li>\u2705 Cross-reference accuracy</li> </ul>"},{"location":"rules-howtodoc/#user-value-metrics","title":"User Value Metrics:","text":"<ul> <li>\u2705 Clear, actionable instructions</li> <li>\u2705 Working code examples</li> <li>\u2705 Comprehensive troubleshooting</li> <li>\u2705 Practical use case demonstrations</li> </ul>"},{"location":"rules-howtodoc/#maintenance-health","title":"Maintenance Health:","text":"<ul> <li>\u2705 Current \"Last Updated\" dates</li> <li>\u2705 No broken links</li> <li>\u2705 Accurate file references</li> <li>\u2705 Valid command examples</li> </ul> <p>\ud83d\udca1 Key Insight: Consistent documentation structure and naming conventions are essential for maintainability and user experience. These rules ensure that all documentation in the Urbalurba infrastructure follows predictable patterns, making it easier for users to find information and for contributors to create high-quality documentation that serves its intended audience effectively.</p>"},{"location":"rules-ingress-traefik/","title":"Traefik Ingress Rules - Cluster Standards Guide","text":"<p>File: <code>docs/rules-ingress-traefik.md</code> Purpose: Explain how ingress is configured in this Kubernetes cluster using Traefik Target Audience: Developers, DevOps engineers, and anyone working with cluster ingress Last Updated: September 01, 2025  </p>"},{"location":"rules-ingress-traefik/#overview","title":"\ud83d\udccb Overview","text":"<p>This cluster uses Traefik as the primary Ingress Controller with Traefik IngressRoute CRDs as the standard pattern. This approach provides more flexibility and features than standard Kubernetes Ingress resources.</p>"},{"location":"rules-ingress-traefik/#cluster-ingress-architecture","title":"\ud83c\udfd7\ufe0f Cluster Ingress Architecture","text":""},{"location":"rules-ingress-traefik/#components","title":"Components:","text":"<ul> <li>Traefik: Ingress Controller (runs as a DaemonSet)</li> <li>IngressRoute CRDs: Custom Resource Definitions for advanced routing</li> <li>Entry Points: HTTP (<code>web</code> port 80) and HTTPS (<code>websecure</code> port 443)</li> <li>Priority System: Determines route matching order</li> </ul>"},{"location":"rules-ingress-traefik/#dns-and-localhost-routing","title":"DNS and Localhost Routing:","text":"<p>This cluster uses the localhost feature for seamless development:</p> <ul> <li>No Hosts File Configuration Required: Developers don't need to modify <code>/etc/hosts</code> or <code>C:\\Windows\\System32\\drivers\\etc\\hosts</code></li> <li>Automatic Routing: Any hostname ending in <code>.localhost</code> automatically routes to <code>127.0.0.1</code> (localhost)</li> <li>Traefik Handles the Rest: Once traffic reaches localhost, Traefik routes it based on the hostname in the request</li> </ul> <p>Example Flow: <pre><code>1. Developer types: http://myapp.localhost\n2. DNS resolves: myapp.localhost \u2192 127.0.0.1 (localhost)\n3. Request reaches: localhost:80 (Traefik)\n4. Traefik matches: Host(`myapp.localhost`) rule\n5. Traefik routes to: myapp-service:8080\n</code></pre></p> <p>Benefits: - \u2705 Zero Configuration: No hosts file editing needed - \u2705 Instant Access: New services immediately accessible - \u2705 Consistent: Same pattern for all developers - \u2705 Clean: No local machine pollution</p>"},{"location":"rules-ingress-traefik/#internal-dns-resolution-for-pod-to-pod-communication","title":"Internal DNS Resolution for Pod-to-Pod Communication:","text":"<p>While the localhost routing works perfectly for browser access, pods within the cluster need different DNS resolution to communicate with services using the same hostnames.</p> <p>The Challenge: - Browser Context: <code>authentik.localhost</code> \u2192 <code>127.0.0.1</code> \u2192 Traefik \u2192 Service \u2705 - Pod Context: <code>authentik.localhost</code> \u2192 <code>127.0.0.1</code> \u2192 \u274c Unreachable from inside pods - OAuth Integration: OpenWebUI pods need to call <code>authentik.localhost</code> for authentication discovery - Service Communication: Internal APIs need consistent hostname resolution</p> <p>The Solution - CoreDNS Rewrite Rules: This cluster implements internal DNS resolution using CoreDNS rewrite rules that map <code>*.localhost</code> hostnames to internal service FQDNs:</p> <pre><code># CoreDNS Configuration (REMOVED - was manifests/005-internal-dns.yaml)\nrewrite name authentik.localhost authentik-server.authentik.svc.cluster.local\nrewrite name openwebui.localhost open-webui.ai.svc.cluster.local\n</code></pre> <p>Dual-Context Architecture: <pre><code># External/Browser Access:\nBrowser \u2192 authentik.localhost \u2192 127.0.0.1 \u2192 Traefik \u2192 authentik-server.authentik\n\n# Internal/Pod Access:\nPod \u2192 authentik.localhost \u2192 CoreDNS \u2192 10.43.x.x (ClusterIP) \u2192 authentik-server.authentik\n</code></pre></p> <p>Key Benefits: - \u2705 Same Hostnames: Applications use identical URLs in all contexts - \u2705 OAuth Integration: Enables OpenWebUI \u2194 Authentik authentication - \u2705 Service Discovery: Internal APIs accessible via consistent hostnames - \u2705 Zero App Changes: No configuration differences between internal/external - \u2705 Automatic Resolution: Pods automatically resolve <code>*.localhost</code> to service IPs</p> <p>Implementation Details: - File: <code>manifests/005-internal-dns.yaml</code> (REMOVED - was deployed early in cluster setup) - Method: Patches existing CoreDNS ConfigMap with rewrite rules - Scope: All pods cluster-wide get the internal DNS resolution - Verification: <code>kubectl exec -it &lt;pod&gt; -- getent hosts authentik.localhost</code></p> <p>Critical for These Use Cases: - \ud83d\udd10 OAuth Authentication: OpenWebUI \u2192 Authentik integration - \ud83c\udf10 API Communication: Service-to-service internal calls - \ud83d\udcca Monitoring: Services calling other services for metrics/health - \ud83d\udd04 Webhooks: Internal callback URLs using consistent hostnames</p> <p>DNS Resolution Flow: <pre><code>Pod Request \u2192 CoreDNS \u2192 Rewrite Rule Applied \u2192 Service ClusterIP \u2192 Target Service\n\nExample:\nauthentik.localhost \u2192 authentik-server.authentik.svc.cluster.local \u2192 10.43.119.98 \u2192 Authentik Service\n</code></pre></p> <p>This dual-context DNS resolution ensures that the same hostname works seamlessly in both browser and pod contexts, enabling complex integrations like OAuth while maintaining the simplicity of the localhost development pattern.</p>"},{"location":"rules-ingress-traefik/#authentication-in-the-cluster","title":"Authentication in the Cluster:","text":"<p>This cluster supports optional authentication using Authentik as the identity provider. Services can be configured as public (no auth) or protected (requires login). Protected services use Traefik middleware (<code>authentik-forward-auth</code>) that forwards authentication requests to Authentik before serving content. See <code>manifests/075-authentik-config.yaml</code>, <code>manifests/077-authentik-forward-auth-middleware.yaml</code>, and <code>manifests/078-whoami-protected-ingressroute.yaml</code> for implementation examples.</p>"},{"location":"rules-ingress-traefik/#external-traffic-access","title":"External Traffic Access:","text":"<p>For external access beyond localhost, this cluster supports Cloudflare Tunnels and Tailscale Funnel to securely route external traffic to Traefik. External domains can be configured to route through Cloudflare (with WAF/DDoS protection) or directly via Tailscale Funnel, while maintaining the same Traefik ingress rules. See <code>docs/networking-cloudflare-setup.md</code>, <code>docs/networking-tailscale-setup.md</code>, and <code>docs/networking-readme.md</code> for setup details.</p>"},{"location":"rules-ingress-traefik/#why-traefik-ingressroute-crds","title":"Why Traefik IngressRoute CRDs?","text":"<ul> <li>More Features: Path rewriting, header manipulation, middleware</li> <li>Better Performance: Direct integration with Traefik</li> <li>Cluster Standard: Consistent with existing infrastructure</li> <li>Advanced Routing: Complex matching rules and conditions</li> </ul>"},{"location":"rules-ingress-traefik/#api-version-and-standards","title":"\ud83d\udd27 API Version and Standards","text":""},{"location":"rules-ingress-traefik/#current-traefik-version","title":"Current Traefik Version:","text":"<p>This cluster is running Traefik 3.3.6 (Rancher Desktop distribution).</p>"},{"location":"rules-ingress-traefik/#available-api-versions","title":"Available API Versions:","text":"<pre><code># Current cluster has these Traefik API resources:\ningressroutes        traefik.io/v1alpha1     true    IngressRoute\ningressroutetcps     traefik.io/v1alpha1     true    IngressRouteTCP\ningressrouteudps     traefik.io/v1alpha1     true    IngressRouteUDP\nmiddlewares          traefik.io/v1alpha1     true    Middleware\nmiddlewaretcps       traefik.io/v1alpha1     true    MiddlewareTCP\nserverstransports    traefik.io/v1alpha1     true    ServersTransport\nserverstransporttcps traefik.io/v1alpha1     true    ServersTransportTCP\ntlsoptions           traefik.io/v1alpha1     true    TLSOption\ntlsstores            traefik.io/v1alpha1     true    TLSStore\ntraefikservices      traefik.io/v1alpha1     true    TraefikService\n</code></pre>"},{"location":"rules-ingress-traefik/#api-version-status","title":"API Version Status:","text":"<ul> <li><code>traefik.io/v1alpha1</code>: \u2705 Currently Supported - This is the working version in Traefik 3.3.6</li> <li><code>traefik.io/v1</code>: \u274c Not Available - This version is not yet available in Traefik 3.3.6</li> <li><code>hub.traefik.io/v1alpha1</code>: \u2705 Available - For newer Traefik Hub features (APIs, Portals, etc.)</li> </ul>"},{"location":"rules-ingress-traefik/#required-api-version-for-ingress","title":"Required API Version for Ingress:","text":"<pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRoute\n</code></pre> <p>\u26a0\ufe0f Important: Use <code>traefik.io/v1alpha1</code> - this is the current working version in Traefik 3.3.6. While <code>traefik.io/v1</code> may be available in future Traefik versions, it's not yet available in this cluster.</p>"},{"location":"rules-ingress-traefik/#standard-structure","title":"Standard Structure:","text":"<pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: &lt;service-name&gt;\n  namespace: default\n  labels:\n    app: &lt;application-name&gt;\n    component: &lt;component-type&gt;\n    protection: &lt;auth-level&gt;\nspec:\n  entryPoints:\n    - web  # HTTP port 80\n  routes:\n    - match: &lt;routing-rule&gt;\n      kind: Rule\n      services:\n        - name: &lt;service-name&gt;\n          port: &lt;port-number&gt;\n</code></pre>"},{"location":"rules-ingress-traefik/#priority-system","title":"\ud83c\udfaf Priority System","text":""},{"location":"rules-ingress-traefik/#how-priorities-work","title":"How Priorities Work:","text":"<ul> <li>Lower numbers = Higher priority (checked first)</li> <li>Higher numbers = Lower priority (checked last)</li> <li>No priority specified = Default higher priority</li> </ul>"},{"location":"rules-ingress-traefik/#priority-guidelines","title":"Priority Guidelines:","text":"<pre><code># High Priority (1-10): Critical services, authentication\npriority: 1\n\n# Medium Priority (10-50): Application services\npriority: 25\n\n# Low Priority (50+): Fallback, catch-all\npriority: 100\n</code></pre>"},{"location":"rules-ingress-traefik/#example-nginx-catch-all","title":"Example - Nginx Catch-All:","text":"<pre><code># File: manifests/020-nginx-root-ingress.yaml\nspec:\n  routes:\n    - match: PathPrefix(`/`)\n      kind: Rule\n      priority: 1  # LOWEST priority - ensures all other routes are checked first\n      services:\n        - name: nginx\n          port: 80\n</code></pre> <p>Purpose: Acts as a fallback for any unmatched requests. Priority 1 ensures it's checked last.</p>"},{"location":"rules-ingress-traefik/#routing-patterns","title":"\ud83c\udf10 Routing Patterns","text":""},{"location":"rules-ingress-traefik/#1-hostregexp-pattern-routing-recommended-for-multi-domain-support","title":"1. HostRegexp Pattern Routing (Recommended - For Multi-Domain Support)","text":"<pre><code># Recommended pattern for all services - enables multi-domain access\nspec:\n  routes:\n    - match: HostRegexp(`myapp\\..+`)\n      kind: Rule\n      services:\n        - name: myapp-service\n          port: 80\n</code></pre> <p>Best For: All services - enables unified internal/external access and future domain support Benefits:  - \u2705 Works on <code>.localhost</code> (internal development) - \u2705 Works on <code>.urbalurba.no</code> (external demo) - \u2705 Works on any future domains automatically - \u2705 No need to update IngressRoute when adding new domains</p>"},{"location":"rules-ingress-traefik/#2-path-based-routing","title":"2. Path-Based Routing","text":"<pre><code>spec:\n  routes:\n    - match: PathPrefix(`/api`)\n      kind: Rule\n      services:\n        - name: api-service\n          port: 8080\n</code></pre> <p>Best For: API endpoints, path-specific routing</p>"},{"location":"rules-ingress-traefik/#3-complex-matching-use-sparingly","title":"3. Complex Matching (Use Sparingly)","text":"<pre><code>spec:\n  routes:\n    - match: Host(`service.localhost`) &amp;&amp; PathPrefix(`/admin`)\n      kind: Rule\n      services:\n        - name: admin-service\n          port: 8080\n</code></pre> <p>Best For: Advanced routing needs, but can cause debugging issues</p>"},{"location":"rules-ingress-traefik/#4-hostregexp-pattern-routing-advanced-for-unified-internalexternal-access","title":"4. HostRegexp Pattern Routing (Advanced - For Unified Internal/External Access)","text":"<pre><code># File: manifests/078-whoami-protected-ingressroute.yaml\nspec:\n  routes:\n    - match: HostRegexp(`whoami\\..+`)\n      kind: Rule\n      services:\n        - name: whoami\n          port: 80\n      middlewares:\n        - name: authentik-forward-auth\n          namespace: default\n</code></pre> <p>Best For: Services that need to work on both internal (.localhost) and external (.urbalurba.no) domains Pattern Explanation: <code>whoami\\..+</code> matches any domain starting with <code>whoami.</code> - \u2705 <code>whoami.localhost</code> (internal development) - \u2705 <code>whoami.urbalurba.no</code> (external demo via Cloudflare tunnel) - \u2705 <code>whoami.example.com</code> (any future domain)</p> <p>Benefits: - Unified Routing: Single IngressRoute handles multiple domains - Future-Proof: Automatically supports new domains without configuration changes - Cloudflare Ready: External access via Cloudflare tunnel without duplicating rules - Seamless Switching: Easy transition between development and demo modes</p>"},{"location":"rules-ingress-traefik/#authentication-with-authentik","title":"\ud83d\udd10 Authentication with Authentik","text":""},{"location":"rules-ingress-traefik/#overview_1","title":"Overview","text":"<p>This cluster supports optional authentication using Authentik as the identity provider. Services can be configured as public (no auth) or protected (requires login). Protected services use Traefik middleware (<code>authentik-forward-auth</code>) that forwards authentication requests to Authentik before serving content.</p>"},{"location":"rules-ingress-traefik/#authentication-flow","title":"Authentication Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Traefik\n    participant Middleware\n    participant Authentik\n    participant Service\n\n    User-&gt;&gt;Traefik: 1. Visit protected service\n    Note over User,Traefik: e.g., http://whoami.localhost\n\n    Traefik-&gt;&gt;Middleware: 2. Intercept request\n    Note over Traefik,Middleware: authentik-forward-auth middleware\n\n    alt Unauthenticated\n        Middleware-&gt;&gt;Authentik: 3. Check authentication\n        Authentik-&gt;&gt;Middleware: 4. Not authenticated\n        Middleware-&gt;&gt;Traefik: 5. Redirect to login\n        Traefik-&gt;&gt;User: 6. Redirect to Authentik login page\n        Note over User,Authentik: User logs in via Authentik UI\n\n        User-&gt;&gt;Authentik: 7. Submit credentials\n        Authentik-&gt;&gt;User: 8. Authentication successful\n        User-&gt;&gt;Traefik: 9. Return to original service\n    else Authenticated\n        Middleware-&gt;&gt;Authentik: 3. Check authentication\n        Authentik-&gt;&gt;Middleware: 4. User authenticated\n        Middleware-&gt;&gt;Traefik: 5. Add auth headers\n        Traefik-&gt;&gt;Service: 6. Forward request with headers\n        Service-&gt;&gt;Traefik: 7. Service response\n        Traefik-&gt;&gt;User: 8. Return service content\n    end\n</code></pre> <p>Step-by-Step Process: 1. Unauthenticated Request: User visits protected service 2. Traefik Intercepts: Middleware catches the request 3. Redirect to Authentik: User is sent to login page 4. User Authentication: User logs in via Authentik 5. Return to Service: After successful auth, user is redirected back 6. Service Access: Service receives authentication headers</p>"},{"location":"rules-ingress-traefik/#required-components","title":"Required Components","text":"<ol> <li>Authentik Deployment: <code>manifests/075-authentik-config.yaml</code></li> <li>CSP Middleware: <code>manifests/076-authentik-csp-middleware.yaml</code> (for external domain support)</li> <li>Forward Auth Middleware: <code>manifests/077-authentik-forward-auth-middleware.yaml</code></li> <li>Protected IngressRoute: Example below</li> </ol>"},{"location":"rules-ingress-traefik/#csp-middleware-for-external-domains","title":"CSP Middleware for External Domains","text":"<p>When using external domains (like <code>authentik.urbalurba.no</code> via Cloudflare tunnel), the authentication UI experiences mixed content errors because: - Page loads over HTTPS: <code>https://authentik.urbalurba.no</code> - But API calls use HTTP: <code>http://authentik.urbalurba.no/api/v3/...</code> - Browsers block HTTP requests from HTTPS pages</p> <p>The CSP middleware solves this by adding the <code>upgrade-insecure-requests</code> header:</p> <pre><code># File: manifests/076-authentik-csp-middleware.yaml\napiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\n  name: authentik-csp-upgrade\n  namespace: authentik\nspec:\n  headers:\n    customResponseHeaders:\n      Content-Security-Policy: \"upgrade-insecure-requests\"\n</code></pre> <p>How it works: - HTTPS domains (external): Get CSP header \u2192 Browser automatically upgrades HTTP API calls to HTTPS \u2705 - HTTP domains (.localhost): No CSP header \u2192 Works normally with HTTP \u2705</p> <p>Integration: The CSP middleware is automatically applied to the Authentik IngressRoute: <pre><code># File: manifests/076-authentik-ingressroute.yaml\nspec:\n  routes:\n    - match: HostRegexp(`authentik\\..+`)\n      middlewares:\n        - name: authentik-csp-upgrade\n          namespace: authentik\n      services:\n        - name: authentik-server\n          port: 80\n</code></pre></p> <p>Benefits: - \u2705 Enables external domain authentication (Cloudflare, Tailscale) - \u2705 Preserves localhost development workflow - \u2705 Uses browser-native mixed content resolution - \u2705 No server-side configuration changes needed</p>"},{"location":"rules-ingress-traefik/#example-protected-service-with-hostregexp","title":"Example: Protected Service with HostRegexp","text":"<pre><code># File: manifests/078-whoami-protected-ingressroute.yaml\napiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: whoami-protected\n  namespace: default\n  labels:\n    app: whoami\n    type: protected\n    routing: unified\n    protection: authentik-forward-auth\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: HostRegexp(`whoami\\..+`)\n      kind: Rule\n      services:\n        - name: whoami\n          port: 80\n      middlewares:\n        - name: authentik-forward-auth\n          namespace: default\n</code></pre> <p>Key Elements: - HostRegexp Pattern: <code>whoami\\..+</code> for unified internal/external routing - Middleware Reference: <code>authentik-forward-auth</code> in default namespace - Labels: <code>type: protected</code> and <code>routing: unified</code> for clarity</p>"},{"location":"rules-ingress-traefik/#authentication-headers","title":"Authentication Headers","text":"<p>When properly configured, the following headers are passed to your service: <pre><code># Headers available in your application\n- X-Forwarded-User          # Username\n- X-Forwarded-Email         # User email\n- X-Forwarded-Groups        # User groups/roles\n- X-Forwarded-Name          # Full name\n- X-Forwarded-Preferred-Username  # Preferred username\n- X-Forwarded-User-Id       # User ID\n</code></pre></p>"},{"location":"rules-ingress-traefik/#ui-configuration-prerequisites","title":"UI Configuration Prerequisites","text":"<p>Before deploying protected services, complete these steps in Authentik UI:</p> <ol> <li>Create Application: </li> <li>Name: <code>whoami</code>, Slug: <code>whoami</code></li> <li> <p>Type: <code>Proxy</code></p> </li> <li> <p>Create Proxy Provider:</p> </li> <li>Name: <code>whoami-provider</code></li> <li>Mode: <code>Forward auth (single application)</code></li> <li> <p>External Host: <code>https://whoami.urbalurba.no</code></p> </li> <li> <p>Link Provider to Application:</p> </li> <li>Edit whoami application</li> <li> <p>Assign whoami-provider</p> </li> <li> <p>Configure Outpost:</p> </li> <li>Edit \"authentik Embedded Outpost\"</li> <li>Add whoami application</li> </ol>"},{"location":"rules-ingress-traefik/#testing-authentication","title":"Testing Authentication","text":"<pre><code># Test unauthenticated access (should redirect to login)\ncurl -L http://whoami.localhost\n\n# Test authenticated access (browser required)\nopen http://whoami.localhost\n\n# Compare with public route (no auth required)\ncurl http://whoami-public.localhost\n</code></pre>"},{"location":"rules-ingress-traefik/#public-vs-protected-routes","title":"Public vs Protected Routes","text":"<pre><code># Public Route (No Authentication)\n# File: manifests/071-whoami-public-ingressroute.yaml\nspec:\n  routes:\n    - match: HostRegexp(`whoami-public\\..+`)\n      services:\n        - name: whoami\n          port: 80\n      # No middlewares = public access\n\n# Protected Route (With Authentication)\n# File: manifests/078-whoami-protected-ingressroute.yaml\nspec:\n  routes:\n    - match: HostRegexp(`whoami\\..+`)\n      services:\n        - name: whoami\n          port: 80\n      middlewares:\n        - name: authentik-forward-auth  # Authentication required\n          namespace: default\n</code></pre> <p>Pattern: Use <code>whoami-public\\..+</code> for public access and <code>whoami\\..+</code> for protected access to the same service.</p>"},{"location":"rules-ingress-traefik/#working-examples","title":"\ud83d\udcc1 Working Examples","text":""},{"location":"rules-ingress-traefik/#file-organization","title":"File Organization:","text":"<p>All ingress configuration files are located in the <code>manifests/</code> folder.</p> <p>\u26a0\ufe0f See doc/rules-naming-conventions.md for complete naming patterns.</p> <p>Quick Reference: - File Naming: IngressRoute files end with <code>-ingressroute.yaml</code> - Examples:   - <code>038-grafana-ingressroute.yaml</code>   - <code>039-otel-collector-ingressroute.yaml</code>   - <code>071-whoami-public-ingressroute.yaml</code> - Numbering: Files numbered to indicate deployment order and component grouping</p>"},{"location":"rules-ingress-traefik/#example-1-nginx-catch-all","title":"Example 1: Nginx Catch-All","text":"<pre><code># File: manifests/020-nginx-root-ingress.yaml\napiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: nginx-root-catch-all\n  namespace: default\n  labels:\n    app: nginx\n    component: catch-all-routing\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: PathPrefix(`/`)\n      kind: Rule\n      priority: 1  # Lowest priority - fallback\n      services:\n        - name: nginx\n          port: 80\n</code></pre> <p>Purpose: Fallback for unmatched requests Priority: 1 (lowest - checked last) Pattern: <code>PathPrefix(/)</code> - matches everything</p>"},{"location":"rules-ingress-traefik/#example-2-whoami-public-service-working-file","title":"Example 2: Whoami Public Service (Working File)","text":"<pre><code># File: manifests/071-whoami-public-ingressroute.yaml\n# This is the ACTUAL working file with HostRegexp pattern\napiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: whoami-public\n  namespace: default\n  labels:\n    app: whoami\n    type: public\n    routing: unified\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: HostRegexp(`whoami-public\\..+`)\n      kind: Rule\n      services:\n        - name: whoami\n          port: 80\n</code></pre> <p>Purpose: Unified internal/external routing with HostRegexp Priority: None specified (defaults to higher than catch-all) Pattern: <code>HostRegexp(whoami-public\\..+)</code> - matches whoami-public.localhost, whoami-public.urbalurba.no, etc. Status: \u2705 This is the ACTUAL working file in your cluster</p>"},{"location":"rules-ingress-traefik/#common-mistakes-to-avoid","title":"\ud83d\udeab Common Mistakes to Avoid","text":""},{"location":"rules-ingress-traefik/#1-wrong-api-version","title":"1. Wrong API Version","text":"<pre><code># \u274c WRONG - Will cause \"no matches for kind\" error\napiVersion: traefik.io/v1\n\n# \u2705 CORRECT - This is the working version\napiVersion: traefik.io/v1alpha1\n</code></pre>"},{"location":"rules-ingress-traefik/#2-over-complex-routing","title":"2. Over-Complex Routing","text":"<pre><code># \u274c AVOID - Complex matching can cause issues\nmatch: Host(`service.localhost`) &amp;&amp; PathPrefix(`/api`) &amp;&amp; Header(`Content-Type`, `application/json`)\n\n# \u2705 PREFER - Simple, reliable routing\nmatch: Host(`api.localhost`)\n</code></pre>"},{"location":"rules-ingress-traefik/#3-missing-priority-for-catch-all","title":"3. Missing Priority for Catch-All","text":"<pre><code># \u274c WRONG - Will interfere with other routes\nmatch: PathPrefix(`/`)\npriority: 100  # Too high - other routes won't work\n\n# \u2705 CORRECT - Low priority for fallback\nmatch: PathPrefix(`/`)\npriority: 1  # Lowest - checked last\n</code></pre>"},{"location":"rules-ingress-traefik/#4-port-mismatches","title":"4. Port Mismatches","text":"<pre><code># \u274c WRONG - Service port doesn't match\nservices:\n  - name: my-service\n    port: 8080  # But service actually runs on port 80\n\n# \u2705 CORRECT - Verify actual service port\nservices:\n  - name: my-service\n    port: 80  # Match actual service port\n</code></pre>"},{"location":"rules-ingress-traefik/#debugging-ingress-issues","title":"\ud83d\udd0d Debugging Ingress Issues","text":""},{"location":"rules-ingress-traefik/#check-ingressroute-status","title":"Check IngressRoute Status:","text":"<pre><code># List all IngressRoutes\nkubectl get ingressroute\n\n# Describe specific IngressRoute\nkubectl describe ingressroute &lt;name&gt;\n\n# Get YAML configuration\nkubectl get ingressroute &lt;name&gt; -o yaml\n</code></pre>"},{"location":"rules-ingress-traefik/#test-service-directly","title":"Test Service Directly:","text":"<pre><code># Port-forward to service\nkubectl port-forward svc/&lt;service-name&gt; &lt;local-port&gt;:&lt;service-port&gt;\n\n# Test locally\ncurl -i http://localhost:&lt;local-port&gt;/\n</code></pre>"},{"location":"rules-ingress-traefik/#check-traefik-logs","title":"Check Traefik Logs:","text":"<pre><code># Get Traefik pod name\nkubectl get pods -l app.kubernetes.io/name=traefik\n\n# View logs\nkubectl logs &lt;traefik-pod-name&gt; -f\n</code></pre>"},{"location":"rules-ingress-traefik/#verify-service-health","title":"Verify Service Health:","text":"<pre><code># Check service endpoints\nkubectl get endpoints &lt;service-name&gt;\n\n# Check pod status\nkubectl get pods -l app=&lt;app-label&gt;\n</code></pre>"},{"location":"rules-ingress-traefik/#debug-internal-dns-resolution","title":"Debug Internal DNS Resolution:","text":"<pre><code># Test DNS resolution from pod\nkubectl exec -it &lt;pod-name&gt; -- getent hosts authentik.localhost\n\n# Verify CoreDNS configuration\nkubectl get configmap coredns -n kube-system -o yaml\n\n# Check CoreDNS logs\nkubectl logs -n kube-system -l k8s-app=kube-dns -f\n\n# Test service connectivity from pod\nkubectl exec -it &lt;pod-name&gt; -- python -c \"import socket; print(socket.gethostbyname('authentik.localhost'))\"\n</code></pre>"},{"location":"rules-ingress-traefik/#best-practices","title":"\ud83d\udcdd Best Practices","text":""},{"location":"rules-ingress-traefik/#1-use-hostregexp-for-all-services-recommended","title":"1. Use HostRegexp for All Services (Recommended)","text":"<pre><code># \u2705 RECOMMENDED - For all services to enable multi-domain support\nmatch: HostRegexp(`myapp\\..+`)\n\n# Pattern Examples:\n# - myapp.localhost (internal development)\n# - myapp.urbalurba.no (external demo)\n# - myapp.example.com (future domains)\n\n# \u274c AVOID - Limited to single domain\nmatch: Host(`myapp.localhost`)\n</code></pre>"},{"location":"rules-ingress-traefik/#2-set-appropriate-priorities","title":"2. Set Appropriate Priorities","text":"<pre><code># High priority for critical services\npriority: 10\n\n# Medium priority for applications\npriority: 25\n\n# Low priority for fallbacks\npriority: 100\n</code></pre>"},{"location":"rules-ingress-traefik/#3-use-descriptive-names-and-labels","title":"3. Use Descriptive Names and Labels","text":"<pre><code>metadata:\n  name: myapp-api-ingress\n  labels:\n    app: myapp\n    component: api\n    protection: public\n    environment: production\n</code></pre>"},{"location":"rules-ingress-traefik/#4-test-before-production","title":"4. Test Before Production","text":"<pre><code># Test with HostRegexp routing first\nmatch: HostRegexp(`test\\..+`)\n\n# Then add complexity if needed\nmatch: HostRegexp(`test\\..+`) &amp;&amp; PathPrefix(`/api`)\n</code></pre>"},{"location":"rules-ingress-traefik/#migration-from-standard-ingress","title":"\ud83d\udd04 Migration from Standard Ingress","text":""},{"location":"rules-ingress-traefik/#before-standard-kubernetes-ingress","title":"Before (Standard Kubernetes Ingress):","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: myapp-ingress\n  annotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: \"web\"\nspec:\n  ingressClassName: traefik\n  rules:\n  - host: myapp.localhost\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: myapp-service\n            port:\n              number: 80\n</code></pre>"},{"location":"rules-ingress-traefik/#after-traefik-ingressroute","title":"After (Traefik IngressRoute):","text":"<pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: myapp-ingress\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: Host(`myapp.localhost`)\n      kind: Rule\n      services:\n        - name: myapp-service\n          port: 80\n</code></pre>"},{"location":"rules-ingress-traefik/#additional-resources","title":"\ud83d\udcda Additional Resources","text":""},{"location":"rules-ingress-traefik/#official-documentation","title":"Official Documentation:","text":"<ul> <li>Traefik IngressRoute Documentation</li> <li>Traefik Routing Rules</li> </ul>"},{"location":"rules-ingress-traefik/#cluster-specific-files","title":"Cluster-Specific Files:","text":"<ul> <li>Internal DNS: <code>manifests/005-internal-dns.yaml</code> (REMOVED)</li> <li>Nginx Catch-All: <code>manifests/020-nginx-root-ingress.yaml</code></li> <li>Whoami Public: <code>manifests/071-whoami-public-ingressroute.yaml</code></li> <li>Authentik CSP Middleware: <code>manifests/076-authentik-csp-middleware.yaml</code></li> <li>Gravitee Examples: <code>manifests/091-gravitee-ingress.yaml</code></li> </ul>"},{"location":"rules-ingress-traefik/#related-documentation","title":"Related Documentation:","text":"<ul> <li>Networking Overview: <code>docs/networking-readme.md</code></li> <li>Infrastructure Guide: <code>docs/infrastructure-readme.md</code></li> </ul>"},{"location":"rules-ingress-traefik/#summary","title":"\ud83c\udfaf Summary","text":""},{"location":"rules-ingress-traefik/#key-points","title":"Key Points:","text":"<ol> <li>Use <code>traefik.io/v1alpha1</code> - this is the current working version in Traefik 3.3.6</li> <li>Deploy internal DNS early - <code>manifests/005-internal-dns.yaml</code> (REMOVED - was used for pod-to-pod communication)</li> <li>Prefer <code>HostRegexp()</code> patterns for all services to enable multi-domain support</li> <li>Use <code>HostRegexp()</code> for unified internal/external access - single rule handles multiple domains</li> <li>Set appropriate priorities (1 = lowest, 100+ = highest)</li> <li>Test services directly before troubleshooting ingress</li> <li>Follow the working examples in the manifests folder</li> <li>Implement authentication using <code>authentik-forward-auth</code> middleware for protected services</li> </ol>"},{"location":"rules-ingress-traefik/#traefik-version-information","title":"Traefik Version Information:","text":"<ul> <li>Current Version: Traefik 3.3.6 (Rancher Desktop)</li> <li>API Version: <code>traefik.io/v1alpha1</code> is current and supported</li> <li>Future Versions: <code>traefik.io/v1</code> may be available in newer Traefik releases</li> <li>Cluster Status: Using the latest stable API version available</li> </ul>"},{"location":"rules-ingress-traefik/#remember","title":"Remember:","text":"<ul> <li>Internal DNS resolution is required for OAuth and service-to-service communication</li> <li>Traefik IngressRoute CRDs are the cluster standard</li> <li>HostRegexp patterns are preferred for all services to enable multi-domain support</li> <li>Dual-context DNS enables same hostnames in browser and pod contexts</li> <li>Unified routing across internal and external domains</li> <li>Priority system determines route matching order</li> <li>Authentication middleware protects services when needed</li> <li>Test incrementally to avoid debugging complexity</li> </ul> <p>This approach ensures consistent, maintainable ingress configuration across the cluster with support for both development and production environments, while enabling complex internal service communication patterns like OAuth integration.</p>"},{"location":"rules-ingress-traefik/#external-domain-authentication-limitations","title":"\u26a0\ufe0f External Domain Authentication Limitations","text":""},{"location":"rules-ingress-traefik/#the-problem","title":"The Problem","text":"<p>While Authentik works seamlessly with <code>*.localhost</code> domains, adding external domains (like <code>urbalurba.no</code>) requires significant manual configuration. This is a fundamental limitation of Authentik's proxy provider architecture, not our cluster setup.</p>"},{"location":"rules-ingress-traefik/#why-this-happens","title":"Why This Happens","text":"<p>Authentik's proxy providers have a single \"External Host\" field that only accepts ONE specific URL. You cannot use patterns or wildcards. This means: - \u274c Cannot have one provider for both <code>whoami.localhost</code> and <code>whoami.urbalurba.no</code> - \u274c Cannot use patterns like <code>https://whoami.*</code> in the External Host field - \u274c Each domain/service combination needs its own provider</p>"},{"location":"rules-ingress-traefik/#manual-steps-required-per-external-domain","title":"Manual Steps Required Per External Domain","text":"<p>When connecting a new external domain (e.g., via Cloudflare tunnel), developers must:</p>"},{"location":"rules-ingress-traefik/#1-update-csrf-trusted-origins-10-minutes","title":"1. Update CSRF Trusted Origins (~10 minutes)","text":"<p>Edit <code>manifests/075-authentik-config.yaml</code> and add ALL external URLs that will be accessed: <pre><code>- name: AUTHENTIK_WEB__CSRF_TRUSTED_ORIGINS\n  value: \"http://authentik.localhost,https://authentik.urbalurba.no,https://whoami.urbalurba.no,https://openwebui.urbalurba.no\"\n</code></pre> Then redeploy Authentik: <pre><code>kubectl delete -f manifests/075-authentik-config.yaml\nkubectl apply -f manifests/075-authentik-config.yaml\n</code></pre></p>"},{"location":"rules-ingress-traefik/#2-create-proxy-provider-in-authentik-ui-5-minutes-per-service","title":"2. Create Proxy Provider in Authentik UI (~5 minutes per service)","text":"<p>For EACH protected service on the external domain: 1. Login to Authentik admin: <code>https://authentik.urbalurba.no/if/flow/initial-setup/</code> 2. Navigate to Applications \u2192 Providers 3. Click Create \u2192 Proxy Provider 4. Configure:    - Name: <code>whoami-urbalurba-provider</code>    - Authorization flow: <code>default-provider-authorization-implicit-consent</code>    - Type: Select Forward auth (single application)    - External host: <code>https://whoami.urbalurba.no</code> (exact URL, no wildcards)    - Token validity: <code>hours=24</code></p>"},{"location":"rules-ingress-traefik/#3-create-application-3-minutes-per-service","title":"3. Create Application (~3 minutes per service)","text":"<ol> <li>Navigate to Applications \u2192 Applications</li> <li>Click Create</li> <li>Configure:</li> <li>Name: <code>whoami-urbalurba</code></li> <li>Slug: <code>whoami-urbalurba</code></li> <li>Provider: Select the provider created in step 2</li> <li>Policy engine mode: <code>any</code></li> </ol>"},{"location":"rules-ingress-traefik/#4-update-embedded-outpost-2-minutes","title":"4. Update Embedded Outpost (~2 minutes)","text":"<ol> <li>Navigate to Applications \u2192 Outposts</li> <li>Edit authentik Embedded Outpost</li> <li>In Applications, add the new application from step 3</li> <li>Click Update</li> </ol>"},{"location":"rules-ingress-traefik/#time-impact","title":"Time Impact","text":"<ul> <li>First external domain: ~45 minutes (includes CSRF update + first few services)</li> <li>Additional services on same domain: ~10 minutes each</li> <li>New external domain: ~20 minutes + 10 minutes per protected service</li> </ul>"},{"location":"rules-ingress-traefik/#example-scenario","title":"Example Scenario","text":"<p>Developer connects <code>company.com</code> via Cloudflare tunnel and wants to protect 5 services: 1. Update CSRF origins with all 5 service URLs: 10 minutes 2. Create 5 proxy providers (one per service): 25 minutes 3. Create 5 applications: 15 minutes 4. Update outpost configuration: 2 minutes Total: ~52 minutes of manual configuration</p>"},{"location":"rules-ingress-traefik/#what-works-without-manual-configuration","title":"What Works Without Manual Configuration","text":"<p>\u2705 All <code>*.localhost</code> services (development) \u2705 Public services on external domains (no authentication) \u2705 Authentik admin UI on external domains (with CSP middleware)</p>"},{"location":"rules-ingress-traefik/#what-requires-manual-configuration","title":"What Requires Manual Configuration","text":"<p>\u274c Each protected service on external domains \u274c CSRF trusted origins for each new domain \u274c Separate provider/application for each domain/service combination</p>"},{"location":"rules-ingress-traefik/#future-solutions-being-considered","title":"Future Solutions Being Considered","text":"<ol> <li>Custom Authentication Proxy: Build domain-agnostic proxy between Traefik and Authentik</li> <li>Automation Scripts: Use Authentik API to auto-configure providers when new domains detected</li> <li>Alternative Auth Systems: Evaluate Authelia or OAuth2-Proxy for better multi-domain support</li> <li>Kubernetes Operator: Auto-configure Authentik based on IngressRoute annotations</li> </ol>"},{"location":"rules-ingress-traefik/#current-workaround","title":"Current Workaround","text":"<p>For now, developers should: 1. Use <code>*.localhost</code> for development (works automatically) 2. Only configure external domain authentication for production/demo services 3. Keep most services public during development 4. Document which services need protection before going to production</p> <p>This limitation is acknowledged and being worked on, but for now represents a \"one-time setup cost\" per external domain that developers need to be aware of.</p>"},{"location":"rules-naming-conventions/","title":"Naming Conventions","text":"<p>Purpose: Define consistent naming patterns for all files, resources, and identifiers in the urbalurba-infrastructure project.</p> <p>Principle: Predictable names enable automation and make the codebase easier to navigate.</p>"},{"location":"rules-naming-conventions/#file-naming-conventions","title":"File Naming Conventions","text":""},{"location":"rules-naming-conventions/#manifest-files-manifestsyaml","title":"Manifest Files (manifests/*.yaml)","text":"<p>Pattern: <code>NNN-component-type.yaml</code></p> <p>Number Ranges: - 000-029: Core infrastructure (storage, ingress, DNS, networking) - 030-039: Monitoring and observability - 040-069: Data services (databases, caches, message queues) - 070-079: Authentication and authorization - 080-099: Reserved for future core services - 200-229: AI and ML services - 230-299: Application services - 600-799: Management and admin tools - 800-899: Development and testing - 900-999: Reserved for custom/experimental</p> <p>Type Suffixes: - <code>-config.yaml</code> - Helm values configuration - <code>-ingressroute.yaml</code> - Traefik IngressRoute definition - <code>-configmap.yaml</code> - Kubernetes ConfigMap - <code>-secret.yaml</code> - Kubernetes Secret (never committed) - <code>-dashboards.yaml</code> - Grafana dashboard ConfigMaps</p> <p>Examples: <pre><code>030-prometheus-config.yaml          # Prometheus Helm values\n031-tempo-config.yaml               # Tempo Helm values\n032-loki-config.yaml                # Loki Helm values\n033-otel-collector-config.yaml      # OTEL Collector Helm values\n034-grafana-config.yaml             # Grafana Helm values\n035-grafana-dashboards.yaml         # Installation test dashboards\n036-grafana-sovdev-verification.yaml # sovdev-logger verification dashboard\n037-grafana-loggeloven-dashboards.yaml # Loggeloven test suite\n038-grafana-ingressroute.yaml       # Grafana UI ingress\n039-otel-collector-ingressroute.yaml # OTEL ingress\n\n040-postgresql-config.yaml          # PostgreSQL\n041-mysql-config.yaml               # MySQL\n042-mongodb-config.yaml             # MongoDB\n043-redis-config.yaml               # Redis\n\n070-authentik-config.yaml           # Authentik SSO\n071-authentik-blueprints.yaml       # Authentik user/group definitions\n076-authentik-csp-middleware.yaml   # CSP headers middleware\n\n200-openwebui-config.yaml           # OpenWebUI\n201-ollama-config.yaml              # Ollama\n202-litellm-config.yaml             # LiteLLM\n</code></pre></p> <p>Numbering Rules: 1. Main component gets base number (e.g., 030 for Prometheus) 2. Related configs use sequential numbers (031, 032, 033...) 3. IngressRoutes use component's number + 8 (e.g., Grafana=034, Ingress=038) 4. Dashboards use component's number + variations (035, 036, 037) 5. Leave gaps for future expansion within each range</p>"},{"location":"rules-naming-conventions/#ansible-playbooks-ansibleplaybooksyml","title":"Ansible Playbooks (ansible/playbooks/*.yml)","text":"<p>Pattern: <code>NNN-action-component.yml</code></p> <p>Number: Must match corresponding manifest file number</p> <p>Actions: - <code>setup-</code> - Deploy/install component - <code>remove-</code> - Uninstall/delete component - <code>update-</code> - Modify existing component - <code>test-</code> - Verification/testing playbook</p> <p>Examples: <pre><code>030-setup-prometheus.yml            # Deploys using manifests/030-prometheus-config.yaml\n030-remove-prometheus.yml           # Removes Prometheus\n031-setup-tempo.yml                 # Deploys using manifests/031-tempo-config.yaml\n031-remove-tempo.yml                # Removes Tempo\n033-setup-otel-collector.yml        # Deploys OTEL Collector\n033-remove-otel-collector.yml       # Removes OTEL Collector\n034-setup-grafana.yml               # Deploys Grafana\n034-remove-grafana.yml              # Removes Grafana\n</code></pre></p> <p>Pattern Rules: 1. Setup playbook references external manifest via <code>values_files: [\"{{ config_file }}\"]</code> or <code>-f {{ config_file }}</code> 2. Remove playbook just removes Helm chart 3. Number MUST match manifest file (030 playbook uses 030 manifest) 4. Never use inline Helm values - always reference external manifest</p>"},{"location":"rules-naming-conventions/#shell-scripts-provision-hostkubernetes","title":"Shell Scripts (provision-host/kubernetes//)","text":"<p>Pattern: <code>NN-action-component.sh</code></p> <p>Script Number: Sequential within directory (01, 02, 03...)</p> <p>Actions: Same as playbooks (<code>setup-</code>, <code>remove-</code>, <code>update-</code>, <code>test-</code>)</p> <p>Relationship to Playbooks: - Script number is independent of playbook number - Script wraps Ansible playbook call - Script provides proper context and parameters</p> <p>Examples: <pre><code>provision-host/kubernetes/11-monitoring/not-in-use/\n\u251c\u2500\u2500 00-setup-all-monitoring.sh      # Orchestrates all setup scripts\n\u251c\u2500\u2500 00-remove-all-monitoring.sh     # Orchestrates all remove scripts\n\u251c\u2500\u2500 01-setup-prometheus.sh          # Calls ansible/playbooks/030-setup-prometheus.yml\n\u251c\u2500\u2500 01-remove-prometheus.sh         # Calls ansible/playbooks/030-remove-prometheus.yml\n\u251c\u2500\u2500 02-setup-tempo.sh               # Calls ansible/playbooks/031-setup-tempo.yml\n\u251c\u2500\u2500 02-remove-tempo.sh              # Calls ansible/playbooks/031-remove-tempo.yml\n\u251c\u2500\u2500 03-setup-loki.sh                # Calls ansible/playbooks/032-setup-loki.yml\n\u251c\u2500\u2500 03-remove-loki.sh               # Calls ansible/playbooks/032-remove-loki.yml\n\u251c\u2500\u2500 04-setup-otel-collector.sh      # Calls ansible/playbooks/033-setup-otel-collector.yml\n\u251c\u2500\u2500 04-remove-otel-collector.sh     # Calls ansible/playbooks/033-remove-otel-collector.yml\n\u251c\u2500\u2500 05-setup-grafana.sh             # Calls ansible/playbooks/034-setup-grafana.yml\n\u2514\u2500\u2500 05-remove-grafana.sh            # Calls ansible/playbooks/034-remove-grafana.yml\n</code></pre></p> <p>Script Content Pattern: <pre><code>#!/bin/bash\n# Description of what this script does\n\n# Check if target host provided\nif [ -z \"$1\" ]; then\n    echo \"Usage: $0 &lt;target_host&gt;\"\n    exit 1\nfi\n\nTARGET_HOST=\"$1\"\n\n# Call Ansible playbook\nansible-playbook /mnt/urbalurbadisk/ansible/playbooks/030-setup-prometheus.yml \\\n    -e \"target_host=${TARGET_HOST}\"\n</code></pre></p>"},{"location":"rules-naming-conventions/#directory-structure","title":"Directory Structure","text":"<p>Pattern: Numbered directories for deployment order</p> <p>Examples: <pre><code>provision-host/kubernetes/\n\u251c\u2500\u2500 00-system/                      # Core system setup\n\u251c\u2500\u2500 01-storage/                     # Storage provisioners\n\u251c\u2500\u2500 02-ingress/                     # Traefik ingress\n\u251c\u2500\u2500 03-dns/                         # DNS services\n\u251c\u2500\u2500 11-monitoring/                  # Monitoring stack\n\u2502   \u2514\u2500\u2500 not-in-use/                # Testing/development area\n\u251c\u2500\u2500 12-databases/                   # Database services\n\u251c\u2500\u2500 13-auth/                        # Authentication services\n\u2514\u2500\u2500 20-applications/                # Application deployments\n</code></pre></p> <p>Rules: 1. Two-digit prefix for ordering 2. Descriptive name after number 3. <code>not-in-use/</code> subdirectory for scripts under development</p>"},{"location":"rules-naming-conventions/#kubernetes-resource-naming","title":"Kubernetes Resource Naming","text":""},{"location":"rules-naming-conventions/#namespaces","title":"Namespaces","text":"<p>Pattern: <code>lowercase-descriptive</code></p> <p>Examples: <pre><code>monitoring          # Monitoring stack (Prometheus, Grafana, Loki, Tempo, OTEL)\ndatabases          # Database services\nauthentik          # Authentik SSO\nopenwebui          # OpenWebUI and AI services\nkube-system        # Kubernetes system (default)\ntraefik            # Traefik ingress controller\n</code></pre></p> <p>Rules: 1. Single word or hyphenated 2. All lowercase 3. Descriptive of purpose 4. No version numbers</p>"},{"location":"rules-naming-conventions/#helm-release-names","title":"Helm Release Names","text":"<p>Pattern: <code>component-name</code></p> <p>Examples: <pre><code>prometheus              # Prometheus monitoring\ntempo                   # Tempo tracing\nloki                    # Loki log aggregation\notel-collector          # OpenTelemetry Collector\ngrafana                 # Grafana visualization\nauthentik               # Authentik SSO\nopenwebui               # OpenWebUI\n</code></pre></p> <p>Rules: 1. Match component name 2. Lowercase with hyphens 3. No namespace prefix (namespace is separate) 4. Use official chart name when possible</p>"},{"location":"rules-naming-conventions/#configmap-names","title":"ConfigMap Names","text":"<p>Pattern: <code>component-purpose</code> or <code>component-purpose-generated</code></p> <p>Examples: <pre><code>grafana-dashboards-installation     # Installation test dashboards\ngrafana-dashboards-sovdev           # sovdev-logger verification\ngrafana-dashboards-loggeloven       # Loggeloven test suite\notel-collector-config               # OTEL Collector configuration\n</code></pre></p> <p>Rules: 1. Start with component name 2. Add descriptive suffix 3. Use <code>-generated</code> suffix for auto-generated content 4. All lowercase with hyphens</p>"},{"location":"rules-naming-conventions/#ingressroute-names","title":"IngressRoute Names","text":"<p>Pattern: <code>component</code> or <code>component-variant</code></p> <p>Examples: <pre><code>grafana                 # Grafana UI ingress\notel-collector          # OTEL Collector ingress\nprometheus              # Prometheus UI ingress (if needed)\nauthentik               # Authentik SSO ingress\nwhoami-protected        # Test service with auth\nwhoami-public          # Test service without auth\n</code></pre></p> <p>Rules: 1. Match component name 2. Add variant suffix if multiple routes for same component 3. Use descriptive suffixes: <code>-protected</code>, <code>-public</code>, <code>-api</code>, <code>-ui</code></p>"},{"location":"rules-naming-conventions/#git-and-version-control","title":"Git and Version Control","text":""},{"location":"rules-naming-conventions/#branch-names","title":"Branch Names","text":"<p>Pattern: <code>type/description</code></p> <p>Types: - <code>feature/</code> - New feature development - <code>fix/</code> - Bug fixes - <code>docs/</code> - Documentation changes - <code>refactor/</code> - Code refactoring - <code>test/</code> - Testing changes</p> <p>Examples: <pre><code>feature/monitoring-stack-migration\nfeature/sovdev-logger-integration\nfix/authentik-csp-headers\ndocs/development-workflow\nrefactor/monitoring-030-039\n</code></pre></p> <p>Rules: 1. Lowercase with hyphens 2. Descriptive but concise 3. Type prefix required 4. No ticket numbers (use PR description)</p>"},{"location":"rules-naming-conventions/#commit-messages","title":"Commit Messages","text":"<p>Pattern: <pre><code>&lt;type&gt;: &lt;subject&gt;\n\n&lt;body&gt;\n\n&lt;footer&gt;\n</code></pre></p> <p>Types: - <code>feat:</code> - New feature - <code>fix:</code> - Bug fix - <code>docs:</code> - Documentation - <code>refactor:</code> - Code refactoring - <code>test:</code> - Tests - <code>chore:</code> - Maintenance</p> <p>Examples: <pre><code>feat: Add sovdev-logger TypeScript implementation\n\n- Multi-transport architecture (Console + File + OTLP)\n- Smart defaults for production vs development\n- Full Loggeloven compliance with credential filtering\n\nCloses #123\n\n---\n\nfix: OTLP collector ingress Host header routing\n\nAdd missing Host header to OTEL Collector IngressRoute\nto enable proper Traefik routing for external OTLP ingestion.\n\n---\n\ndocs: Create development workflow rules\n\nAdd docs/rules-development-workflow.md explaining\nClaude Code vs manual workflows and path conventions.\n</code></pre></p> <p>Rules: 1. Subject line: 50 chars max, imperative mood, no period 2. Body: Wrap at 72 chars, explain what and why 3. Footer: Reference issues/PRs 4. Use type prefix</p>"},{"location":"rules-naming-conventions/#summary","title":"Summary","text":"<p>Key Principles: 1. \u2705 Numbers indicate deployment order and relationships 2. \u2705 Manifest number = Playbook number (030 manifest \u2192 030 playbook) 3. \u2705 Scripts are sequential within directory (01, 02, 03...) 4. \u2705 All lowercase with hyphens for most names 5. \u2705 Descriptive suffixes for variants (-config, -ingressroute, -dashboards) 6. \u2705 Leave gaps in numbering for future expansion 7. \u2705 External manifest files, never inline Helm values 8. \u2705 Consistent patterns across all file types</p> <p>When adding new components: 1. Choose appropriate number range 2. Create manifest file with proper suffix 3. Create matching Ansible playbooks (setup + remove) 4. Create shell script wrappers 5. Follow established patterns 6. Leave room for related components</p> <p>Reference: - doc/rules-development-workflow.md - Workflow and command execution - doc/rules-automated-kubernetes-deployment.md - Ansible patterns (to be created) - doc/rules-ingress-traefik.md - IngressRoute patterns (to be created)</p>"},{"location":"rules-provisioning/","title":"Provisioning Rules and Standards","text":"<p>File: <code>docs/rules-provisioning.md</code> Purpose: Define the IMPLEMENTATION LAYER - how to write individual deployment scripts and playbooks Target Audience: Developers, DevOps engineers, and LLMs creating deployment scripts Scope: Script/playbook patterns, testing standards, error handling, and implementation best practices</p>"},{"location":"rules-provisioning/#relationship-to-other-rules","title":"Relationship to Other Rules","text":"<p>This document covers how to implement individual deployment scripts: - Shell script + Ansible playbook pattern - Testing and verification standards - Error handling and progress feedback - Implementation best practices</p> <p>For how scripts are organized and executed automatically, see: \u2192 Rules for Automated Kubernetes Deployment - Orchestration and automation framework</p>"},{"location":"rules-provisioning/#overview","title":"\ud83d\udccb Overview","text":"<p>This document establishes mandatory patterns for writing deployment scripts and playbooks in the Urbalurba Infrastructure. These patterns ensure reliability, consistency, and maintainability.</p>"},{"location":"rules-provisioning/#core-deployment-architecture","title":"\ud83c\udfaf Core Deployment Architecture","text":""},{"location":"rules-provisioning/#rule-1-script-ansible-pattern","title":"Rule 1: Script + Ansible Pattern","text":"<p>All deployments MUST follow the Script + Ansible pattern:</p> <pre><code>scripts/packages/[service].sh  \u2192  ansible/playbooks/[nnn]-setup-[service].yml\n     \u2191 Minimal orchestration      \u2191 Heavy lifting implementation\n</code></pre>"},{"location":"rules-provisioning/#script-responsibilities-keep-minimal","title":"Script Responsibilities (Keep Minimal):","text":"<ul> <li>\u2705 Check prerequisites (kubectl access, basic dependencies)</li> <li>\u2705 Call Ansible playbook with proper parameters</li> <li>\u2705 Display final success/failure message</li> <li>\u274c NO business logic - delegate to Ansible</li> <li>\u274c NO complex operations - keep scripts simple</li> </ul>"},{"location":"rules-provisioning/#ansible-playbook-responsibilities-heavy-lifting","title":"Ansible Playbook Responsibilities (Heavy Lifting):","text":"<ul> <li>\u2705 All deployment logic and verification</li> <li>\u2705 Resource creation and configuration</li> <li>\u2705 Comprehensive testing and validation</li> <li>\u2705 Error handling with proper retry mechanisms</li> <li>\u2705 Status reporting and troubleshooting information</li> </ul>"},{"location":"rules-provisioning/#example-structure","title":"Example Structure:","text":"<pre><code># scripts/packages/litellm.sh\n#!/bin/bash\nset -e\necho \"\ud83d\ude80 Deploying LiteLLM AI Gateway...\"\nansible-playbook ansible/playbooks/210-setup-litellm.yml\necho \"\u2705 LiteLLM deployment complete\"\n</code></pre> <pre><code># ansible/playbooks/210-setup-litellm.yml\n- name: Deploy LiteLLM with comprehensive validation\n  # ... all the actual deployment logic\n</code></pre>"},{"location":"rules-provisioning/#script-template-pattern","title":"\ud83d\udcdd Script Template Pattern","text":""},{"location":"rules-provisioning/#rule-1b-script-naming-convention","title":"Rule 1B: Script Naming Convention","text":"<p>\u26a0\ufe0f See doc/rules-naming-conventions.md for complete naming patterns.</p> <p>Quick Reference: - Setup Script: <code>[NN]-setup-[service-name].sh</code> (e.g., <code>05-setup-postgres.sh</code>) - Remove Script: <code>[NN]-remove-[service-name].sh</code> (same number prefix)</p> <p>MANDATORY: Every setup script MUST have a corresponding remove script for clean uninstallation.</p>"},{"location":"rules-provisioning/#rule-1c-check-existing-playbooks-first","title":"Rule 1C: Check Existing Playbooks First","text":"<p>MANDATORY: Before creating any new Ansible playbook, you MUST:</p> <ol> <li> <p>Search existing playbooks: Check <code>ansible/playbooks/</code> for existing implementations    <pre><code># Search for similar service names\nfind ansible/playbooks -name \"*[service-name]*\" -type f\n\n# Search for functionality in playbook content\ngrep -r \"service-functionality\" ansible/playbooks/\n</code></pre></p> </li> <li> <p>Review existing playbook capabilities: Many existing playbooks support multiple operations via parameters</p> </li> <li>Look for <code>operation</code> parameter (e.g., <code>deploy</code>, <code>delete</code>, <code>verify</code>)</li> <li>Check variable definitions and supported modes</li> <li> <p>Review task blocks for conditional logic</p> </li> <li> <p>Extend existing playbooks rather than create new ones when possible:</p> </li> <li>Add new <code>operation</code> modes to existing playbooks</li> <li>Add conditional blocks for new functionality</li> <li> <p>Maintain consistency with existing patterns</p> </li> <li> <p>Create new playbooks ONLY when:</p> </li> <li>No existing playbook handles the service</li> <li>Functionality is completely different from existing patterns</li> <li>Combining would make existing playbook overly complex</li> </ol> <p>Example: The whoami service already has <code>025-setup-whoami-testpod.yml</code> with both deploy and delete operations. Use this instead of creating new playbooks.</p>"},{"location":"rules-provisioning/#rule-1d-standard-script-structure","title":"Rule 1D: Standard Script Structure","text":"<p>All deployment scripts MUST follow this template pattern:</p> <pre><code>#!/bin/bash\n# filename: [NN]-setup-[service].sh\n# description: Deploy [service] to Kubernetes cluster\n\nTARGET_HOST=${1:-\"rancher-desktop\"}\nSTATUS=()\nERROR=0\n\necho \"Starting [service] setup on $TARGET_HOST\"\necho \"---------------------------------------------------\"\n\n# Step 1: Verify prerequisites\n# Step 2: Apply configurations\n# Step 3: Deploy via Helm/manifests\n# Step 4: Verify deployment\n\nprint_summary() {\n    echo \"---------- Installation Summary ----------\"\n    for step in \"${STATUS[@]}\"; do\n        echo \"$step\"\n    done\n    if [ $ERROR -eq 0 ]; then\n        echo \"All steps completed successfully.\"\n    else\n        echo \"Some steps failed. Please check the logs.\"\n    fi\n}\n\nmain() {\n    # Implementation here\n    print_summary\n}\n\nmain \"$@\"\nexit $ERROR\n</code></pre> <p>Key Requirements: - Accept <code>TARGET_HOST</code> as first parameter - Use <code>STATUS</code> array to track step completion - Use <code>ERROR</code> variable for exit code - Include <code>print_summary()</code> function - Call <code>main \"$@\"</code> and <code>exit $ERROR</code></p>"},{"location":"rules-provisioning/#testing-requirements","title":"\ud83e\uddea Testing Requirements","text":""},{"location":"rules-provisioning/#rule-2-no-localhost-testing-from-host-context","title":"Rule 2: No .localhost Testing from Host Context","text":"<p>\u274c CRITICAL ERROR - Never Do This: <pre><code># WRONG: Testing .localhost from Ansible (host context)\n- name: Test service\n  ansible.builtin.uri:\n    url: \"http://service.localhost/health\"  # Will fail!\n</code></pre></p> <p>Problem: Ansible runs on the host machine where <code>.localhost</code> domains resolve to <code>127.0.0.1</code> (the host itself), not to the Traefik ingress controller running in the cluster.</p> <p>Background: The cluster uses a dual-context DNS architecture (detailed in <code>docs/rules-ingress-traefik.md</code>): - External/Browser Context: <code>service.localhost</code> \u2192 <code>127.0.0.1</code> \u2192 Traefik \u2192 Service \u2705 - Internal/Pod Context: <code>service.localhost</code> \u2192 CoreDNS rewrite \u2192 ClusterIP \u2192 Service \u2705 - Host/Ansible Context: <code>service.localhost</code> \u2192 <code>127.0.0.1</code> (host machine) \u274c</p>"},{"location":"rules-provisioning/#rule-3-mandatory-cluster-internal-testing","title":"Rule 3: Mandatory Cluster-Internal Testing","text":"<p>\u2705 CORRECT: Use kubectl run for all service tests: <pre><code># CORRECT: Test from within cluster using temporary pod\n- name: Test service connectivity from within cluster\n  ansible.builtin.shell: |\n    kubectl run curl-test --image=curlimages/curl --rm -i --restart=Never -n {{ namespace }} -- \\\n    curl -s -w \"HTTP_CODE:%{http_code}\" http://{{ service }}:{{ port }}/health\n  register: service_test\n  retries: 5\n  delay: 5\n  until: service_test.rc == 0 and (service_test.stdout.find('HTTP_CODE:200') != -1 or service_test.stdout.find('HTTP_CODE:401') != -1)\n</code></pre></p>"},{"location":"rules-provisioning/#why-this-works","title":"Why This Works:","text":"<ul> <li>\u2705 True Cluster Context: Test pod runs inside cluster with proper DNS resolution</li> <li>\u2705 Service-to-Service Testing: Tests actual communication paths other pods will use</li> <li>\u2705 Temporary &amp; Clean: <code>--rm</code> automatically removes test pod</li> <li>\u2705 No Dependencies: Doesn't require existing pods to have curl/python</li> <li>\u2705 Reliable: Uses proven pattern from working playbooks</li> </ul>"},{"location":"rules-provisioning/#testing-sequence-requirements","title":"Testing Sequence Requirements:","text":"<ol> <li>Internal Service Test: Verify service responds within cluster</li> <li>API Functionality Test: Test actual API endpoints with authentication</li> <li>IngressRoute Verification: Confirm Traefik routing is configured</li> <li>Integration Test: Verify service integrates with dependencies</li> </ol>"},{"location":"rules-provisioning/#error-handling-rules","title":"\ud83d\udd04 Error Handling Rules","text":""},{"location":"rules-provisioning/#rule-4-no-error-ignoring-for-critical-dependencies","title":"Rule 4: No Error Ignoring for Critical Dependencies","text":"<p>\u274c WRONG: Ignoring errors when next steps depend on success: <pre><code>- name: Deploy database\n  command: helm install postgres ...\n  ignore_errors: true  # WRONG! Next steps need this to succeed\n\n- name: Create application tables  # Will fail if database not deployed\n  command: kubectl exec postgres -- psql ...\n</code></pre></p> <p>\u2705 CORRECT: Fail fast for critical dependencies: <pre><code>- name: Deploy database\n  command: helm install postgres ...\n  # No ignore_errors - let it fail if database can't deploy\n\n- name: Wait for database to be ready\n  kubernetes.core.k8s_info:\n    kind: Pod\n    namespace: \"{{ namespace }}\"\n    label_selectors:\n      - app=postgresql\n  register: db_pods\n  retries: 20\n  delay: 15\n  until: db_pods.resources | length &gt; 0 and db_pods.resources[0].status.phase == \"Running\"\n\n- name: Create application tables\n  command: kubectl exec postgres -- psql ...\n  # Now safe to run because database is verified ready\n</code></pre></p>"},{"location":"rules-provisioning/#rule-5-when-to-use-ignore_errors","title":"Rule 5: When to Use ignore_errors","text":"<p>\u2705 Safe to ignore errors: - Cleanup operations (<code>pkill kubectl proxy</code>) - Optional optimizations (cache warmup) - Non-critical status reporting - Tests that don't block deployment progress</p> <p>\u274c Never ignore errors for: - Service deployment steps - Database/storage setup - Required secret creation - Network/ingress configuration - Any step that subsequent steps depend on</p>"},{"location":"rules-provisioning/#verification-standards","title":"\ud83d\udd0d Verification Standards","text":""},{"location":"rules-provisioning/#rule-6-comprehensive-verification-required","title":"Rule 6: Comprehensive Verification Required","text":"<p>Every deployment MUST include:</p> <ol> <li>Pod Readiness Check with Progress Feedback (Two-Stage Pattern):</li> </ol> <p>RECOMMENDED: Two-Stage Pod Readiness Verification</p> <p>For robust deployment verification, use the two-stage pattern:</p> <p>Stage 1: Wait for Pod Running <pre><code>- name: Wait for service pods to be ready (with progress indicators)\n  kubernetes.core.k8s_info:\n    kind: Pod\n    namespace: \"{{ namespace }}\"\n    label_selectors:\n      - app.kubernetes.io/name={{ service_name }}\n  register: service_pods\n  retries: 20\n  delay: 15\n  until: &gt;\n    service_pods.resources | length &gt; 0 and\n    service_pods.resources[0].status.phase == \"Running\"\n</code></pre></p> <p>Stage 2: Wait for Container Ready <pre><code>- name: Wait for service pods to be fully ready (1/1)\n  kubernetes.core.k8s_info:\n    kind: Pod\n    namespace: \"{{ namespace }}\"\n    label_selectors:\n      - app.kubernetes.io/name={{ service_name }}\n  register: service_pods_ready\n  retries: 30\n  delay: 10\n  until: &gt;\n    service_pods_ready.resources | length &gt; 0 and\n    service_pods_ready.resources[0].status.containerStatuses[0].ready == true\n</code></pre></p> <p>Why Two Stages? - Stage 1 (<code>Running</code>): Pod scheduled, containers started, image pulled - Stage 2 (<code>Ready</code>): Application initialized, readiness probes passing, ready for traffic - Benefits: Prevents false positives where pod exists but application isn't ready - Use Cases: Databases, message queues, complex applications with startup sequences</p> <p>Alternative: Single-Stage Pattern (Minimum Requirement) <pre><code>- name: Wait for service pods to be ready (with progress indicators)\n  kubernetes.core.k8s_info:\n    kind: Pod\n    namespace: \"{{ namespace }}\"\n    label_selectors:\n      - app.kubernetes.io/name={{ service_name }}\n  register: service_pods\n  retries: 20\n  delay: 15\n  until: &gt;\n    service_pods.resources | length &gt; 0 and\n    service_pods.resources[0].status.phase == \"Running\"\n</code></pre></p> <ol> <li> <p>Service Connectivity Test: <pre><code>- name: Test service connectivity from within cluster\n  ansible.builtin.shell: |\n    kubectl run curl-test --image=curlimages/curl --rm -i --restart=Never -n {{ namespace }} -- \\\n    curl -s -w \"HTTP_CODE:%{http_code}\" http://{{ service }}:{{ port }}/health\n</code></pre></p> </li> <li> <p>IngressRoute Verification: <pre><code>- name: Verify IngressRoute is created\n  kubernetes.core.k8s_info:\n    api_version: traefik.io/v1alpha1\n    kind: IngressRoute\n    namespace: \"{{ namespace }}\"\n    name: \"{{ service_name }}\"\n  register: ingress_check\n  retries: 5\n  delay: 2\n  until: ingress_check.resources | length &gt; 0\n</code></pre></p> </li> <li> <p>Functional API Test: <pre><code>- name: Test API functionality\n  # Use kubectl proxy or port-forward for API-specific tests\n</code></pre></p> </li> </ol>"},{"location":"rules-provisioning/#rule-7-progress-feedback-for-long-running-tasks","title":"Rule 7: Progress Feedback for Long-Running Tasks","text":"<p>All tasks that may take longer than 30 seconds MUST provide progress feedback to prevent the appearance of hanging.</p> <p>\u274c WRONG: Silent long-running tasks: <pre><code># WRONG: 10 minutes of silence - appears to hang\n- name: Wait for pods to be ready\n  shell: kubectl wait --timeout=600s ...\n</code></pre></p> <p>\u2705 CORRECT: Ansible retries with progress indicators: <pre><code># CORRECT: Progress every 15 seconds with retry counters\n- name: Wait for pods to be ready (with progress indicators)\n  kubernetes.core.k8s_info:\n    kind: Pod\n    label_selectors: [...]\n  retries: 40     # Clear total attempt count\n  delay: 15       # Regular progress intervals\n  until: condition_met\n</code></pre></p> <p>Expected User Experience: <pre><code>FAILED - RETRYING: [localhost]: Wait for pods (40 retries left).\nFAILED - RETRYING: [localhost]: Wait for pods (39 retries left).\nFAILED - RETRYING: [localhost]: Wait for pods (38 retries left).\n...\nok: [localhost]\n</code></pre></p> <p>Benefits: - \u2705 User sees system is active, not hanging - \u2705 Clear countdown shows progress and remaining time - \u2705 Predictable feedback rhythm (every 15 seconds) - \u2705 Transparent about retry attempts and timeouts</p>"},{"location":"rules-provisioning/#rule-8-task-naming-and-numbering-standards","title":"Rule 8: Task Naming and Numbering Standards","text":"<p>All Ansible tasks MUST follow consistent naming and numbering conventions.</p> <p>\u26a0\ufe0f See doc/rules-naming-conventions.md for complete patterns.</p> <p>Mandatory Requirements: - \u2705 Sequential numbering: Tasks numbered 1, 2, 3... in execution order - \u2705 Descriptive names: Clear action description after the number - \u2705 Consistent format: <code>- name: N. Action description</code></p> <p>\u2705 CORRECT Examples: <pre><code>- name: 1. Deploy database via Helm\n- name: 2. Wait for database pods to be ready\n- name: 3. Test database connectivity from within cluster\n- name: 4. Apply database ingress configuration\n- name: 5. Display database deployment status\n</code></pre></p> <p>\u274c WRONG Examples: <pre><code>- name: Deploy database        # Missing number\n- name: 3. Deploy database     # Wrong sequence (should be 1)\n- name: 2. Wait               # Not descriptive enough\n- name: Step 2 - Deploy       # Wrong format\n</code></pre></p> <p>Benefits: - \u2705 Easy debugging: Error messages show exact task sequence - \u2705 Clear progress: Users see completion percentage - \u2705 Maintainability: Easy to reference specific tasks in documentation - \u2705 Troubleshooting: \"Failed at task 7\" immediately identifies the problem</p> <p>Refactoring Rule: When adding/removing tasks, renumber all subsequent tasks to maintain sequence.</p>"},{"location":"rules-provisioning/#rule-8b-first-task-must-display-deployment-information","title":"Rule 8B: First Task MUST Display Deployment Information","text":"<p>MANDATORY: Every Ansible playbook MUST start with Task 1 that displays deployment context information.</p> <p>Required Format: <pre><code>- name: 1. Display deployment information\n  ansible.builtin.debug:\n    msg:\n      - \"======================================\"\n      - \"[Service Name] Deployment\"\n      - \"File: ansible/playbooks/[nnn]-setup-[service].yml\"\n      - \"======================================\"\n      - \"Target Host: {{ target_host }}\"\n      - \"Namespace: {{ namespace }}\"\n      - \"Component: {{ component_name }}\"\n      - \"[Additional context as needed]\"\n</code></pre></p> <p>Why This Matters: - \u2705 Immediate Context: User sees what playbook is running and where - \u2705 Debugging: Log files clearly show which playbook generated output - \u2705 Parameter Verification: Confirms correct target host and namespace before deployment - \u2705 Documentation: File path shows exact source for troubleshooting - \u2705 Consistency: Uniform format across all playbooks</p> <p>Real Example from Grafana Setup: <pre><code>tasks:\n  - name: 1. Display deployment information\n    ansible.builtin.debug:\n      msg:\n        - \"======================================\"\n        - \"Grafana Deployment\"\n        - \"File: ansible/playbooks/034-setup-grafana.yml\"\n        - \"======================================\"\n        - \"Target Host: {{ target_host }}\"\n        - \"Namespace: {{ namespace }}\"\n        - \"Component: {{ component_name }}\"\n        - \"Config File: {{ grafana_config_file }}\"\n</code></pre></p> <p>Output Example: <pre><code>TASK [1. Display deployment information] ***********************\nok: [localhost] =&gt; {\n    \"msg\": [\n        \"======================================\",\n        \"Grafana Deployment\",\n        \"File: ansible/playbooks/034-setup-grafana.yml\",\n        \"======================================\",\n        \"Target Host: rancher-desktop\",\n        \"Namespace: monitoring\",\n        \"Component: grafana\",\n        \"Config File: /mnt/urbalurbadisk/manifests/034-grafana-config.yaml\"\n    ]\n}\n</code></pre></p> <p>Removal Playbooks: Use the same format but with \"[Service Name] Removal\" as the title and file path pointing to the remove playbook.</p>"},{"location":"rules-provisioning/#rule-9-status-reporting-standards","title":"Rule 9: Status Reporting Standards","text":"<p>Every playbook MUST end with a comprehensive status report:</p> <pre><code>- name: Display final deployment status\n  ansible.builtin.debug:\n    msg:\n      - \"===============================================\"\n      - \"\ud83d\ude80 {{ service_name | title }} Deployment Status\"\n      - \"===============================================\"\n      - \"\"\n      - \"\u2705 SUCCESS - All components verified and running\"\n      - \"\"\n      - \"\ud83d\udd04 Status:\"\n      - \"\u2022 Service connectivity: \u2705 Internal cluster communication verified\"\n      - \"\u2022 API responding: \u2705 Functional tests passed\"\n      - \"\u2022 IngressRoute: \u2705 Traefik routing configured\"\n      - \"\"\n      - \"\ud83c\udf10 Access Instructions:\"\n      - \"\u2022 Port-forward: kubectl port-forward svc/{{ service_name }} {{ port }}:{{ port }} -n {{ namespace }}\"\n      - \"\u2022 Ingress: http://{{ service_name }}.localhost\"\n      - \"\"\n      - \"\ud83d\udd27 Troubleshooting:\"\n      - \"\u2022 Check pod status: kubectl get pods -n {{ namespace }}\"\n      - \"\u2022 View logs: kubectl logs -f &lt;pod-name&gt; -n {{ namespace }}\"\n      - \"===============================================\"\n</code></pre>"},{"location":"rules-provisioning/#file-organization-rules","title":"\ud83d\udcc1 File Organization Rules","text":""},{"location":"rules-provisioning/#rule-10-utility-playbook-structure","title":"Rule 10: Utility Playbook Structure","text":"<p>All files in <code>ansible/playbooks/utility/</code> MUST be complete playbooks, not just task lists.</p> <p>\u274c WRONG: Task list format: <pre><code># utility/u06-database-setup.yml - WRONG!\n- name: Create database user\n  postgresql_user: ...\n- name: Create database\n  postgresql_db: ...\n</code></pre></p> <p>\u2705 CORRECT: Complete playbook format: <pre><code># utility/u06-database-setup.yml - CORRECT!\n---\n- name: Database setup utility\n  hosts: localhost\n  gather_facts: false\n  vars:\n    database_name: \"{{ database_name | default('myapp') }}\"\n    database_user: \"{{ database_user | default('myuser') }}\"\n  tasks:\n    - name: 1. Create database user\n      postgresql_user: ...\n    - name: 2. Create database\n      postgresql_db: ...\n</code></pre></p> <p>Usage in Main Playbooks: <pre><code># Main playbook imports utility\n- import_playbook: utility/u06-database-setup.yml\n  vars:\n    database_name: \"openwebui\"\n    database_user: \"openwebui\"\n</code></pre></p> <p>Benefits: - \u2705 Reusable: Can be run standalone or imported - \u2705 Testable: Can be tested independently - \u2705 Parameterized: Accepts variables for different use cases - \u2705 Complete: Has proper playbook structure with hosts, vars, tasks - \u2705 Maintainable: Clear separation of concerns</p>"},{"location":"rules-provisioning/#rule-11-helm-repository-management","title":"Rule 11: Helm Repository Management","text":"<p>Every playbook that uses Helm charts MUST be responsible for managing its required Helm repositories.</p> <p>Mandatory Requirements: - \u2705 Check existing repositories: Verify what's already configured - \u2705 Add missing repositories: Add only repositories that are needed and missing - \u2705 Update repositories: Refresh repository indexes before installation - \u2705 Self-contained: Never assume repositories are pre-configured</p> <p>\u2705 CORRECT Pattern: <pre><code>- name: N. Check existing Helm repositories\n  ansible.builtin.command: helm repo list\n  register: helm_repo_list\n  changed_when: false\n\n- name: N+1. Add required Helm repositories if missing\n  kubernetes.core.helm_repository:\n    name: \"{{ item.name }}\"\n    repo_url: \"{{ item.url }}\"\n  loop:\n    - { name: 'bitnami', url: 'https://charts.bitnami.com/bitnami' }\n    - { name: 'open-webui', url: 'https://helm.openwebui.com/' }\n  when: item.name not in helm_repo_list.stdout\n\n- name: N+2. Update Helm repositories\n  ansible.builtin.command: helm repo update\n  changed_when: false\n\n- name: N+3. Deploy service via Helm\n  ansible.builtin.command: &gt;\n    helm upgrade --install {{ service_name }} {{ chart_name }}\n    -f {{ config_file }}\n    --namespace {{ namespace }}\n</code></pre></p> <p>Benefits: - \u2705 Self-contained: Playbook doesn't depend on external setup - \u2705 Idempotent: Safe to run multiple times - \u2705 Efficient: Only adds missing repositories - \u2705 Reliable: Fresh repository indexes before deployment - \u2705 Debuggable: Clear separation of repository and deployment steps</p> <p>\u274c WRONG: Assuming pre-configured repositories: <pre><code># WRONG: Assumes repositories are already configured\n- name: Deploy service via Helm\n  helm: chart=some-chart/service-name ...  # May fail if repo missing\n</code></pre></p>"},{"location":"rules-provisioning/#rule-12-utility-playbook-error-handling","title":"Rule 12: Utility Playbook Error Handling","text":"<p>When calling utility playbooks from main playbooks, MUST implement \"quiet success, verbose failure\" pattern.</p> <p>Mandatory Requirements: - \u2705 Capture output: Always <code>register</code> the result of utility playbook calls - \u2705 Silent success: No output display when utility playbook succeeds - \u2705 Verbose failure: Show full utility playbook output when it fails - \u2705 Proper error handling: Use <code>failed_when</code> to catch non-zero exit codes</p> <p>\u274c WRONG: No error diagnostics: <pre><code># WRONG: Utility failure provides no diagnostic information\n- name: 1. Set up database\n  ansible.builtin.shell: |\n    ansible-playbook utility/database-setup.yml\n  register: db_result\n  failed_when: db_result.rc != 0\n</code></pre></p> <p>\u2705 CORRECT: Error diagnostics on failure: <pre><code># CORRECT: Shows utility output only when debugging is needed\n- name: 1. Set up database\n  ansible.builtin.shell: |\n    ansible-playbook utility/database-setup.yml -e operation=create\n  args:\n    chdir: /path/to/playbooks\n  register: db_result\n  changed_when: db_result.rc == 0\n  failed_when: db_result.rc != 0\n\n- name: 1.1. Display utility playbook output on failure\n  ansible.builtin.debug:\n    msg:\n      - \"\u274c Database setup failed!\"\n      - \"Full output from utility playbook:\"\n      - \"{{ db_result.stdout_lines }}\"\n  when: db_result.rc != 0\n</code></pre></p> <p>Benefits: - \u2705 Clean output during normal operations (quiet success) - \u2705 Full diagnostic information when troubleshooting is needed (verbose failure) - \u2705 No subprocess output buffering issues - \u2705 Maintains utility playbook independence</p>"},{"location":"rules-provisioning/#rule-13-consistent-file-naming-and-numbering","title":"Rule 13: Consistent File Naming and Numbering","text":"<p>TODO: we need to revise numbering (someday)</p> <pre><code>scripts/packages/[service-name].sh\nansible/playbooks/[nnn]-setup-[service-name].yml\nansible/playbooks/utility/[unn]-[purpose].yml\nmanifests/[nnn]-[service-name]-[component].yaml\nprovision-host/kubernetes/[nn]-[category]/[nn]-setup-[service].sh\n</code></pre>"},{"location":"rules-provisioning/#rule-14-retry-and-timeout-patterns","title":"Rule 14: Retry and Timeout Patterns","text":"<p>All deployment tasks MUST use appropriate retry patterns with visible progress indicators instead of silent long-running operations.</p> <p>\ud83d\udd04 Retry Patterns by Use Case:</p> <p>1. Pod Startup (Standard Services): <pre><code># Most services: 20 retries \u00d7 15s = 5 minutes\n- name: Wait for service pods to be ready\n  kubernetes.core.k8s_info:\n    kind: Pod\n    namespace: \"{{ namespace }}\"\n    label_selectors:\n      - app={{ service_name }}\n  register: service_pods\n  retries: 20\n  delay: 15\n  until: &gt;\n    service_pods.resources | length &gt; 0 and\n    service_pods.resources[0].status.phase == \"Running\"\n</code></pre></p> <p>2. Pod Startup (Heavy Container Images): <pre><code># OpenWebUI with large container: 80 retries \u00d7 15s = 20 minutes\n- name: Wait for OpenWebUI pods (large container download)\n  kubernetes.core.k8s_info:\n    kind: Pod\n    namespace: \"{{ ai_namespace }}\"\n    label_selectors:\n      - app=open-webui\n  register: openwebui_pods\n  retries: 80      # Extra time for container image download\n  delay: 15\n  until: &gt;\n    openwebui_pods.resources | length &gt; 0 and\n    openwebui_pods.resources[0].status.phase == \"Running\"\n</code></pre></p> <p>3. Service Connectivity Tests: <pre><code># HTTP health checks: 15 retries \u00d7 15s = ~4 minutes\n- name: Test OpenWebUI HTTP response\n  ansible.builtin.shell: |\n    kubectl run curl-test --image=curlimages/curl --rm -i --restart=Never -n {{ namespace }} -- \\\n    curl -s -w \"HTTP_CODE:%{http_code}\" http://open-webui/health\n  register: openwebui_http_response\n  retries: 15\n  delay: 15\n  until: openwebui_http_response.rc == 0 and openwebui_http_response.stdout.find('HTTP_CODE:200') != -1\n</code></pre></p> <p>4. Resource Creation Checks: <pre><code># Quick resource checks: 5 retries \u00d7 2s = 10 seconds\n- name: Verify IngressRoute is created\n  kubernetes.core.k8s_info:\n    api_version: traefik.io/v1alpha1\n    kind: IngressRoute\n    namespace: \"{{ namespace }}\"\n    name: \"{{ service_name }}\"\n  register: ingress_check\n  retries: 5\n  delay: 2\n  until: ingress_check.resources | length &gt; 0\n</code></pre></p> <p>\ud83d\udccf Timeout Guidelines: - Lightweight services: 20 retries \u00d7 15s = 5 minutes - Heavy container images: 80 retries \u00d7 15s = 20 minutes (OpenWebUI pattern) - HTTP connectivity tests: 15 retries \u00d7 15s = ~4 minutes - Resource existence checks: 5 retries \u00d7 2s = 10 seconds</p> <p>\ud83d\udca1 Key Benefits of This Pattern: - \u2705 Visible Progress: User sees \"RETRYING (N retries left)\" messages every 15 seconds - \u2705 Predictable Timing: Clear expectation of maximum wait time - \u2705 No Silent Hangs: Never appears frozen or unresponsive - \u2705 Appropriate Timeouts: Different timeouts for different complexity levels</p> <p>\u274c What NOT to do: <pre><code># WRONG: Silent long-running operations\n- name: Wait for deployment\n  shell: kubectl wait --timeout=600s --for=condition=ready pod/service-pod\n  # Problem: 10 minutes of silence - appears to hang\n</code></pre></p> <p>Numbering Convention: - <code>000-099</code>: Core infrastructure (storage, networking, DNS) - <code>040-099</code>: Data services (databases, caches, message queues) - <code>200-229</code>: AI services (OpenWebUI, LiteLLM, Ollama) - <code>070-079</code>: Authentication (Authentik, OAuth providers) - <code>030-039</code>: Observability (monitoring, logging, tracing) - <code>600-799</code>: Management tools (admin interfaces, development tools)</p>"},{"location":"rules-provisioning/#rule-15-ingress-standards","title":"Rule 15: Ingress Standards","text":"<p>All services MUST follow the Traefik Ingress Standards defined in <code>docs/rules-ingress-traefik.md</code>.</p> <p>Mandatory Requirements: - \u2705 Use only Traefik IngressRoute CRDs (never standard Kubernetes Ingress) - \u2705 Follow HostRegexp patterns for multi-domain support - \u2705 Use correct API version and proper labeling - \u2705 Apply authentication middleware for protected services</p> <p>See <code>docs/rules-ingress-traefik.md</code> for: - Complete IngressRoute templates and examples - HostRegexp pattern explanations - Authentication integration patterns - API version requirements - Working examples from the codebase</p>"},{"location":"rules-provisioning/#common-anti-patterns-to-avoid","title":"\ud83d\udea8 Common Anti-Patterns to Avoid","text":""},{"location":"rules-provisioning/#anti-pattern-1-shell-script-logic","title":"\u274c Anti-Pattern 1: Shell Script Logic","text":"<p><pre><code># WRONG: Business logic in shell script\nif helm list | grep -q postgres; then\n  echo \"Postgres exists, upgrading...\"\n  helm upgrade postgres ...\nelse\n  echo \"Installing postgres...\"\n  helm install postgres ...\nfi\n</code></pre> Solution: Move all logic to Ansible playbooks.</p>"},{"location":"rules-provisioning/#anti-pattern-2-localhost-testing","title":"\u274c Anti-Pattern 2: localhost Testing","text":"<p><pre><code># WRONG: Testing localhost from host context\n- name: Test service\n  uri:\n    url: \"http://service.localhost/api\"\n</code></pre> Solution: Use kubectl run with curl container.</p>"},{"location":"rules-provisioning/#anti-pattern-3-missing-retry-logic","title":"\u274c Anti-Pattern 3: Missing Retry Logic","text":"<p><pre><code># WRONG: No retry for potentially slow operations\n- name: Wait for pod\n  shell: kubectl get pod service-pod\n</code></pre> Solution: Use retries/delay/until pattern.</p>"},{"location":"rules-provisioning/#anti-pattern-4-ignoring-critical-errors","title":"\u274c Anti-Pattern 4: Ignoring Critical Errors","text":"<p><pre><code># WRONG: Ignoring deployment failures\n- name: Deploy service\n  command: helm install service ...\n  ignore_errors: true\n</code></pre> Solution: Let critical failures fail fast.</p>"},{"location":"rules-provisioning/#anti-pattern-5-silent-long-running-tasks","title":"\u274c Anti-Pattern 5: Silent Long-Running Tasks","text":"<p><pre><code># WRONG: No progress feedback for long operations\n- name: Wait for deployment\n  shell: kubectl wait --timeout=600s ...\n</code></pre> Solution: Use Ansible retries with progress indicators (see Rule 14: Retry and Timeout Patterns).</p>"},{"location":"rules-provisioning/#anti-pattern-6-utility-files-as-task-lists","title":"\u274c Anti-Pattern 6: Utility Files as Task Lists","text":"<p><pre><code># WRONG: utility/database-setup.yml as task list\n- name: Create user\n  postgresql_user: ...\n- name: Create database\n  postgresql_db: ...\n</code></pre> Solution: Write complete playbooks with hosts, vars, and tasks sections.</p>"},{"location":"rules-provisioning/#anti-pattern-7-assuming-pre-configured-helm-repositories","title":"\u274c Anti-Pattern 7: Assuming Pre-configured Helm Repositories","text":"<p><pre><code># WRONG: Assuming repositories are already configured\n- name: Deploy service\n  helm: chart=some-repo/service-name ...  # May fail if repo missing\n</code></pre> Solution: Manage Helm repositories within the playbook (check, add, update).</p>"},{"location":"rules-provisioning/#reference-documentation","title":"\ud83d\udcda Reference Documentation","text":""},{"location":"rules-provisioning/#related-cluster-documentation","title":"Related Cluster Documentation:","text":"<ul> <li>\ud83d\udea6 Ingress Standards: <code>docs/rules-ingress-traefik.md</code> - Comprehensive Traefik IngressRoute patterns</li> <li>\ud83c\udf10 Networking Overview: <code>docs/networking-readme.md</code> - Cluster networking architecture</li> <li>\ud83c\udfd7\ufe0f Infrastructure Guide: <code>docs/infrastructure-readme.md</code> - Overall cluster architecture</li> <li>\ud83e\udd16 AI Environment: <code>docs/package-ai-environment-management.md</code> - AI-specific deployment patterns</li> </ul>"},{"location":"rules-provisioning/#key-concepts-from-traefik-documentation","title":"Key Concepts from Traefik Documentation:","text":"<ul> <li>HostRegexp Patterns: Multi-domain routing with <code>HostRegexp(\\</code>service\\..+`)`</li> <li>API Version: Use <code>traefik.io/v1alpha1</code> (not <code>traefik.io/v1</code>)</li> <li>DNS Resolution: Dual-context architecture for localhost routing</li> <li>Authentication: Forward auth middleware patterns for protected services</li> <li>Priority System: Route matching order and conflict resolution</li> </ul>"},{"location":"rules-provisioning/#working-examples-in-codebase","title":"Working Examples in Codebase:","text":"<ul> <li>\u2705 Good: <code>ansible/playbooks/020-setup-nginx.yml</code> - Proper testing with kubectl run</li> <li>\u2705 Good: <code>ansible/playbooks/210-setup-litellm.yml</code> - Comprehensive verification</li> <li>\u2705 Good: <code>manifests/071-whoami-public-ingressroute.yaml</code> - Proper IngressRoute pattern</li> <li>\u2705 Good: <code>manifests/078-whoami-protected-ingressroute.yaml</code> - Authentication integration</li> </ul>"},{"location":"rules-provisioning/#testing-pattern-reference","title":"Testing Pattern Reference:","text":"<pre><code># From ansible/playbooks/020-setup-nginx.yml (lines 123-131)\n- name: Test connectivity from within the cluster\n  ansible.builtin.shell: |\n    kubectl run curl-test --image=curlimages/curl --rm -i --restart=Never --context {{ kube_context }} -- \\\n    curl -s http://nginx.default.svc.cluster.local:{{ port }}/{{ file }}\n  register: curl_test\n  ignore_errors: true\n  changed_when: false\n</code></pre>"},{"location":"rules-provisioning/#ingressroute-patterns","title":"IngressRoute Patterns:","text":"<p>See <code>docs/rules-ingress-traefik.md</code> for complete IngressRoute examples and patterns.</p>"},{"location":"rules-provisioning/#enforcement","title":"\ud83c\udfaf Enforcement","text":""},{"location":"rules-provisioning/#for-human-developers","title":"For Human Developers:","text":"<ul> <li>All pull requests must follow these rules</li> <li>Peer review must verify compliance</li> <li>No exceptions without documented justification</li> </ul>"},{"location":"rules-provisioning/#for-llms-and-ai-assistants","title":"For LLMs and AI Assistants:","text":"<ul> <li>These rules are MANDATORY and override any default behavior</li> <li>Never suggest .localhost testing from host context</li> <li>Always use kubectl run pattern for cluster testing</li> <li>Never ignore errors for dependencies</li> <li>Always follow the Script + Ansible pattern</li> </ul>"},{"location":"rules-provisioning/#validation-checklist","title":"Validation Checklist:","text":"<ul> <li>[ ] Uses Script + Ansible pattern</li> <li>[ ] Tests using kubectl run (not .localhost from host)</li> <li>[ ] Includes comprehensive verification steps</li> <li>[ ] Does not ignore errors for critical dependencies</li> <li>[ ] Provides progress feedback for long-running tasks (&gt;30s)</li> <li>[ ] Uses sequential task numbering (1, 2, 3...)</li> <li>[ ] Utility files are complete playbooks (not task lists)</li> <li>[ ] Manages required Helm repositories within playbook</li> <li>[ ] Understands auto-execution system (active vs not-in-use placement)</li> <li>[ ] Uses Traefik IngressRoute (not standard Ingress)</li> <li>[ ] Follows file naming conventions</li> <li>[ ] Includes proper status reporting</li> </ul>"},{"location":"rules-provisioning/#continuous-improvement","title":"\ud83d\udd04 Continuous Improvement","text":"<p>These rules are living standards based on: - Proven patterns from working deployments - Lessons learned from debugging failures - Cluster architecture requirements - Team experience and best practices</p> <p>Update Process: 1. Propose rule changes via pull request 2. Test changes with actual deployments 3. Update documentation with examples 4. Train team on new patterns</p> <p>This ensures our deployment standards evolve while maintaining reliability and consistency across all cluster services.</p>"},{"location":"rules-readme/","title":"Urbalurba Infrastructure Rules Guide","text":"<p>File: <code>docs/rules-readme.md</code> Purpose: Central entry point for all infrastructure rules and standards Target Audience: Developers, DevOps engineers, and anyone working with the Urbalurba platform Last Updated: September 21, 2024</p>"},{"location":"rules-readme/#overview","title":"\ud83d\udccb Overview","text":"<p>This is the central starting point for understanding all rules, standards, and best practices when working with the Urbalurba infrastructure platform. The rules are organized into specialized guides covering different aspects of the platform.</p>"},{"location":"rules-readme/#rule-categories","title":"\ud83d\ude80 Rule Categories","text":""},{"location":"rules-readme/#infrastructure-provisioning-rules","title":"Infrastructure Provisioning Rules","text":"<p>\ud83d\udcd6 Provisioning Rules Guide</p> <p>Comprehensive rules for deploying and managing infrastructure on Kubernetes using the Urbalurba platform patterns:</p> <ul> <li>Shell Script + Ansible Patterns: Separation of orchestration and implementation</li> <li>Cluster Testing Standards: kubectl run patterns for reliable service verification</li> <li>Progress Feedback Requirements: User experience during long-running operations</li> <li>Task Organization: Sequential numbering and proper structure</li> <li>Error Handling: Quiet success, verbose failure patterns</li> <li>Helm Repository Management: Consistent chart deployment practices</li> </ul> <p>When to use: Infrastructure deployment, service provisioning, cluster setup automation</p>"},{"location":"rules-readme/#automated-kubernetes-deployment-rules","title":"Automated Kubernetes Deployment Rules","text":"<p>\u2638\ufe0f Automated Kubernetes Deployment Rules Guide</p> <p>Mandatory patterns and standards for the automated deployment system in provision-host/kubernetes:</p> <ul> <li>Directory Structure: Numbered categories for dependency management</li> <li>Script Requirements: Parameter handling, status tracking, error resilience</li> <li>Active/Inactive Management: Using not-in-use folders for optional services</li> <li>Dependency Order: Ensuring prerequisites are met through proper numbering</li> <li>Namespace Standards: Consistent namespace usage across services</li> <li>Testing Requirements: Health checks and deployment verification</li> </ul> <p>When to use: Adding new applications, creating deployment scripts, managing service dependencies</p>"},{"location":"rules-readme/#ingress-and-networking-rules","title":"Ingress and Networking Rules","text":"<p>\ud83d\udea6 Ingress Rules Guide</p> <p>Detailed standards for configuring ingress and networking using Traefik in the Kubernetes cluster:</p> <ul> <li>Traefik IngressRoute Standards: CRD usage patterns and API versions</li> <li>Multi-Domain Routing: HostRegexp patterns for flexible domain handling</li> <li>Authentication Integration: Authentik middleware configuration</li> <li>DNS Architecture: Dual-context routing for localhost and external domains</li> <li>Security Patterns: CSP middleware and forward auth configurations</li> </ul> <p>When to use: Service exposure, domain routing, authentication setup, external access</p>"},{"location":"rules-readme/#secrets-management-rules","title":"Secrets Management Rules","text":"<p>\ud83d\udd12 Secrets Management Rules Guide</p> <p>Comprehensive rules for the modular secrets management system ensuring security and maintainability:</p> <ul> <li>Template + Gitignore Pattern: Separation of base templates from actual secrets</li> <li>Variable Substitution Standards: Centralized configuration with <code>${VARIABLE}</code> patterns</li> <li>Security Verification: Git safety checks and validation requirements</li> <li>Service Integration: Proper namespace organization and secret structure</li> <li>Rotation Procedures: Safe secret rotation and emergency response protocols</li> <li>Cross-System Dependencies: Integration with provisioning, ingress, and git workflows</li> </ul> <p>When to use: All secrets management, configuration updates, service deployments requiring credentials</p>"},{"location":"rules-readme/#git-workflow-and-development-rules","title":"Git Workflow and Development Rules","text":"<p>\ud83d\udd00 Git Workflow Rules Guide</p> <p>Professional Git workflow standards for maintaining code quality and enabling collaboration:</p> <ul> <li>Feature Branch Workflow: Branch strategy and naming conventions</li> <li>Pull Request Requirements: Mandatory PR process with detailed descriptions</li> <li>Code Review Standards: Quality criteria and review processes</li> <li>Commit Message Standards: Clear, descriptive commit practices</li> <li>Branch Management: Creation, merging, and cleanup procedures</li> <li>Emergency Procedures: Hotfix and rollback processes</li> </ul> <p>When to use: All code contributions, feature development, bug fixes, documentation updates</p>"},{"location":"rules-readme/#development-workflow-rules","title":"Development Workflow Rules","text":"<p>\ud83d\udcbb Development Workflow Rules Guide</p> <p>Standards for working with the urbalurba-infrastructure codebase, covering file operations, command execution, and project conventions:</p> <ul> <li>Path Conventions: Relative paths from repository root for consistency</li> <li>Workflow Types: Claude Code AI vs. human developer workflows</li> <li>File Operations: Read, write, edit patterns for different contexts</li> <li>Command Execution: Mac host vs. provision-host container operations</li> <li>Kubernetes Operations: kubectl commands and resource management</li> <li>Testing Patterns: Verification and validation approaches</li> </ul> <p>When to use: Daily development work, AI assistant operations, file management, command execution</p>"},{"location":"rules-readme/#naming-conventions-rules","title":"Naming Conventions Rules","text":"<p>\ud83c\udff7\ufe0f Naming Conventions Rules Guide</p> <p>Standardized naming patterns for files, manifests, scripts, and resources across the infrastructure:</p> <ul> <li>Manifest Numbering: Sequential numbering scheme (000-999) by service category</li> <li>File Naming: Consistent patterns for YAML, scripts, and documentation</li> <li>Resource Naming: Kubernetes resources, namespaces, and labels</li> <li>Script Naming: Shell scripts and Ansible playbooks conventions</li> <li>Service Categories: Numbered ranges for different infrastructure layers</li> </ul> <p>When to use: Creating new manifests, scripts, services, or any infrastructure resources</p>"},{"location":"rules-readme/#documentation-standards","title":"Documentation Standards","text":"<p>\ud83d\udcdd Documentation Standards Guide</p> <p>Comprehensive guide for writing consistent, high-quality documentation across the infrastructure:</p> <ul> <li>Documentation Structure: Standardized sections and formatting</li> <li>Metadata Headers: File, purpose, audience, last updated fields</li> <li>Writing Style: Clarity, conciseness, and technical accuracy</li> <li>Examples and Code Blocks: Proper formatting and syntax highlighting</li> <li>Cross-References: Linking between related documentation</li> <li>Update Requirements: Keeping documentation synchronized with code</li> </ul> <p>When to use: Writing or updating any documentation files, README files, or inline comments</p>"},{"location":"rules-readme/#quick-reference","title":"\ud83c\udfaf Quick Reference","text":""},{"location":"rules-readme/#for-new-developers","title":"For New Developers","text":"<ol> <li>Start with: Git Workflow Rules - Learn development workflow and collaboration</li> <li>Then read: Development Workflow Rules - Learn daily development practices</li> <li>Next: Naming Conventions Rules - Learn file and resource naming</li> <li>Then: Documentation Standards - Learn documentation practices</li> <li>Next: Secrets Management Rules - Learn secure secrets handling</li> <li>Then: Provisioning Rules - Learn infrastructure deployment patterns</li> <li>Finally: Ingress Rules - Understand service exposure</li> </ol>"},{"location":"rules-readme/#for-development-work","title":"For Development Work","text":"<ul> <li>Making code changes: Follow Git Workflow Rules</li> <li>Daily development: Follow Development Workflow Rules</li> <li>Naming files/resources: Follow Naming Conventions Rules</li> <li>Writing documentation: Follow Documentation Standards</li> <li>Managing secrets: Follow Secrets Management Rules</li> <li>Deploying services: Follow Provisioning Rules</li> <li>Exposing services: Follow Ingress Rules</li> </ul>"},{"location":"rules-readme/#for-troubleshooting","title":"For Troubleshooting","text":"<ul> <li>Deployment issues: Check Rule 3 in Provisioning Rules for cluster testing patterns</li> <li>Access issues: Check DNS resolution in Ingress Rules</li> </ul>"},{"location":"rules-readme/#rule-enforcement","title":"\ud83d\udd27 Rule Enforcement","text":""},{"location":"rules-readme/#automated-validation","title":"Automated Validation","text":"<ul> <li>CI/CD pipelines should validate compliance with these rules</li> <li>Use the patterns documented in each guide as templates</li> </ul>"},{"location":"rules-readme/#manual-review","title":"Manual Review","text":"<ul> <li>All pull requests must demonstrate rule compliance</li> <li>Peer reviews should verify adherence to documented patterns</li> <li>No exceptions without documented justification</li> </ul>"},{"location":"rules-readme/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>\ud83c\udf10 Networking Overview: <code>docs/networking-readme.md</code> - High-level cluster networking</li> <li>\ud83c\udfd7\ufe0f Infrastructure Guide: <code>docs/infrastructure-readme.md</code> - Overall platform architecture</li> <li>\ud83e\udd16 AI Environment: <code>docs/package-ai-environment-management.md</code> - AI-specific patterns</li> </ul>"},{"location":"rules-readme/#contributing-to-rules","title":"\ud83c\udd95 Contributing to Rules","text":""},{"location":"rules-readme/#when-to-add-new-rules","title":"When to Add New Rules","text":"<ul> <li>Recurring anti-patterns are discovered</li> <li>New deployment patterns are established</li> <li>Infrastructure standards evolve</li> </ul>"},{"location":"rules-readme/#rule-documentation-standards","title":"Rule Documentation Standards","text":"<ul> <li>Each rule must include examples (both \u2705 correct and \u274c incorrect)</li> <li>Background explanation of why the rule exists</li> <li>Clear enforcement criteria</li> <li>Links to working examples in the codebase</li> </ul>"},{"location":"rules-readme/#update-process","title":"Update Process","text":"<ol> <li>Propose rule changes via pull request</li> <li>Update relevant rule guide (Provisioning or Ingress)</li> <li>Update this central guide if categories change</li> <li>Ensure all existing code complies with new rules</li> </ol> <p>\ud83d\udca1 Remember: These rules exist to ensure reliable, maintainable, and scalable infrastructure. They represent lessons learned from real deployment challenges and should be followed consistently across all Urbalurba infrastructure work.</p>"},{"location":"rules-secrets-management/","title":"Secrets Management Rules and Standards","text":"<p>File: <code>docs/rules-secrets-management.md</code> Purpose: Define mandatory rules and patterns for secrets management in the Urbalurba Infrastructure Target Audience: Developers, DevOps engineers, and LLMs working with secrets and configuration Last Updated: September 21, 2025</p>"},{"location":"rules-secrets-management/#overview","title":"\ud83d\udccb Overview","text":"<p>This document establishes mandatory rules for managing secrets using the modular secrets management system. These rules ensure security, maintainability, and prevent accidental exposure of sensitive information.</p>"},{"location":"rules-secrets-management/#core-security-architecture","title":"\ud83d\udd12 Core Security Architecture","text":""},{"location":"rules-secrets-management/#rule-1-never-commit-secrets-pattern","title":"Rule 1: Never Commit Secrets Pattern","text":"<p>All secrets handling MUST follow the Template + Gitignore pattern:</p> <pre><code>secrets-templates/     \u2192  secrets-config/     \u2192  kubernetes-secrets.yml\n  \u2191 Git tracked           \u2191 Gitignored           \u2191 Gitignored\n  \u2191 ${VARIABLES}          \u2191 Actual secrets       \u2191 Final YAML\n</code></pre>"},{"location":"rules-secrets-management/#safe-operations","title":"\u2705 Safe Operations:","text":"<ul> <li>Edit files in <code>secrets-templates/</code> with <code>${VARIABLE}</code> placeholders only</li> <li>Edit files in <code>secrets-config/</code> for actual secret values</li> <li>Use <code>./create-kubernetes-secrets.sh</code> to generate final secrets</li> <li>Backup secrets to <code>terchris/</code> folder if needed</li> </ul>"},{"location":"rules-secrets-management/#forbidden-operations","title":"\u274c Forbidden Operations:","text":"<ul> <li>NEVER put actual secrets in <code>secrets-templates/</code></li> <li>NEVER commit files from <code>secrets-config/</code> or <code>secrets-generated/</code></li> <li>NEVER create copies of <code>kubernetes-secrets.yml</code> in git-tracked areas</li> <li>NEVER store secrets in documentation, comments, or README files</li> </ul>"},{"location":"rules-secrets-management/#rule-2-variable-substitution-pattern","title":"Rule 2: Variable Substitution Pattern","text":"<p>ALL secrets MUST use centralized variable management:</p>"},{"location":"rules-secrets-management/#correct-pattern","title":"\u2705 Correct Pattern:","text":"<pre><code># In secrets-config/00-common-values.env.template\nDEFAULT_DATABASE_PASSWORD=YourSecurePassword123\nDEFAULT_ADMIN_EMAIL=admin@yourcompany.com\n\n# In secrets-config/00-master-secrets.yml.template\nPGPASSWORD: \"${DEFAULT_DATABASE_PASSWORD}\"\nAUTHENTIK_BOOTSTRAP_EMAIL: \"${DEFAULT_ADMIN_EMAIL}\"\n</code></pre>"},{"location":"rules-secrets-management/#anti-pattern","title":"\u274c Anti-Pattern:","text":"<pre><code># DON'T: Hard-code different passwords for each service\nPGPASSWORD: \"postgres-specific-password\"\nMYSQL_ROOT_PASSWORD: \"mysql-different-password\"\nREDIS_PASSWORD: \"redis-another-password\"\n</code></pre> <p>Why: Centralized variables enable password rotation across all services simultaneously.</p>"},{"location":"rules-secrets-management/#rule-3-modular-system-usage-pattern","title":"Rule 3: Modular System Usage Pattern","text":"<p>The generation script MUST be used correctly:</p>"},{"location":"rules-secrets-management/#correct-workflow","title":"\u2705 Correct Workflow:","text":"<pre><code># 1. Edit your configuration\nnano secrets-config/00-common-values.env.template\nnano secrets-config/00-master-secrets.yml.template\n\n# 2. Generate secrets\n./create-kubernetes-secrets.sh\n\n# 3. Validate before applying\nkubectl apply --dry-run=client -f kubernetes/kubernetes-secrets.yml\n\n# 4. Deploy to cluster\nkubectl apply -f kubernetes/kubernetes-secrets.yml\n</code></pre>"},{"location":"rules-secrets-management/#anti-pattern_1","title":"\u274c Anti-Pattern:","text":"<pre><code># DON'T: Edit generated files directly\nnano kubernetes/kubernetes-secrets.yml\n\n# DON'T: Skip validation\nkubectl apply -f kubernetes/kubernetes-secrets.yml  # Without dry-run\n\n# DON'T: Edit base templates with secrets\nnano secrets-templates/00-master-secrets.yml.template  # Putting actual values\n</code></pre>"},{"location":"rules-secrets-management/#security-rules","title":"\ud83d\udee1\ufe0f Security Rules","text":""},{"location":"rules-secrets-management/#rule-4-git-safety-verification","title":"Rule 4: Git Safety Verification","text":"<p>Before ANY git operation, MUST verify no secrets are staged:</p>"},{"location":"rules-secrets-management/#required-verification-commands","title":"\u2705 Required Verification Commands:","text":"<pre><code># 1. Check what's staged for commit\ngit status\n\n# 2. Verify gitignore is working\ngit check-ignore secrets-config/\ngit check-ignore secrets-generated/\n\n# 3. Check for secret patterns in staged files\ngit diff --cached | grep -i \"password\\|secret\\|key\"\n</code></pre>"},{"location":"rules-secrets-management/#forbidden-git-operations","title":"\u274c Forbidden Git Operations:","text":"<ul> <li>Committing without running verification commands</li> <li>Adding <code>secrets-config/</code> or <code>secrets-generated/</code> to git</li> <li>Using <code>git add .</code> without checking what's included</li> <li>Creating documentation that contains actual secret values</li> </ul>"},{"location":"rules-secrets-management/#rule-5-service-integration-pattern","title":"Rule 5: Service Integration Pattern","text":"<p>When adding secrets for new services, follow the established pattern:</p>"},{"location":"rules-secrets-management/#correct-service-integration","title":"\u2705 Correct Service Integration:","text":"<pre><code># 1. Add variables to common values (if reusable)\nMYSERVICE_DATABASE_PASSWORD: \"${DEFAULT_DATABASE_PASSWORD}\"\nMYSERVICE_ADMIN_EMAIL: \"${DEFAULT_ADMIN_EMAIL}\"\n\n# 2. Add service-specific secrets (if unique)\nMYSERVICE_API_KEY: \"your-service-specific-key\"\nMYSERVICE_JWT_SECRET: \"your-jwt-secret\"\n\n# 3. Use proper namespace structure\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: urbalurba-secrets\n  namespace: myservice\ntype: Opaque\nstringData:\n  MYSERVICE_DATABASE_PASSWORD: \"${DEFAULT_DATABASE_PASSWORD}\"\n  MYSERVICE_API_KEY: \"${MYSERVICE_API_KEY}\"\n</code></pre>"},{"location":"rules-secrets-management/#anti-pattern_2","title":"\u274c Anti-Pattern:","text":"<pre><code># DON'T: Hard-code secrets in templates\nstringData:\n  MYSERVICE_DATABASE_PASSWORD: \"hardcoded-password\"\n\n# DON'T: Skip namespace organization\nmetadata:\n  name: myservice-specific-secret  # Should use urbalurba-secrets\n  namespace: default              # Should use service namespace\n</code></pre>"},{"location":"rules-secrets-management/#rule-6-no-helm-chart-defaults-for-security-values","title":"Rule 6: No Helm Chart Defaults for Security Values","text":"<p>NEVER use Helm chart default values for security-sensitive parameters:</p>"},{"location":"rules-secrets-management/#correct-pattern-override-all-security-defaults","title":"\u2705 Correct Pattern: Override All Security Defaults","text":"<pre><code># In Ansible playbook deployment\nhelm upgrade --install {{ service_name }} bitnami/rabbitmq \\\n  --set auth.username={{ rabbitmq_username_fact | quote }} \\\n  --set auth.password={{ rabbitmq_password_fact | quote }} \\\n  --set auth.erlangCookie={{ rabbitmq_erlang_cookie_fact | quote }}\n</code></pre> <pre><code># In secrets configuration\nRABBITMQ_USERNAME: \"${DEFAULT_ADMIN_USERNAME}\"\nRABBITMQ_PASSWORD: \"${DEFAULT_DATABASE_PASSWORD}\"\nRABBITMQ_ERLANG_COOKIE: \"${RABBITMQ_ERLANG_COOKIE}\"\n</code></pre>"},{"location":"rules-secrets-management/#anti-pattern-using-chart-defaults","title":"\u274c Anti-Pattern: Using Chart Defaults","text":"<pre><code># DON'T: Let Helm chart use default credentials\nauth:\n  username: user          # Bitnami default - predictable!\n  password: bitnami       # Chart default - insecure!\n\n# DON'T: Only override passwords but leave usernames as defaults\nauth:\n  username: user          # Still using chart default\n  password: \"${SECURE_PASSWORD}\"  # Good, but incomplete\n</code></pre>"},{"location":"rules-secrets-management/#security-sensitive-helm-parameters-to-always-override","title":"Security-Sensitive Helm Parameters to Always Override","text":"<ul> <li>Usernames: <code>auth.username</code>, <code>rootUser.username</code>, <code>adminUser</code></li> <li>Passwords: <code>auth.password</code>, <code>rootPassword</code>, <code>adminPassword</code></li> <li>API Keys: <code>apiKey</code>, <code>secretKey</code>, <code>accessKey</code></li> <li>Tokens: <code>authToken</code>, <code>jwtSecret</code>, <code>sessionSecret</code></li> <li>Cookies: <code>erlangCookie</code>, <code>sessionCookie</code></li> <li>Database URLs: Connection strings with embedded credentials</li> </ul>"},{"location":"rules-secrets-management/#why-this-matters","title":"Why This Matters","text":"<ul> <li>\u2705 Predictable defaults are security vulnerabilities</li> <li>\u2705 Chart documentation publishes default values publicly</li> <li>\u2705 Centralized management enables credential rotation</li> <li>\u2705 Consistent security across all services</li> </ul> <p>Example: Common Bitnami Chart Defaults to Avoid <pre><code># PostgreSQL defaults\nauth.postgresPassword: \"postgres\"\nauth.username: \"postgres\"\n\n# Redis defaults\nauth.password: \"bitnami\"\n\n# RabbitMQ defaults\nauth.username: \"user\"\nauth.password: \"bitnami\"\n\n# MongoDB defaults\nauth.rootPassword: \"root\"\nauth.username: \"root\"\n</code></pre></p> <p>All of these MUST be overridden with values from <code>urbalurba-secrets</code>.</p>"},{"location":"rules-secrets-management/#rule-7-configmap-management-pattern","title":"Rule 7: ConfigMap Management Pattern","text":"<p>ConfigMaps follow the same template pattern as secrets but for non-sensitive configuration data:</p>"},{"location":"rules-secrets-management/#configmap-directory-structure","title":"\u2705 ConfigMap Directory Structure:","text":"<pre><code>secrets-templates/\n\u251c\u2500\u2500 configmaps/\n\u2502   \u251c\u2500\u2500 [namespace]/\n\u2502   \u2502   \u251c\u2500\u2500 [category]/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 [config-name].[ext].template\n\u2502   \u2502   \u2514\u2500\u2500 dashboards/     # Special: Auto-labeled\n\u2502   \u2502       \u2514\u2500\u2500 my-dashboard.json.template\n</code></pre>"},{"location":"rules-secrets-management/#auto-discovery-pattern","title":"\u2705 Auto-Discovery Pattern:","text":"<ul> <li>Templates: Place files in <code>secrets-templates/configmaps/[namespace]/[category]/</code></li> <li>Processing: Any <code>*.template</code> file is automatically discovered</li> <li>Variables: Use same <code>${VARIABLE}</code> pattern as secrets</li> <li>Labeling: Automatic based on directory conventions</li> </ul>"},{"location":"rules-secrets-management/#directory-conventions-auto-labeling","title":"\u2705 Directory Conventions &amp; Auto-Labeling:","text":"<pre><code># dashboards/ \u2192 Label: grafana_dashboard: \"1\"\nconfigmaps/monitoring/dashboards/*.json.template\n\n# nginx/ \u2192 Label: app: nginx\nconfigmaps/[namespace]/nginx/*.conf.template\n\n# otel/ \u2192 Label: app.kubernetes.io/name: otel-collector\nconfigmaps/monitoring/otel/*.yaml.template\n\n# Default \u2192 Label: managed-by: secrets-pipeline\nconfigmaps/[namespace]/configs/*.yaml.template\n</code></pre>"},{"location":"rules-secrets-management/#developer-workflow","title":"\u2705 Developer Workflow:","text":"<pre><code># 1. Add new ConfigMap template\necho 'server: ${MY_SERVER}' &gt; secrets-templates/configmaps/myapp/configs/app.conf.template\n\n# 2. Run generation (discovers automatically)\n./create-kubernetes-secrets.sh\n\n# 3. Deploy everything together\nkubectl apply -f kubernetes/kubernetes-secrets.yml\n</code></pre>"},{"location":"rules-secrets-management/#customization-pattern","title":"\u2705 Customization Pattern:","text":"<pre><code># Templates copied to secrets-config/ on first run\nsecrets-config/configmaps/monitoring/dashboards/my-dashboard.json.template\n\n# Edit for customization (variables substituted, file preserved)\nnano secrets-config/configmaps/monitoring/configs/custom.yaml.template\n\n# Regenerate (preserves edits, updates variables)\n./create-kubernetes-secrets.sh\n</code></pre>"},{"location":"rules-secrets-management/#configmap-anti-patterns","title":"\u274c ConfigMap Anti-Patterns:","text":"<ul> <li>NEVER put sensitive data in ConfigMaps (use Secrets instead)</li> <li>NEVER create ConfigMaps outside the pipeline (use template system)</li> <li>NEVER hardcode values (use <code>${VARIABLES}</code> for dynamic data)</li> <li>NEVER edit generated YAML directly (edit templates instead)</li> </ul>"},{"location":"rules-secrets-management/#configmap-vs-secret-decision-matrix","title":"ConfigMap vs Secret Decision Matrix:","text":"<pre><code># \u2705 ConfigMaps (Non-sensitive)\n- Application configuration files\n- Dashboard definitions (JSON/YAML)\n- Service discovery settings\n- Public certificates\n- Database connection hosts/ports\n\n# \u274c Secrets (Sensitive)\n- Passwords, API keys, tokens\n- Private certificates/keys\n- Database connection strings with passwords\n- OAuth client secrets\n</code></pre>"},{"location":"rules-secrets-management/#operational-rules","title":"\ud83d\udd27 Operational Rules","text":""},{"location":"rules-secrets-management/#rule-8-testing-and-validation","title":"Rule 8: Testing and Validation","text":"<p>ALL secret changes MUST be validated before deployment:</p>"},{"location":"rules-secrets-management/#required-testing-steps","title":"\u2705 Required Testing Steps:","text":"<pre><code># 1. Generate and validate YAML syntax\n./create-kubernetes-secrets.sh\nkubectl apply --dry-run=client -f kubernetes/kubernetes-secrets.yml\n\n# 2. Check for variable substitution errors\ngrep '${' kubernetes/kubernetes-secrets.yml  # Should return no results\n\n# 3. Verify critical services have required secrets\ngrep -c \"PGPASSWORD\\|REDIS_PASSWORD\\|AUTHENTIK_SECRET_KEY\" kubernetes/kubernetes-secrets.yml\n</code></pre>"},{"location":"rules-secrets-management/#skip-testing","title":"\u274c Skip Testing:","text":"<ul> <li>Deploying without YAML validation</li> <li>Not checking for unresolved variables</li> <li>Missing verification that critical secrets are present</li> </ul>"},{"location":"rules-secrets-management/#rule-9-backup-and-recovery-pattern","title":"Rule 9: Backup and Recovery Pattern","text":"<p>Secret backups MUST follow secure patterns:</p>"},{"location":"rules-secrets-management/#correct-backup","title":"\u2705 Correct Backup:","text":"<pre><code># Backup to gitignored terchris folder\ncp kubernetes/kubernetes-secrets.yml terchris/secrets-backup/backup-$(date +%Y%m%d).yml\n\n# Backup configuration (not generated files)\ncp -r secrets-config/ terchris/config-backup-$(date +%Y%m%d)/\n</code></pre>"},{"location":"rules-secrets-management/#insecure-backup","title":"\u274c Insecure Backup:","text":"<pre><code># DON'T: Backup to git-tracked areas\ncp kubernetes/kubernetes-secrets.yml docs/backup.yml\ncp secrets-config/00-common-values.env.template examples/\n</code></pre>"},{"location":"rules-secrets-management/#emergency-procedures","title":"\ud83d\udea8 Emergency Procedures","text":""},{"location":"rules-secrets-management/#rule-8-secret-rotation-process","title":"Rule 8: Secret Rotation Process","text":"<p>When rotating secrets, follow this sequence:</p>"},{"location":"rules-secrets-management/#safe-rotation-process","title":"\u2705 Safe Rotation Process:","text":"<pre><code># 1. Update central variables first\nnano secrets-config/00-common-values.env.template\n# Change DEFAULT_DATABASE_PASSWORD, DEFAULT_ADMIN_PASSWORD, etc.\n\n# 2. Update service-specific secrets if needed\nnano secrets-config/00-master-secrets.yml.template\n\n# 3. Generate and validate\n./create-kubernetes-secrets.sh\nkubectl apply --dry-run=client -f kubernetes/kubernetes-secrets.yml\n\n# 4. Deploy during maintenance window\nkubectl apply -f kubernetes/kubernetes-secrets.yml\n\n# 5. Restart affected services\nkubectl rollout restart deployment/service-name -n namespace\n</code></pre>"},{"location":"rules-secrets-management/#rule-9-incident-response","title":"Rule 9: Incident Response","text":"<p>If secrets are accidentally exposed:</p>"},{"location":"rules-secrets-management/#immediate-response","title":"\u2705 Immediate Response:","text":"<pre><code># 1. Remove from git immediately\ngit reset HEAD~1  # If not pushed\ngit filter-branch  # If pushed (contact team)\n\n# 2. Rotate ALL exposed secrets\n# Update secrets-config/00-common-values.env.template with new values\n\n# 3. Audit access logs\n# Check who had access to exposed secrets\n\n# 4. Update documentation\n# Record incident and lessons learned\n</code></pre>"},{"location":"rules-secrets-management/#integration-rules","title":"\ud83d\udcda Integration Rules","text":""},{"location":"rules-secrets-management/#rule-10-cross-system-dependencies","title":"Rule 10: Cross-System Dependencies","text":"<p>Secrets management integrates with other systems:</p>"},{"location":"rules-secrets-management/#required-coordination","title":"\u2705 Required Coordination:","text":"<ul> <li>Provisioning: Secrets MUST be generated before running deployment scripts</li> <li>Ingress: Domain names in secrets MUST match ingress configurations</li> <li>Git Workflow: Secret changes MUST go through pull request process</li> <li>Monitoring: Failed secret deployments MUST trigger alerts</li> </ul>"},{"location":"rules-secrets-management/#verification-commands","title":"\u2705 Verification Commands:","text":"<pre><code># Verify domain consistency with ingress\ngrep -E \"BASE_DOMAIN_|TAILSCALE_DOMAIN|CLOUDFLARE\" secrets-config/00-common-values.env.template\n\n# Verify secrets exist before deployment\nkubectl get secret urbalurba-secrets -n default\nkubectl get secret urbalurba-secrets -n ai\nkubectl get secret urbalurba-secrets -n authentik\n</code></pre>"},{"location":"rules-secrets-management/#enforcement","title":"\ud83c\udfaf Enforcement","text":""},{"location":"rules-secrets-management/#automated-validation","title":"Automated Validation","text":"<p>These rules SHOULD be enforced by: - Pre-commit hooks checking for secret patterns - CI/CD pipeline validation of YAML syntax - Automated testing of secret generation process</p>"},{"location":"rules-secrets-management/#manual-review-requirements","title":"Manual Review Requirements","text":"<p>ALL secret changes MUST be reviewed for: - Compliance with variable substitution patterns - Proper gitignore coverage - Security best practices - Integration with existing services</p>"},{"location":"rules-secrets-management/#template-update-rules","title":"\ud83d\udd04 Template Update Rules","text":""},{"location":"rules-secrets-management/#rule-11-user-config-override-pattern","title":"Rule 11: User Config Override Pattern","text":"<p>When updating base templates, you MUST also update the user's config for immediate effect:</p>"},{"location":"rules-secrets-management/#correct-workflow-for-template-updates","title":"\u2705 Correct Workflow for Template Updates:","text":"<pre><code># 1. Update base template (for team sharing)\nnano secrets-templates/00-master-secrets.yml.template\ngit add secrets-templates/\ngit commit -m \"Add new secret template\"\n\n# 2. Update user's config (for immediate use)\nnano secrets-config/00-master-secrets.yml.template\n./create-kubernetes-secrets.sh\n\n# 3. Deploy updated secrets\nkubectl apply -f kubernetes/kubernetes-secrets.yml\n</code></pre>"},{"location":"rules-secrets-management/#anti-pattern_3","title":"\u274c Anti-Pattern:","text":"<pre><code># DON'T: Only update base templates\nnano secrets-templates/00-master-secrets.yml.template\n./create-kubernetes-secrets.sh  # Will use old user config!\n</code></pre>"},{"location":"rules-secrets-management/#why-this-rule-exists","title":"Why This Rule Exists:","text":"<p>The generation script prioritizes <code>secrets-config/</code> over <code>secrets-templates/</code> to enable user customization. Base templates are for team sharing, but user configs are for immediate use.</p>"},{"location":"rules-secrets-management/#verification-commands_1","title":"Verification Commands:","text":"<pre><code># Check if user config exists and is up to date\nls -la secrets-config/00-master-secrets.yml.template\n\n# Compare with base template\ndiff secrets-templates/00-master-secrets.yml.template secrets-config/00-master-secrets.yml.template\n\n# Verify generation uses user config\n./create-kubernetes-secrets.sh\ngrep \"Your new secret\" kubernetes/kubernetes-secrets.yml\n</code></pre> <p>\ud83d\udca1 Remember: These rules exist to prevent security breaches and maintain system reliability. They represent lessons learned from real incidents and should be followed consistently across all Urbalurba infrastructure work.</p>"},{"location":"secrets-management-readme/","title":"Secrets Management System","text":""},{"location":"secrets-management-readme/#the-golden-rule","title":"The Golden Rule","text":"<pre><code>EDIT HERE:        secrets-config/           (your machine's values)\nNEVER EDIT:       secrets-generated/        (auto-generated output)\nNEVER EDIT:       kubernetes/               (final output for kubectl)\n</code></pre>"},{"location":"secrets-management-readme/#how-it-works","title":"How It Works","text":"<pre><code>secrets-templates/     \u2192     secrets-config/     \u2192     kubernetes-secrets.yml\n     \u2191                            \u2191                           \u2191\n  Git tracked              EDIT THIS ONE               Final output\n  ${VARIABLES}             Your actual values          kubectl apply this\n  Team shared              Machine-specific\n</code></pre> <p>The script reads from <code>secrets-config/</code> and writes to <code>kubernetes/kubernetes-secrets.yml</code>.</p>"},{"location":"secrets-management-readme/#quick-start","title":"Quick Start","text":"<pre><code>cd topsecret/\n\n# 1. Edit your values (the SOURCE)\nnano secrets-config/00-common-values.env.template\n\n# 2. Generate the output\n./create-kubernetes-secrets.sh\n\n# 3. Apply to cluster\nkubectl apply -f kubernetes/kubernetes-secrets.yml\n</code></pre>"},{"location":"secrets-management-readme/#file-structure","title":"File Structure","text":"<pre><code>topsecret/\n\u251c\u2500\u2500 secrets-templates/              # Git tracked - base templates with ${VARIABLES}\n\u251c\u2500\u2500 secrets-config/                 # Gitignored - EDIT THIS (your values)\n\u251c\u2500\u2500 secrets-generated/              # Gitignored - DO NOT EDIT (temp files)\n\u2514\u2500\u2500 kubernetes/kubernetes-secrets.yml  # Gitignored - DO NOT EDIT (final output)\n</code></pre> Directory Edit? Git Purpose <code>secrets-templates/</code> Only for new variables Tracked Base templates for team <code>secrets-config/</code> YES - Edit this Ignored Your machine's values <code>secrets-generated/</code> NO Ignored Temporary processing <code>kubernetes/</code> NO Ignored Final output"},{"location":"secrets-management-readme/#common-tasks","title":"Common Tasks","text":""},{"location":"secrets-management-readme/#change-a-password-or-value","title":"Change a password or value","text":"<pre><code>nano secrets-config/00-common-values.env.template\n./create-kubernetes-secrets.sh\nkubectl apply -f kubernetes/kubernetes-secrets.yml\n</code></pre>"},{"location":"secrets-management-readme/#add-a-new-secret-variable","title":"Add a new secret variable","text":"<pre><code># 1. Add to base template (for team)\nnano secrets-templates/00-master-secrets.yml.template\n\n# 2. ALSO add to your config (script prioritizes secrets-config/)\nnano secrets-config/00-master-secrets.yml.template\n\n# 3. Regenerate\n./create-kubernetes-secrets.sh\n</code></pre>"},{"location":"secrets-management-readme/#reset-to-defaults","title":"Reset to defaults","text":"<pre><code>rm -rf secrets-config/\n./create-kubernetes-secrets.sh\n</code></pre>"},{"location":"secrets-management-readme/#sync-with-updated-templates","title":"Sync with updated templates","text":"<pre><code># Check what's different\ndiff secrets-templates/00-master-secrets.yml.template secrets-config/00-master-secrets.yml.template\n\n# Copy new template structure (preserves your values if you re-add them)\ncp secrets-templates/00-master-secrets.yml.template secrets-config/\n</code></pre>"},{"location":"secrets-management-readme/#multi-machine-setup","title":"Multi-Machine Setup","text":"<p>Each machine has its own <code>secrets-config/</code> with machine-specific values:</p> <pre><code># MacBook secrets-config/00-common-values.env.template\nTAILSCALE_INTERNAL_HOSTNAME=k8s-terje\n\n# iMac secrets-config/00-common-values.env.template\nTAILSCALE_INTERNAL_HOSTNAME=k8s-imac\n</code></pre> <p>Important: <code>secrets-config/</code> is gitignored and must be maintained separately on each machine.</p>"},{"location":"secrets-management-readme/#variable-substitution","title":"Variable Substitution","text":"<p>Central values in <code>secrets-config/00-common-values.env.template</code>: <pre><code>DEFAULT_DATABASE_PASSWORD=YourSecurePassword123\nDEFAULT_ADMIN_EMAIL=admin@yourcompany.com\n</code></pre></p> <p>Used in templates: <pre><code>PGPASSWORD: \"${DEFAULT_DATABASE_PASSWORD}\"\nAUTHENTIK_BOOTSTRAP_EMAIL: \"${DEFAULT_ADMIN_EMAIL}\"\n</code></pre></p>"},{"location":"secrets-management-readme/#troubleshooting","title":"Troubleshooting","text":"Problem Solution Changes not appearing Did you edit <code>secrets-config/</code> (not <code>secrets-generated/</code>)? New template variables missing Copy updated template to <code>secrets-config/</code> YAML validation failed <code>kubectl apply --dry-run=client -f kubernetes/kubernetes-secrets.yml</code> Variable not substituted Check spelling in <code>secrets-config/00-common-values.env.template</code>"},{"location":"secrets-management-readme/#safety-rules","title":"Safety Rules","text":"<p>DO: - Edit files in <code>secrets-config/</code> - Run <code>./create-kubernetes-secrets.sh</code> after changes - Verify with <code>kubectl apply --dry-run=client</code></p> <p>DON'T: - Edit files in <code>secrets-generated/</code> or <code>kubernetes/</code> - Put actual secrets in <code>secrets-templates/</code> - Commit anything from gitignored directories</p>"},{"location":"secrets-management-readme/#related-docs","title":"Related Docs","text":"<ul> <li>rules-secrets-management.md - Detailed rules and patterns</li> <li>hosts-cloud-init-secrets.md - SSH key setup</li> </ul>"},{"location":"troubleshooting-readme/","title":"Troubleshooting Guide","text":"<p>File: <code>docs/troubleshooting-readme.md</code> Purpose: Comprehensive troubleshooting guide for common issues and solutions Target Audience: All users, developers, and administrators Last Updated: September 22, 2024</p>"},{"location":"troubleshooting-readme/#overview","title":"\ud83d\udccb Overview","text":"<p>This guide covers common issues encountered in the Urbalurba Infrastructure platform and their solutions. Problems are organized by category to help you quickly find relevant troubleshooting steps.</p>"},{"location":"troubleshooting-readme/#quick-diagnostic-commands","title":"\ud83d\ude80 Quick Diagnostic Commands","text":"<p>Before diving into specific issues, these commands help identify the problem area:</p>"},{"location":"troubleshooting-readme/#automated-debugging-recommended","title":"Automated Debugging (Recommended)","text":"<pre><code># Complete cluster analysis (from provision-host)\n./troubleshooting/debug-cluster.sh\n\n# Service-specific debugging\n./troubleshooting/debug-traefik.sh        # Ingress issues\n./troubleshooting/debug-ai-litellm.sh     # AI platform issues\n</code></pre>"},{"location":"troubleshooting-readme/#manual-commands","title":"Manual Commands","text":"<pre><code># Check overall cluster health\nkubectl get nodes\nkubectl get pods -A --field-selector=status.phase!=Running\n\n# Check provision host\ndocker ps | grep provision-host\ndocker logs provision-host --tail=50\n\n# Check ingress and services\nkubectl get ingressroute -A\nkubectl get svc -A\n\n# Check storage\nkubectl get pvc -A\nkubectl get pv\n</code></pre>"},{"location":"troubleshooting-readme/#installation-setup-issues","title":"\ud83d\udd27 Installation &amp; Setup Issues","text":""},{"location":"troubleshooting-readme/#rancher-desktop-not-starting","title":"Rancher Desktop Not Starting","text":"<p>Symptoms: Rancher Desktop fails to start or Kubernetes is not available</p> <p>Solutions: 1. Reset Rancher Desktop:    - Settings \u2192 Troubleshooting \u2192 Reset Kubernetes    - Wait for complete reset, then restart</p> <ol> <li> <p>Check system resources:    <pre><code># Ensure sufficient memory (minimum 8GB recommended)\nfree -h\n# Check disk space\ndf -h\n</code></pre></p> </li> <li> <p>Verify Docker context:    <pre><code>docker context list\ndocker context use rancher-desktop\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting-readme/#provision-host-container-issues","title":"Provision Host Container Issues","text":"<p>Symptoms: Cannot access provision-host or container not running</p> <p>Solutions: 1. Check container status:    <pre><code>docker ps | grep provision-host\ndocker logs provision-host --tail=20\n</code></pre></p> <ol> <li> <p>Restart provision host:    <pre><code>docker stop provision-host\ndocker start provision-host\n</code></pre></p> </li> <li> <p>Volume mount issues:    <pre><code># Verify mount point exists\nls -la /mnt/urbalurbadisk/\n# Check Docker volume mounts\ndocker inspect provision-host | grep Mounts -A 20\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting-readme/#kubeconfig-issues","title":"Kubeconfig Issues","text":"<p>Symptoms: kubectl commands fail with connection errors</p> <p>Solutions: 1. Verify kubeconfig location:    <pre><code>echo $KUBECONFIG\nls -la ~/.kube/config\n</code></pre></p> <ol> <li> <p>Test connection:    <pre><code>kubectl cluster-info\nkubectl get nodes\n</code></pre></p> </li> <li> <p>Reset kubeconfig (from provision-host):    <pre><code>cp ~/.kube/config ~/.kube/config.backup\n# Re-run cluster connection setup\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting-readme/#service-deployment-issues","title":"\ud83c\udfd7\ufe0f Service Deployment Issues","text":""},{"location":"troubleshooting-readme/#pod-stuck-in-pending-state","title":"Pod Stuck in Pending State","text":"<p>Symptoms: Pods remain in Pending status</p> <p>Diagnosis: <pre><code>kubectl describe pod -n &lt;namespace&gt; &lt;pod-name&gt;\nkubectl get events -n &lt;namespace&gt; --sort-by=.metadata.creationTimestamp\n</code></pre></p> <p>Common Causes &amp; Solutions:</p> <ol> <li> <p>Resource constraints:    <pre><code>kubectl top nodes\nkubectl describe node\n</code></pre></p> </li> <li> <p>Storage issues:    <pre><code>kubectl get pvc -A\nkubectl describe pvc -n &lt;namespace&gt; &lt;pvc-name&gt;\n</code></pre></p> </li> <li> <p>Image pull problems:    <pre><code># Check image availability\ndocker pull &lt;image-name&gt;\n# Check image pull secrets\nkubectl get secrets -n &lt;namespace&gt;\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting-readme/#pod-crashloopbackoff","title":"Pod CrashLoopBackOff","text":"<p>Symptoms: Pods continuously restart</p> <p>Diagnosis: <pre><code>kubectl logs -n &lt;namespace&gt; &lt;pod-name&gt; --previous\nkubectl describe pod -n &lt;namespace&gt; &lt;pod-name&gt;\n</code></pre></p> <p>Solutions: 1. Check application logs:    <pre><code>kubectl logs -n &lt;namespace&gt; &lt;pod-name&gt; -f\n</code></pre></p> <ol> <li> <p>Review resource limits:    <pre><code>kubectl get pod -n &lt;namespace&gt; &lt;pod-name&gt; -o yaml | grep -A 5 resources\n</code></pre></p> </li> <li> <p>Inspect configuration:    <pre><code>kubectl get configmap -n &lt;namespace&gt;\nkubectl get secrets -n &lt;namespace&gt;\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting-readme/#service-connection-issues","title":"Service Connection Issues","text":"<p>Symptoms: Services unreachable or timing out</p> <p>Diagnosis: <pre><code>kubectl get svc -A\nkubectl get endpoints -n &lt;namespace&gt;\nkubectl describe svc -n &lt;namespace&gt; &lt;service-name&gt;\n</code></pre></p> <p>Solutions: 1. Test internal connectivity:    <pre><code>kubectl run test-pod --image=curlimages/curl -it --rm -- sh\n# From inside pod:\ncurl &lt;service-name&gt;.&lt;namespace&gt;:port\n</code></pre></p> <ol> <li>Check service selectors:    <pre><code>kubectl get pod -n &lt;namespace&gt; --show-labels\nkubectl describe svc -n &lt;namespace&gt; &lt;service-name&gt;\n</code></pre></li> </ol>"},{"location":"troubleshooting-readme/#networking-ingress-issues","title":"\ud83c\udf10 Networking &amp; Ingress Issues","text":""},{"location":"troubleshooting-readme/#traefik-ingress-not-working","title":"Traefik Ingress Not Working","text":"<p>Symptoms: Services not accessible via ingress URLs</p> <p>Diagnosis: <pre><code>kubectl get ingressroute -A\nkubectl logs -n kube-system -l app.kubernetes.io/name=traefik\nkubectl describe ingressroute -n &lt;namespace&gt; &lt;route-name&gt;\n</code></pre></p> <p>Solutions: 1. Verify IngressRoute configuration:    <pre><code># Check host patterns and rules\nkubectl get ingressroute -n &lt;namespace&gt; &lt;route-name&gt; -o yaml\n</code></pre></p> <ol> <li> <p>Test Traefik dashboard:    <pre><code>kubectl port-forward -n kube-system svc/traefik 8080:8080\n# Access http://localhost:8080\n</code></pre></p> </li> <li> <p>Check DNS resolution:    <pre><code>nslookup &lt;service&gt;.localhost\n# Should resolve to 127.0.0.1\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting-readme/#localhost-domain-issues","title":"localhost Domain Issues","text":"<p>Symptoms: <code>http://service.localhost</code> not accessible</p> <p>Solutions: 1. Check /etc/hosts (macOS/Linux):    <pre><code>cat /etc/hosts | grep localhost\n# Should include: 127.0.0.1 localhost\n</code></pre></p> <ol> <li> <p>Verify port forwarding:    <pre><code>kubectl get svc -n kube-system traefik\n# Should show ports 80:xxxxx and 443:xxxxx\n</code></pre></p> </li> <li> <p>Test with IP directly:    <pre><code>curl -H \"Host: service.localhost\" http://127.0.0.1/\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting-readme/#authentication-issues","title":"\ud83d\udd10 Authentication Issues","text":""},{"location":"troubleshooting-readme/#authentik-sso-problems","title":"Authentik SSO Problems","text":"<p>Symptoms: Cannot access Authentik or authentication loops</p> <p>Diagnosis: <pre><code>kubectl logs -n authentik -l app.kubernetes.io/name=authentik\nkubectl get pod -n authentik\nkubectl describe ingressroute -n authentik authentik\n</code></pre></p> <p>Solutions: 1. Reset Authentik password:    <pre><code>kubectl exec -n authentik &lt;authentik-pod&gt; -- ak create_admin_token\n</code></pre></p> <ol> <li> <p>Check Authentik configuration:    <pre><code>kubectl get configmap -n authentik authentik-config -o yaml\n</code></pre></p> </li> <li> <p>Verify database connectivity:    <pre><code>kubectl logs -n authentik &lt;authentik-pod&gt; | grep -i database\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting-readme/#forward-auth-middleware-issues","title":"Forward Auth Middleware Issues","text":"<p>Symptoms: Protected services show authentication errors</p> <p>Diagnosis: <pre><code>kubectl get middleware -A\nkubectl logs -n kube-system -l app.kubernetes.io/name=traefik | grep -i auth\n</code></pre></p> <p>Solutions: 1. Check middleware configuration:    <pre><code>kubectl describe middleware -n &lt;namespace&gt; authentik-forward-auth\n</code></pre></p> <ol> <li>Test whoami service:    <pre><code># Should work: http://whoami-public.localhost\n# Should redirect: http://whoami.localhost\n</code></pre></li> </ol>"},{"location":"troubleshooting-readme/#database-issues","title":"\ud83d\udcbe Database Issues","text":""},{"location":"troubleshooting-readme/#postgresql-connection-problems","title":"PostgreSQL Connection Problems","text":"<p>Symptoms: Applications cannot connect to PostgreSQL</p> <p>Diagnosis: <pre><code>kubectl logs -n postgresql -l app=postgresql\nkubectl get svc -n postgresql\nkubectl exec -n postgresql &lt;postgres-pod&gt; -- pg_isready\n</code></pre></p> <p>Solutions: 1. Test database connection:    <pre><code>kubectl exec -n postgresql &lt;postgres-pod&gt; -- psql -U postgres -c \"SELECT version();\"\n</code></pre></p> <ol> <li> <p>Check connection limits:    <pre><code>kubectl exec -n postgresql &lt;postgres-pod&gt; -- psql -U postgres -c \"SHOW max_connections;\"\nkubectl exec -n postgresql &lt;postgres-pod&gt; -- psql -U postgres -c \"SELECT count(*) FROM pg_stat_activity;\"\n</code></pre></p> </li> <li> <p>Review configuration:    <pre><code>kubectl get configmap -n postgresql postgresql-config -o yaml\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting-readme/#database-migration-issues","title":"Database Migration Issues","text":"<p>Symptoms: Applications report database schema problems</p> <p>Solutions: 1. Check migration logs:    <pre><code>kubectl logs -n &lt;namespace&gt; &lt;app-pod&gt; | grep -i migration\n</code></pre></p> <ol> <li>Manual migration (if needed):    <pre><code>kubectl exec -it -n &lt;namespace&gt; &lt;app-pod&gt; -- &lt;migration-command&gt;\n</code></pre></li> </ol>"},{"location":"troubleshooting-readme/#ai-platform-issues","title":"\ud83e\udd16 AI Platform Issues","text":""},{"location":"troubleshooting-readme/#openwebui-not-loading","title":"OpenWebUI Not Loading","text":"<p>Symptoms: OpenWebUI interface shows errors or won't load</p> <p>Diagnosis: <pre><code>kubectl logs -n openwebui -l app=openwebui\nkubectl get svc -n openwebui\nkubectl describe pod -n openwebui\n</code></pre></p> <p>Solutions: 1. Check OpenWebUI configuration:    <pre><code>kubectl get configmap -n openwebui openwebui-config -o yaml\n</code></pre></p> <ol> <li>Verify LiteLLM connectivity:    <pre><code>kubectl exec -n openwebui &lt;openwebui-pod&gt; -- curl http://litellm.litellm:4000/health\n</code></pre></li> </ol>"},{"location":"troubleshooting-readme/#litellm-api-issues","title":"LiteLLM API Issues","text":"<p>Symptoms: AI models not responding or API errors</p> <p>Diagnosis: <pre><code>kubectl logs -n litellm -l app=litellm\nkubectl exec -n litellm &lt;litellm-pod&gt; -- curl http://localhost:4000/health\n</code></pre></p> <p>Solutions: 1. Check API keys:    <pre><code>kubectl get secrets -n litellm litellm-secrets\n</code></pre></p> <ol> <li>Test model availability:    <pre><code>kubectl exec -n litellm &lt;litellm-pod&gt; -- curl -X POST http://localhost:4000/v1/models\n</code></pre></li> </ol>"},{"location":"troubleshooting-readme/#monitoring-issues","title":"\ud83d\udcca Monitoring Issues","text":""},{"location":"troubleshooting-readme/#grafana-dashboard-problems","title":"Grafana Dashboard Problems","text":"<p>Symptoms: Grafana not accessible or missing data</p> <p>Diagnosis: <pre><code>kubectl logs -n grafana -l app.kubernetes.io/name=grafana\nkubectl get svc -n grafana\n</code></pre></p> <p>Solutions: 1. Reset Grafana admin password:    <pre><code>kubectl get secrets -n grafana grafana-admin-secret\n</code></pre></p> <ol> <li>Check data sources:    <pre><code># Access Grafana and verify Prometheus connection\n# URL: http://grafana.localhost\n</code></pre></li> </ol>"},{"location":"troubleshooting-readme/#cloud-deployment-issues","title":"\u2601\ufe0f Cloud Deployment Issues","text":""},{"location":"troubleshooting-readme/#azure-aks-connection-problems","title":"Azure AKS Connection Problems","text":"<p>Symptoms: Cannot connect to AKS cluster</p> <p>Solutions: 1. Update kubeconfig:    <pre><code>az aks get-credentials --resource-group &lt;rg&gt; --name &lt;cluster&gt;\n</code></pre></p> <ol> <li>Check Azure CLI authentication:    <pre><code>az account show\naz aks list\n</code></pre></li> </ol>"},{"location":"troubleshooting-readme/#tailscale-vpn-issues","title":"Tailscale VPN Issues","text":"<p>Symptoms: Cannot access remote hosts via Tailscale</p> <p>Solutions: 1. Check Tailscale status:    <pre><code>tailscale status\ntailscale ping &lt;host&gt;\n</code></pre></p> <ol> <li>Restart Tailscale:    <pre><code>sudo systemctl restart tailscaled  # Linux\n# Or restart Tailscale app on macOS/Windows\n</code></pre></li> </ol>"},{"location":"troubleshooting-readme/#recovery-procedures","title":"\ud83d\udd04 Recovery Procedures","text":""},{"location":"troubleshooting-readme/#complete-cluster-reset","title":"Complete Cluster Reset","text":"<p>When multiple issues persist, a complete reset may be needed:</p> <ol> <li> <p>Backup important data:    <pre><code># Export important configurations\nkubectl get secrets -A -o yaml &gt; secrets-backup.yaml\nkubectl get configmap -A -o yaml &gt; configmaps-backup.yaml\n</code></pre></p> </li> <li> <p>Reset Rancher Desktop:</p> </li> <li>Settings \u2192 Troubleshooting \u2192 Reset Kubernetes</li> <li> <p>Wait for complete reset</p> </li> <li> <p>Restore provision-host:    <pre><code># Restart provision-host container\ndocker stop provision-host\ndocker start provision-host\n</code></pre></p> </li> <li> <p>Re-provision services:    <pre><code>docker exec -it provision-host bash\ncd /mnt/urbalurbadisk/\n./provision-host/kubernetes/provision-kubernetes.sh\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting-readme/#data-recovery","title":"Data Recovery","text":"<p>If persistent data is lost:</p> <ol> <li> <p>Check persistent volumes:    <pre><code>kubectl get pv\nkubectl describe pv &lt;volume-name&gt;\n</code></pre></p> </li> <li> <p>Restore from backups (if available):    <pre><code># Restore database backups\nkubectl exec -i -n postgresql &lt;postgres-pod&gt; -- psql -U postgres &lt; backup.sql\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting-readme/#getting-additional-help","title":"\ud83c\udd98 Getting Additional Help","text":""},{"location":"troubleshooting-readme/#log-collection","title":"Log Collection","text":"<p>When reporting issues, include these logs:</p> <pre><code># Cluster overview\nkubectl get all -A &gt; cluster-overview.txt\n\n# Pod issues\nkubectl describe pod -n &lt;namespace&gt; &lt;pod-name&gt; &gt; pod-details.txt\nkubectl logs -n &lt;namespace&gt; &lt;pod-name&gt; --previous &gt; pod-logs.txt\n\n# Events\nkubectl get events -A --sort-by=.metadata.creationTimestamp &gt; events.txt\n\n# Provision host logs\ndocker logs provision-host &gt; provision-host.log\n</code></pre>"},{"location":"troubleshooting-readme/#system-information","title":"System Information","text":"<p>Include system details when requesting help:</p> <pre><code># Kubernetes version\nkubectl version\n\n# Node information\nkubectl get nodes -o wide\n\n# Docker information\ndocker version\ndocker system info\n\n# Host system\nuname -a\ndf -h\nfree -h\n</code></pre>"},{"location":"troubleshooting-readme/#automated-debugging-scripts","title":"\ud83e\udd16 Automated Debugging Scripts","text":"<p>The platform includes comprehensive debugging scripts in the <code>troubleshooting/</code> folder:</p>"},{"location":"troubleshooting-readme/#cluster-wide-debugging","title":"Cluster-Wide Debugging","text":"<p><code>debug-cluster.sh</code> - Complete cluster health analysis: <pre><code># Run from provision-host\n./troubleshooting/debug-cluster.sh [namespace]\n</code></pre></p> <p>Features: - Collects all resource information across namespaces - Identifies unhealthy pods and retrieves their logs - Analyzes resource usage and storage issues - Generates timestamped output files with cleanup - Provides actionable recommendations</p> <p><code>export-cluster-status.sh</code> - Full cluster snapshot: <pre><code># Export complete cluster configuration\n./troubleshooting/export-cluster-status.sh [cluster-name]\n</code></pre></p> <p>Creates: - Individual files for each Kubernetes resource type - Compressed archive for easy sharing with support - Version information for key services - Complete cluster configuration snapshot</p>"},{"location":"troubleshooting-readme/#service-specific-debugging","title":"Service-Specific Debugging","text":"<p>Traefik Ingress (<code>debug-traefik.sh</code>): <pre><code>./troubleshooting/debug-traefik.sh\n</code></pre> - IngressRoute and Middleware analysis - Traefik pod and service diagnostics - Custom resource validation - Network connectivity checks</p> <p>AI Platform (<code>debug-ai-litellm.sh</code>): <pre><code>./troubleshooting/debug-ai-litellm.sh [namespace]\n</code></pre> - LiteLLM configuration and API health - Model availability and routing - Secret and ConfigMap validation - API connectivity testing</p> <p>Other Service Scripts: - <code>debug-ai-openwebui.sh</code> - OpenWebUI diagnostics - <code>debug-ai-ollama-cluster.sh</code> - Ollama cluster debugging - <code>debug-ai-qdrant.sh</code> - Vector database diagnostics - <code>debug-redis.sh</code> - Redis connectivity and performance - <code>debug-mongodb.sh</code> - MongoDB cluster analysis - <code>debug-elasticsearch.sh</code> - Elasticsearch cluster health</p>"},{"location":"troubleshooting-readme/#using-the-debug-scripts","title":"Using the Debug Scripts","text":"<ol> <li> <p>Access provision-host:    <pre><code>docker exec -it provision-host bash\ncd /mnt/urbalurbadisk/\n</code></pre></p> </li> <li> <p>Run appropriate debug script:    <pre><code># For general cluster issues\n./troubleshooting/debug-cluster.sh\n\n# For specific service issues\n./troubleshooting/debug-traefik.sh\n./troubleshooting/debug-ai-litellm.sh\n</code></pre></p> </li> <li> <p>Review generated output:    <pre><code># Output saved to troubleshooting/output/\nls troubleshooting/output/\ncat troubleshooting/output/debug-cluster-*.txt\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting-readme/#debug-output-features","title":"Debug Output Features","text":"<ul> <li>Timestamped files - Each run creates uniquely named output</li> <li>Automatic cleanup - Keeps only the 3 most recent debug files</li> <li>Structured sections - Organized by problem area</li> <li>Status tracking - Success/failure indicators for each check</li> <li>Log extraction - Automatic collection from problematic pods</li> <li>Recommendations - Specific next steps based on findings</li> </ul>"},{"location":"troubleshooting-readme/#contact-and-resources","title":"Contact and Resources","text":"<ul> <li>\ud83d\udcd6 Documentation: Start with doc/README.md</li> <li>\ud83c\udfd7\ufe0f Architecture: Review doc/overview-system-architecture.md</li> <li>\ud83d\udd27 Commands: See doc/provision-host-commands.md</li> <li>\ud83e\udd16 Debug Scripts: Use automated tools in <code>troubleshooting/</code> folder</li> <li>\ud83d\udc1b Issues: Report at GitHub repository issues</li> </ul> <p>\ud83d\udca1 Remember: Most issues can be resolved by checking logs, verifying configuration, and ensuring services are running. When in doubt, start with the automated debugging scripts or the basic diagnostic commands at the top of this guide.</p>"},{"location":"api-first/api-first-csharp/","title":"API-First Development for C# Developers","text":""},{"location":"api-first/api-first-csharp/#what-is-api-first","title":"What is API-First?","text":"<p>Instead of writing code first, you design your API contract first in an OpenAPI specification. Then you generate code from that contract. This ensures everyone agrees on what the API should look like before any coding starts.</p>"},{"location":"api-first/api-first-csharp/#workflow-overview","title":"Workflow Overview","text":""},{"location":"api-first/api-first-csharp/#phase-1-design","title":"Phase 1: Design","text":"<p>\u2022 Write OpenAPI spec (<code>api-spec.yaml</code>)   - Note: This is your contract - think carefully about data models and endpoints   - Tip: Use Swagger Editor online to validate as you write</p> <p>\u2022 Validate specification   - <code>nswag validate /input:api-spec.yaml</code>   - Note: Fix all validation errors before proceeding</p>"},{"location":"api-first/api-first-csharp/#phase-2-generate-code","title":"Phase 2: Generate Code","text":"<p>\u2022 Generate server stubs   - <code>nswag openapi2cscontroller</code> \u2192 Creates controller templates   - Note: Generated code has <code>NotImplementedException()</code> - that's expected</p> <p>\u2022 Generate client code   - <code>nswag openapi2csclient</code> \u2192 Creates typed HTTP client   - Note: Client automatically handles serialization/deserialization</p>"},{"location":"api-first/api-first-csharp/#phase-3-implement","title":"Phase 3: Implement","text":"<p>\u2022 Implement server logic   - Fill in the <code>NotImplementedException()</code> methods   - Note: Use partial classes to keep your code separate from generated code</p> <p>\u2022 Test with generated client   - Use the generated client interfaces for testing   - Note: Perfect contract compliance guaranteed</p>"},{"location":"api-first/api-first-csharp/#phase-4-iterate","title":"Phase 4: Iterate","text":"<p>\u2022 Update spec \u2192 Regenerate \u2192 Update implementation   - Note: Breaking changes in spec = compile errors in code (this is good!)   - Tip: Version your API specs (<code>v1.yaml</code>, <code>v2.yaml</code>) for major changes</p>"},{"location":"api-first/api-first-csharp/#key-success-factors","title":"Key Success Factors","text":"<p>\ud83c\udfaf Start simple - Few endpoints, basic models \ud83c\udfaf Validate early - Fix spec issues before generating code \ud83c\udfaf Separate concerns - Generated code vs. implementation code \ud83c\udfaf Version control specs - Treat them like source code \ud83c\udfaf Share specs first - Get agreement before implementation  </p>"},{"location":"api-first/api-first-csharp/#common-gotchas","title":"Common Gotchas","text":"<p>\u26a0\ufe0f Don't edit generated files - They get overwritten \u26a0\ufe0f Use <code>operationId</code> - Controls method names in generated code \u26a0\ufe0f Be specific with data types - <code>integer</code> vs <code>number</code>, <code>date-time</code> formats \u26a0\ufe0f Handle errors in spec - Define 400, 404, 500 responses  </p>"},{"location":"api-first/api-first-csharp/#workflow-summary","title":"Workflow Summary","text":"<pre><code>YAML Spec \u2192 Validate \u2192 Generate \u2192 Implement \u2192 Test \u2192 Repeat\n</code></pre> <p>The beauty: Your API contract becomes executable code that stays in sync!</p>"},{"location":"api-first/api-first-csharp/#step-by-step-tutorial","title":"Step-by-Step Tutorial","text":""},{"location":"api-first/api-first-csharp/#step-1-install-tools","title":"Step 1: Install Tools","text":"<pre><code># Install NSwag globally\ndotnet tool install -g NSwag.ConsoleCore\n\n# Create your solution\ndotnet new sln -n TodoApi\nmkdir TodoApi.Specs\nmkdir TodoApi.Server\nmkdir TodoApi.Client\n</code></pre>"},{"location":"api-first/api-first-csharp/#step-2-create-your-first-api-specification","title":"Step 2: Create Your First API Specification","text":"<p>Create <code>TodoApi.Specs/todo-api.yaml</code>:</p> <pre><code>openapi: 3.0.0\ninfo:\n  title: Todo API\n  version: 1.0.0\n  description: A simple Todo API for learning\n\nservers:\n  - url: https://localhost:7001\n    description: Development server\n\npaths:\n  /todos:\n    get:\n      summary: Get all todos\n      operationId: GetTodos\n      tags:\n        - Todos\n      responses:\n        '200':\n          description: List of todos\n          content:\n            application/json:\n              schema:\n                type: array\n                items:\n                  $ref: '#/components/schemas/Todo'\n\n    post:\n      summary: Create a new todo\n      operationId: CreateTodo\n      tags:\n        - Todos\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/CreateTodoRequest'\n      responses:\n        '201':\n          description: Todo created\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Todo'\n\n  /todos/{id}:\n    get:\n      summary: Get todo by ID\n      operationId: GetTodo\n      tags:\n        - Todos\n      parameters:\n        - name: id\n          in: path\n          required: true\n          schema:\n            type: integer\n      responses:\n        '200':\n          description: Todo found\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Todo'\n        '404':\n          description: Todo not found\n\ncomponents:\n  schemas:\n    Todo:\n      type: object\n      required:\n        - id\n        - title\n        - completed\n      properties:\n        id:\n          type: integer\n          example: 1\n        title:\n          type: string\n          example: \"Buy groceries\"\n        completed:\n          type: boolean\n          example: false\n        createdAt:\n          type: string\n          format: date-time\n\n    CreateTodoRequest:\n      type: object\n      required:\n        - title\n      properties:\n        title:\n          type: string\n          example: \"Buy groceries\"\n</code></pre>"},{"location":"api-first/api-first-csharp/#step-3-validate-your-specification","title":"Step 3: Validate Your Specification","text":"<pre><code>cd TodoApi.Specs\nnswag validate /input:todo-api.yaml\n</code></pre> <p>\u2705 If you see \"Document is valid\" - you're good to go!</p>"},{"location":"api-first/api-first-csharp/#step-4-generate-server-code","title":"Step 4: Generate Server Code","text":"<p>Create <code>TodoApi.Server/nswag-server.json</code>:</p> <pre><code>{\n  \"runtime\": \"Net80\",\n  \"documentGenerator\": {\n    \"fromDocument\": {\n      \"url\": \"../TodoApi.Specs/todo-api.yaml\"\n    }\n  },\n  \"codeGenerators\": {\n    \"openApiToCSharpController\": {\n      \"className\": \"{controller}\",\n      \"namespace\": \"TodoApi.Server.Controllers\",\n      \"controllerBaseClass\": \"Microsoft.AspNetCore.Mvc.ControllerBase\",\n      \"generateModelValidationAttributes\": true,\n      \"routeNamingStrategy\": \"OperationId\",\n      \"output\": \"Controllers/TodosController.cs\"\n    }\n  }\n}\n</code></pre> <p>Generate the server:</p> <pre><code>cd TodoApi.Server\ndotnet new webapi --no-openapi\nnswag run nswag-server.json\n</code></pre>"},{"location":"api-first/api-first-csharp/#step-5-implement-your-controller","title":"Step 5: Implement Your Controller","text":"<p>The generated <code>Controllers/TodosController.cs</code> will look like this:</p> <pre><code>// This is GENERATED - don't edit directly\n[ApiController]\npublic partial class TodosController : ControllerBase\n{\n    [HttpGet]\n    [Route(\"/todos\")]\n    public virtual async Task&lt;ActionResult&lt;ICollection&lt;Todo&gt;&gt;&gt; GetTodos()\n    {\n        throw new NotImplementedException();\n    }\n\n    [HttpPost]\n    [Route(\"/todos\")]\n    public virtual async Task&lt;ActionResult&lt;Todo&gt;&gt; CreateTodo([FromBody] CreateTodoRequest body)\n    {\n        throw new NotImplementedException();\n    }\n\n    [HttpGet]\n    [Route(\"/todos/{id}\")]\n    public virtual async Task&lt;ActionResult&lt;Todo&gt;&gt; GetTodo([FromRoute] int id)\n    {\n        throw new NotImplementedException();\n    }\n}\n</code></pre> <p>Create your implementation in <code>Controllers/TodosController.Implementation.cs</code>:</p> <pre><code>using Microsoft.AspNetCore.Mvc;\n\nnamespace TodoApi.Server.Controllers;\n\n// This is YOUR code - implement the actual logic\npublic partial class TodosController\n{\n    private static readonly List&lt;Todo&gt; _todos = new();\n    private static int _nextId = 1;\n\n    public override async Task&lt;ActionResult&lt;ICollection&lt;Todo&gt;&gt;&gt; GetTodos()\n    {\n        return Ok(_todos);\n    }\n\n    public override async Task&lt;ActionResult&lt;Todo&gt;&gt; CreateTodo([FromBody] CreateTodoRequest request)\n    {\n        var todo = new Todo\n        {\n            Id = _nextId++,\n            Title = request.Title,\n            Completed = false,\n            CreatedAt = DateTime.UtcNow\n        };\n\n        _todos.Add(todo);\n        return CreatedAtAction(nameof(GetTodo), new { id = todo.Id }, todo);\n    }\n\n    public override async Task&lt;ActionResult&lt;Todo&gt;&gt; GetTodo([FromRoute] int id)\n    {\n        var todo = _todos.FirstOrDefault(t =&gt; t.Id == id);\n        if (todo == null)\n            return NotFound();\n\n        return Ok(todo);\n    }\n}\n</code></pre>"},{"location":"api-first/api-first-csharp/#step-6-generate-client-code","title":"Step 6: Generate Client Code","text":"<p>Create <code>TodoApi.Client/nswag-client.json</code>:</p> <pre><code>{\n  \"runtime\": \"Net80\",\n  \"documentGenerator\": {\n    \"fromDocument\": {\n      \"url\": \"../TodoApi.Specs/todo-api.yaml\"\n    }\n  },\n  \"codeGenerators\": {\n    \"openApiToCSharpClient\": {\n      \"className\": \"TodoApiClient\",\n      \"namespace\": \"TodoApi.Client\",\n      \"generateClientInterfaces\": true,\n      \"generateExceptionClasses\": true,\n      \"output\": \"TodoApiClient.cs\"\n    }\n  }\n}\n</code></pre> <p>Generate the client:</p> <pre><code>cd TodoApi.Client\ndotnet new classlib\ndotnet add package Microsoft.Extensions.Http\nnswag run nswag-client.json\n</code></pre>"},{"location":"api-first/api-first-csharp/#step-7-test-everything","title":"Step 7: Test Everything","text":"<p>Start your server:</p> <pre><code>cd TodoApi.Server\ndotnet run\n</code></pre> <p>Create a simple console app to test:</p> <pre><code>dotnet new console -n TodoApi.TestApp\ncd TodoApi.TestApp\ndotnet add reference ../TodoApi.Client\ndotnet add package Microsoft.Extensions.Http\ndotnet add package Microsoft.Extensions.DependencyInjection\n</code></pre> <p>Test code in <code>Program.cs</code>:</p> <pre><code>using Microsoft.Extensions.DependencyInjection;\nusing TodoApi.Client;\n\n// Setup DI\nvar services = new ServiceCollection();\nservices.AddHttpClient&lt;ITodoApiClient, TodoApiClient&gt;(client =&gt;\n{\n    client.BaseAddress = new Uri(\"https://localhost:7001\");\n});\n\nvar provider = services.BuildServiceProvider();\nvar apiClient = provider.GetRequiredService&lt;ITodoApiClient&gt;();\n\n// Test the API\ntry\n{\n    // Create a todo\n    var newTodo = await apiClient.CreateTodoAsync(new CreateTodoRequest \n    { \n        Title = \"Learn API-First Development\" \n    });\n    Console.WriteLine($\"Created: {newTodo.Title}\");\n\n    // Get all todos\n    var todos = await apiClient.GetTodosAsync();\n    Console.WriteLine($\"Total todos: {todos.Count}\");\n\n    // Get specific todo\n    var todo = await apiClient.GetTodoAsync(newTodo.Id);\n    Console.WriteLine($\"Retrieved: {todo.Title}\");\n}\ncatch (Exception ex)\n{\n    Console.WriteLine($\"Error: {ex.Message}\");\n}\n</code></pre>"},{"location":"api-first/api-first-csharp/#key-benefits-you-just-experienced","title":"Key Benefits You Just Experienced","text":"<p>\u2705 Contract-First: Everyone agrees on the API before coding \u2705 Type Safety: Generated code prevents runtime errors \u2705 Consistency: Client and server match perfectly \u2705 Documentation: Your spec IS your documentation \u2705 Testing: Easy to mock generated interfaces  </p>"},{"location":"api-first/api-first-csharp/#next-steps","title":"Next Steps","text":"<p>\u2022 Update your spec \u2192 Regenerate code \u2192 Implement new features \u2022 Version your API by creating <code>todo-api-v2.yaml</code> \u2022 Share your spec with frontend developers \u2022 Add authentication to your OpenAPI spec \u2022 Deploy and share the spec URL with consumers</p> <p>The magic: Change the spec file, regenerate, and both server and client are automatically updated! \ud83c\udf89</p>"},{"location":"api-first/csharp-developer/","title":"Urbalurba API Project - Quick Start Template","text":"<p>\ud83d\ude80 Get your Urbalurba API project running in 5 minutes!</p>"},{"location":"api-first/csharp-developer/#for-external-consultants","title":"\ud83c\udfaf For External Consultants","text":"<p>This template provides everything you need to build an Urbalurba API following our API-First approach. You don't need to know API-First development - just follow these steps!</p>"},{"location":"api-first/csharp-developer/#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<p>Before starting, make sure you have: - \u2705 .NET 8 SDK installed - \u2705 Git installed - \u2705 Your favorite code editor (VS Code, Visual Studio, etc.)</p>"},{"location":"api-first/csharp-developer/#quick-start-5-steps","title":"\ud83d\ude80 Quick Start (5 Steps)","text":""},{"location":"api-first/csharp-developer/#step-1-clone-this-template","title":"Step 1: Clone This Template","text":"<pre><code># Replace 'my-project-name' with your actual project name\ngit clone [TEMPLATE-REPO-URL] my-project-name\ncd my-project-name\n</code></pre>"},{"location":"api-first/csharp-developer/#step-2-run-one-command-setup","title":"Step 2: Run One-Command Setup","text":"<pre><code># This installs tools and pulls Urbalurba shared schemas\n./tools/setup.ps1\n</code></pre>"},{"location":"api-first/csharp-developer/#step-3-design-your-api","title":"Step 3: Design Your API","text":"<pre><code># Edit your API specification using shared Urbalurba schemas\ncode api/specs/my-api-v1.yaml\n</code></pre> <p>\ud83d\udcd6 IMPORTANT: Always use the shared schemas! See <code>shared-schemas/examples/how-to-reference.yaml</code> for examples.</p>"},{"location":"api-first/csharp-developer/#step-4-generate-validate","title":"Step 4: Generate &amp; Validate","text":"<pre><code># Validate your API specification\n./tools/validate.ps1\n\n# Generate server and client code\n./tools/generate.ps1\n</code></pre>"},{"location":"api-first/csharp-developer/#step-5-implement-your-logic","title":"Step 5: Implement Your Logic","text":"<pre><code># Implement business logic in the generated controllers\ncode api/server/src/Controllers/\n</code></pre>"},{"location":"api-first/csharp-developer/#project-structure","title":"\ud83d\udcc1 Project Structure","text":"<pre><code>my-project/\n\u251c\u2500\u2500 README.md                   # This file\n\u251c\u2500\u2500 .gitmodules                 # Git submodule configuration\n\u251c\u2500\u2500 shared-schemas/             # Urbalurba shared schemas (READ-ONLY)\n\u2502   \u251c\u2500\u2500 fields/v1/              # Individual field types\n\u2502   \u251c\u2500\u2500 entities/v1/            # Complete business entities  \n\u2502   \u2514\u2500\u2500 examples/               # Usage examples\n\u251c\u2500\u2500 api/\n\u2502   \u251c\u2500\u2500 specs/\n\u2502   \u2502   \u2514\u2500\u2500 my-api-v1.yaml      # YOUR API specification\n\u2502   \u251c\u2500\u2500 server/\n\u2502   \u2502   \u251c\u2500\u2500 nswag-server.json   # Server generation config\n\u2502   \u2502   \u2514\u2500\u2500 src/                # Generated server code goes here\n\u2502   \u2514\u2500\u2500 client/\n\u2502       \u251c\u2500\u2500 nswag-client.json   # Client generation config\n\u2502       \u2514\u2500\u2500 generated/          # Generated client code goes here\n\u251c\u2500\u2500 tools/\n\u2502   \u251c\u2500\u2500 setup.ps1               # One-time setup script\n\u2502   \u251c\u2500\u2500 validate.ps1            # Validate your API spec\n\u2502   \u251c\u2500\u2500 generate.ps1            # Generate server &amp; client code\n\u2502   \u2514\u2500\u2500 test.ps1                # Run tests\n\u2514\u2500\u2500 docs/\n    \u251c\u2500\u2500 GETTING-STARTED.md      # Detailed getting started guide\n    \u251c\u2500\u2500 EXAMPLES.md             # More examples\n    \u2514\u2500\u2500 TROUBLESHOOTING.md      # Common problems &amp; solutions\n</code></pre>"},{"location":"api-first/csharp-developer/#daily-development-workflow","title":"\u26a1 Daily Development Workflow","text":"<pre><code># 1. Edit your API specification\ncode api/specs/my-api-v1.yaml\n\n# 2. Validate changes\n./tools/validate.ps1\n\n# 3. Generate code when ready\n./tools/generate.ps1\n\n# 4. Implement business logic\ncode api/server/src/Controllers/\n\n# 5. Test your changes\n./tools/test.ps1\n\n# 6. Commit your work (generated code is automatically ignored)\ngit add .\ngit commit -m \"Implemented feature X\"\n</code></pre>"},{"location":"api-first/csharp-developer/#rules-for-success","title":"\ud83d\udea8 Rules for Success","text":""},{"location":"api-first/csharp-developer/#do","title":"\u2705 DO:","text":"<ul> <li>\u2705 Use shared schemas from <code>shared-schemas/</code> folder</li> <li>\u2705 Follow examples in <code>shared-schemas/examples/</code></li> <li>\u2705 Run <code>./tools/validate.ps1</code> before committing</li> <li>\u2705 Ask Urbalurba team before creating new field types</li> <li>\u2705 Read the documentation in <code>docs/</code> folder</li> </ul>"},{"location":"api-first/csharp-developer/#dont","title":"\u274c DON'T:","text":"<ul> <li>\u274c Edit anything in <code>shared-schemas/</code> folder (it's READ-ONLY)</li> <li>\u274c Create your own versions of existing types</li> <li>\u274c Skip validation steps</li> <li>\u274c Commit generated code (it's auto-ignored)</li> <li>\u274c Hardcode values that should be configurable</li> </ul>"},{"location":"api-first/csharp-developer/#need-help","title":"\ud83d\udcde Need Help?","text":"<ol> <li>First: Check <code>docs/TROUBLESHOOTING.md</code></li> <li>Second: Look at <code>shared-schemas/examples/how-to-reference.yaml</code></li> <li>Still stuck? Contact Urbalurba API Team:</li> <li>\ud83d\udce7 Email: api-team@urbalurba.no</li> <li>\ud83d\udcac Teams: Urbalurba API Support</li> </ol>"},{"location":"api-first/csharp-developer/#what-youll-build","title":"\ud83c\udfaf What You'll Build","text":"<p>Following this template, you'll create: - \ud83d\udccb OpenAPI specification using Urbalurba standards - \ud83d\udda5\ufe0f ASP.NET Core server with generated controllers - \ud83d\udcf1 C# client library for consuming your API - \u2705 Automated validation to prevent mistakes - \ud83d\udcd6 Auto-generated documentation</p>"},{"location":"api-first/csharp-developer/#pro-tips","title":"\u2b50 Pro Tips","text":"<p>\ud83d\udca1 Start Simple: Begin with just 1-2 endpoints, then expand \ud83d\udca1 Use Examples: Copy patterns from <code>shared-schemas/examples/</code> \ud83d\udca1 Validate Often: Run <code>./tools/validate.ps1</code> frequently \ud83d\udca1 Ask Questions: The Urbalurba team is here to help!</p> <p>\ud83c\udf89 Happy coding! You're building APIs the Urbalurba way!</p>"},{"location":"api-first/urbalurba-schemas/","title":"Urbalurba Shared OpenAPI Schemas","text":"<p>This repository contains the official shared OpenAPI schema definitions for Urbalurba APIs.</p>"},{"location":"api-first/urbalurba-schemas/#important-for-consultants","title":"\ud83d\udea8 Important for Consultants","text":"<p>This repository is READ-ONLY for external consultants!</p> <ul> <li>\u2705 DO: Reference these schemas in your API specifications</li> <li>\u274c DON'T: Modify, copy, or create custom versions of these schemas</li> <li>\ud83c\udd98 Need a new field type? Contact the Urbalurba API team</li> </ul>"},{"location":"api-first/urbalurba-schemas/#repository-structure","title":"Repository Structure","text":"<pre><code>urbalurba-schemas/\n\u251c\u2500\u2500 fields/v1/              # Individual field type definitions\n\u2502   \u251c\u2500\u2500 branch-id.yaml      # Branch identifier\n\u2502   \u251c\u2500\u2500 email.yaml          # Email address\n\u2502   \u251c\u2500\u2500 geo-location.yaml   # Geographic coordinates\n\u2502   \u2514\u2500\u2500 ...                 # All standard field types\n\u251c\u2500\u2500 entities/v1/            # Complete entity schemas\n\u2502   \u251c\u2500\u2500 branch.yaml         # Branch entity with all properties\n\u2502   \u2514\u2500\u2500 ...                 # Other business entities\n\u251c\u2500\u2500 examples/               # Usage examples for consultants\n\u2502   \u2514\u2500\u2500 how-to-reference.yaml  # Shows correct reference patterns\n\u251c\u2500\u2500 README.md               # This file\n\u2514\u2500\u2500 CHANGELOG.md            # Version history\n</code></pre>"},{"location":"api-first/urbalurba-schemas/#how-to-use-these-schemas","title":"How to Use These Schemas","text":""},{"location":"api-first/urbalurba-schemas/#1-as-a-git-submodule-recommended","title":"1. As a Git Submodule (Recommended)","text":"<pre><code># Add as submodule to your project\ngit submodule add https://github.com/urbalurba/urbalurba-schemas.git shared-schemas\n\n# Update to latest version\ngit submodule update --remote\n</code></pre>"},{"location":"api-first/urbalurba-schemas/#2-reference-in-your-openapi-specs","title":"2. Reference in Your OpenAPI Specs","text":"<pre><code># In your API specification\ncomponents:\n  schemas:\n    MyResponse:\n      type: object\n      properties:\n        branchId:\n          $ref: '../shared-schemas/fields/v1/branch-id.yaml#/components/schemas/BranchId'\n        email:\n          $ref: '../shared-schemas/fields/v1/email.yaml#/components/schemas/Email'\n</code></pre>"},{"location":"api-first/urbalurba-schemas/#3-use-complete-entities","title":"3. Use Complete Entities","text":"<pre><code>paths:\n  /branches:\n    get:\n      responses:\n        '200':\n          content:\n            application/json:\n              schema:\n                type: array\n                items:\n                  $ref: '../shared-schemas/entities/v1/branch.yaml#/components/schemas/Branch'\n</code></pre>"},{"location":"api-first/urbalurba-schemas/#available-field-types","title":"Available Field Types","text":"Field File Description <code>BranchId</code> <code>fields/v1/branch-id.yaml</code> Unique branch identifier (L123 format) <code>BranchName</code> <code>fields/v1/branch-name.yaml</code> Official branch name <code>BranchNumber</code> <code>fields/v1/branch-number.yaml</code> Internal branch number <code>BranchType</code> <code>fields/v1/branch-type.yaml</code> Type of organization unit <code>Email</code> <code>fields/v1/email.yaml</code> Email address with validation <code>NorwegianPhoneNumber</code> <code>fields/v1/norwegian-phone.yaml</code> Norwegian phone with +47 format <code>NorwegianPostalCode</code> <code>fields/v1/postal-code.yaml</code> 4-digit Norwegian postal code <code>OrganizationNumber</code> <code>fields/v1/organization-number.yaml</code> Norwegian org number <code>County</code> <code>fields/v1/county.yaml</code> Norwegian county (fylke) <code>Municipality</code> <code>fields/v1/municipality.yaml</code> Norwegian municipality (kommune) <code>GeoLocation</code> <code>fields/v1/geo-location.yaml</code> GeoJSON coordinates <code>GlobalActivityId</code> <code>fields/v1/global-activity-id.yaml</code> UUID for standard activities <code>GlobalActivityName</code> <code>fields/v1/global-activity-name.yaml</code> Standard activity name <code>MemberStatus</code> <code>fields/v1/member-status.yaml</code> Boolean member status <code>VolunteerStatus</code> <code>fields/v1/volunteer-status.yaml</code> Boolean volunteer status <code>Web</code> <code>fields/v1/web.yaml</code> Website URL <code>PrivacyMaskedString</code> <code>fields/v1/privacy-masked-string.yaml</code> String that can be masked"},{"location":"api-first/urbalurba-schemas/#available-entities","title":"Available Entities","text":"Entity File Description <code>Branch</code> <code>entities/v1/branch.yaml</code> Complete branch information <code>CreateBranchRequest</code> <code>entities/v1/branch.yaml</code> Request for creating branch <code>UpdateBranchRequest</code> <code>entities/v1/branch.yaml</code> Request for updating branch"},{"location":"api-first/urbalurba-schemas/#validation","title":"Validation","text":"<p>All schemas include appropriate validation rules: - Patterns for format validation (email, phone, IDs) - Length limits for strings - Required fields clearly marked - Examples for all types</p>"},{"location":"api-first/urbalurba-schemas/#examples","title":"Examples","text":"<p>See <code>examples/how-to-reference.yaml</code> for complete examples showing: - \u2705 Correct reference patterns - \u274c What NOT to do - \ud83d\udd27 How to combine shared types with custom fields</p>"},{"location":"api-first/urbalurba-schemas/#version-policy","title":"Version Policy","text":"<ul> <li>v1 - Current stable version</li> <li>Breaking changes will create new versions (v2, v3, etc.)</li> <li>Patch updates (bug fixes) maintain backward compatibility</li> <li>See <code>CHANGELOG.md</code> for detailed version history</li> </ul>"},{"location":"api-first/urbalurba-schemas/#support","title":"Support","text":"<p>For questions about these schemas: - \ud83d\udce7 Email: api-team@urbalurba.no - \ud83d\udcd6 Documentation: Urbalurba API Guidelines - \ud83d\udc1b Issues: Contact the API team</p>"},{"location":"api-first/urbalurba-schemas/#contributing","title":"Contributing","text":"<p>External consultants: Contact the Urbalurba API team for schema changes or additions.</p> <p>Urbalurba team members: Follow the internal schema change process documented in the team wiki.</p>"},{"location":"api-first/urbalurba-schemas/CHANGELOG/","title":"Changelog","text":"<p>All notable changes to the Urbalurba shared schemas will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"api-first/urbalurba-schemas/CHANGELOG/#100-2025-01-09","title":"[1.0.0] - 2025-01-09","text":""},{"location":"api-first/urbalurba-schemas/CHANGELOG/#added","title":"Added","text":"<ul> <li>Initial release of Urbalurba shared OpenAPI schemas</li> <li>Field types (v1):</li> <li><code>BranchId</code> - Unique branch identifier with L### pattern</li> <li><code>BranchName</code> - Official branch name with Norwegian character support</li> <li><code>BranchNumber</code> - 7-digit internal branch number</li> <li><code>BranchType</code> - Enum for organization unit types</li> <li><code>Email</code> - RFC-compliant email address validation</li> <li><code>NorwegianPhoneNumber</code> - +47 format validation</li> <li><code>NorwegianPostalCode</code> - 4-digit postal code validation</li> <li><code>OrganizationNumber</code> - 9-digit Norwegian organization number</li> <li><code>OrganizationLevel</code> - Internal organization level code</li> <li><code>County</code> - Norwegian county (fylke) with character validation</li> <li><code>Municipality</code> - Norwegian municipality (kommune) with character validation</li> <li><code>GeoLocation</code> - GeoJSON Feature format for coordinates</li> <li><code>GlobalActivityId</code> - UUID for standard activities</li> <li><code>GlobalActivityName</code> - Standard activity name (max 50 chars)</li> <li><code>MemberStatus</code> - Boolean member status indicator</li> <li><code>VolunteerStatus</code> - Boolean volunteer status indicator</li> <li><code>Web</code> - Website URL with protocol validation</li> <li> <p><code>PrivacyMaskedString</code> - String with privacy masking support</p> </li> <li> <p>Entity schemas (v1):</p> </li> <li><code>Branch</code> - Complete branch entity with all properties</li> <li><code>CreateBranchRequest</code> - Request schema for branch creation</li> <li> <p><code>UpdateBranchRequest</code> - Request schema for branch updates</p> </li> <li> <p>Documentation:</p> </li> <li>Usage examples for consultants</li> <li>Complete field reference table</li> <li>Reference patterns and anti-patterns</li> <li>Integration guidelines</li> </ul>"},{"location":"api-first/urbalurba-schemas/CHANGELOG/#notes","title":"Notes","text":"<ul> <li>All schemas include validation patterns and examples</li> <li>Norwegian character support (\u00e6, \u00f8, \u00e5) included where appropriate</li> <li>GeoJSON compliance for geographic data</li> <li>Privacy-aware field definitions</li> </ul>"},{"location":"draft/doc-gravitee-apim/","title":"Gravitee APIM Configuration Documentation","text":"<p>This document provides a comprehensive technical overview of the Gravitee API Management (APIM) installation in the Kubernetes cluster.</p>"},{"location":"draft/doc-gravitee-apim/#1-installation-overview","title":"1. Installation Overview","text":"<p>Gravitee APIM is deployed using Helm in the default namespace with all core components running in dedicated pods. The installation is based on Gravitee APIM version 4.6.6.</p>"},{"location":"draft/doc-gravitee-apim/#helm-release-information","title":"Helm Release Information","text":"<pre><code>NAME         NAMESPACE REVISION UPDATED                                STATUS  CHART               APP VERSION\ngravitee-apim default    1      2025-03-19 18:05:21.740952492 +0100 CET deployed apim-4.6.6       4.6.6\n</code></pre>"},{"location":"draft/doc-gravitee-apim/#component-pods","title":"Component Pods","text":"<pre><code>gravitee-apim-api-6c9685b599-2fkk8       1/1     Running   1 (7h15m ago)   21h\ngravitee-apim-gateway-789bdb786b-g649f   1/1     Running   1 (7h15m ago)   21h\ngravitee-apim-portal-5dfb7fd6fb-d6wtl    1/1     Running   1 (7h15m ago)   21h\ngravitee-apim-ui-777b869bbf-8dhmf        1/1     Running   1 (7h15m ago)   21h\n</code></pre>"},{"location":"draft/doc-gravitee-apim/#other-helm-deployments-in-the-cluster","title":"Other Helm Deployments in the Cluster","text":"<p>The following Helm deployments exist in the same namespace as Gravitee APIM:</p> <pre><code># Output from: kubectl get helm -n default\nNAME         NAMESPACE REVISION UPDATED                                STATUS  CHART               APP VERSION\nelasticsearch default    1      2025-03-19 17:59:22.738818446 +0100 CET deployed elasticsearch-21.4.8 8.17.3     \nnginx        default    1      2025-03-19 17:52:45.576041048 +0100 CET deployed nginx-19.0.2       1.27.4     \npostgresql   default    1      2025-03-19 17:55:22.001477165 +0100 CET deployed postgresql-16.5.2  17.4.0     \nrabbitmq     default    1      2025-03-19 17:57:37.640287854 +0100 CET deployed rabbitmq-15.4.0    4.0.7      \nredis        default    1      2025-03-19 17:56:31.653719573 +0100 CET deployed redis-20.11.3      7.4.2      \n</code></pre> <p>Of these deployments, Gravitee APIM specifically requires: - MongoDB (for the main datastore) - Elasticsearch (for analytics)</p> <p>The others (PostgreSQL, RabbitMQ, Redis) may be used by other applications in the cluster but are not direct dependencies of Gravitee APIM.</p>"},{"location":"draft/doc-gravitee-apim/#2-component-architecture","title":"2. Component Architecture","text":""},{"location":"draft/doc-gravitee-apim/#pod-configuration","title":"Pod Configuration","text":"<p>Each component is deployed as a separate pod with specific labels and resource allocations. For example, the API component has the following configuration:</p> <pre><code># Output from: kubectl describe pod $(kubectl get pods -n default | grep gravitee-apim-api | head -1 | awk '{print $1}')\n# This configuration is defined in the Helm chart values and templates\n\nLabels:\n  app.kubernetes.io/component: api\n  app.kubernetes.io/instance: gravitee-apim\n  app.kubernetes.io/name: apim\n  app.kubernetes.io/version: 4.6.6\n  pod-template-hash: 6c9685b599\n\nResources:\n  Limits:\n    cpu: 1\n    memory: 1Gi\n  Requests:\n    cpu: 200m\n    memory: 512Mi\n</code></pre> <p>Probes are configured for pod healthiness: <pre><code># These probe configurations are defined in the Helm chart's deployment templates\n# File path in the chart: templates/api/deployment.yaml\n\nLiveness:   tcp-socket :http delay=30s timeout=1s period=30s #success=1 #failure=3\nReadiness:  tcp-socket :http delay=30s timeout=1s period=30s #success=1 #failure=3\nStartup:    tcp-socket :http delay=0s timeout=1s period=10s #success=1 #failure=30\n</code></pre></p>"},{"location":"draft/doc-gravitee-apim/#configmaps","title":"ConfigMaps","text":"<p>The Gravitee components use ConfigMaps for configuration:</p> <pre><code># Output from: kubectl get configmaps -n default | grep gravitee\ngravitee-apim-api       1      21h\ngravitee-apim-gateway   1      21h\ngravitee-apim-portal    5      21h\ngravitee-apim-ui        4      21h\n</code></pre> <p>Each component mounts its respective ConfigMap for configuration:</p> <pre><code># From the pod description: kubectl describe pod gravitee-apim-api-6c9685b599-2fkk8\n# These volume mounts are defined in the Helm chart's deployment templates\n# File path in the chart: templates/api/deployment.yaml\n\nVolumes:\n  config:\n    Type:      ConfigMap (a volume populated by a ConfigMap)\n    Name:      gravitee-apim-api\n    Optional:  false\n</code></pre> <p>The contents of these ConfigMaps can be viewed with: <pre><code>kubectl get configmap gravitee-apim-api -n default -o yaml\nkubectl get configmap gravitee-apim-gateway -n default -o yaml\n</code></pre></p>"},{"location":"draft/doc-gravitee-apim/#service-configuration","title":"Service Configuration","text":"<p>The services are configured as ClusterIP services for internal communication:</p> <pre><code># Output from: kubectl get svc -n default | grep gravitee\ngravitee-apim-api         ClusterIP   10.43.177.164   &lt;none&gt;        83/TCP\ngravitee-apim-gateway     ClusterIP   10.43.138.3     &lt;none&gt;        82/TCP\ngravitee-apim-portal      ClusterIP   10.43.230.113   &lt;none&gt;        8003/TCP\ngravitee-apim-ui          ClusterIP   10.43.2.156     &lt;none&gt;        8002/TCP\n</code></pre> <p>These service definitions are created by the Helm chart and can be found in the chart's template files: - templates/api/service.yaml - templates/gateway/service.yaml - templates/portal/service.yaml - templates/ui/service.yaml</p>"},{"location":"draft/doc-gravitee-apim/#3-networking-and-ingress-configuration","title":"3. Networking and Ingress Configuration","text":""},{"location":"draft/doc-gravitee-apim/#ingress-resources","title":"Ingress Resources","text":"<p>The installation uses 5 separate ingress resources to handle different paths on the same host (<code>apim.example.com</code>):</p> <ol> <li> <p>Management API (gravitee-apim-api-management):    <pre><code># Output from: kubectl describe ingress -n default | grep -A20 gravitee\n# These ingress resources are defined in the Helm chart's template files\n# File path in the chart: templates/api/ingress-management.yaml\n\nHost: apim.example.com\nPath: /management\nBackend: gravitee-apim-api:83 (10.42.0.56:8083)\nAnnotations: kubernetes.io/ingress.class: nginx\n</code></pre></p> </li> <li> <p>Portal API (gravitee-apim-api-portal):    <pre><code>Host: apim.example.com\nPath: /portal\nBackend: gravitee-apim-api:83 (10.42.0.56:8083)\nAnnotations: kubernetes.io/ingress.class: nginx\n</code></pre></p> </li> <li> <p>Gateway (gravitee-apim-gateway):    <pre><code>Host: apim.example.com\nPath: /\nBackend: gravitee-apim-gateway:82 (10.42.0.54:8082)\nAnnotations: kubernetes.io/ingress.class: nginx\n</code></pre></p> </li> <li> <p>Portal UI (gravitee-apim-portal):    <pre><code>Host: apim.example.com\nPath: /\nBackend: gravitee-apim-portal:8003 (10.42.0.60:8080)\nAnnotations: \n  kubernetes.io/ingress.class: nginx\n  nginx.ingress.kubernetes.io/rewrite-target: /\n</code></pre></p> </li> <li> <p>Management UI (gravitee-apim-ui):    <pre><code>Host: apim.example.com\nPath: /console(/.*)?\nBackend: gravitee-apim-ui:8002 (10.42.0.58:8080)\nAnnotations: \n  kubernetes.io/ingress.class: nginx\n  nginx.ingress.kubernetes.io/rewrite-target: /$1\n</code></pre></p> </li> </ol> <p>Note: The path configurations create potential routing conflicts, especially with multiple root paths (\"/\"). The nginx ingress controller likely uses the first match or most specific match for routing.</p>"},{"location":"draft/doc-gravitee-apim/#4-environment-configuration","title":"4. Environment Configuration","text":"<p>The API component has the following environment variables configured:</p> <pre><code># Output from: kubectl exec -it $(kubectl get pods -n default | grep gravitee-apim-api | head -1 | awk '{print $1}') -- env | sort\n\n# Portal configuration - defined in the Helm values\nportal.entrypoint=https://apim.example.com/\n\n# Automatically injected Kubernetes service discovery env vars for all components\nGRAVITEE_APIM_API_SERVICE_HOST=10.43.177.164\nGRAVITEE_APIM_API_SERVICE_PORT=83\nGRAVITEE_APIM_GATEWAY_SERVICE_HOST=10.43.138.3\nGRAVITEE_APIM_GATEWAY_SERVICE_PORT=82\nGRAVITEE_APIM_PORTAL_SERVICE_HOST=10.43.230.113\nGRAVITEE_APIM_PORTAL_SERVICE_PORT=8003\nGRAVITEE_APIM_UI_SERVICE_HOST=10.43.2.156\nGRAVITEE_APIM_UI_SERVICE_PORT=8002\n\n# Database service discovery - automatically injected by Kubernetes\nELASTICSEARCH_SERVICE_HOST=10.43.70.158\nELASTICSEARCH_SERVICE_PORT=9200\nMONGODB_SERVICE_HOST=10.43.147.6\nMONGODB_SERVICE_PORT=27017\nPOSTGRESQL_SERVICE_HOST=10.43.51.252\nPOSTGRESQL_SERVICE_PORT=5432\nRABBITMQ_SERVICE_HOST=10.43.128.62\nRABBITMQ_SERVICE_PORT=5672\nREDIS_MASTER_SERVICE_HOST=10.43.79.100\nREDIS_MASTER_SERVICE_PORT=6379\n</code></pre> <p>Custom environment variables can be defined in the Helm values file under the <code>api.env</code> section.</p>"},{"location":"draft/doc-gravitee-apim/#5-authentication-configuration","title":"5. Authentication Configuration","text":"<p>The installation includes the following user accounts (as defined in the Helm values):</p> <ul> <li>Admin User:</li> <li>Username: admin</li> <li>Password: adminadmin</li> <li>Email: admin@example.com</li> <li> <p>Roles: MANAGEMENT:ADMIN, PORTAL:ADMIN</p> </li> <li> <p>API User:</p> </li> <li>Username: api_user</li> <li>Password: api_user</li> <li>Email: api_user@example.com</li> <li>First Name: API</li> <li>Last Name: User</li> <li>Roles: MANAGEMENT:USER, PORTAL:USER</li> </ul>"},{"location":"draft/doc-gravitee-apim/#6-database-configuration","title":"6. Database Configuration","text":""},{"location":"draft/doc-gravitee-apim/#mongodb-settings","title":"MongoDB Settings","text":"<p>Gravitee is configured to use MongoDB for its primary data store:</p> <pre><code># From the Helm values: helm get values gravitee-apim -n default\n# These settings can be configured in the values.yaml file when installing or upgrading the chart\n\nmongo:\n  auth:\n    enabled: true\n    password: gravitee\n    username: gravitee\n  dbhost: mongodb.default.svc.cluster.local\n  dbname: graviteedb\n  dbport: 27017\n  rsEnabled: false\n</code></pre>"},{"location":"draft/doc-gravitee-apim/#elasticsearch-configuration","title":"Elasticsearch Configuration","text":"<p>Elasticsearch is configured for analytics storage:</p> <pre><code># From the Helm values: helm get values gravitee-apim -n default\n# These settings can be configured in the values.yaml file when installing or upgrading the chart\n\nes:\n  enabled: false  # Not managed by the Gravitee Helm chart\n  endpoints:\n  - http://elasticsearch.default.svc.cluster.local:9200\n  security:\n    enabled: false\n    password: Secretp@ssword1\n    username: elastic\n</code></pre>"},{"location":"draft/doc-gravitee-apim/#7-resource-allocations","title":"7. Resource Allocations","text":"<p>Resources are allocated differently based on component type:</p> <p>API and Gateway Components: <pre><code>resources:\n  limits:\n    cpu: 1000m\n    memory: 1Gi\n  requests:\n    cpu: 200m\n    memory: 512Mi\n</code></pre></p> <p>UI Components (Management UI and Portal): <pre><code>resources:\n  limits:\n    cpu: 300m\n    memory: 256Mi\n  requests:\n    cpu: 100m\n    memory: 128Mi\n</code></pre></p>"},{"location":"draft/doc-gravitee-apim/#8-security-configuration","title":"8. Security Configuration","text":""},{"location":"draft/doc-gravitee-apim/#encryption-settings","title":"Encryption Settings","text":"<p>Encryption secrets are defined for sensitive data:</p> <pre><code>properties:\n  encryption:\n    secret: urbalurba-encryption-1234\n</code></pre>"},{"location":"draft/doc-gravitee-apim/#kubernetes-security-context","title":"Kubernetes Security Context","text":"<p>The pods run with default security contexts and service accounts:</p> <pre><code>Service Account:  gravitee-apim-apim\n</code></pre> <p>No Persistent Volume Claims are associated with the Gravitee deployment, suggesting ephemeral storage for all components.</p>"},{"location":"draft/doc-gravitee-apim/#9-accessing-the-platform","title":"9. Accessing the Platform","text":""},{"location":"draft/doc-gravitee-apim/#external-access","title":"External Access","text":"<p>The system is configured with ingress rules for external access through the host <code>apim.example.com</code>. Access URLs:</p> <ul> <li>Management UI: https://apim.example.com/console</li> <li>Portal UI: https://apim.example.com/</li> <li>Management API: https://apim.example.com/management</li> <li>Gateway API: https://apim.example.com/</li> <li>Portal API: https://apim.example.com/portal</li> </ul> <p>Note: There are potential routing conflicts with multiple components using the root path (\"/\").</p>"},{"location":"draft/doc-gravitee-apim/#port-forwarding-developmenttesting","title":"Port Forwarding (Development/Testing)","text":"<p>For local access, use the following port-forwarding commands:</p> <pre><code># Management UI\nkubectl port-forward svc/gravitee-apim-ui 8002:8002 -n default\n\n# Developer Portal\nkubectl port-forward svc/gravitee-apim-portal 8003:8003 -n default\n\n# API Gateway\nkubectl port-forward svc/gravitee-apim-gateway 82:82 -n default\n\n# Management API\nkubectl port-forward svc/gravitee-apim-api 83:83 -n default\n</code></pre>"},{"location":"draft/doc-gravitee-apim/#10-maintenance-operations","title":"10. Maintenance Operations","text":""},{"location":"draft/doc-gravitee-apim/#checking-component-status","title":"Checking Component Status","text":"<pre><code># Get all Gravitee pods\nkubectl get pods -n default | grep gravitee-apim\n\n# Check pod details\nkubectl describe pod &lt;pod-name&gt; -n default\n\n# Check pod logs\nkubectl logs &lt;pod-name&gt; -n default\n\n# Check ConfigMap content\nkubectl describe configmap gravitee-apim-api -n default\n</code></pre>"},{"location":"draft/doc-gravitee-apim/#scaling-components","title":"Scaling Components","text":"<pre><code># Scale the API component\nkubectl scale deployment gravitee-apim-api --replicas=2 -n default\n\n# Scale the Gateway component\nkubectl scale deployment gravitee-apim-gateway --replicas=2 -n default\n</code></pre>"},{"location":"draft/doc-gravitee-apim/#upgrading-gravitee","title":"Upgrading Gravitee","text":"<pre><code># Update Helm repositories\nhelm repo update\n\n# Upgrade the Gravitee installation\nhelm upgrade gravitee-apim graviteeio/apim -n default --reuse-values\n</code></pre>"},{"location":"draft/doc-gravitee-apim/#configuration-changes","title":"Configuration Changes","text":"<p>To modify the Gravitee configuration:</p> <ol> <li> <p>Get the current values:    <pre><code>helm get values gravitee-apim -n default -o yaml &gt; values.yaml\n</code></pre></p> </li> <li> <p>Edit the values.yaml file</p> </li> <li> <p>Apply the changes:    <pre><code>helm upgrade gravitee-apim graviteeio/apim -n default -f values.yaml\n</code></pre></p> </li> </ol>"},{"location":"draft/doc-gravitee-apim/#11-troubleshooting","title":"11. Troubleshooting","text":""},{"location":"draft/doc-gravitee-apim/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Ingress Routing Conflicts: With multiple components sharing the same host and overlapping paths, routing issues may occur. Check the nginx ingress controller logs.</p> </li> <li> <p>Database Connectivity: Ensure MongoDB and Elasticsearch are running and accessible from the Gravitee pods.</p> </li> <li> <p>Resource Constraints: Monitor CPU and memory usage to ensure pods aren't being throttled.</p> </li> </ol>"},{"location":"draft/doc-gravitee-apim/#checking-logs","title":"Checking Logs","text":"<pre><code># API logs\nkubectl logs deployments/gravitee-apim-api -n default\n\n# Gateway logs\nkubectl logs deployments/gravitee-apim-gateway -n default\n\n# Check events for issues\nkubectl get events -n default | grep gravitee\n</code></pre>"},{"location":"draft/doc-gravitee-apim/#component-health-checks","title":"Component Health Checks","text":"<p>For direct health checks (requires access to the pods):</p> <pre><code># Check API health\nkubectl exec -it &lt;api-pod-name&gt; -n default -- curl -s http://localhost:8083/_node/health\n\n# Check Gateway health\nkubectl exec -it &lt;gateway-pod-name&gt; -n default -- curl -s http://localhost:8082/_node/health\n</code></pre>"},{"location":"presentation/azure/deployment-benefits/","title":"Deployment Benefits","text":""},{"location":"presentation/azure/deployment-benefits/#key-benefits","title":"Key Benefits","text":""},{"location":"presentation/azure/deployment-benefits/#speed-efficiency","title":"\ud83d\ude80 Speed &amp; Efficiency","text":"<ul> <li>Instant Developer Onboarding: Working \"Hello World\" setup ready immediately</li> <li>Automated Deployments: One-click deployment to Dev/Test/Prod via CI/CD</li> <li>Infrastructure as Code: No manual server configuration needed</li> <li>Self-Service Development: Developers can deploy without waiting for ops</li> </ul>"},{"location":"presentation/azure/deployment-benefits/#standardization","title":"\ud83d\udccb Standardization","text":"<ul> <li>Consistent Process: Every integration follows the same 8-step workflow</li> <li>Uniform Naming: Standardized repository and API naming conventions</li> <li>Template-Based: New integrations use proven, tested patterns</li> <li>Predictable Structure: Same services and setup across all projects</li> </ul>"},{"location":"presentation/azure/deployment-benefits/#developer-experience","title":"\ud83d\udd27 Developer Experience","text":"<ul> <li>DevContainer Ready: Consistent development environment out of the box</li> <li>Clear Guidelines: Branching strategy and coding standards provided</li> <li>Quick Iteration: Fast dev/test cycles for rapid feature delivery</li> <li>Focus on Code: Developers concentrate on business logic, not infrastructure</li> </ul>"},{"location":"presentation/azure/deployment-benefits/#quality-reliability","title":"\ud83d\udee1\ufe0f Quality &amp; Reliability","text":"<ul> <li>Automated Testing: CI pipeline ensures code quality before deployment</li> <li>Gradual Rollout: Dev \u2192 Test \u2192 Prod progression with verification</li> <li>Environment Parity: Identical setups prevent \"works on my machine\" issues</li> <li>Easy Rollbacks: Semantic versioning enables quick problem resolution</li> </ul>"},{"location":"presentation/azure/deployment-benefits/#cost-resource-benefits","title":"\ud83d\udcb0 Cost &amp; Resource Benefits","text":"<ul> <li>Shared Services: Reuse databases, service bus, logging across integrations</li> <li>Reduced Ops Overhead: Less manual intervention and maintenance</li> <li>Scalable Infrastructure: Auto-scaling based on demand</li> <li>Faster Time-to-Market: Quicker delivery of new integrations</li> </ul>"},{"location":"presentation/azure/deployment-benefits/#business-value","title":"\ud83c\udfaf Business Value","text":"<ul> <li>API-First Approach: All integrations accessible through consistent gateway</li> <li>Microservices Ready: Independent, deployable services</li> <li>Platform Thinking: Foundation for future integrations</li> <li>Risk Reduction: Controlled, auditable deployment process</li> </ul> <p>Bottom Line: Enterprise-grade deployment automation with developer-friendly simplicity, enabling rapid, reliable delivery of new integrations.</p>"},{"location":"presentation/azure/deployment/","title":"Azure Integration Deployment Process","text":""},{"location":"presentation/azure/deployment/#actors","title":"Actors","text":"<ul> <li>Internal Owner: Business decision maker</li> <li>Developer: Technical implementer</li> <li>API Team: Infrastructure and platform management</li> </ul>"},{"location":"presentation/azure/deployment/#process-flow","title":"Process Flow","text":"<pre><code>graph TD\n    A[Internal Owner: Decision to Create Integration] --&gt; B[Developer: Technical Planning]\n    B --&gt; C[API Team: Integration Registration]\n    C --&gt; D[API Team: Repository and Infrastructure Setup]\n    D --&gt; E[API Team: Developer Onboarding]\n    E --&gt; F[Developer: Development Deployment]\n    F --&gt; G[Developer: Test Deployment - Merge to Main]\n    G --&gt; H[Developer: Production Deployment]\n\n    A1[Define roles: Business Owner and IT Owner&lt;br/&gt;Create executive description&lt;br/&gt;Approve integration creation] -.-&gt; A\n    B1[Create technical specification&lt;br/&gt;Define required Azure services&lt;br/&gt;Choose implementation approach] -.-&gt; B\n    C1[Assign unique integration ID&lt;br/&gt;Assign unique API path&lt;br/&gt;Define integration name&lt;br/&gt;Generate repository name] -.-&gt; C\n    D1[Create repository with IaC&lt;br/&gt;Generate CI/CD pipelines&lt;br/&gt;Configure programming language&lt;br/&gt;Set up 3 environments] -.-&gt; D\n    E1[Send welcome email&lt;br/&gt;Provide working setup&lt;br/&gt;Share development guidelines] -.-&gt; E\n    F1[Clone repository&lt;br/&gt;Start development in devcontainer&lt;br/&gt;Create feature branch&lt;br/&gt;Deploy to Dev environment] -.-&gt; F\n    G1[Merge feature branch to main&lt;br/&gt;Pipelines deploy to Test&lt;br/&gt;Verify functionality] -.-&gt; G\n    H1[Tag main commit with version&lt;br/&gt;Pipelines deploy to Production&lt;br/&gt;Verify functionality] -.-&gt; H\n\n    classDef owner stroke:#01579b,stroke-width:3px\n    classDef developer stroke:#4a148c,stroke-width:3px\n    classDef apiteam stroke:#1b5e20,stroke-width:3px\n\n    class A,A1 owner\n    class B,B1,F,F1,G,G1,H,H1 developer\n    class C,C1,D,D1,E,E1 apiteam\n</code></pre>"},{"location":"presentation/azure/deployment/#1-internal-owner-decision-to-create-integration","title":"1. Internal Owner: Decision to Create Integration","text":"<ul> <li>Define roles: Business Owner and IT Owner</li> <li>Create executive description of integration purpose and scope</li> <li>Approve integration creation</li> </ul>"},{"location":"presentation/azure/deployment/#2-developer-technical-planning","title":"2. Developer: Technical Planning","text":"<ul> <li>Create technical specification (data requirements, performance needs)</li> <li>Define required Azure services (database, service bus, logging, storage, key vault)</li> <li>Choose implementation approach (Azure Functions: C#/TypeScript/Python, App Service, Container Apps)</li> </ul>"},{"location":"presentation/azure/deployment/#3-api-team-integration-registration","title":"3. API Team: Integration Registration","text":"<ul> <li>Assign unique integration ID (e.g., INT0001007)</li> <li>Assign unique API path (e.g., /organizations)</li> <li>Define integration name (e.g., Local-Unions)</li> <li>Generate repository name: <code>&lt;integration-id&gt;-&lt;integration-name&gt;</code> (e.g., INT0001007-Local-Unions)</li> </ul>"},{"location":"presentation/azure/deployment/#4-api-team-repository-and-infrastructure-setup","title":"4. API Team: Repository and Infrastructure Setup","text":"<ul> <li>Create repository with infrastructure as code</li> <li>Automatically generate CI/CD pipelines:</li> <li>CI (Continuous Integration)</li> <li>CD (Continuous Deployment)</li> <li>APIM registration for API developer portal</li> <li>Configure programming language for Azure Functions</li> <li>Set up for 3 environments: Dev, Test, Production</li> <li>Prepare services requiring manual configuration (database, service bus, etc.)</li> </ul>"},{"location":"presentation/azure/deployment/#5-api-team-developer-onboarding","title":"5. API Team: Developer Onboarding","text":"<ul> <li>Send welcome email to developer</li> <li>Provide working setup with \"Hello World\" skeleton</li> <li>Share development guidelines (branching strategy, coding standards)</li> </ul>"},{"location":"presentation/azure/deployment/#6-developer-development-deployment","title":"6. Developer: Development Deployment","text":"<ul> <li>Clone repository</li> <li>Start development in devcontainer (developer toolbox)</li> <li>Create feature branch according to branching rules</li> <li>Modify the test path from printing \"Hello World\" to \"Hello My World\"</li> <li>Deploy infrastructure to Dev environment</li> <li>Deploy to APIM</li> <li>Verify functionality at: <code>https://api-dev.urbalurba.no/&lt;API-path&gt;/helloworld</code></li> <li>Example: <code>https://api-dev.urbalurba.no/organizations/helloworld</code></li> <li>Continue development iterations</li> </ul>"},{"location":"presentation/azure/deployment/#7-developer-test-deployment-merge-to-main","title":"7. Developer: Test Deployment (Merge to Main)","text":"<ul> <li>Merge feature branch to main in DevOps portal</li> <li>Pipelines automatically deploy to Test environment</li> <li>Verify functionality at: <code>https://api-test.urbalurba.no/&lt;API-path&gt;/helloworld</code></li> </ul>"},{"location":"presentation/azure/deployment/#8-developer-production-deployment","title":"8. Developer: Production Deployment","text":"<ul> <li>Tag main commit with semantic version (e.g., 1.0.2)</li> <li>Pipelines automatically deploy to Production</li> <li>Verify functionality at: <code>https://api.urbalurba.no/&lt;API-path&gt;/helloworld</code></li> </ul>"},{"location":"presentation/azure/deployment/#key-principles","title":"Key Principles","text":"<ul> <li>Infrastructure as Code for all deployments</li> <li>Automated CI/CD pipelines across all environments</li> <li>Manual configuration only for services requiring it</li> <li>Semantic versioning for production releases</li> </ul>"},{"location":"presentation/azure/developer-howto/","title":"Developer HOWTO","text":""},{"location":"presentation/azure/developer-howto/#slide-developer-guidelines","title":"Slide: Developer Guidelines","text":"<p>Core Requirements: - Shared Resources - Use centralized services, not separate instances - Private Networking - VNet integration only, no public IPs - Security First - Managed Identities, Key Vault, RBAC - Governance - Tag resources, follow naming, use Bicep</p> <p>Development Process: - Trunk-Based Git - Short feature branches, continuous integration - 3 Environments - Dev (manual), Test (auto), Prod (tag-based) - APIM - Separate pipeline, manual deployment</p> <p>Logging Standards: - Structured JSON - Machine-readable format required - System ID - Unique identifier per integration (INT0001001) - Correlation IDs - Enable cross-service tracing</p>"},{"location":"presentation/azure/developer-howto/#speaker-notes","title":"Speaker Notes","text":"<p>Opening (30 seconds) \"Our platform has clear standards that ensure every integration is secure and maintainable. These aren't restrictions - they're what make us reliable.\"</p> <p>Key Points (1 minute) \"Every developer must: - Use shared services instead of creating separate instances - Follow security standards with private networking and managed identities - Use structured logging with system IDs for easy troubleshooting - Follow our trunk-based development process with clear deployment rules\"</p> <p>Closing (30 seconds) \"When everyone follows the same patterns, we move faster and solve problems quicker. These standards are our competitive advantage.\"</p>"},{"location":"presentation/azure/developer-portal/","title":"Developer Portal","text":""},{"location":"presentation/azure/developer-portal/#slide-developer-portal","title":"Slide: Developer Portal","text":"<p>What is it? A self-service platform where developers can discover, explore, and consume APIs published by your organization.</p> <p>How developers get access to APIs: - Browse &amp; Discover - Search and explore available APIs - Self-Register - Sign up for API access independently - Get API Keys - Receive authentication credentials instantly - Test APIs - Try APIs directly in the portal before integration - Download SDKs - Access client libraries and code samples</p>"},{"location":"presentation/azure/developer-portal/#speaker-notes","title":"Speaker Notes","text":"<p>Opening (30 seconds) \"The Developer Portal is like a marketplace for our APIs. Instead of developers having to email us or go through lengthy approval processes, they can discover what's available and get access on their own.\"</p> <p>Key Points (1 minute) \"Here's how it works: - Developers visit the portal and browse our available APIs - They can see documentation, examples, and even test APIs right there - When they find what they need, they can self-register and get API keys immediately - No more waiting for IT approval or lengthy onboarding processes - They get everything they need: documentation, SDKs, and working examples\"</p> <p>Closing (30 seconds) \"This dramatically reduces the friction for developers who want to use our APIs, while giving us better visibility into who's using what and how.\"</p>"},{"location":"presentation/azure/developer-toolbox/","title":"Developer Toolbox","text":""},{"location":"presentation/azure/developer-toolbox/#slide-developer-portal","title":"Slide: Developer Portal","text":"<p>What is it? A centralized platform providing developers with tools, documentation, and resources for efficient development.</p> <p>Why do we have it? - Single Point of Access - All development resources in one place - Self-Service - Developers manage their own environments - Consistency - Standardized tools and practices across teams - Version Control - Same libraries for everyone, no version conflicts - Efficiency - Faster onboarding and reduced support overhead</p>"},{"location":"presentation/azure/developer-toolbox/#speaker-notes","title":"Speaker Notes","text":"<p>Opening (30 seconds) \"The Developer Portal is our central hub for all development activities. Think of it as a one-stop shop where developers can find everything they need to build, test, and deploy applications.\"</p> <p>Key Points (1 minute) \"Instead of developers hunting through different systems and documentation scattered across the organization, they have one place to go. This means: - New team members can get up and running faster - Everyone uses the same tools and follows the same processes - No more 'works on my machine' problems - libraries are checked into the repo, everyone uses identical versions - Less time spent on setup, more time on actual development - Reduced support tickets because developers can help themselves\"</p> <p>Closing (30 seconds) \"This isn't just about convenience - it's about creating a more efficient, consistent development environment that helps us deliver better software faster.\"</p>"},{"location":"presentation/azure/dns-names/","title":"DNS Names","text":"<p>The API system has the following dns names:</p> <p>For production</p> <ul> <li>https://api.urbalurba.no - API endpoint URL for production</li> <li>https://developer.urbalurba.no - Developer API portal</li> </ul> <p>For test</p> <ul> <li>https://api-test.urbalurba.no - API endpoint URL for test</li> <li>https://developer-test.urbalurba.no - Developer API portal for test</li> </ul> <p>For Development</p> <ul> <li>https://api-dev.urbalurba.no - API endpoint URL for development</li> <li>https://developer-dev.urbalurba.no - Developer API portal for development</li> </ul>"},{"location":"presentation/azure/integration-platform/","title":"Integration Platform","text":"<pre><code>graph TB\n    CLOUD[\"\u2601\ufe0f Internet\"]\n\n    FIREWALL[\"\ud83d\udee1\ufe0f Azure Firewall\"]\n\n    classDef prodSecure stroke:#d63031,stroke-width:5px\n\n    subgraph \"Prod Shared Services\"\n        subgraph \"dev-sharedservices\"\n            FRONTDOORDEV[\"\ud83d\udeaa Azure Front Door&lt;br/&gt;(Dev)\"]\n            APIMDEV[\"\ud83d\udd27 APIM-Dev\"]\n            PORTALDEV[\"\ud83d\udc68\u200d\ud83d\udcbb Developer Portal&lt;br/&gt;(Dev)\"]\n            SERVICEBUSDEV[\"\ud83d\ude8c Service Bus&lt;br/&gt;(Dev)\"]\n        end\n\n        subgraph \"test-sharedservices\"\n            FRONTDOORTEST[\"\ud83d\udeaa Azure Front Door&lt;br/&gt;(Test)\"]\n            APIMTEST[\"\ud83d\udd27 APIM-Test\"]\n            PORTALTEST[\"\ud83d\udc68\u200d\ud83d\udcbb Developer Portal&lt;br/&gt;(Test)\"]\n            SERVICEBUSTEST[\"\ud83d\ude8c Service Bus&lt;br/&gt;(Test)\"]\n        end\n\n        subgraph \"prod-sharedservices\"\n            FRONTDOORPROD[\"\ud83d\udeaa Azure Front Door&lt;br/&gt;(Prod)\"]\n            APIMPROD[\"\ud83d\udd27 APIM-Prod\"]\n            PORTALPROD[\"\ud83d\udc68\u200d\ud83d\udcbb Developer Portal&lt;br/&gt;(Prod)\"]\n            SERVICEBUSPROD[\"\ud83d\ude8c Service Bus&lt;br/&gt;(Prod)\"]\n            REGISTRYPROD[\"\ud83d\udce6 Container Registry&lt;br/&gt;(Prod)\"]\n            CERTRENEWPROD[\"\ud83d\udd10 Certificate Renew&lt;br/&gt;(Prod)\"]\n        end\n    end\n\n    %% Connections\n    CLOUD --&gt; FIREWALL\n    FIREWALL --&gt; FRONTDOORDEV\n    FIREWALL --&gt; FRONTDOORTEST\n    FIREWALL --&gt; FRONTDOORPROD\n\n    FRONTDOORDEV --&gt; APIMDEV\n    FRONTDOORTEST --&gt; APIMTEST\n    FRONTDOORPROD --&gt; APIMPROD\n\n    APIMDEV --&gt; PORTALDEV\n    APIMTEST --&gt; PORTALTEST\n    APIMPROD --&gt; PORTALPROD\n\n    class SERVICEBUSPROD,APIMPROD prodSecure\n</code></pre>"},{"location":"presentation/azure/integration-platform/#alternative-flow-diagram-version","title":"Alternative Flow Diagram Version","text":"<pre><code>flowchart LR\n    CLOUD[\"\u2601\ufe0f Internet\"]\n    FIREWALL[\"\ud83d\udee1\ufe0f Azure Firewall\"]\n\n    classDef prodSecure stroke:#d63031,stroke-width:5px\n\n    subgraph PSS[\"\ud83c\udfe2 Prod Shared Services\"]\n        direction TB\n        subgraph DEV[\"dev-sharedservices\"]\n            direction TB\n            FRONTDOORDEV[\"\ud83d\udeaa Azure Front Door&lt;br/&gt;(Dev)\"]\n            APIMDEV[\"\ud83d\udd27 APIM-Dev\"]\n            PORTALDEV[\"\ud83d\udc68\u200d\ud83d\udcbb Developer Portal&lt;br/&gt;(Dev)\"]\n            SERVICEBUSDEV[\"\ud83d\ude8c Service Bus&lt;br/&gt;(Dev)\"]\n        end\n\n        subgraph TEST[\"test-sharedservices\"]\n            direction TB\n            FRONTDOORTEST[\"\ud83d\udeaa Azure Front Door&lt;br/&gt;(Test)\"]\n            APIMTEST[\"\ud83d\udd27 APIM-Test\"]\n            PORTALTEST[\"\ud83d\udc68\u200d\ud83d\udcbb Developer Portal&lt;br/&gt;(Test)\"]\n            SERVICEBUSTEST[\"\ud83d\ude8c Service Bus&lt;br/&gt;(Test)\"]\n        end\n\n        subgraph PROD[\"prod-sharedservices\"]\n            direction TB\n            FRONTDOORPROD[\"\ud83d\udeaa Azure Front Door&lt;br/&gt;(Prod)\"]\n            APIMPROD[\"\ud83d\udd27 APIM-Prod\"]\n            PORTALPROD[\"\ud83d\udc68\u200d\ud83d\udcbb Developer Portal&lt;br/&gt;(Prod)\"]\n            SERVICEBUSPROD[\"\ud83d\ude8c Service Bus&lt;br/&gt;(Prod)\"]\n            REGISTRYPROD[\"\ud83d\udce6 Container Registry&lt;br/&gt;(Prod)\"]\n            CERTRENEWPROD[\"\ud83d\udd10 Certificate Renew&lt;br/&gt;(Prod)\"]\n        end\n    end\n\n    CLOUD --&gt; FIREWALL\n    FIREWALL --&gt; FRONTDOORDEV\n    FIREWALL --&gt; FRONTDOORTEST\n    FIREWALL --&gt; FRONTDOORPROD\n\n    FRONTDOORDEV --&gt; APIMDEV\n    FRONTDOORTEST --&gt; APIMTEST\n    FRONTDOORPROD --&gt; APIMPROD\n\n    APIMDEV --&gt; PORTALDEV\n    APIMTEST --&gt; PORTALTEST\n    APIMPROD --&gt; PORTALPROD\n\n    class SERVICEBUSPROD,APIMPROD prodSecure\n</code></pre>"},{"location":"presentation/azure/landing-zone/","title":"Landing Zone Architecture","text":"<p>This is the production Landing Zone</p> <pre><code>graph TB\n    subgraph \"Internet\"\n        CLOUD[\"\u2601\ufe0f Internet\"]\n    end\n\n    subgraph \"Azure Firewall\"\n        FIREWALL[\"\ud83d\udee1\ufe0f Azure Firewall\"]\n    end\n\n    SERVICENOW[\"\ud83c\udfab ServiceNow Incident\"]\n\n    subgraph \"Shared Landing Zone\"\n        FRONTDOOR[\"\ud83d\udeaa Azure Front Door\"]\n        SERVICEBUS[\"\ud83d\ude8c Service Bus\"]\n        REGISTRY[\"\ud83d\udce6 Container Registry\"]\n        DANIELLOG[\"\ud83d\udcca Log Alert Processor\"]\n        CERTRENEW[\"\ud83d\udd10 Certificate Renew\"]\n\n        subgraph \"API Management\"\n            APIM[\"\ud83d\udd27 APIM&lt;br/&gt;(API Management)\"]\n            PORTAL[\"\ud83d\udc68\u200d\ud83d\udcbb Developer Portal\"]\n        end\n    end\n\n    subgraph \"Application Landing Zone\"\n        subgraph \"API1 Resource Group\"\n            API1[\"\ud83d\udce1 API 1\"]\n            INSIGHTS1[\"\ud83d\udcca Application Insights\"]\n            VAULT1[\"\ud83d\udd10 Key Vault\"]\n            STORAGE1[\"\ud83d\udcbe Storage Account\"]\n        end\n\n        subgraph \"API2 Resource Group\"\n            API2[\"\ud83d\udce1 API 2&lt;br/&gt;\ud83d\udcca Application Insights&lt;br/&gt;\ud83d\udd10 Key Vault&lt;br/&gt;\ud83d\udcbe Storage Account\"]\n        end\n\n        subgraph \"Common Services\"\n            POSTGRES[\"\ud83d\uddc4\ufe0f Azure PostgreSQL\"]\n            COSMOS[\"\ud83c\udf0c Cosmos DB\"]\n            PLAN[\"\ud83d\udccb App Service Plan&lt;br/&gt;(Function Apps)\"]\n            CONTAINER[\"\ud83d\udc33 Container App&lt;br/&gt;Environment\"]\n            LOGS[\"\ud83d\udcdd Log Analytics\"]\n            ALERTS[\"\ud83d\udea8 Log Search Alert Rule\"]\n            SENDGRID[\"\ud83d\udce7 SendGrid\"]\n        end\n    end\n\n    %% Connections\n    CLOUD --&gt; FIREWALL\n    FIREWALL --&gt; FRONTDOOR\n    FRONTDOOR --&gt; APIM\n    APIM --&gt; PORTAL\n    APIM --&gt; API1\n    APIM --&gt; API2\n\n    API1 -.-&gt; POSTGRES\n    API1 -.-&gt; COSMOS\n    API1 -.-&gt; SERVICEBUS\n    API1 --&gt; INSIGHTS1\n    INSIGHTS1 --&gt; LOGS\n    LOGS --&gt; ALERTS\n    ALERTS --&gt; DANIELLOG\n    DANIELLOG --&gt; SERVICENOW\n    API1 -.-&gt; SENDGRID\n</code></pre>"},{"location":"presentation/azure/landing-zone/#alternative-flow-diagram-version","title":"Alternative Flow Diagram Version","text":"<pre><code>flowchart LR\n    CLOUD[\"\u2601\ufe0f Internet\"]\n    FIREWALL[\"\ud83d\udee1\ufe0f Azure Firewall\"]\n    SERVICENOW[\"\ud83c\udfab ServiceNow Incident\"]\n\n    subgraph SLZ[\"\ud83c\udfe2 Shared Landing Zone\"]\n        direction TB\n        FRONTDOOR[\"\ud83d\udeaa Azure Front Door\"]\n        SERVICEBUS[\"\ud83d\ude8c Service Bus\"]\n        REGISTRY[\"\ud83d\udce6 Container Registry\"]\n        DANIELLOG[\"\ud83d\udcca Log Alert Processor\"]\n        CERTRENEW[\"\ud83d\udd10 Certificate Renew\"]\n        APIM[\"\ud83d\udd27 APIM\"]\n        PORTAL[\"\ud83d\udc68\u200d\ud83d\udcbb Developer&lt;br/&gt;Portal\"]\n    end\n\n    subgraph ALZ[\"\ud83c\udfe2 Application Landing Zone\"]\n        direction TB\n        subgraph \"API1 Resource Group\"\n            API1[\"\ud83d\udce1 API 1\"]\n            INSIGHTS1[\"\ud83d\udcca Application Insights\"]\n            VAULT1[\"\ud83d\udd10 Key Vault\"]\n            STORAGE1[\"\ud83d\udcbe Storage Account\"]\n        end\n\n        subgraph \"API2 Resource Group\"\n            API2[\"\ud83d\udce1 API 2&lt;br/&gt;\ud83d\udcca Application Insights&lt;br/&gt;\ud83d\udd10 Key Vault&lt;br/&gt;\ud83d\udcbe Storage Account\"]\n        end\n\n        subgraph SS[\"\ud83d\udd27 Common Services\"]\n            direction TB\n            POSTGRES[\"\ud83d\uddc4\ufe0f Azure PostgreSQL\"]\n            COSMOS[\"\ud83c\udf0c Cosmos DB\"]\n            PLAN[\"\ud83d\udccb App Service Plan&lt;br/&gt;(Function Apps)\"]\n            CONTAINER[\"\ud83d\udc33 Container App&lt;br/&gt;Environment\"]\n            LOGS[\"\ud83d\udcdd Log Analytics\"]\n            ALERTS[\"\ud83d\udea8 Log Search Alert Rule\"]\n            SENDGRID[\"\ud83d\udce7 SendGrid\"]\n        end\n    end\n\n    CLOUD --&gt; FIREWALL\n    FIREWALL --&gt; FRONTDOOR\n    FRONTDOOR --&gt; APIM\n    APIM --&gt; PORTAL\n    APIM --&gt; API1\n    APIM --&gt; API2\n\n    API1 -.-&gt; POSTGRES\n    API1 -.-&gt; COSMOS\n    API1 -.-&gt; SERVICEBUS\n    API1 --&gt; INSIGHTS1\n    INSIGHTS1 --&gt; LOGS\n    LOGS --&gt; ALERTS\n    ALERTS --&gt; DANIELLOG\n    DANIELLOG --&gt; SERVICENOW\n    API1 -.-&gt; SENDGRID\n</code></pre>"},{"location":"presentation/azure/powerpoint-text/","title":"1- Urbalurba Integration Platform","text":"<p>From ClickOps to DevOps</p> <p>API Endpoints - Production API Base: https://api.urbalurba.no   Developer portal: https://developer.urbalurba.no - Test API Base: https://api.urbalurba.no   Developer portal: https://developer.urbalurba.no - Development API Base: https://api.urbalurba.no   Developer portal: https://developer.urbalurba.no  </p>"},{"location":"presentation/azure/powerpoint-text/#2-clickops-vs-devops","title":"2- ClickOps vs DevOps","text":"<p>ClickOps Characteristics - Manual repetition \u2014 same clicking process for every deployment - \"It worked yesterday\" \u2014 no audit trail when things break unexpectedly - Knowledge silos \u2014 everything breaks when the \u201cAzure expert\u201d is on vacation - Configuration drift \u2014 dev, test, and prod environments are never identical - Fear of change \u2014 \u201cDon\u2019t touch it if it\u2019s working\u201d mentality - Costly mistakes \u2014 one wrong click in production = major incident  </p> <p>DevOps Characteristics - Deploy in seconds \u2014 one command replaces hundreds of clicks - Full traceability \u2014 every change tracked (who, what, when, why) - Knowledge in code \u2014 new team members productive on day one - Identical environments \u2014 what works in dev will work in production - Confident changes \u2014 test everything before production - Automatic rollback \u2014 mistakes reverted in seconds, not hours  </p>"},{"location":"presentation/azure/powerpoint-text/#3-deployment-process","title":"3- Deployment Process","text":"<ol> <li>Internal Owner: Decision to create integration  </li> <li>Developer: Technical planning  </li> <li>Developer: OpenAPI/Swagger spec  </li> <li>API Team: Integration registration  </li> <li>API Team: Repository and infrastructure setup  </li> <li>API Team: Developer onboarding  </li> <li>Developer: Development deployment  </li> <li>Developer: Test deployment (merge to main)  </li> <li>Developer: Production deployment  </li> </ol> <pre><code>graph TD\n    A[Internal Owner: Decision to Create Integration] --&gt; B[Developer: Technical Planning]\n    B --&gt; C[API Team: Integration Registration]\n    C --&gt; D[API Team: Repository and Infrastructure Setup]\n    D --&gt; E[API Team: Developer Onboarding]\n    E --&gt; F[Developer: Development Deployment]\n    F --&gt; G[Developer: Test Deployment - Merge to Main]\n    G --&gt; H[Developer: Production Deployment]\n\n    A1[Define roles: Business Owner and IT Owner&lt;br/&gt;Create executive description&lt;br/&gt;Approve integration creation] -.-&gt; A\n    B1[Create technical specification&lt;br/&gt;Define required Azure services&lt;br/&gt;Choose implementation approach] -.-&gt; B\n    C1[Assign unique integration ID&lt;br/&gt;Assign unique API path&lt;br/&gt;Define integration name&lt;br/&gt;Generate repository name] -.-&gt; C\n    D1[Create repository with IaC&lt;br/&gt;Generate CI/CD pipelines&lt;br/&gt;Configure programming language&lt;br/&gt;Set up 3 environments] -.-&gt; D\n    E1[Send welcome email&lt;br/&gt;Provide working setup&lt;br/&gt;Share development guidelines] -.-&gt; E\n    F1[Clone repository&lt;br/&gt;Start development in devcontainer&lt;br/&gt;Create feature branch&lt;br/&gt;Deploy to Dev environment] -.-&gt; F\n    G1[Merge feature branch to main&lt;br/&gt;Pipelines deploy to Test&lt;br/&gt;Verify functionality] -.-&gt; G\n    H1[Tag main commit with version&lt;br/&gt;Pipelines deploy to Production&lt;br/&gt;Verify functionality] -.-&gt; H\n\n    classDef owner stroke:#01579b,stroke-width:3px\n    classDef developer stroke:#4a148c,stroke-width:3px\n    classDef apiteam stroke:#1b5e20,stroke-width:3px\n\n    class A,A1 owner\n    class B,B1,F,F1,G,G1,H,H1 developer\n    class C,C1,D,D1,E,E1 apiteam\n</code></pre>"},{"location":"presentation/azure/powerpoint-text/#1-internal-owner-decision-to-create-integration","title":"1. Internal Owner: Decision to Create Integration","text":"<ul> <li>Define roles: Business Owner and IT Owner</li> <li>Create executive description of integration purpose and scope</li> <li>Approve integration creation</li> </ul>"},{"location":"presentation/azure/powerpoint-text/#2-developer-technical-planning","title":"2. Developer: Technical Planning","text":"<ul> <li>Create technical specification (data requirements, performance needs)</li> <li>Define required Azure services (database, service bus, logging, storage, key vault)</li> <li>Choose implementation approach (Azure Functions: C#/TypeScript/Python, App Service, Container Apps)</li> </ul>"},{"location":"presentation/azure/powerpoint-text/#3-api-team-integration-registration","title":"3. API Team: Integration Registration","text":"<ul> <li>Assign unique integration ID (e.g., INT0001007)</li> <li>Assign unique API path (e.g., /organizations)</li> <li>Define integration name (e.g., Local-Unions)</li> <li>Generate repository name: <code>&lt;integration-id&gt;-&lt;integration-name&gt;</code> (e.g., INT0001007-Local-Unions)</li> </ul>"},{"location":"presentation/azure/powerpoint-text/#4-api-team-repository-and-infrastructure-setup","title":"4. API Team: Repository and Infrastructure Setup","text":"<ul> <li>Create repository with infrastructure as code</li> <li>Automatically generate CI/CD pipelines:</li> <li>CI (Continuous Integration)</li> <li>CD (Continuous Deployment)</li> <li>APIM registration for API developer portal</li> <li>Configure programming language for Azure Functions</li> <li>Set up for 3 environments: Dev, Test, Production</li> <li>Prepare services requiring manual configuration (database, service bus, etc.)</li> </ul>"},{"location":"presentation/azure/powerpoint-text/#5-api-team-developer-onboarding","title":"5. API Team: Developer Onboarding","text":"<ul> <li>Send welcome email to developer</li> <li>Provide working setup with \"Hello World\" skeleton</li> <li>Share development guidelines (branching strategy, coding standards)</li> </ul>"},{"location":"presentation/azure/powerpoint-text/#6-developer-development-deployment","title":"6. Developer: Development Deployment","text":"<ul> <li>Clone repository</li> <li>Start development in devcontainer (developer toolbox)</li> <li>Create feature branch according to branching rules</li> <li>Modify the test path from printing \"Hello World\" to \"Hello My World\"</li> <li>Deploy infrastructure to Dev environment</li> <li>Deploy to APIM</li> <li>Verify functionality at: <code>https://api-dev.urbalurba.no/&lt;API-path&gt;/helloworld</code></li> <li>Example: <code>https://api-dev.urbalurba.no/organizations/helloworld</code></li> <li>Continue development iterations</li> </ul>"},{"location":"presentation/azure/powerpoint-text/#7-developer-test-deployment-merge-to-main","title":"7. Developer: Test Deployment (Merge to Main)","text":"<ul> <li>Merge feature branch to main in DevOps portal</li> <li>Pipelines automatically deploy to Test environment</li> <li>Verify functionality at: <code>https://api-test.urbalurba.no/&lt;API-path&gt;/helloworld</code></li> </ul>"},{"location":"presentation/azure/powerpoint-text/#8-developer-production-deployment","title":"8. Developer: Production Deployment","text":"<ul> <li>Tag main commit with semantic version (e.g., 1.0.2)</li> <li>Pipelines automatically deploy to Production</li> <li>Verify functionality at: <code>https://api.urbalurba.no/&lt;API-path&gt;/helloworld</code></li> </ul>"},{"location":"presentation/azure/powerpoint-text/#key-principles","title":"Key Principles","text":"<ul> <li>Infrastructure as Code for all deployments</li> <li>Automated CI/CD pipelines across all environments</li> <li>Manual configuration only for services requiring it</li> <li>Semantic versioning for production releases</li> </ul>"},{"location":"presentation/azure/powerpoint-text/#4-what-happens-inside-iac-infrastructure-as-code","title":"4- What happens inside IaC Infrastructure as Code","text":"<ul> <li>Fully automated setup  </li> <li>Works the same every time  </li> <li>Provisions: landing zones, systems, networking, security, APIM, owners, and more  </li> </ul> <pre><code>graph TD\n    A[Infrastructure as Code IaC] --&gt; B[1. Create Repository&lt;br/&gt;with Infrastructure as Code]\n\n    B --&gt; C[2. Auto-Generate CI/CD Pipelines]\n    C --&gt; C1[CI - Continuous Integration]\n    C --&gt; C2[CD - Continuous Deployment]\n    C --&gt; C3[APIM Registration&lt;br/&gt;for API Developer Portal]\n\n    C1 --&gt; D[3. Configure Programming Language&lt;br/&gt;for Azure Functions]\n    C2 --&gt; D\n    C3 --&gt; D\n\n    D --&gt; E[4. Set Up 3 Environments]\n    E --&gt; E1[DEV Environment]\n    E --&gt; E2[TEST Environment]\n    E --&gt; E3[PROD Environment]\n\n    E1 --&gt; F[5. Prepare Services&lt;br/&gt;Requiring Manual Configuration]\n    E2 --&gt; F\n    E3 --&gt; F\n\n    F --&gt; F1[Database]\n    F --&gt; F2[Service Bus]\n    F --&gt; F3[Other Services]\n\n    F1 --&gt; G[Infrastructure Deployed \u2713]\n    F2 --&gt; G\n    F3 --&gt; G\n</code></pre>"},{"location":"presentation/azure/powerpoint-text/#5-landing-zone-architecture-production-landing-zone","title":"5- Landing Zone Architecture \u2013 Production Landing Zone","text":"<ul> <li>Landing zones for Development, Test, and Production <pre><code>flowchart LR\n    CLOUD[\"\u2601\ufe0f Internet\"]\n    FIREWALL[\"\ud83d\udee1\ufe0f Azure Firewall\"]\n    SERVICENOW[\"\ud83c\udfab ServiceNow Incident\"]\n\n    subgraph SLZ[\"\ud83c\udfe2 Shared Landing Zone\"]\n        direction TB\n        FRONTDOOR[\"\ud83d\udeaa Azure Front Door\"]\n        SERVICEBUS[\"\ud83d\ude8c Service Bus\"]\n        REGISTRY[\"\ud83d\udce6 Container Registry\"]\n        DANIELLOG[\"\ud83d\udcca Log Alert Processor\"]\n        CERTRENEW[\"\ud83d\udd10 Certificate Renew\"]\n        APIM[\"\ud83d\udd27 APIM\"]\n        PORTAL[\"\ud83d\udc68\u200d\ud83d\udcbb Developer&lt;br/&gt;Portal\"]\n    end\n\n    subgraph ALZ[\"\ud83c\udfe2 Application Landing Zone\"]\n        direction TB\n        subgraph \"API1 Resource Group\"\n            API1[\"\ud83d\udce1 API 1\"]\n            INSIGHTS1[\"\ud83d\udcca Application Insights\"]\n            VAULT1[\"\ud83d\udd10 Key Vault\"]\n            STORAGE1[\"\ud83d\udcbe Storage Account\"]\n        end\n\n        subgraph \"API2 Resource Group\"\n            API2[\"\ud83d\udce1 API 2&lt;br/&gt;\ud83d\udcca Application Insights&lt;br/&gt;\ud83d\udd10 Key Vault&lt;br/&gt;\ud83d\udcbe Storage Account\"]\n        end\n\n        subgraph SS[\"\ud83d\udd27 Common Services\"]\n            direction TB\n            POSTGRES[\"\ud83d\uddc4\ufe0f Azure PostgreSQL\"]\n            COSMOS[\"\ud83c\udf0c Cosmos DB\"]\n            PLAN[\"\ud83d\udccb App Service Plan&lt;br/&gt;(Function Apps)\"]\n            CONTAINER[\"\ud83d\udc33 Container App&lt;br/&gt;Environment\"]\n            LOGS[\"\ud83d\udcdd Log Analytics\"]\n            ALERTS[\"\ud83d\udea8 Log Search Alert Rule\"]\n            SENDGRID[\"\ud83d\udce7 SendGrid\"]\n        end\n    end\n\n    CLOUD --&gt; FIREWALL\n    FIREWALL --&gt; FRONTDOOR\n    FRONTDOOR --&gt; APIM\n    APIM --&gt; PORTAL\n    APIM --&gt; API1\n    APIM --&gt; API2\n\n    API1 -.-&gt; POSTGRES\n    API1 -.-&gt; COSMOS\n    API1 -.-&gt; SERVICEBUS\n    API1 --&gt; INSIGHTS1\n    INSIGHTS1 --&gt; LOGS\n    LOGS --&gt; ALERTS\n    ALERTS --&gt; DANIELLOG\n    DANIELLOG --&gt; SERVICENOW\n    API1 -.-&gt; SENDGRID\n</code></pre></li> </ul>"},{"location":"presentation/azure/powerpoint-text/#6-integration-platform-overview-landing-zones-dev-test-and-prod","title":"6- Integration Platform Overview - Landing zones Dev, Test and Prod","text":"<p>Overview of the dev, test and prod landing zones</p> <pre><code>graph TB\n    CLOUD[\"\u2601\ufe0f Internet\"]\n\n    FIREWALL[\"\ud83d\udee1\ufe0f Azure Firewall\"]\n\n    classDef prodSecure stroke:#d63031,stroke-width:5px\n\n    subgraph \"Prod Shared Services\"\n        subgraph \"dev-sharedservices\"\n            FRONTDOORDEV[\"\ud83d\udeaa Azure Front Door&lt;br/&gt;(Dev)\"]\n            APIMDEV[\"\ud83d\udd27 APIM-Dev\"]\n            PORTALDEV[\"\ud83d\udc68\u200d\ud83d\udcbb Developer Portal&lt;br/&gt;(Dev)\"]\n            SERVICEBUSDEV[\"\ud83d\ude8c Service Bus&lt;br/&gt;(Dev)\"]\n        end\n\n        subgraph \"test-sharedservices\"\n            FRONTDOORTEST[\"\ud83d\udeaa Azure Front Door&lt;br/&gt;(Test)\"]\n            APIMTEST[\"\ud83d\udd27 APIM-Test\"]\n            PORTALTEST[\"\ud83d\udc68\u200d\ud83d\udcbb Developer Portal&lt;br/&gt;(Test)\"]\n            SERVICEBUSTEST[\"\ud83d\ude8c Service Bus&lt;br/&gt;(Test)\"]\n        end\n\n        subgraph \"prod-sharedservices\"\n            FRONTDOORPROD[\"\ud83d\udeaa Azure Front Door&lt;br/&gt;(Prod)\"]\n            APIMPROD[\"\ud83d\udd27 APIM-Prod\"]\n            PORTALPROD[\"\ud83d\udc68\u200d\ud83d\udcbb Developer Portal&lt;br/&gt;(Prod)\"]\n            SERVICEBUSPROD[\"\ud83d\ude8c Service Bus&lt;br/&gt;(Prod)\"]\n            REGISTRYPROD[\"\ud83d\udce6 Container Registry&lt;br/&gt;(Prod)\"]\n            CERTRENEWPROD[\"\ud83d\udd10 Certificate Renew&lt;br/&gt;(Prod)\"]\n        end\n    end\n\n    %% Connections\n    CLOUD --&gt; FIREWALL\n    FIREWALL --&gt; FRONTDOORDEV\n    FIREWALL --&gt; FRONTDOORTEST\n    FIREWALL --&gt; FRONTDOORPROD\n\n    FRONTDOORDEV --&gt; APIMDEV\n    FRONTDOORTEST --&gt; APIMTEST\n    FRONTDOORPROD --&gt; APIMPROD\n\n    APIMDEV --&gt; PORTALDEV\n    APIMTEST --&gt; PORTALTEST\n    APIMPROD --&gt; PORTALPROD\n\n    class SERVICEBUSPROD,APIMPROD prodSecure\n</code></pre>"},{"location":"presentation/azure/powerpoint-text/#7-deployment-portal-the-api-shop","title":"7- Deployment Portal \u2013 \u201cThe API Shop\u201d","text":"<ul> <li>Browse &amp; Discover: Search and explore available APIs  </li> <li>Self-Register: Sign up for API access independently  </li> <li>Get API Keys: Receive authentication credentials instantly  </li> <li>Test APIs: Try APIs directly in the portal before integration  </li> <li>Download SDKs: Access client libraries and code samples  </li> </ul>"},{"location":"presentation/azure/powerpoint-text/#8-developer-guidelines","title":"8- Developer Guidelines","text":"<ul> <li>Core Requirements  </li> <li>Development Process  </li> <li>Logging Standards  </li> </ul> <p>**image from Integrations.wiki showing the Developer handbook\"  TODO: Add the most important text as speaker notes</p>"},{"location":"presentation/azure/powerpoint-text/#9-developer-toolbox","title":"9- Developer Toolbox","text":"<ul> <li>Works across Windows / Mac / Linux  </li> <li>Efficiency: Faster onboarding, reduced support overhead  </li> <li>Version Control: Same libraries for everyone, no version conflicts  </li> <li>Consistency: Standardized tools and practices across teams  </li> <li>Accessibility: Anyone can check out code and fix bugs  </li> </ul> <p>Tools, programming languages, and libraries are included in the Devcontainer. TODO: Add speakder notes and link</p>"},{"location":"presentation/azure/powerpoint-text/#8-our-runtime-menu","title":"8- Our \u201cRuntime\u201d Menu","text":"<ul> <li>Container Apps  </li> <li>App Service  </li> <li>Serverless / Functions  </li> </ul> <p>Images showing: a) what runtimes we offer and how they relate to offerings from azure b) what runtimes are suited for what*</p>"},{"location":"presentation/azure/powerpoint-text/#9-devops-benefits","title":"9- DevOps Benefits","text":"<p>For the Company - Faster time-to-market: features in days, not months - Reduced operational costs: fewer incidents, less manual work, optimal resource usage  </p> <p>For Developers - Focus on creating, not configuring: write code, not deployment guides - Less stress: no more weekend deployments or 2 AM emergencies  </p> <p>For IT Department - From firefighting to engineering: prevent problems instead of reacting - Standardization without bureaucracy: governance built-in, not blocking  </p> <p>DevOps Benefits  </p> <ul> <li>Automated &amp; Reliable  </li> <li>Documented  </li> </ul>"},{"location":"presentation/azure/powerpoint-text/#10-devops-challenges","title":"10- DevOps Challenges","text":"<p>For the Company - Upfront investment: time, training, tools cost money before ROI - Cultural resistance: \u201cwe\u2019ve always done it this way\u201d friction  </p> <p>For Developers - Steep learning curve: Git, pipelines, IaC, testing (not just coding) - Less freedom: only pre-approved services/patterns, no experimenting  </p> <p>For IT Department - Identity shift: from infrastructure gatekeepers to platform enablers - Legacy system pressure: maintaining old while building new practices  </p> <p>Image (described): Illustrations contrasting ClickOps and DevOps work styles.  </p>"},{"location":"presentation/azure/presentation/","title":"1-Azure Integration Platform","text":"<p>The Urbalurba API integration platform</p> <p>Environment-Specific URLs</p>"},{"location":"presentation/azure/presentation/#development","title":"Development","text":"<ul> <li>API Base: <code>https://api-dev.urbalurba.no</code></li> <li>Developer Portal: <code>https://developer-dev.urbalurba.no</code></li> </ul>"},{"location":"presentation/azure/presentation/#test","title":"Test","text":"<ul> <li>API Base: <code>https://api-test.urbalurba.no</code></li> <li>Developer Portal: <code>https://developer-test.urbalurba.no</code></li> </ul>"},{"location":"presentation/azure/presentation/#production","title":"Production","text":"<ul> <li>API Base: <code>https://api.urbalurba.no</code></li> <li>Developer Portal: <code>https://developer.urbalurba.no</code></li> </ul>"},{"location":"presentation/azure/presentation/#2-landing-zone-architecture","title":"2-Landing Zone Architecture","text":"<p>Security-First Design: API Production Landing Zone</p> <pre><code>flowchart LR\n    CLOUD[\"\u2601\ufe0f Internet\"]\n    FIREWALL[\"\ud83d\udee1\ufe0f Azure Firewall\"]\n    SERVICENOW[\"\ud83c\udfab ServiceNow Incident\"]\n\n    subgraph SLZ[\"\ud83c\udfe2 Shared Landing Zone\"]\n        direction TB\n        FRONTDOOR[\"\ud83d\udeaa Azure Front Door\"]\n        SERVICEBUS[\"\ud83d\ude8c Service Bus\"]\n        REGISTRY[\"\ud83d\udce6 Container Registry\"]\n        DANIELLOG[\"\ud83d\udcca Log Alert Processor\"]\n        CERTRENEW[\"\ud83d\udd10 Certificate Renew\"]\n        APIM[\"\ud83d\udd27 APIM\"]\n        PORTAL[\"\ud83d\udc68\u200d\ud83d\udcbb Developer&lt;br/&gt;Portal\"]\n    end\n\n    subgraph ALZ[\"\ud83c\udfe2 Application Landing Zone\"]\n        direction TB\n        subgraph \"API1 Resource Group\"\n            API1[\"\ud83d\udce1 API 1\"]\n            INSIGHTS1[\"\ud83d\udcca Application Insights\"]\n            VAULT1[\"\ud83d\udd10 Key Vault\"]\n            STORAGE1[\"\ud83d\udcbe Storage Account\"]\n        end\n\n        subgraph \"API2 Resource Group\"\n            API2[\"\ud83d\udce1 API 2&lt;br/&gt;\ud83d\udcca Application Insights&lt;br/&gt;\ud83d\udd10 Key Vault&lt;br/&gt;\ud83d\udcbe Storage Account\"]\n        end\n\n        subgraph SS[\"\ud83d\udd27 Common Services\"]\n            direction TB\n            POSTGRES[\"\ud83d\uddc4\ufe0f Azure PostgreSQL\"]\n            COSMOS[\"\ud83c\udf0c Cosmos DB\"]\n            PLAN[\"\ud83d\udccb App Service Plan&lt;br/&gt;(Function Apps)\"]\n            CONTAINER[\"\ud83d\udc33 Container App&lt;br/&gt;Environment\"]\n            LOGS[\"\ud83d\udcdd Log Analytics\"]\n            ALERTS[\"\ud83d\udea8 Log Search Alert Rule\"]\n            SENDGRID[\"\ud83d\udce7 SendGrid\"]\n        end\n    end\n\n    CLOUD --&gt; FIREWALL\n    FIREWALL --&gt; FRONTDOOR\n    FRONTDOOR --&gt; APIM\n    APIM --&gt; PORTAL\n    APIM --&gt; API1\n    APIM --&gt; API2\n\n    API1 -.-&gt; POSTGRES\n    API1 -.-&gt; COSMOS\n    API1 -.-&gt; SERVICEBUS\n    API1 --&gt; INSIGHTS1\n    INSIGHTS1 --&gt; LOGS\n    LOGS --&gt; ALERTS\n    ALERTS --&gt; DANIELLOG\n    DANIELLOG --&gt; SERVICENOW\n    API1 -.-&gt; SENDGRID\n</code></pre> <p>This is the Production Landing Zone architecture. Notice how we have a clear separation between the Shared Landing Zone and the Application Landing Zone. The Shared Landing Zone contains services that are used across multiple APIs like Azure Front Door, API Management, Service Bus, and monitoring tools. The Application Landing Zone shows how individual APIs are organized in their own resource groups with their dedicated Application Insights, Key Vault, and Storage Account. The dotted lines show how APIs connect to shared services like databases and messaging, while solid lines show the monitoring flow from API to Application Insights to Log Analytics to alerting. This architecture ensures security, scalability, and proper monitoring across all our integrations.</p>"},{"location":"presentation/azure/presentation/#3-integration-platform-overview","title":"3-Integration Platform Overview","text":"<p>The 3 API Landing zones Dev, Test and Production</p> <pre><code>graph TB\n    subgraph \"Internet\"\n        CLOUD[\"\u2601\ufe0f Internet\"]\n    end\n\n    subgraph \"Azure Firewall\"\n        FIREWALL[\"\ud83d\udee1\ufe0f Azure Firewall\"]\n    end\n\n    classDef prodSecure stroke:#d63031,stroke-width:5px\n\n    subgraph \"Prod Shared Services\"\n        subgraph \"dev-sharedservices\"\n            FRONTDOORDEV[\"\ud83d\udeaa Azure Front Door&lt;br/&gt;(Dev)\"]\n            APIMDEV[\"\ud83d\udd27 APIM-Dev\"]\n            PORTALDEV[\"\ud83d\udc68\u200d\ud83d\udcbb Developer Portal&lt;br/&gt;(Dev)\"]\n            SERVICEBUSDEV[\"\ud83d\ude8c Service Bus&lt;br/&gt;(Dev)\"]\n        end\n\n        subgraph \"test-sharedservices\"\n            FRONTDOORTEST[\"\ud83d\udeaa Azure Front Door&lt;br/&gt;(Test)\"]\n            APIMTEST[\"\ud83d\udd27 APIM-Test\"]\n            PORTALTEST[\"\ud83d\udc68\u200d\ud83d\udcbb Developer Portal&lt;br/&gt;(Test)\"]\n            SERVICEBUSTEST[\"\ud83d\ude8c Service Bus&lt;br/&gt;(Test)\"]\n        end\n\n        subgraph \"prod-sharedservices\"\n            FRONTDOORPROD[\"\ud83d\udeaa Azure Front Door&lt;br/&gt;(Prod)\"]\n            APIMPROD[\"\ud83d\udd27 APIM-Prod\"]\n            PORTALPROD[\"\ud83d\udc68\u200d\ud83d\udcbb Developer Portal&lt;br/&gt;(Prod)\"]\n            SERVICEBUSPROD[\"\ud83d\ude8c Service Bus&lt;br/&gt;(Prod)\"]\n            REGISTRYPROD[\"\ud83d\udce6 Container Registry&lt;br/&gt;(Prod)\"]\n            CERTRENEWPROD[\"\ud83d\udd10 Certificate Renew&lt;br/&gt;(Prod)\"]\n        end\n    end\n\n    %% Connections\n    CLOUD --&gt; FIREWALL\n    FIREWALL --&gt; FRONTDOORDEV\n    FIREWALL --&gt; FRONTDOORTEST\n    FIREWALL --&gt; FRONTDOORPROD\n\n    FRONTDOORDEV --&gt; APIMDEV\n    FRONTDOORTEST --&gt; APIMTEST\n    FRONTDOORPROD --&gt; APIMPROD\n\n    APIMDEV --&gt; PORTALDEV\n    APIMTEST --&gt; PORTALTEST\n    APIMPROD --&gt; PORTALPROD\n\n    class SERVICEBUSPROD prodSecure\n</code></pre> <p>This slide shows how the 3 landing zones (Dev, Test, and Production) each have their own shared services. Notice that each environment has its own Azure Front Door, APIM, Developer Portal, and Service Bus. This isolation ensures that development and testing don't interfere with production. The Production environment has additional services like Container Registry and Certificate Renew that are only needed in production. The red border around the Production Service Bus indicates it's the most secure and critical service. This multi-environment setup allows developers to work safely in Dev, test thoroughly in Test, and deploy confidently to Production.</p>"},{"location":"presentation/azure/presentation/#4-deployment-process","title":"4-Deployment Process","text":"<p>9-Step Workflow</p> <pre><code>graph TD\n    A[Internal Owner: Decision to Create Integration] --&gt; B[Developer: Technical Planning]\n    B --&gt; C[Developer: OpenAPI /Swagger spec]\n    C --&gt; D[API Team: Integration Registration]\n    D --&gt; E[API Team: Repository and Infrastructure Setup]\n    E --&gt; F[API Team: Developer Onboarding]\n    F --&gt; G[Developer: Development Deployment]\n    G --&gt; H[Developer: Test Deployment - Merge to Main]\n    H --&gt; I[Developer: Production Deployment]\n\n    A1[Define roles: Business Owner and IT Owner&lt;br/&gt;Create executive description&lt;br/&gt;Approve integration creation] -.-&gt; A\n    B1[Create technical specification&lt;br/&gt;Define required Azure services&lt;br/&gt;Choose implementation approach] -.-&gt; B\n    C1[Create OpenAPI specification&lt;br/&gt;Document API endpoints&lt;br/&gt;Define request/response schemas&lt;br/&gt;Validate API documentation] -.-&gt; C\n    D1[Assign unique integration ID&lt;br/&gt;Assign unique API path&lt;br/&gt;Define integration name&lt;br/&gt;Generate repository name] -.-&gt; D\n    E1[Create repository with IaC&lt;br/&gt;Generate CI/CD pipelines&lt;br/&gt;Configure programming language&lt;br/&gt;Set up 3 environments] -.-&gt; E\n    F1[Send welcome email&lt;br/&gt;Provide working setup&lt;br/&gt;Share development guidelines] -.-&gt; F\n    G1[Clone repository&lt;br/&gt;Start development in devcontainer&lt;br/&gt;Create feature branch&lt;br/&gt;Deploy to Dev environment] -.-&gt; G\n    H1[Merge feature branch to main&lt;br/&gt;Pipelines deploy to Test&lt;br/&gt;Verify functionality] -.-&gt; H\n    I1[Tag main commit with version&lt;br/&gt;Pipelines deploy to Production&lt;br/&gt;Verify functionality] -.-&gt; I\n\n    classDef owner stroke:#01579b,stroke-width:3px\n    classDef developer stroke:#4a148c,stroke-width:3px\n    classDef apiteam stroke:#1b5e20,stroke-width:3px\n\n    class A,A1 owner\n    class B,B1,C,C1,G,G1,H,H1,I,I1 developer\n    class D,D1,E,E1,F,F1 apiteam\n</code></pre> <p>This 8-step deployment process shows the clear division of responsibilities between three key actors. The Internal Owner makes the business decision and defines roles. The Developer handles technical planning and all development work including Dev, Test, and Production deployments. The API Team manages the infrastructure setup, integration registration, and developer onboarding. Notice the color coding: blue for business decisions, purple for development work, and green for infrastructure management. The dotted lines show the detailed actions for each step. This process ensures proper governance while enabling developers to work independently once they're onboarded. The automated CI/CD pipelines handle the actual deployments, making the process reliable and repeatable.</p>"},{"location":"presentation/azure/presentation/#5-developer-portal-the-api-shop","title":"5-Developer Portal - The API shop","text":"<p>API Discovery &amp; Access</p>"},{"location":"presentation/azure/presentation/#what-is-it","title":"What is it?","text":"<p>A self-service platform where developers can discover, explore, and consume APIs published by your organization.</p>"},{"location":"presentation/azure/presentation/#how-developers-get-access-to-apis","title":"How developers get access to APIs:","text":"<ul> <li>Browse &amp; Discover - Search and explore available APIs</li> <li>Self-Register - Sign up for API access independently</li> <li>Get API Keys - Receive authentication credentials instantly</li> <li>Test APIs - Try APIs directly in the portal before integration</li> <li>Download SDKs - Access client libraries and code samples</li> </ul>"},{"location":"presentation/azure/presentation/#6-developer-guidelines","title":"6-Developer Guidelines","text":"<p>Standards That Enable Success</p>"},{"location":"presentation/azure/presentation/#core-requirements","title":"Core Requirements","text":"<ul> <li>Shared Resources - Use common services (databases, service bus, logging)</li> <li>Private Networking - VNet integration for security</li> <li>Security First - Managed Identities, Key Vault, RBAC</li> <li>Governance - Follow naming conventions and standards</li> </ul>"},{"location":"presentation/azure/presentation/#development-process","title":"Development Process","text":"<ul> <li>Trunk-Based Git - Feature branches, PRs, automated deployments</li> <li>3 Environments - Dev \u2192 Test \u2192 Production promotion</li> <li>APIM Integration - Automatic API registration and management</li> </ul>"},{"location":"presentation/azure/presentation/#logging-standards","title":"Logging Standards","text":"<ul> <li>Structured JSON - Consistent log format across all services</li> <li>System ID - Unique identifier for each integration</li> <li>Correlation IDs - Track requests across service boundaries</li> </ul>"},{"location":"presentation/azure/presentation/#7-developer-experience","title":"7-Developer Experience","text":"<p>Self-Service Development</p>"},{"location":"presentation/azure/presentation/#what-is-it_1","title":"What is it?","text":"<p>A centralized platform providing developers with tools, documentation, and resources for efficient development.</p>"},{"location":"presentation/azure/presentation/#why-do-we-have-it","title":"Why do we have it?","text":"<ul> <li>Single Point of Access - All development resources in one place</li> <li>Self-Service - Developers manage their own environments</li> <li>Consistency - Standardized tools and practices across teams</li> <li>Version Control - Same libraries for everyone, no version conflicts</li> <li>Efficiency - Faster onboarding and reduced support overhead</li> </ul>"},{"location":"presentation/azure/presentation/#deployment-benefits","title":"Deployment Benefits","text":"<p>Automated &amp; Reliable</p>"},{"location":"presentation/azure/presentation/#key-benefits","title":"Key Benefits","text":"<ul> <li>Infrastructure as Code - All deployments automated</li> <li>Consistent Environments - Dev, Test, Production parity</li> <li>Quality Gates - Automated testing and validation</li> <li>Rollback Capability - Quick recovery from issues</li> <li>Audit Trail - Complete deployment history</li> </ul>"},{"location":"presentation/azure/presentation/#process-efficiency","title":"Process Efficiency","text":"<ul> <li>Faster Deployments - Minutes instead of hours</li> <li>Reduced Errors - Automated validation and testing</li> <li>Self-Service - Developers can deploy independently</li> <li>Standardization - Consistent deployment patterns</li> </ul>"},{"location":"presentation/azure/presentation/#next-steps","title":"Next Steps","text":"<p>Ready to Get Started?</p>"},{"location":"presentation/azure/presentation/#for-developers","title":"For Developers","text":"<ol> <li>Access Developer Portal: <code>https://developer.urbalurba.no</code></li> <li>Review development guidelines</li> <li>Set up DevContainer environment</li> <li>Start building integrations</li> </ol>"},{"location":"presentation/azure/presentation/#for-management","title":"For Management","text":"<ol> <li>Review deployment benefits</li> <li>Understand the 8-step process</li> <li>Plan integration projects</li> <li>Assign business and IT owners</li> </ol>"},{"location":"presentation/azure/presentation/#for-it-teams","title":"For IT Teams","text":"<ol> <li>Review security architecture</li> <li>Understand monitoring and alerting</li> <li>Set up operational procedures</li> <li>Configure ServiceNow integration</li> </ol>"},{"location":"presentation/azure/presentation/#questions","title":"Questions?","text":"<p>Let's discuss your integration needs</p> <ul> <li>Architecture questions?</li> <li>Development process?</li> <li>Security requirements?</li> <li>Deployment timeline?</li> </ul>"},{"location":"presentation/azure/story/","title":"Our Azure Integration Platform Story","text":""},{"location":"presentation/azure/story/#the-challenge-we-solved","title":"The Challenge We Solved","text":"<p>For Management: \"We needed a way to quickly build and deploy integrations without the traditional 6-month development cycles and complex approval processes.\"</p> <p>For IT Department: \"We were drowning in manual server setups, inconsistent environments, and support tickets from developers who couldn't get their tools working.\"</p> <p>For Developers: \"We were spending more time fighting with environments and waiting for access than actually building features.\"</p>"},{"location":"presentation/azure/story/#our-solution-the-azure-integration-platform","title":"Our Solution: The Azure Integration Platform","text":""},{"location":"presentation/azure/story/#landing-zone-architecture","title":"\ud83c\udfd7\ufe0f Landing Zone Architecture","text":"<ul> <li>Secure by Design: Azure Firewall \u2192 Front Door \u2192 API Management</li> <li>Multi-Environment: Dev, Test, and Production with proper isolation</li> <li>Shared Services: Common infrastructure (databases, monitoring, certificates)</li> <li>Resource Groups: Each API gets its own dedicated resources</li> </ul>"},{"location":"presentation/azure/story/#developer-experience","title":"\ud83d\udd27 Developer Experience","text":"<ul> <li>DevContainer Toolbox: Same libraries for everyone, no version conflicts</li> <li>Self-Service Portal: Developers can discover and access APIs instantly</li> <li>Automated Deployments: One-click deployment to any environment</li> <li>Consistent Standards: Every project follows the same proven patterns</li> </ul>"},{"location":"presentation/azure/story/#monitoring-operations","title":"\ud83d\udcca Monitoring &amp; Operations","text":"<ul> <li>Complete Observability: Application Insights \u2192 Log Analytics \u2192 Alert Processing</li> <li>Automated Incident Management: Alerts flow directly to ServiceNow</li> <li>Centralized Logging: All services feed into unified monitoring</li> <li>Certificate Management: Automated renewal for production services</li> </ul>"},{"location":"presentation/azure/story/#the-business-impact","title":"The Business Impact","text":""},{"location":"presentation/azure/story/#for-management","title":"For Management \ud83d\udcbc","text":"<ul> <li>Faster Time-to-Market: New integrations deployed in days, not months</li> <li>Reduced Costs: No more manual server management or lengthy approval processes</li> <li>Better Visibility: Clear tracking of API usage and performance</li> <li>Risk Mitigation: Standardized security and compliance across all integrations</li> </ul>"},{"location":"presentation/azure/story/#for-it-department","title":"For IT Department \ud83d\udee0\ufe0f","text":"<ul> <li>Reduced Support Load: Self-service capabilities mean fewer tickets</li> <li>Consistent Infrastructure: No more \"works on my machine\" problems</li> <li>Automated Operations: Monitoring and alerting handle routine issues</li> <li>Predictable Scaling: Infrastructure grows with demand automatically</li> </ul>"},{"location":"presentation/azure/story/#for-developers","title":"For Developers \ud83d\udc68\u200d\ud83d\udcbb","text":"<ul> <li>Instant Productivity: Working environment ready in minutes</li> <li>Self-Service Access: Get API keys and documentation without waiting</li> <li>Modern Tooling: Latest development tools and libraries pre-configured</li> <li>Clear Guidelines: Know exactly how to build, test, and deploy</li> </ul>"},{"location":"presentation/azure/story/#real-world-results","title":"Real-World Results","text":""},{"location":"presentation/azure/story/#before-our-platform","title":"Before Our Platform \u274c","text":"<ul> <li>Inconsistent development environments</li> <li>Manual provisioning - ClickOps</li> <li>No standardiced way to develop, document, deploy </li> <li>Frequent \"works on my machine\" issues</li> </ul>"},{"location":"presentation/azure/story/#after-our-platform","title":"After Our Platform \u2705","text":"<ul> <li>New integrations deployed in days</li> <li>Identical environments for all developers</li> <li>Automated infrastructure provisioning</li> <li>Self-service API discovery and access</li> <li>Zero environment-related issues</li> <li>Automated incidents to ServiceNow when needed</li> </ul>"},{"location":"presentation/azure/story/#the-technical-foundation","title":"The Technical Foundation","text":""},{"location":"presentation/azure/story/#security-first","title":"Security First \ud83d\udd12","text":"<ul> <li>Azure Firewall provides network-level protection</li> <li>API Management handles authentication and rate limiting</li> <li>Separate environments prevent cross-contamination</li> <li>Automated certificate management ensures security</li> </ul>"},{"location":"presentation/azure/story/#developer-productivity","title":"Developer Productivity \u26a1","text":"<ul> <li>DevContainer ensures everyone uses identical tools</li> <li>Self-service portal eliminates access bottlenecks</li> <li>Automated CI/CD pipelines handle deployments</li> <li>Comprehensive monitoring catches issues early</li> </ul>"},{"location":"presentation/azure/story/#operational-excellence","title":"Operational Excellence \ud83d\udcc8","text":"<ul> <li>Centralized logging and monitoring</li> <li>Automated alerting and incident management</li> <li>Standardized processes across all projects</li> <li>Predictable scaling and cost management</li> </ul>"},{"location":"presentation/azure/story/#why-this-matters","title":"Why This Matters","text":"<p>We've transformed integration development from a bottleneck into a competitive advantage.</p> <p>Instead of spending months on infrastructure setup and approval processes, our teams can focus on building features that matter to our users. We've eliminated the friction that was slowing down innovation while maintaining the security and compliance standards that protect our organization.</p> <p>The result? Faster delivery, happier developers, and more reliable systems that scale with our growing needs.</p>"},{"location":"presentation/azure/story/#next-steps","title":"Next Steps","text":"<p>Ready to see this platform in action? Let's explore how it can accelerate your next integration project.</p> <p>For Developers: Check out the Developer Portal at <code>developer.urbalurba.no</code> For Management: Review the deployment benefits and cost savings For IT: Examine the operational monitoring and security features</p>"},{"location":"presentation/sovdev/goal/","title":"Goal for the presentation of SovDev","text":"<p>Goal:  - Put forward SovDev as a solution for:     - Rapid prototyping     - Creating MVP (Minimum Viable Product)     - PoC (Profe of concept)</p> <p>Actors: - Developer - IT Operations - Innovation department - Management</p> <p>Setting: - IT Opreations team is focused on running and maintaining infrastructure, not familiar with innovation as described in Goal. - Management need to get an easy to understand explanaton. - Innovation department has knowledge about the goal concepts, but not the technical stuff that make it happen. - Developers might be skeptic to do development in a unfamiliar setup.</p> <p>To get everyone on the same page: - Basic innovation principes, why the goals are important, trends, what is proven and tested  - \"Build, Measure, Learn\" vs Waterfall - Why speed is important in itterations</p> <p>Backgrouund info: We have a DevOps regime described in the ../azure/powerpoint-text.md </p>"},{"location":"presentation/sovdev/sovdev-developer-toolbox/","title":"The Developer Toolbox and how it is connected to SovDev","text":"<pre><code>graph TB\n    subgraph \"Developer Machine\"\n        subgraph \"VSCode IDE\"\n            VSCODE[\"\ud83d\udcbb VSCode&lt;br/&gt;Code Editor\"]\n\n            subgraph \"DevContainer\"\n                CONTAINER[\"\ud83d\udc33 DevContainer&lt;br/&gt;Development Environment\"]\n                CODE[\"\ud83d\udcdd Application Code&lt;br/&gt;Node.js, Python, Go, etc.\"]\n                TOOLS[\"\ud83d\udee0\ufe0f Dev Tools&lt;br/&gt;SDKs, CLIs, Debuggers\"]\n            end\n        end\n\n        subgraph \"SovDev Cluster (devSov)\"\n            K8S[\"\u2638\ufe0f Kubernetes&lt;br/&gt;Container Orchestration\"]\n            TRAEFIK[\"\ud83c\udf10 Traefik&lt;br/&gt;Reverse Proxy\"]\n            POSTGRES[\"\ud83d\udc18 PostgreSQL&lt;br/&gt;Database\"]\n            REDIS[\"\ud83d\udd34 Redis&lt;br/&gt;Cache\"]\n            MONGODB[\"\ud83c\udf43 MongoDB&lt;br/&gt;Document DB\"]\n            AUTHENTIK[\"\ud83d\udd10 Authentik&lt;br/&gt;Authentication\"]\n            AI[\"\ud83e\udd16 AI Platform&lt;br/&gt;OpenWebUI + LiteLLM\"]\n            GRAFANA[\"\ud83d\udcca Grafana&lt;br/&gt;Monitoring\"]\n        end\n    end\n\n    %% Developer workflow connections\n    VSCODE --&gt; CONTAINER\n    CONTAINER --&gt; CODE\n    CONTAINER --&gt; TOOLS\n\n    %% DevContainer to SovDev connections\n    CONTAINER -.-&gt;|\"Database Queries\"| POSTGRES\n    CONTAINER -.-&gt;|\"Cache Operations\"| REDIS\n    CONTAINER -.-&gt;|\"Document Storage\"| MONGODB\n    CONTAINER -.-&gt;|\"Authentication\"| AUTHENTIK\n    CONTAINER -.-&gt;|\"AI Services\"| AI\n    CONTAINER -.-&gt;|\"Monitoring\"| GRAFANA\n\n    %% Internal SovDev connections\n    TRAEFIK --&gt; POSTGRES\n    TRAEFIK --&gt; REDIS\n    TRAEFIK --&gt; MONGODB\n    TRAEFIK --&gt; AUTHENTIK\n    TRAEFIK --&gt; AI\n    TRAEFIK --&gt; GRAFANA\n</code></pre>"},{"location":"presentation/sovdev/sovdev-developer-toolbox/#developer-workflow","title":"\ud83d\udd27 Developer Workflow","text":""},{"location":"presentation/sovdev/sovdev-developer-toolbox/#development-environment-setup","title":"Development Environment Setup","text":"<ol> <li>VSCode opens the project with devcontainer configuration</li> <li>DevContainer starts with all necessary development tools</li> <li>Application Code runs inside the container</li> <li>SovDev Cluster provides backend services</li> </ol>"},{"location":"presentation/sovdev/sovdev-developer-toolbox/#service-connections","title":"Service Connections","text":"Service Connection Type Purpose Example PostgreSQL Database connection Primary data storage <code>postgresql://postgres:5432/mydb</code> Redis Cache connection Session storage, caching <code>redis://redis:6379</code> MongoDB Document DB NoSQL data storage <code>mongodb://mongodb:27017/mydb</code> Authentik OAuth/OIDC User authentication <code>https://authentik.localhost</code> AI Platform REST API AI/ML services <code>http://openwebui.localhost/api</code> Grafana Monitoring API Metrics and logging <code>http://grafana.localhost/api</code>"},{"location":"presentation/sovdev/sovdev-developer-toolbox/#devcontainer-configuration-example","title":"DevContainer Configuration Example","text":"<pre><code>{\n  \"name\": \"MyApp DevContainer\",\n  \"image\": \"mcr.microsoft.com/devcontainers/typescript-node:18\",\n  \"features\": {\n    \"ghcr.io/devcontainers/features/docker-in-docker:2\": {},\n    \"ghcr.io/devcontainers/features/kubectl-helm-minikube:1\": {}\n  },\n  \"forwardPorts\": [3000, 5432, 6379],\n  \"postCreateCommand\": \"npm install &amp;&amp; npm run dev\"\n}\n</code></pre>"},{"location":"presentation/sovdev/sovdev-developer-toolbox/#environment-variables","title":"Environment Variables","text":"<pre><code># Database connections\nDATABASE_URL=postgresql://postgres:password@postgres.localhost:5432/mydb\nREDIS_URL=redis://redis.localhost:6379\nMONGODB_URL=mongodb://mongodb.localhost:27017/mydb\n\n# Authentication\nAUTHENTIK_URL=https://authentik.localhost\nAUTHENTIK_CLIENT_ID=myapp\nAUTHENTIK_CLIENT_SECRET=secret\n\n# AI Services\nAI_API_URL=http://openwebui.localhost/api\nAI_API_KEY=your-api-key\n\n# Monitoring\nGRAFANA_URL=http://grafana.localhost\nGRAFANA_API_KEY=your-grafana-key\n</code></pre>"},{"location":"presentation/sovdev/sovdev-developer-toolbox/#key-benefits","title":"\ud83d\ude80 Key Benefits","text":""},{"location":"presentation/sovdev/sovdev-developer-toolbox/#isolated-development","title":"Isolated Development","text":"<ul> <li>DevContainer provides consistent environment</li> <li>All dependencies containerized</li> <li>Easy onboarding for new developers</li> </ul>"},{"location":"presentation/sovdev/sovdev-developer-toolbox/#real-service-integration","title":"Real Service Integration","text":"<ul> <li>Connect to actual SovDev services</li> <li>Test against production-like environment</li> <li>Validate integrations early</li> </ul>"},{"location":"presentation/sovdev/sovdev-developer-toolbox/#local-development-experience","title":"Local Development Experience","text":"<ul> <li>VSCode integration with SovDev</li> <li>Hot reloading and debugging</li> <li>Full IDE features with container benefits</li> </ul>"},{"location":"presentation/sovdev/sovdev-developer-toolbox/#service-discovery","title":"Service Discovery","text":"<ul> <li>Services accessible via localhost subdomains</li> <li>Automatic service discovery</li> <li>Consistent naming convention</li> </ul>"},{"location":"presentation/sovdev/sovdev-developer-toolbox/#development-flow","title":"\ud83d\udd04 Development Flow","text":"<ol> <li>Start SovDev: <code>./provision-host/kubernetes/provision-kubernetes.sh</code></li> <li>Open VSCode: <code>code .</code> in project directory</li> <li>DevContainer Starts: Automatically with all tools</li> <li>Connect to Services: Application connects to SovDev services</li> <li>Develop &amp; Test: Full development experience with real backend</li> <li>Deploy: Same services available in staging/production</li> </ol>"},{"location":"presentation/sovdev/sovdev-developer-toolbox/#value-proposition","title":"\ud83d\udca1 Value Proposition","text":"<p>SovDev + DevContainer provides developers with: - Real Backend Services instead of mocks - Consistent Environment across team members - Production Parity from day one - Easy Service Integration with localhost access - Full Development Experience with VSCode + containers</p>"},{"location":"presentation/sovdev/sovdev-overview/","title":"Sovereign Developer Infrastructure - SovDev","text":"<pre><code>graph TB\n    subgraph \"SovDev - Developer Machine\"\n        BROWSER[\"\ud83c\udf10 Web Browser&lt;br/&gt;http://service.localhost\"]\n\n        subgraph \"Host Operating System\"\n            HOST[\"\ud83d\udcbb Host OS&lt;br/&gt;macOS / Linux / Windows\"]\n\n            subgraph \"Provision Host Container\"\n                PH[\"\ud83c\udfd7\ufe0f Provision Host&lt;br/&gt;Management Environment\"]\n                TOOLS[\"\ud83d\udee0\ufe0f Management Tools&lt;br/&gt;kubectl, helm, ansible&lt;br/&gt;cloud CLIs, terraform\"]\n                SCRIPTS[\"\ud83d\udcdc Orchestration Scripts&lt;br/&gt;provision-kubernetes.sh&lt;br/&gt;category-based automation\"]\n                CONFIG[\"\u2699\ufe0f Configuration&lt;br/&gt;playbooks, manifests&lt;br/&gt;secrets, kubeconfig\"]\n            end\n\n            subgraph \"Kubernetes Cluster - Landing Zone\"\n                subgraph \"Core Infrastructure\"\n                    K8S[\"\u2638\ufe0f Kubernetes&lt;br/&gt;Container Orchestration\"]\n                    TRAEFIK[\"\ud83c\udf10 Traefik&lt;br/&gt;Reverse Proxy &amp; Ingress\"]\n                end\n\n                subgraph \"Common Services\"\n                    POSTGRES[\"\ud83d\udc18 PostgreSQL&lt;br/&gt;(Azure PostgreSQL)\"]\n                    SERVICEBUS[\"\ud83d\ude8c RabbitMQ&lt;br/&gt;(Service Bus)\"]\n                    VAULT[\"\ud83d\udd10 Key Vault&lt;br/&gt;(Secrets Management)\"]\n                    LOGS[\"\ud83d\udcdd Log Analytics&lt;br/&gt;(Monitoring)\"]\n                    SENDGRID[\"\ud83d\udce7 SendGrid&lt;br/&gt;(Email Services)\"]\n                    COSMOS[\"\ud83c\udf0c MongoDB&lt;br/&gt;(Cosmos DB)\"]\n                    INSIGHTS[\"\ud83d\udcca Application Insights&lt;br/&gt;(Telemetry)\"]\n                    PLAN[\"\ud83d\udccb App Service Plan&lt;br/&gt;(Function Apps)\"]\n                    CONTAINER[\"\ud83d\udc33 Container App&lt;br/&gt;(Environment)\"]\n                    REGISTRY[\"\ud83d\udce6 Container Registry&lt;br/&gt;(Image Storage)\"]\n                end\n            end\n        end\n    end\n\n    %% Internal SovDev connections\n    BROWSER --&gt; TRAEFIK\n    HOST --&gt; PH\n    PH -.-&gt;|\"Manages &amp; Deploys\"| K8S\n    PH -.-&gt;|\"Manages &amp; Deploys\"| TRAEFIK\n    PH -.-&gt;|\"Manages &amp; Deploys\"| POSTGRES\n    PH -.-&gt;|\"Manages &amp; Deploys\"| SERVICEBUS\n    PH -.-&gt;|\"Manages &amp; Deploys\"| VAULT\n    PH -.-&gt;|\"Manages &amp; Deploys\"| LOGS\n    PH -.-&gt;|\"Manages &amp; Deploys\"| SENDGRID\n    PH -.-&gt;|\"Manages &amp; Deploys\"| COSMOS\n    PH -.-&gt;|\"Manages &amp; Deploys\"| INSIGHTS\n    PH -.-&gt;|\"Manages &amp; Deploys\"| PLAN\n    PH -.-&gt;|\"Manages &amp; Deploys\"| CONTAINER\n    PH -.-&gt;|\"Manages &amp; Deploys\"| REGISTRY\n\n    TRAEFIK --&gt; POSTGRES\n    TRAEFIK --&gt; SERVICEBUS\n    TRAEFIK --&gt; VAULT\n    TRAEFIK --&gt; LOGS\n    TRAEFIK --&gt; SENDGRID\n    TRAEFIK --&gt; COSMOS\n    TRAEFIK --&gt; INSIGHTS\n    TRAEFIK --&gt; PLAN\n    TRAEFIK --&gt; CONTAINER\n    TRAEFIK --&gt; REGISTRY\n</code></pre>"},{"location":"presentation/sovdev/sovdev-overview/#alternative-layered-architecture-view","title":"Alternative Layered Architecture View","text":"<pre><code>flowchart TB\n    subgraph \"\ud83c\udf10 Presentation Layer\"\n        BROWSER[\"Web Browser&lt;br/&gt;http://service.localhost\"]\n    end\n\n    subgraph \"\ud83d\udd27 Management Layer\"\n        PH[\"Provision Host Container&lt;br/&gt;Management Environment\"]\n        TOOLS[\"Management Tools&lt;br/&gt;kubectl, helm, ansible\"]\n    end\n\n    subgraph \"\u2638\ufe0f Orchestration Layer\"\n        K8S[\"Kubernetes&lt;br/&gt;Container Orchestration\"]\n        TRAEFIK[\"Traefik&lt;br/&gt;Ingress Controller\"]\n    end\n\n    subgraph \"\ud83c\udfd7\ufe0f Landing Zone - Common Services\"\n        subgraph \"Data Services\"\n            POSTGRES[\"PostgreSQL&lt;br/&gt;(Azure PostgreSQL)\"]\n            SERVICEBUS[\"RabbitMQ&lt;br/&gt;(Service Bus)\"]\n            VAULT[\"Key Vault&lt;br/&gt;(Secrets Management)\"]\n            COSMOS[\"MongoDB&lt;br/&gt;(Cosmos DB)\"]\n        end\n\n        subgraph \"Observability\"\n            LOGS[\"Log Analytics&lt;br/&gt;(Monitoring)\"]\n            INSIGHTS[\"Application Insights&lt;br/&gt;(Telemetry)\"]\n        end\n\n        subgraph \"Communication\"\n            SENDGRID[\"SendGrid&lt;br/&gt;(Email Services)\"]\n        end\n\n        subgraph \"Compute\"\n            PLAN[\"App Service Plan&lt;br/&gt;(Function Apps)\"]\n            CONTAINER[\"Container App&lt;br/&gt;(Environment)\"]\n        end\n\n        subgraph \"Storage\"\n            REGISTRY[\"Container Registry&lt;br/&gt;(Image Storage)\"]\n        end\n    end\n\n    %% SovDev internal flow\n    BROWSER --&gt; TRAEFIK\n    PH --&gt; K8S\n    K8S --&gt; TRAEFIK\n    TRAEFIK --&gt; POSTGRES\n    TRAEFIK --&gt; SERVICEBUS\n    TRAEFIK --&gt; VAULT\n    TRAEFIK --&gt; LOGS\n    TRAEFIK --&gt; SENDGRID\n    TRAEFIK --&gt; COSMOS\n    TRAEFIK --&gt; INSIGHTS\n    TRAEFIK --&gt; PLAN\n    TRAEFIK --&gt; CONTAINER\n    TRAEFIK --&gt; REGISTRY\n</code></pre>"},{"location":"presentation/sovdev/sovdev-prod/","title":"SovDev Production - Multi-Cluster Deployment","text":"<pre><code>graph TB\n    subgraph \"Provision Host Container\"\n        PH[\"\ud83c\udfd7\ufe0f Provision Host&lt;br/&gt;Management Environment\"]\n    end\n\n    subgraph \"Development\"\n        DEV[\"\ud83d\udda5\ufe0f Rancher Desktop&lt;br/&gt;\u2638\ufe0f Kubernetes&lt;br/&gt;\ud83c\udf10 Traefik&lt;br/&gt;\ud83d\udc18 PostgreSQL&lt;br/&gt;\ud83d\udd34 Redis&lt;br/&gt;\ud83d\udd10 Authentik&lt;br/&gt;\ud83e\udd16 AI Platform&lt;br/&gt;\ud83d\udcca Grafana\"]\n    end\n\n    subgraph \"Staging\"\n        STAGING[\"\u26a1 MicroK8s&lt;br/&gt;\u2638\ufe0f Kubernetes&lt;br/&gt;\ud83c\udf10 Traefik&lt;br/&gt;\ud83d\udc18 PostgreSQL&lt;br/&gt;\ud83d\udd34 Redis&lt;br/&gt;\ud83d\udd10 Authentik&lt;br/&gt;\ud83e\udd16 AI Platform&lt;br/&gt;\ud83d\udcca Grafana\"]\n    end\n\n    subgraph \"Production\"\n        PROD[\"\u2601\ufe0f Azure AKS&lt;br/&gt;\u2638\ufe0f Kubernetes&lt;br/&gt;\ud83c\udf10 Traefik&lt;br/&gt;\ud83d\udc18 PostgreSQL&lt;br/&gt;\ud83d\udd34 Redis&lt;br/&gt;\ud83d\udd10 Authentik&lt;br/&gt;\ud83e\udd16 AI Platform&lt;br/&gt;\ud83d\udcca Grafana\"]\n    end\n\n    subgraph \"Edge\"\n        EDGE[\"\ud83c\udf10 Edge MicroK8s&lt;br/&gt;\u2638\ufe0f Kubernetes&lt;br/&gt;\ud83c\udf10 Traefik&lt;br/&gt;\ud83d\udc18 PostgreSQL&lt;br/&gt;\ud83d\udd34 Redis&lt;br/&gt;\ud83d\udd10 Authentik&lt;br/&gt;\ud83e\udd16 AI Platform&lt;br/&gt;\ud83d\udcca Grafana\"]\n    end\n\n    %% Management connections\n    PH -.-&gt;|\"Same tools &amp; scripts\"| DEV\n    PH -.-&gt;|\"Same tools &amp; scripts\"| STAGING\n    PH -.-&gt;|\"Same tools &amp; scripts\"| PROD\n    PH -.-&gt;|\"Same tools &amp; scripts\"| EDGE\n</code></pre>"},{"location":"presentation/sovdev/sovdev-prod/#environment-specific-configurations","title":"Environment-Specific Configurations","text":""},{"location":"presentation/sovdev/sovdev-prod/#development-rancher-desktop","title":"\ud83d\udda5\ufe0f Development (Rancher Desktop)","text":"Service Configuration Access Kubernetes Single node, local storage Internal Traefik Localhost routing http://service.localhost PostgreSQL 8GB storage, pgvector enabled Internal Redis In-memory cache Internal Authentik Development SSO http://authentik.localhost AI Platform OpenWebUI + LiteLLM http://openwebui.localhost Grafana Development dashboards http://grafana.localhost"},{"location":"presentation/sovdev/sovdev-prod/#staging-microk8s","title":"\u26a1 Staging (MicroK8s)","text":"Service Configuration Access Kubernetes Multi-node, production-like Internal Traefik Load balancer routing http://staging.service.localhost PostgreSQL 20GB storage, HA setup Internal Redis Persistent cache Internal Authentik Staging SSO http://staging.authentik.localhost AI Platform Full AI stack http://staging.openwebui.localhost Grafana Production dashboards http://staging.grafana.localhost"},{"location":"presentation/sovdev/sovdev-prod/#production-azure-aks","title":"\u2601\ufe0f Production (Azure AKS)","text":"Service Configuration Access Kubernetes Managed cluster, auto-scaling Internal Traefik Azure Load Balancer https://service.company.com PostgreSQL Azure Database for PostgreSQL Internal Redis Azure Cache for Redis Internal Authentik Production SSO https://auth.company.com AI Platform Enterprise AI stack https://ai.company.com Grafana Enterprise monitoring https://monitoring.company.com"},{"location":"presentation/sovdev/sovdev-prod/#edge-microk8s-on-iot","title":"\ud83c\udf10 Edge (MicroK8s on IoT)","text":"Service Configuration Access Kubernetes Lightweight, ARM64 Internal Traefik Edge routing http://edge.service.localhost PostgreSQL 4GB storage, optimized Internal Redis Minimal cache Internal Authentik Edge SSO http://edge.authentik.localhost AI Platform Lightweight AI http://edge.openwebui.localhost Grafana Edge monitoring http://edge.grafana.localhost"},{"location":"presentation/sovdev/sovdev-prod/#deployment-strategy","title":"\ud83d\ude80 Deployment Strategy","text":""},{"location":"presentation/sovdev/sovdev-prod/#single-command-deployment","title":"Single Command Deployment","text":"<pre><code># Deploy to all environments\n./provision-host/kubernetes/provision-kubernetes.sh\n\n# Deploy to specific environment\nkubectl config use-context rancher-desktop\n./provision-host/kubernetes/provision-kubernetes.sh\n\nkubectl config use-context azure-aks\n./provision-host/kubernetes/provision-kubernetes.sh\n</code></pre>"},{"location":"presentation/sovdev/sovdev-prod/#environment-specific-configuration","title":"Environment-Specific Configuration","text":"<ul> <li>Development: Local storage, single node, localhost access</li> <li>Staging: Persistent storage, multi-node, staging subdomains</li> <li>Production: Managed storage, auto-scaling, custom domains</li> <li>Edge: Optimized storage, ARM64, edge-optimized services</li> </ul>"},{"location":"presentation/sovdev/sovdev-prod/#key-benefits","title":"\ud83d\udd27 Key Benefits","text":""},{"location":"presentation/sovdev/sovdev-prod/#consistent-service-stack","title":"Consistent Service Stack","text":"<ul> <li>Same services across all environments</li> <li>Identical configuration management</li> <li>Unified monitoring and logging</li> </ul>"},{"location":"presentation/sovdev/sovdev-prod/#environment-parity","title":"Environment Parity","text":"<ul> <li>Development mirrors production</li> <li>Same tools and scripts everywhere</li> <li>Predictable deployments</li> </ul>"},{"location":"presentation/sovdev/sovdev-prod/#multi-cloud-support","title":"Multi-Cloud Support","text":"<ul> <li>Works on any Kubernetes cluster</li> <li>Cloud-agnostic service deployment</li> <li>Vendor lock-in avoidance</li> </ul>"},{"location":"presentation/sovdev/sovdev-prod/#edge-computing-ready","title":"Edge Computing Ready","text":"<ul> <li>Lightweight deployments for IoT</li> <li>ARM64 support for edge devices</li> <li>Optimized resource usage</li> </ul>"},{"location":"presentation/sovdev/sovdev-prod/#service-availability-matrix","title":"\ud83d\udcca Service Availability Matrix","text":"Service Dev Staging Production Edge Kubernetes \u2705 \u2705 \u2705 \u2705 Traefik \u2705 \u2705 \u2705 \u2705 PostgreSQL \u2705 \u2705 \u2705 \u2705 Redis \u2705 \u2705 \u2705 \u2705 Authentik \u2705 \u2705 \u2705 \u2705 AI Platform \u2705 \u2705 \u2705 \u2705 Grafana \u2705 \u2705 \u2705 \u2705"},{"location":"presentation/sovdev/sovdev-prod/#value-proposition","title":"\ud83d\udca1 Value Proposition","text":"<p>SovDev enables true multi-environment consistency by using the same provision-host container and orchestration scripts across all deployment targets, from local development to production cloud environments and edge computing scenarios.</p>"},{"location":"presentation/sovdev/sovdev-services/","title":"SovDev Services - Complete Service Catalog","text":"<pre><code>graph TB\n    subgraph \"SovDev - Complete Service Ecosystem\"\n        BROWSER[\"\ud83c\udf10 Web Browser&lt;br/&gt;http://service.localhost\"]\n\n        subgraph \"Host Operating System\"\n            HOST[\"\ud83d\udcbb Host OS&lt;br/&gt;macOS / Linux / Windows\"]\n\n            subgraph \"Provision Host Container\"\n                PH[\"\ud83c\udfd7\ufe0f Provision Host&lt;br/&gt;Management Environment\"]\n                TOOLS[\"\ud83d\udee0\ufe0f Management Tools&lt;br/&gt;kubectl, helm, ansible&lt;br/&gt;cloud CLIs, terraform\"]\n                SCRIPTS[\"\ud83d\udcdc Orchestration Scripts&lt;br/&gt;provision-kubernetes.sh&lt;br/&gt;category-based automation\"]\n                CONFIG[\"\u2699\ufe0f Configuration&lt;br/&gt;playbooks, manifests&lt;br/&gt;secrets, kubeconfig\"]\n            end\n\n            subgraph \"Kubernetes Cluster - Complete Service Stack\"\n                subgraph \"Core Infrastructure\"\n                    K8S[\"\u2638\ufe0f Kubernetes&lt;br/&gt;Container Orchestration\"]\n                    TRAEFIK[\"\ud83c\udf10 Traefik&lt;br/&gt;Reverse Proxy &amp; Ingress\"]\n                    NGINX[\"\ud83d\udcc4 NGINX&lt;br/&gt;Web Server\"]\n                end\n\n                subgraph \"Authentication &amp; Security\"\n                    AUTHENTIK[\"\ud83d\udd10 Authentik&lt;br/&gt;SSO &amp; Authentication\"]\n                    TAILSCALE[\"\ud83d\udd12 Tailscale&lt;br/&gt;VPN Connectivity\"]\n                    CLOUDFLARE[\"\u2601\ufe0f Cloudflare&lt;br/&gt;Tunnel &amp; CDN\"]\n                end\n\n                subgraph \"Data &amp; Storage Services\"\n                    POSTGRES[\"\ud83d\udc18 PostgreSQL&lt;br/&gt;Primary Database\"]\n                    REDIS[\"\ud83d\udd34 Redis&lt;br/&gt;Cache &amp; Sessions\"]\n                    MONGODB[\"\ud83c\udf43 MongoDB&lt;br/&gt;Document Database\"]\n                    MYSQL[\"\ud83d\udc2c MySQL&lt;br/&gt;Alternative SQL\"]\n                    ELASTICSEARCH[\"\ud83d\udd0d Elasticsearch&lt;br/&gt;Search Engine\"]\n                    QDRANT[\"\ud83e\udde0 Qdrant&lt;br/&gt;Vector Database\"]\n                end\n\n                subgraph \"Message &amp; Communication\"\n                    RABBITMQ[\"\ud83d\udc30 RabbitMQ&lt;br/&gt;Message Broker\"]\n                end\n\n                subgraph \"AI Platform\"\n                    OPENWEBUI[\"\ud83e\udd16 OpenWebUI&lt;br/&gt;AI Chat Interface\"]\n                    LITELLM[\"\ud83d\udd17 LiteLLM&lt;br/&gt;LLM Proxy &amp; Router\"]\n                    TIKA[\"\ud83d\udcc4 Tika&lt;br/&gt;Document Processing\"]\n                end\n\n                subgraph \"Observability Stack\"\n                    GRAFANA[\"\ud83d\udcca Grafana&lt;br/&gt;Monitoring Dashboards\"]\n                    PROMETHEUS[\"\ud83d\udcc8 Prometheus&lt;br/&gt;Metrics Collection\"]\n                    LOKI[\"\ud83d\udcdd Loki&lt;br/&gt;Log Aggregation\"]\n                    TEMPO[\"\ud83d\udd0d Tempo&lt;br/&gt;Distributed Tracing\"]\n                    OTEL[\"\ud83d\udce1 OpenTelemetry&lt;br/&gt;Observability\"]\n                end\n\n                subgraph \"Data Platform\"\n                    SPARK[\"\u26a1 Apache Spark&lt;br/&gt;Data Processing\"]\n                    JUPYTER[\"\ud83d\udcd3 JupyterHub&lt;br/&gt;Notebook Environment\"]\n                    UNITY[\"\ud83d\udcda Unity Catalog&lt;br/&gt;Data Governance\"]\n                end\n\n                subgraph \"Management &amp; Administration\"\n                    PGADMIN[\"\ud83d\uddc4\ufe0f pgAdmin&lt;br/&gt;PostgreSQL Admin\"]\n                    ARGOCD[\"\ud83d\ude80 ArgoCD&lt;br/&gt;GitOps CD\"]\n                    REDISINSIGHT[\"\ud83d\udd0d RedisInsight&lt;br/&gt;Redis Admin\"]\n                    GRAVITEE[\"\ud83d\udd27 Gravitee&lt;br/&gt;API Management\"]\n                end\n\n                subgraph \"Container &amp; Registry\"\n                    REGISTRY[\"\ud83d\udce6 Container Registry&lt;br/&gt;Image Storage\"]\n                end\n\n                subgraph \"Testing &amp; Development\"\n                    WHOAMI[\"\ud83e\uddea Whoami&lt;br/&gt;Test Service&lt;br/&gt;(public/protected)\"]\n                end\n            end\n        end\n    end\n\n    %% Connections\n    BROWSER --&gt; TRAEFIK\n    HOST --&gt; PH\n    PH -.-&gt;|\"Manages &amp; Deploys\"| K8S\n    PH -.-&gt;|\"Manages &amp; Deploys\"| TRAEFIK\n    PH -.-&gt;|\"Manages &amp; Deploys\"| NGINX\n    PH -.-&gt;|\"Manages &amp; Deploys\"| AUTHENTIK\n    PH -.-&gt;|\"Manages &amp; Deploys\"| POSTGRES\n    PH -.-&gt;|\"Manages &amp; Deploys\"| REDIS\n    PH -.-&gt;|\"Manages &amp; Deploys\"| MONGODB\n    PH -.-&gt;|\"Manages &amp; Deploys\"| MYSQL\n    PH -.-&gt;|\"Manages &amp; Deploys\"| ELASTICSEARCH\n    PH -.-&gt;|\"Manages &amp; Deploys\"| QDRANT\n    PH -.-&gt;|\"Manages &amp; Deploys\"| RABBITMQ\n    PH -.-&gt;|\"Manages &amp; Deploys\"| OPENWEBUI\n    PH -.-&gt;|\"Manages &amp; Deploys\"| LITELLM\n    PH -.-&gt;|\"Manages &amp; Deploys\"| TIKA\n    PH -.-&gt;|\"Manages &amp; Deploys\"| GRAFANA\n    PH -.-&gt;|\"Manages &amp; Deploys\"| PROMETHEUS\n    PH -.-&gt;|\"Manages &amp; Deploys\"| LOKI\n    PH -.-&gt;|\"Manages &amp; Deploys\"| TEMPO\n    PH -.-&gt;|\"Manages &amp; Deploys\"| OTEL\n    PH -.-&gt;|\"Manages &amp; Deploys\"| SPARK\n    PH -.-&gt;|\"Manages &amp; Deploys\"| JUPYTER\n    PH -.-&gt;|\"Manages &amp; Deploys\"| UNITY\n    PH -.-&gt;|\"Manages &amp; Deploys\"| PGADMIN\n    PH -.-&gt;|\"Manages &amp; Deploys\"| ARGOCD\n    PH -.-&gt;|\"Manages &amp; Deploys\"| REDISINSIGHT\n    PH -.-&gt;|\"Manages &amp; Deploys\"| GRAVITEE\n    PH -.-&gt;|\"Manages &amp; Deploys\"| REGISTRY\n    PH -.-&gt;|\"Manages &amp; Deploys\"| WHOAMI\n\n    TRAEFIK --&gt; NGINX\n    TRAEFIK --&gt; AUTHENTIK\n    TRAEFIK --&gt; OPENWEBUI\n    TRAEFIK --&gt; GRAFANA\n    TRAEFIK --&gt; PGADMIN\n    TRAEFIK --&gt; ARGOCD\n    TRAEFIK --&gt; REDISINSIGHT\n    TRAEFIK --&gt; GRAVITEE\n    TRAEFIK --&gt; WHOAMI\n</code></pre>"},{"location":"presentation/sovdev/sovdev-services/#service-categories-status","title":"Service Categories &amp; Status","text":""},{"location":"presentation/sovdev/sovdev-services/#core-infrastructure-always-active","title":"\ud83c\udfd7\ufe0f Core Infrastructure (Always Active)","text":"Service Description Status Access URL Kubernetes Container orchestration platform \u2705 Active Internal Traefik Reverse proxy and ingress controller \u2705 Active Internal NGINX Web server and static content \u2705 Active http://nginx.localhost"},{"location":"presentation/sovdev/sovdev-services/#authentication-security","title":"\ud83d\udd10 Authentication &amp; Security","text":"Service Description Status Access URL Authentik SSO &amp; Authentication \u2705 Active http://authentik.localhost Tailscale VPN Connectivity \u2705 Available Internal Cloudflare Tunnel &amp; CDN \u2705 Available External"},{"location":"presentation/sovdev/sovdev-services/#data-storage-services","title":"\ud83d\udcbe Data &amp; Storage Services","text":"Service Description Status Access URL PostgreSQL Primary relational database \u2705 Active Internal Redis Cache &amp; session management \u2705 Active Internal MongoDB Document database \u2705 Available Internal MySQL Alternative SQL database \u2705 Available Internal Elasticsearch Full-text search engine \u2705 Available Internal Qdrant Vector database \u2705 Available Internal"},{"location":"presentation/sovdev/sovdev-services/#message-communication","title":"\ud83d\udce8 Message &amp; Communication","text":"Service Description Status Access URL RabbitMQ Message broker \u2705 Available Internal"},{"location":"presentation/sovdev/sovdev-services/#ai-platform","title":"\ud83e\udd16 AI Platform","text":"Service Description Status Access URL OpenWebUI AI chat interface \u2705 Active http://openwebui.localhost LiteLLM LLM proxy &amp; router \u2705 Active Internal Tika Document processing \u2705 Available Internal"},{"location":"presentation/sovdev/sovdev-services/#observability-stack","title":"\ud83d\udcca Observability Stack","text":"Service Description Status Access URL Grafana Monitoring dashboards \u2705 Available http://grafana.localhost Prometheus Metrics collection \u2705 Available Internal Loki Log aggregation \u2705 Available Internal Tempo Distributed tracing \u2705 Available Internal OpenTelemetry Observability instrumentation \u2705 Available Internal"},{"location":"presentation/sovdev/sovdev-services/#data-platform","title":"\ud83d\udd2c Data Platform","text":"Service Description Status Access URL Apache Spark Data processing \u2705 Available Internal JupyterHub Notebook environment \u2705 Available Internal Unity Catalog Data governance \u2705 Available Internal"},{"location":"presentation/sovdev/sovdev-services/#management-administration","title":"\ud83d\udee0\ufe0f Management &amp; Administration","text":"Service Description Status Access URL pgAdmin PostgreSQL administration \u2705 Available http://pgadmin.localhost ArgoCD GitOps continuous delivery \u2705 Available http://argocd.localhost RedisInsight Redis administration \u2705 Available Internal Gravitee API management \u2705 Available Internal"},{"location":"presentation/sovdev/sovdev-services/#container-registry","title":"\ud83d\udce6 Container &amp; Registry","text":"Service Description Status Access URL Container Registry Image storage \u2705 Available Internal"},{"location":"presentation/sovdev/sovdev-services/#testing-development","title":"\ud83e\uddea Testing &amp; Development","text":"Service Description Status Access URL Whoami Test service (public/protected) \u2705 Active http://whoami.localhost"},{"location":"presentation/sovdev/sovdev-services/#service-deployment","title":"\ud83d\ude80 Service Deployment","text":""},{"location":"presentation/sovdev/sovdev-services/#automatic-deployment-core-services","title":"Automatic Deployment (Core Services)","text":"<pre><code>./provision-host/kubernetes/provision-kubernetes.sh\n</code></pre>"},{"location":"presentation/sovdev/sovdev-services/#on-demand-deployment","title":"On-Demand Deployment","text":"<pre><code># AI Platform\n./provision-host/kubernetes/07-ai/01-setup-litellm-openwebui.sh\n\n# Observability Stack\n./provision-host/kubernetes/08-observability/01-setup-prometheus-grafana.sh\n\n# Data Platform\n./provision-host/kubernetes/10-datascience/01-setup-jupyterhub.sh\n</code></pre>"},{"location":"presentation/sovdev/sovdev-services/#service-statistics","title":"\ud83d\udcc8 Service Statistics","text":"<ul> <li>Total Services: 24+</li> <li>Always Active: 6 services</li> <li>Available on Demand: 18+ services</li> <li>Azure Equivalents: 19+ services</li> <li>Coverage: 85% of Azure enterprise capabilities</li> </ul>"},{"location":"presentation/sovdev/sovdev-services/#access-pattern","title":"\ud83c\udf10 Access Pattern","text":"<p>All services follow the consistent access pattern: - Web Interfaces: <code>http://service-name.localhost</code> - Internal APIs: Kubernetes service discovery - External APIs: Direct service endpoints</p>"},{"location":"presentation/sovdev/sovdev-services/#value-proposition","title":"\ud83d\udca1 Value Proposition","text":"<p>SovDev provides 85% of Azure's enterprise capabilities in a local development environment, enabling teams to build, test, and iterate without cloud dependencies or costs.</p>"}]}